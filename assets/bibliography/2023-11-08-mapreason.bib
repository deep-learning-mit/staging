@article{llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  url={https://arxiv.org/abs/2307.09288},
  year={2023},
}

@electronic{gemini,
  author = {Google-Gemini-Team},
  title = {Gemini: A Family of Highly Capable Multimodal Models},
  url = {https://deepmind.google/technologies/gemini/},
  year={2023},
}

@article{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  url={https://arxiv.org/abs/2303.08774},
  year={2023},
}

@inproceedings{retro_reader,
  title={Retrospective Reader for Machine Reading Comprehension},
  author={Zhang, Zhuosheng and Yang, Junjie and Zhao, Hai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={16},
  pages={14506--14514},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/17705},
  year={2021}
}

@article{squad2,
  title={Know What You Don't Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  url={https://arxiv.org/abs/1806.03822},
  year={2018}
}

@article{st_moe,
  title={ST-MoE: Designing Stable and Transferable Sparse Expert Models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  url={https://arxiv.org/abs/2202.08906},
  year={2022}
}

@article{super_glue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  url={https://proceedings.neurips.cc/paper_files/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
  year={2019}
}

@article{lvlm_ehub,
  title={LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  url={https://arxiv.org/abs/2306.09265},
  year={2023}
}

@article{visual_reasoning,
  title={Visual Spatial Reasoning},
  author={Liu, Fangyu and Emerson, Guy and Collier, Nigel},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={635--651},
  url={https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00566/116470},
  year={2023},
  publisher={MIT Press}
}

@inproceedings{visual_dialog,
  title={Visual Dialog},
  author={Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={326--335},
  url={https://openaccess.thecvf.com/content_cvpr_2017/html/Das_Visual_Dialog_CVPR_2017_paper.html},
  year={2017}
}

@article{science_qa,
  title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/11332b6b6cf4485b84afadb1352d3a9a-Abstract-Conference.html},
  year={2022}
}

@article{llama_adapter,
  title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  url={https://arxiv.org/abs/2303.16199},
  year={2023}
}

@article{acl_fig,
  title={ACL-Fig: A Dataset for Scientific Figure Classification},
  author={Karishma, Zeba and Rohatgi, Shaurya and Puranik, Kavya Shrinivas and Wu, Jian and Giles, C Lee},
  journal={arXiv preprint arXiv:2301.12293},
  url={https://arxiv.org/abs/2301.12293},
  year={2023}
}

@inproceedings{info_vqa,
  title={InfographicVQA},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1697--1706},
  url={https://openaccess.thecvf.com/content/WACV2022/html/Mathew_InfographicVQA_WACV_2022_paper.html},
  year={2022}
}

@article{chart_qa,
  title={ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  url={https://arxiv.org/abs/2203.10244},
  year={2022}
}

@article{deplot,
  title={DePlot: One-shot visual language reasoning by plot-to-table translation},
  author={Liu, Fangyu and Eisenschlos, Julian Martin and Piccinno, Francesco and Krichene, Syrine and Pang, Chenxi and Lee, Kenton and Joshi, Mandar and Chen, Wenhu and Collier, Nigel and Altun, Yasemin},
  journal={arXiv preprint arXiv:2212.10505},
  url={https://arxiv.org/abs/2212.10505},
  year={2022}
}

@inproceedings{coco,
  title={Microsoft COCO: Common Objects in Context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  url={https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48},
  organization={Springer}
}

@inproceedings{image_to_map,
  title={Image-To-Image Translation With Conditional Adversarial Networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  url={https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html},
  year={2017}
}

@article{map_segmentation,
  title={Semi-supervised learning for topographic map analysis over time: a study of bridge segmentation},
  author={Wong, Cheng-Shih and Liao, Hsiung-Ming and Tsai, Richard Tzong-Han and Chang, Ming-Ching},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={18997},
  year={2022},
  url={https://www.nature.com/articles/s41598-022-23364-w},
  publisher={Nature Publishing Group UK London}
}

@article{intersection_map,
  title={Automatic extraction of road intersection points from USGS historical map series using deep convolutional neural networks},
  author={Saeedimoghaddam, Mahmoud and Stepinski, Tomasz F},
  journal={International Journal of Geographical Information Science},
  volume={34},
  number={5},
  pages={947--968},
  year={2020},
  url={https://www.tandfonline.com/doi/abs/10.1080/13658816.2019.1696968},
  publisher={Taylor \& Francis}
}

@article{street_name_map,
  title={Aligning geographic entities from historical maps for building knowledge graphs},
  author={Sun, Kai and Hu, Yingjie and Song, Jia and Zhu, Yunqiang},
  journal={International Journal of Geographical Information Science},
  volume={35},
  number={10},
  pages={2078--2107},
  year={2021},
  url={https://www.tandfonline.com/doi/full/10.1080/13658816.2020.1845702},
  publisher={Taylor \& Francis}
}

@article{aerial_understanding,
  title={Semi-Supervised Perception Augmentation for Aerial Photo Topologies Understanding},
  author={Zhang, Luming and Pan, Zhigeng and Shao, Ling},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={7803--7814},
  year={2021},
  url={https://ieeexplore.ieee.org/abstract/document/9435071},
  publisher={IEEE}
}

@article{blip,
  title={BBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  url={https://arxiv.org/abs/2301.12597},
  year={2023}
}

@inproceedings{vilt,
  title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={International Conference on Machine Learning},
  pages={5583--5594},
  year={2021},
  url={https://proceedings.mlr.press/v139/kim21k.html},
  organization={PMLR}
}

@article{lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  url={https://arxiv.org/abs/1908.07490},
  year={2019}
}

@inproceedings{flava,
  title={FLAVA: A Foundational Language and Vision Alignment Model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15638--15650},
  url={https://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html},
  year={2022}
}

@article{git,
  title={GIT: A Generative Image-to-text Transformer for Vision and Language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv preprint arXiv:2205.14100},
  url={https://arxiv.org/abs/2205.14100},
  year={2022}
}

@article{vision_prompt,
  title={A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models},
  author={Gu, Jindong and Han, Zhen and Chen, Shuo and Beirami, Ahmad and He, Bailan and Zhang, Gengyuan and Liao, Ruotong and Qin, Yao and Tresp, Volker and Torr, Philip},
  journal={arXiv preprint arXiv:2307.12980},
  url={https://arxiv.org/abs/2307.12980},
  year={2023}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  url={https://arxiv.org/abs/2005.14165},
  year={2020}
}

@electronic{palm2,
  author = {Google-DeepMind},
  title = {Introducing PaLM 2},
  url = {https://blog.google/technology/ai/google-palm-2-ai-large-language-model/},
  year={2023},
}

@article{wizardlm,
  title={WizardLM: Empowering Large Language Models to Follow Complex Instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  url={https://arxiv.org/abs/2304.12244},
  year={2023}
}

@article{vision_llm,
  title={VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2305.11175},
  url={https://arxiv.org/abs/2305.11175},
  year={2023}
}

@article{yang2022prompt,
  title={Prompt tuning for generative multimodal pretrained models},
  author={Yang, Hao and Lin, Junyang and Yang, An and Wang, Peng and Zhou, Chang and Yang, Hongxia},
  journal={arXiv preprint arXiv:2208.02532},
  url={https://arxiv.org/abs/2208.02532},
  year={2022}
}

@article{yang2023large,
  title={Large language models as optimizers},
  author={Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  journal={arXiv preprint arXiv:2309.03409},
  url={https://arxiv.org/abs/2309.03409},
  year={2023}
}

@article{white2023prompt,
  title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT},
  author={White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C},
  journal={arXiv preprint arXiv:2302.11382},
  url={https://arxiv.org/abs/2302.11382},
  year={2023}
}

@article{zhou2022large,
  title={Large Language Models Are Human-Level Prompt Engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  url={https://arxiv.org/abs/2211.01910},
  year={2022}
}

@article{chain_Thought,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and ichter, brian and Xia, Fei and Chi, Ed and V Le, Quoc and Zhou, Denny},
  journal={Advances in neural information processing systems},
  volume={35},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  year={2022}
}

@article{map_QA,
  title={MapQA: A Dataset for Question Answering on Choropleth Maps},
  author={Chang, Shuaichen and Palzer, David and Li, Jialin and Fosler-Lussier, Eric and Xiao, Ningchuan},
  journal={Neurips 2nd Table Representation Learning Workshop},
  url={https://arxiv.org/abs/2211.08545},
  year={2022}
}


@misc{sap_socialiqa_2019,
	title = {{SocialIQA}: {Commonsense} {Reasoning} about {Social} {Interactions}},
	shorttitle = {{SocialIQA}},
	url = {http://arxiv.org/abs/1904.09728},
	doi = {10.48550/arXiv.1904.09728},
	abstract = {We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?" A: "Make sure no one else could hear"). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20\% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
	month = sep,
	year = {2019},
	note = {arXiv:1904.09728 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: the first two authors contributed equally; accepted to EMNLP 2019; camera ready version},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/BJVQKQJH/Sap et al. - 2019 - SocialIQA Commonsense Reasoning about Social Inte.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/SEAIQEKC/1904.html:text/html},
}

@misc{ziems_normbank_2023,
	title = {{NormBank}: {A} {Knowledge} {Bank} of {Situational} {Social} {Norms}},
	shorttitle = {{NormBank}},
	url = {http://arxiv.org/abs/2305.17008},
	doi = {10.48550/arXiv.2305.17008},
	abstract = {We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents' contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the temperature or the country of operation). In total, NormBank contains 63k unique constraints from a taxonomy that we introduce and iteratively refine here. Constraints then apply in different combinations to frame social norms. Under these manipulations, norms are non-monotonic - one can cancel an inference by updating its frame even slightly. Still, we find evidence that neural models can help reliably extend the scope and coverage of NormBank. We further demonstrate the utility of this resource with a series of transfer experiments.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Ziems, Caleb and Dwivedi-Yu, Jane and Wang, Yi-Chia and Halevy, Alon and Yang, Diyi},
	month = jul,
	year = {2023},
	note = {arXiv:2305.17008 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/JYTX6KM8/Ziems et al. - 2023 - NormBank A Knowledge Bank of Situational Social N.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/SFX4K5PX/2305.html:text/html},
}

@misc{sap_neural_2023,
	title = {Neural {Theory}-of-{Mind}? {On} the {Limits} of {Social} {Intelligence} in {Large} {LMs}},
	shorttitle = {Neural {Theory}-of-{Mind}?},
	url = {http://arxiv.org/abs/2210.13312},
	doi = {10.48550/arXiv.2210.13312},
	abstract = {Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations. Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55\% and 60\% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind. In our updated version, we also analyze newer instruction tuned and RLFH models for neural ToM. We find that even ChatGPT and GPT-4 do not display emergent Theory of Mind; strikingly even GPT-4 performs only 60\% accuracy on the ToMi questions related to mental states and realities.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi, Yejin},
	month = apr,
	year = {2023},
	note = {arXiv:2210.13312 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Originally published at EMNLP 2022, extended to include ChatGPT and GPT-4 models on March 30th 2023 (extension not peer reviewed)},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/7WCU8CD6/Sap et al. - 2023 - Neural Theory-of-Mind On the Limits of Social Int.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/VE2T9KMG/2210.html:text/html},
}

@misc{li_systematic_2022,
	title = {A {Systematic} {Investigation} of {Commonsense} {Knowledge} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2111.00607},
	doi = {10.48550/arXiv.2111.00607},
	abstract = {Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge -- a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation are insufficient to achieve human-level commonsense performance.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Li, Xiang Lorraine and Kuncoro, Adhiguna and Hoffmann, Jordan and d'Autume, Cyprien de Masson and Blunsom, Phil and Nematzadeh, Aida},
	month = oct,
	year = {2022},
	note = {arXiv:2111.00607 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2022},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/8GL5SURD/Li et al. - 2022 - A Systematic Investigation of Commonsense Knowledg.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/R3ANLN9I/2111.html:text/html},
}

@misc{gandhi_understanding_2023,
	title = {Understanding {Social} {Reasoning} in {Language} {Models} with {Language} {Models}},
	url = {http://arxiv.org/abs/2306.15448},
	abstract = {As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Gandhi, Kanishk and Fränken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15448 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/VVQKYVP7/2306.html:text/html;Full Text PDF:/Users/ningerlei/Zotero/storage/7BWUSP5N/Gandhi et al. - 2023 - Understanding Social Reasoning in Language Models .pdf:application/pdf},
}

@inproceedings{chang_incorporating_2020,
	address = {Online},
	title = {Incorporating {Commonsense} {Knowledge} {Graph} in {Pretrained} {Models} for {Social} {Commonsense} {Tasks}},
	url = {https://aclanthology.org/2020.deelio-1.9},
	doi = {10.18653/v1/2020.deelio-1.9},
	abstract = {Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to perform commonsense reasoning besides fitting the specific downstream tasks. External commonsense knowledge graphs (KGs), such as ConceptNet, provide rich information about words and their relationships. Thus, towards general commonsense learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of {Deep} {Learning} {Inside} {Out} ({DeeLIO}): {The} {First} {Workshop} on {Knowledge} {Extraction} and {Integration} for {Deep} {Learning} {Architectures}},
	publisher = {Association for Computational Linguistics},
	author = {Chang, Ting-Yun and Liu, Yang and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Zhou, Pei and Hakkani-Tur, Dilek},
	editor = {Agirre, Eneko and Apidianaki, Marianna and Vulić, Ivan},
	month = nov,
	year = {2020},
	pages = {74--79},
	file = {Full Text PDF:/Users/ningerlei/Zotero/storage/UI2QXD64/Chang et al. - 2020 - Incorporating Commonsense Knowledge Graph in Pretr.pdf:application/pdf},
}

@misc{mitra_how_2020,
	title = {How {Additional} {Knowledge} can {Improve} {Natural} {Language} {Commonsense} {Question} {Answering}?},
	url = {http://arxiv.org/abs/1909.08855},
	doi = {10.48550/arXiv.1909.08855},
	abstract = {Recently several datasets have been proposed to encourage research in Question Answering domains where commonsense knowledge is expected to play an important role. Recent language models such as ROBERTA, BERT and GPT that have been pre-trained on Wikipedia articles and books have shown reasonable performance with little fine-tuning on several such Multiple Choice Question-Answering (MCQ) datasets. Our goal in this work is to develop methods to incorporate additional (commonsense) knowledge into language model-based approaches for better question-answering in such domains. In this work, we first categorize external knowledge sources, and show performance does improve on using such sources. We then explore three different strategies for knowledge incorporation and four different models for question-answering using external commonsense knowledge. We analyze our predictions to explore the scope of further improvements.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Mitra, Arindam and Banerjee, Pratyay and Pal, Kuntal Kumar and Mishra, Swaroop and Baral, Chitta},
	month = apr,
	year = {2020},
	note = {arXiv:1909.08855 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: 14 pages, 14 figures, 3 tables},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/DZ5PQP29/Mitra et al. - 2020 - How Additional Knowledge can Improve Natural Langu.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/YHTL66QK/1909.html:text/html},
}

@misc{wang_semantic_2021,
	title = {Semantic {Categorization} of {Social} {Knowledge} for {Commonsense} {Question} {Answering}},
	url = {http://arxiv.org/abs/2109.05168},
	doi = {10.48550/arXiv.2109.05168},
	abstract = {Large pre-trained language models (PLMs) have led to great success on various commonsense question answering (QA) tasks in an end-to-end fashion. However, little attention has been paid to what commonsense knowledge is needed to deeply characterize these QA tasks. In this work, we proposed to categorize the semantics needed for these tasks using the SocialIQA as an example. Building upon our labeled social knowledge categories dataset on top of SocialIQA, we further train neural QA models to incorporate such social knowledge categories and relation information from a knowledge base. Unlike previous work, we observe our models with semantic categorizations of social knowledge can achieve comparable performance with a relatively simple model and smaller size compared to other complex approaches.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Wang, Gengyu and Hou, Xiaochen and Yang, Diyi and McKeown, Kathleen and Huang, Jing},
	month = sep,
	year = {2021},
	note = {arXiv:2109.05168 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by SustaiNLP 2021 on EMNLP 2021},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/D9BII9EH/Wang et al. - 2021 - Semantic Categorization of Social Knowledge for Co.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/KYTMCG2U/2109.html:text/html},
}

@misc{nematzadeh_evaluating_2018,
	title = {Evaluating {Theory} of {Mind} in {Question} {Answering}},
	url = {http://arxiv.org/abs/1808.09352},
	doi = {10.48550/arXiv.1808.09352},
	abstract = {We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models' accuracy decreases notably when random sentences are introduced to the tasks at test.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Nematzadeh, Aida and Burns, Kaylee and Grant, Erin and Gopnik, Alison and Griffiths, Thomas L.},
	month = aug,
	year = {2018},
	note = {arXiv:1808.09352 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/B37NXFRQ/Nematzadeh et al. - 2018 - Evaluating Theory of Mind in Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/X82TZAL6/1808.html:text/html},
}

@inproceedings{zellers_hellaswag_2019,
	address = {Florence, Italy},
	title = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
	shorttitle = {{HellaSwag}},
	url = {https://aclanthology.org/P19-1472},
	doi = {10.18653/v1/P19-1472},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textbackslash}textgreater95\% accuracy), state-of-the-art models struggle ({\textbackslash}textless48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical `Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {4791--4800},
	file = {Full Text PDF:/Users/ningerlei/Zotero/storage/8B9TEHT3/Zellers et al. - 2019 - HellaSwag Can a Machine Really Finish Your Senten.pdf:application/pdf},
}

@misc{forbes_social_2021,
	title = {Social {Chemistry} 101: {Learning} to {Reason} about {Social} and {Moral} {Norms}},
	shorttitle = {Social {Chemistry} 101},
	url = {http://arxiv.org/abs/2011.00620},
	doi = {10.48550/arXiv.2011.00620},
	abstract = {Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes." We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Forbes, Maxwell and Hwang, Jena D. and Shwartz, Vered and Sap, Maarten and Choi, Yejin},
	month = aug,
	year = {2021},
	note = {arXiv:2011.00620 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Published at EMNLP 2020},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/2NDCVH6E/Forbes et al. - 2021 - Social Chemistry 101 Learning to Reason about Soc.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/5JQIHLGP/2011.html:text/html},
}

@inproceedings{hovy_importance_2021,
	address = {Online},
	title = {The {Importance} of {Modeling} {Social} {Factors} of {Language}: {Theory} and {Practice}},
	shorttitle = {The {Importance} of {Modeling} {Social} {Factors} of {Language}},
	url = {https://aclanthology.org/2021.naacl-main.49},
	doi = {10.18653/v1/2021.naacl-main.49},
	abstract = {Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language's social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Yang, Diyi},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {588--602},
	file = {Full Text PDF:/Users/ningerlei/Zotero/storage/FN8SBV4Z/Hovy and Yang - 2021 - The Importance of Modeling Social Factors of Langu.pdf:application/pdf},
}

@inproceedings{boratko_protoqa_2020,
	address = {Online},
	title = {{ProtoQA}: {A} {Question} {Answering} {Dataset} for {Prototypical} {Common}-{Sense} {Reasoning}},
	shorttitle = {{ProtoQA}},
	url = {https://aclanthology.org/2020.emnlp-main.85},
	doi = {10.18653/v1/2020.emnlp-main.85},
	abstract = {Given questions regarding some prototypical situation — such as Name something that people usually do before they leave the house for work? — a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international trivia game show – Family Feud. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Boratko, Michael and Li, Xiang and O'Gorman, Tim and Das, Rajarshi and Le, Dan and McCallum, Andrew},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {1122--1136},
	file = {Full Text PDF:/Users/ningerlei/Zotero/storage/7LUK4AC8/Boratko et al. - 2020 - ProtoQA A Question Answering Dataset for Prototyp.pdf:application/pdf},
}

@misc{davis_benchmarks_2023,
	title = {Benchmarks for {Automated} {Commonsense} {Reasoning}: {A} {Survey}},
	shorttitle = {Benchmarks for {Automated} {Commonsense} {Reasoning}},
	url = {http://arxiv.org/abs/2302.04752},
	doi = {10.48550/arXiv.2302.04752},
	abstract = {More than one hundred benchmarks have been developed to test the commonsense knowledge and commonsense reasoning abilities of artificial intelligence (AI) systems. However, these benchmarks are often flawed and many aspects of common sense remain untested. Consequently, we do not currently have any reliable way of measuring to what extent existing AI systems have achieved these abilities. This paper surveys the development and uses of AI commonsense benchmarks. We discuss the nature of common sense; the role of common sense in AI; the goals served by constructing commonsense benchmarks; and desirable features of commonsense benchmarks. We analyze the common flaws in benchmarks, and we argue that it is worthwhile to invest the work needed ensure that benchmark examples are consistently high quality. We survey the various methods of constructing commonsense benchmarks. We enumerate 139 commonsense benchmarks that have been developed: 102 text-based, 18 image-based, 12 video based, and 7 simulated physical environments. We discuss the gaps in the existing benchmarks and aspects of commonsense reasoning that are not addressed in any existing benchmark. We conclude with a number of recommendations for future development of commonsense AI benchmarks.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Davis, Ernest},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04752 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/KHHCYDCI/Davis - 2023 - Benchmarks for Automated Commonsense Reasoning A .pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/QXYS2ADY/2302.html:text/html},
}

@article{davis_commonsense_2015,
	title = {Commonsense reasoning and commonsense knowledge in artificial intelligence},
	volume = {58},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2701413},
	doi = {10.1145/2701413},
	abstract = {AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.},
	language = {en},
	number = {9},
	urldate = {2023-11-13},
	journal = {Communications of the ACM},
	author = {Davis, Ernest and Marcus, Gary},
	month = aug,
	year = {2015},
	pages = {92--103},
	annote = {[TLDR] AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.},
}

@misc{mitra_how_2020-1,
	title = {How {Additional} {Knowledge} can {Improve} {Natural} {Language} {Commonsense} {Question} {Answering}?},
	url = {http://arxiv.org/abs/1909.08855},
	doi = {10.48550/arXiv.1909.08855},
	abstract = {Recently several datasets have been proposed to encourage research in Question Answering domains where commonsense knowledge is expected to play an important role. Recent language models such as ROBERTA, BERT and GPT that have been pre-trained on Wikipedia articles and books have shown reasonable performance with little fine-tuning on several such Multiple Choice Question-Answering (MCQ) datasets. Our goal in this work is to develop methods to incorporate additional (commonsense) knowledge into language model-based approaches for better question-answering in such domains. In this work, we first categorize external knowledge sources, and show performance does improve on using such sources. We then explore three different strategies for knowledge incorporation and four different models for question-answering using external commonsense knowledge. We analyze our predictions to explore the scope of further improvements.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Mitra, Arindam and Banerjee, Pratyay and Pal, Kuntal Kumar and Mishra, Swaroop and Baral, Chitta},
	month = apr,
	year = {2020},
	note = {arXiv:1909.08855 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: 14 pages, 14 figures, 3 tables},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/JSW9KZQD/Mitra et al. - 2020 - How Additional Knowledge can Improve Natural Langu.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/QERCRKYA/1909.html:text/html},
}

@misc{huang_towards_2023,
	title = {Towards {Reasoning} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Towards {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.10403},
	doi = {10.48550/arXiv.2212.10403},
	abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan},
	month = may,
	year = {2023},
	note = {arXiv:2212.10403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: ACL 2023 Findings, 15 pages},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/EUINZVEN/Huang and Chang - 2023 - Towards Reasoning in Large Language Models A Surv.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/8YFN9VTL/2212.html:text/html},
}

@misc{qiao_reasoning_2023,
	title = {Reasoning with {Language} {Model} {Prompting}: {A} {Survey}},
	shorttitle = {Reasoning with {Language} {Model} {Prompting}},
	url = {http://arxiv.org/abs/2212.09597},
	doi = {10.48550/arXiv.2212.09597},
	abstract = {Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
	month = sep,
	year = {2023},
	note = {arXiv:2212.09597 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: ACL 2023, 24 pages, add references of theoretical analysis},
	file = {arXiv Fulltext PDF:/Users/ningerlei/Zotero/storage/9RW7XYNX/Qiao et al. - 2023 - Reasoning with Language Model Prompting A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/ningerlei/Zotero/storage/NXJFCHQ8/2212.html:text/html},
}

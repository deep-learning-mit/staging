@misc{chenGeneptSimpleHardtoBeat2023,
  title = {Genept: A Simple but Hard-to-Beat Foundation Model for Genes and Cells Built from Chatgpt},
  shorttitle = {Genept},
  author = {Chen, Yiqun T. and Zou, James},
  year = {2023},
  month = oct,
  primaryclass = {New Results},
  pages = {2023.10.16.562533},
  publisher = {bioRxiv},
  doi = {10.1101/2023.10.16.562533},
  urldate = {2023-11-03},
  abstract = {There has been significant recent progress in leveraging large-scale gene expression data to develop foundation models for single-cell transcriptomes such as Geneformer [1], scGPT [2], and scBERT [3]. These models infer gene functions and interrelations from the gene expression profiles of millions of cells, which requires extensive data curation and resource-intensive training. Here, we explore a much simpler alternative by leveraging ChatGPT embeddings of genes based on literature. Our proposal, GenePT, uses NCBI text descriptions of individual genes with GPT-3.5 to generate gene embeddings. From there, GenePT generates single-cell embeddings in two ways: (i) by averaging the gene embeddings, weighted by each gene's expression level; or (ii) by creating a sentence embedding for each cell, using gene names ordered by the expression level. Without the need for dataset curation and additional pretraining, GenePT is efficient and easy to use. On many downstream tasks used to evaluate recent single-cell foundation models \textemdash{} e.g., classifying gene properties and cell types \textemdash{} GenePT achieves comparable, and often better, performance than Geneformer and other methods. GenePT demonstrates that large language model embedding of literature is a simple and effective path for biological foundation models.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/RB3JFP9K/Chen and Zou - 2023 - Genept A Simple but Hard-to-Beat Foundation Model.pdf}
}

@misc{cuiScGPTBuildingFoundation2023,
  title = {scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI},
  shorttitle = {scGPT},
  author = {Cui, Haotian and Wang, Chloe and Maan, Hassaan and Wang, Bo},
  year = {2023},
  month = may,
  primaryclass = {New Results},
  pages = {2023.04.30.538439},
  publisher = {bioRxiv},
  doi = {10.1101/2023.04.30.538439},
  urldate = {2023-11-08},
  abstract = {Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. While texts are made up of words, cells can be characterized by genes. This analogy inspires us to explore the potential of foundation models for cell and gene biology. By leveraging the exponentially growing single-cell sequencing data, we present the first attempt to construct a single-cell foundation model through generative pre-training on over 10 million cells. We demonstrate that the generative pre-trained transformer, scGPT, effectively captures meaningful biological insights into genes and cells. Furthermore, the model can be readily finetuned to achieve state-of-the-art performance across a variety of downstream tasks, including multi-batch integration, multi-omic integration, cell-type annotation, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at https://github.com/bowang-lab/scGPT.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/2S6TRQIB/Cui et al. - 2023 - scGPT Towards Building a Foundation Model for Sin.pdf}
}

@article{gunawanIntroductionRepresentationLearning2023,
  title = {An introduction to representation learning for single-cell data analysis},
  author = {Gunawan, Ihuan and Vafaee, Fatemeh and Meijering, Erik and Lock, John George},
  year = {2023},
  month = aug,
  journal = {Cell Reports Methods},
  volume = {3},
  number = {8},
  pages = {100547},
  issn = {2667-2375},
  doi = {10.1016/j.crmeth.2023.100547},
  urldate = {2023-11-06},
  abstract = {Single-cell-resolved systems biology methods, including omics- and imaging-based measurement modalities, generate a wealth of high-dimensional data characterizing the heterogeneity of cell populations. Representation learning methods are routinely used to analyze these complex, high-dimensional data by projecting them into lower-dimensional embeddings. This facilitates the interpretation and interrogation of the structures, dynamics, and regulation of cell heterogeneity. Reflecting their central role in analyzing diverse single-cell data types, a myriad of representation learning methods exist, with new approaches continually emerging. Here, we contrast general features of representation learning methods spanning statistical, manifold learning, and neural network approaches. We consider key steps involved in representation learning with single-cell data, including data pre-processing, hyperparameter optimization, downstream analysis, and biological validation. Interdependencies and contingencies linking these steps are also highlighted. This overview is intended to guide researchers in the selection, application, and optimization of representation learning strategies for current and future single-cell research applications., High-dimensional data generated by single-cell systems biology (omics) methods require powerful representation learning approaches to enable interpretation and interrogation of the structures, dynamics, and regulation of cell heterogeneity. In this perspective, Gunawan et~al. elucidate key steps involved in representation learning to guide optimized method application by researchers analyzing diverse single-cell data modalities.},
  pmcid = {PMC10475795},
  pmid = {37671013},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/NGUXKNUZ/Gunawan et al. - 2023 - An introduction to representation learning for sin.pdf}
}

@techreport{haoLargeScaleFoundation2023,
  type = {preprint},
  title = {Large Scale Foundation Model on Single-cell Transcriptomics},
  author = {Hao, Minsheng and Gong, Jing and Zeng, Xin and Liu, Chiming and Guo, Yucheng and Cheng, Xingyi and Wang, Taifeng and Ma, Jianzhu and Song, Le and Zhang, Xuegong},
  year = {2023},
  month = may,
  institution = {Bioinformatics},
  doi = {10.1101/2023.05.29.542705},
  urldate = {2023-11-06},
  abstract = {Large-scale pretrained models have become foundation models leading to breakthroughs in natural language processing and related fields. Developing foundation models in life science for deciphering the "languages" of cells and facilitating biomedical research is promising yet challenging. We developed a large-scale pretrained model scFoundation with 100M parameters for this purpose. scFoundation was trained on over 50 million human single-cell transcriptomics data, which contain high-throughput observations on the complex molecular features in all known types of cells. scFoundation is currently the largest model in terms of the size of trainable parameters, dimensionality of genes and the number of cells used in the pre-training. Experiments showed that scFoundation can serve as a foundation model for single-cell transcriptomics and achieve state-of-the-art performances in a diverse array of downstream tasks, such as gene expression enhancement, tissue drug response prediction, single-cell drug response classification, and single-cell perturbation prediction.},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/6IFWM67I/Hao et al. - 2023 - Large Scale Foundation Model on Single-cell Transc.pdf}
}

@article{hwangSinglecellRNASequencing2018,
  title = {Single-cell RNA sequencing technologies and bioinformatics pipelines},
  author = {Hwang, Byungjin and Lee, Ji Hyun and Bang, Duhee},
  year = {2018},
  month = aug,
  journal = {Experimental \& Molecular Medicine},
  volume = {50},
  number = {8},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2092-6413},
  doi = {10.1038/s12276-018-0071-8},
  urldate = {2023-11-08},
  abstract = {Rapid progress in the development of next-generation sequencing (NGS)technologies in recent years has provided many valuable insights into complexbiological systems, ranging from cancer genomics to diverse microbial communities.NGS-based technologies for genomics, transcriptomics, and epigenomics are nowincreasingly focused on the characterization of individual cells. These single-cellanalyses will allow researchers to uncover new and potentially unexpected biologicaldiscoveries relative to traditional profiling methods that assess bulk populations.Single-cell RNA sequencing (scRNA-seq), for example, can reveal complex and rarecell populations, uncover regulatory relationships between genes, and track thetrajectories of distinct cell lineages in development. In this review, we will focuson technical challenges in single-cell isolation and library preparation and oncomputational analysis pipelines available for analyzing scRNA-seq data. Furthertechnical improvements at the level of molecular and cell biology and in availablebioinformatics tools will greatly facilitate both the basic science and medicalapplications of these sequencing technologies.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {6.S898 project,Bioinformatics,Sequencing},
  file = {/Users/rcalef/Zotero/storage/N3UGZQ8I/Hwang et al. - 2018 - Single-cell RNA sequencing technologies and bioinf.pdf}
}

@article{kedzierskaAssessingLimitsZeroshot,
  title = {Assessing the limits of zero-shot foundation models in single-cell biology},
  author = {Kedzierska, Kasia Z and Crawford, Lorin and Amini, Ava P and Lu, Alex X},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/ZUGP4SWS/Kedzierska et al. - Assessing the limits of zero-shot foundation model.pdf}
}

@article{lahnemannElevenGrandChallenges2020,
  title = {Eleven grand challenges in single-cell data science},
  author = {L{\"a}hnemann, David and K{\"o}ster, Johannes and Szczurek, Ewa and McCarthy, Davis J. and Hicks, Stephanie C. and Robinson, Mark D. and Vallejos, Catalina A. and Campbell, Kieran R. and Beerenwinkel, Niko and Mahfouz, Ahmed and Pinello, Luca and Skums, Pavel and Stamatakis, Alexandros and Attolini, Camille Stephan-Otto and Aparicio, Samuel and Baaijens, Jasmijn and Balvert, Marleen and de Barbanson, Buys and Cappuccio, Antonio and Corleone, Giacomo and Dutilh, Bas E. and Florescu, Maria and Guryev, Victor and Holmer, Rens and Jahn, Katharina and Lobo, Thamar Jessurun and Keizer, Emma M. and Khatri, Indu and Kielbasa, Szymon M. and Korbel, Jan O. and Kozlov, Alexey M. and Kuo, Tzu-Hao and Lelieveldt, Boudewijn P.F. and Mandoiu, Ion I. and Marioni, John C. and Marschall, Tobias and M{\"o}lder, Felix and Niknejad, Amir and R{\k{a}}czkowska, Alicja and Reinders, Marcel and de Ridder, Jeroen and Saliba, Antoine-Emmanuel and Somarakis, Antonios and Stegle, Oliver and Theis, Fabian J. and Yang, Huan and Zelikovsky, Alex and McHardy, Alice C. and Raphael, Benjamin J. and Shah, Sohrab P. and Sch{\"o}nhuth, Alexander},
  year = {2020},
  month = feb,
  journal = {Genome Biology},
  volume = {21},
  number = {1},
  pages = {31},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-1926-6},
  urldate = {2023-11-08},
  abstract = {The recent boom in microfluidics and combinatorial indexing strategies, combined with low sequencing costs, has empowered single-cell sequencing technology. Thousands\textemdash or even millions\textemdash of cells analyzed in a single experiment amount to a data revolution in single-cell biology and pose unique data science problems. Here, we outline eleven challenges that will be central to bringing this emerging field of single-cell data science forward. For each challenge, we highlight motivating research questions, review prior work, and formulate open problems. This compendium is for established researchers, newcomers, and students alike, highlighting interesting and rewarding problems for the coming years.},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/DT2NV9J8/LÃ¤hnemann et al. - 2020 - Eleven grand challenges in single-cell data scienc.pdf}
}

@techreport{levineCell2SentenceTeachingLarge2023,
  type = {preprint},
  title = {Cell2Sentence: Teaching Large Language Models the Language of Biology},
  shorttitle = {Cell2Sentence},
  author = {Levine, Daniel and Rizvi, Syed Asad and L{\'e}vy, Sacha and Pallikkavaliyaveetil, Nazreen and Wu, Ruiming and Zheng, Zihe and Fonseca, Antonio Oliveira and Chen, Xingyu and Ghadermarzi, Sina and Dhodapkar, Rahul M. and Van Dijk, David},
  year = {2023},
  month = sep,
  institution = {Bioinformatics},
  doi = {10.1101/2023.09.11.557287},
  urldate = {2023-10-18},
  abstract = {Large language models like GPT have shown impressive performance on natural language tasks. Here, we present a novel method to directly adapt these pretrained models to a biological context, specifically single-cell transcriptomics, by representing gene expression data as text. Our Cell2Sentence approach converts each cell's gene expression profile into a sequence of gene names ordered by expression level. We show that these gene sequences, which we term "cell sentences", can be used to fine-tune causal language models like GPT-2. Critically, we find that natural language pretraining boosts model performance on cell sentence tasks. When fine-tuned on cell sentences, GPT-2 generates biologically valid cells when prompted with a cell type. Conversely, it can also accurately predict cell type labels when prompted with cell sentences. This demonstrates that language models fine-tuned using Cell2Sentence can gain a biological understanding of single-cell data, while retaining their ability to generate text. Our approach provides a simple, adaptable framework to combine natural language and transcriptomics using existing models and libraries. Our code is available at: https://github.com/vandijklab/cell2sentence-ft.},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/VR8Q6N4V/Levine et al. - 2023 - Cell2Sentence Teaching Large Language Models the .pdf}
}

@article{lopezDeepGenerativeModeling2018,
  title = {Deep generative modeling for single-cell transcriptomics},
  author = {Lopez, Romain and Regier, Jeffrey and Cole, Michael B. and Jordan, Michael I. and Yosef, Nir},
  year = {2018},
  month = dec,
  journal = {Nature Methods},
  volume = {15},
  number = {12},
  pages = {1053--1058},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-018-0229-2},
  urldate = {2023-11-08},
  abstract = {Single-cell transcriptome measurements can reveal unexplored biological diversity, but they suffer from technical noise and bias that must be modeled to account for the resulting uncertainty in downstream analyses. Here we introduce single-cell variational inference (scVI), a ready-to-use scalable framework for the probabilistic representation and analysis of gene expression in single cells ( https://github.com/YosefLab/scVI ). scVI uses stochastic optimization and deep neural networks to aggregate information across similar cells and genes and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity. We used scVI for a range of fundamental analysis tasks including batch correction, visualization, clustering, and differential expression, and achieved high accuracy for each task. scVI is a ready-to-use generative deep learning tool for large-scale single-cell RNA-seq data that enables raw data processing and a wide range of rapid and accurate downstream analyses.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {6.S898 project,Computational biology and bioinformatics,Computational models},
  file = {/Users/rcalef/Zotero/storage/NLNPACIB/Lopez et al. - 2018 - Deep generative modeling for single-cell transcrip.pdf}
}

@article{lotfollahiScGenPredictsSinglecell2019,
  title = {scGen predicts single-cell perturbation responses},
  author = {Lotfollahi, Mohammad and Wolf, F. Alexander and Theis, Fabian J.},
  year = {2019},
  month = aug,
  journal = {Nature Methods},
  volume = {16},
  number = {8},
  pages = {715--721},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0494-8},
  urldate = {2023-11-06},
  abstract = {Accurately modeling cellular response to perturbations is a central goal of computational biology. While such modeling has been based on statistical, mechanistic and machine learning models in specific settings, no generalization of predictions to phenomena absent from training data (out-of-sample) has yet been demonstrated. Here, we present scGen (https://github.com/theislab/scgen), a model combining variational autoencoders and latent space vector arithmetics for high-dimensional single-cell gene expression data. We show that scGen accurately models perturbation and infection response of cells across cell types, studies and species. In particular, we demonstrate that scGen learns cell-type and species-specific responses implying that it captures features that distinguish responding from non-responding genes and cells. With the upcoming availability of large-scale atlases of organs in a healthy state, we envision scGen to become a tool for experimental design through in silico screening of perturbation response in the context of disease and drug treatment.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {6.S898 project,Cell biology,Gene expression,Machine learning},
  file = {/Users/rcalef/Zotero/storage/X9ZLCBQ7/Lotfollahi et al. - 2019 - scGen predicts single-cell perturbation responses.pdf}
}

@article{lueckenBenchmarkingAtlaslevelData2022,
  title = {Benchmarking atlas-level data integration in single-cell genomics},
  author = {Luecken, Malte D. and Bttner, M. and Chaichoompu, K. and Danese, A. and Interlandi, M. and Mueller, M. F. and Strobl, D. C. and Zappia, L. and Dugas, M. and {Colom{\'e}-Tatch{\'e}}, M. and Theis, Fabian J.},
  year = {2022},
  month = jan,
  journal = {Nature Methods},
  volume = {19},
  number = {1},
  pages = {41--50},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01336-8},
  urldate = {2023-11-09},
  abstract = {Single-cell atlases often include samples that span locations, laboratories and conditions, leading to complex, nested batch effects in data. Thus, joint analysis of atlas datasets requires reliable data integration. To guide integration method choice, we benchmarked 68 method and preprocessing combinations on 85 batches of gene expression, chromatin accessibility and simulation data from 23 publications, altogether representing {$>$}1.2 million cells distributed in 13 atlas-level integration tasks. We evaluated methods according to scalability, usability and their ability to remove batch effects while retaining biological variation using 14 evaluation metrics. We show that highly variable gene selection improves the performance of data integration methods, whereas scaling pushes methods to prioritize batch removal over conservation of biological variation. Overall, scANVI, Scanorama, scVI and scGen perform well, particularly on complex integration tasks, while single-cell ATAC-sequencing integration performance is strongly affected by choice of feature space. Our freely available Python module and benchmarking pipeline can identify optimal data integration methods for new data, benchmark new methods and improve method development.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {6.S898 project,Data integration,Machine learning,Software,Transcriptomics},
  file = {/Users/rcalef/Zotero/storage/ZQRLWN98/Luecken et al. - 2022 - Benchmarking atlas-level data integration in singl.pdf}
}

@article{mathysSinglecellAtlasReveals2023,
  title = {Single-cell atlas reveals correlates of high cognitive function, dementia, and resilience to Alzheimer's disease pathology},
  author = {Mathys, Hansruedi and Peng, Zhuyu and Boix, Carles A. and Victor, Matheus B. and Leary, Noelle and Babu, Sudhagar and Abdelhady, Ghada and Jiang, Xueqiao and Ng, Ayesha P. and Ghafari, Kimia and Kunisky, Alexander K. and Mantero, Julio and Galani, Kyriaki and Lohia, Vanshika N. and Fortier, Gabrielle E. and Lotfi, Yasmine and Ivey, Jason and Brown, Hannah P. and Patel, Pratham R. and Chakraborty, Nehal and Beaudway, Jacob I. and Imhoff, Elizabeth J. and Keeler, Cameron F. and McChesney, Maren M. and Patel, Haishal H. and Patel, Sahil P. and Thai, Megan T. and Bennett, David A. and Kellis, Manolis and Tsai, Li-Huei},
  year = {2023},
  month = sep,
  journal = {Cell},
  volume = {186},
  number = {20},
  pages = {4365-4385.e27},
  publisher = {Elsevier},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2023.08.039},
  urldate = {2023-11-09},
  langid = {english},
  pmid = {37774677},
  keywords = {6.S898 project,Alzheimer's disease,cognitive impairment,cognitive resilience,cohesin complex,DNA damage response,inhibitory neurons,neurodegeneration,single-cell transcriptomic atlas},
  file = {/Users/rcalef/Zotero/storage/IFBR7EZY/Mathys et al. - 2023 - Single-cell atlas reveals correlates of high cogni.pdf}
}

@article{panMicrofluidicsFacilitatesDevelopment2022,
  title = {Microfluidics Facilitates the Development of Single-Cell RNA Sequencing},
  author = {Pan, Yating and Cao, Wenjian and Mu, Ying and Zhu, Qiangyuan},
  year = {2022},
  month = jul,
  journal = {Biosensors},
  volume = {12},
  number = {7},
  pages = {450},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-6374},
  doi = {10.3390/bios12070450},
  urldate = {2023-12-05},
  abstract = {Single-cell RNA sequencing (scRNA-seq) technology provides a powerful tool for understanding complex biosystems at the single-cell and single-molecule level. The past decade has been a golden period for the development of single-cell sequencing, with scRNA-seq undergoing a tremendous leap in sensitivity and throughput. The application of droplet- and microwell-based microfluidics in scRNA-seq has contributed greatly to improving sequencing throughput. This review introduces the history of development and important technical factors of scRNA-seq. We mainly focus on the role of microfluidics in facilitating the development of scRNA-seq technology. To end, we discuss the future directions for scRNA-seq.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {6.S898 project,droplet,microfluidics,microwell,scRNA-seq},
  file = {/Users/rcalef/Zotero/storage/3WW3SVGB/Pan et al. - 2022 - Microfluidics Facilitates the Development of Singl.pdf}
}

@article{theodorisTransferLearningEnables2023,
  title = {Transfer learning enables predictions in network biology},
  author = {Theodoris, Christina V. and Xiao, Ling and Chopra, Anant and Chaffin, Mark D. and Al Sayed, Zeina R. and Hill, Matthew C. and Mantineo, Helene and Brydon, Elizabeth M. and Zeng, Zexian and Liu, X. Shirley and Ellinor, Patrick T.},
  year = {2023},
  month = jun,
  journal = {Nature},
  volume = {618},
  number = {7965},
  pages = {616--624},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06139-9},
  urldate = {2023-10-18},
  abstract = {Mapping gene networks requires large amounts of transcriptomic data to learn the connections between genes, which impedes discoveries in settings with limited data, including rare diseases and diseases affecting clinically inaccessible tissues. Recently, transfer learning has revolutionized fields such as natural language understanding1,2 and computer vision3 by leveraging deep learning models pretrained on large-scale general datasets that can then be fine-tuned towards a vast array of downstream tasks with limited task-specific data. Here, we developed a context-aware, attention-based deep learning model, Geneformer, pretrained on a large-scale corpus of about 30\,million single-cell transcriptomes to enable context-specific predictions in settings with limited data in network biology. During pretraining, Geneformer gained a fundamental understanding of network dynamics, encoding network hierarchy in the attention weights of the model in a completely self-supervised manner. Fine-tuning towards a diverse panel of downstream tasks relevant to chromatin and network dynamics using limited task-specific data demonstrated that Geneformer consistently boosted predictive accuracy. Applied to disease modelling with limited patient data, Geneformer identified candidate therapeutic targets for cardiomyopathy. Overall, Geneformer represents a pretrained deep learning model from which fine-tuning towards a broad range of downstream applications can be pursued to accelerate discovery of key network regulators and candidate therapeutic targets.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {6.S898 project,Cardiomyopathies,Computational models,Gene regulatory networks,Machine learning,Regulatory networks},
  file = {/Users/rcalef/Zotero/storage/D399P6II/Theodoris et al. - 2023 - Transfer learning enables predictions in network b.pdf}
}

@misc{yangGeneCompassDecipheringUniversal2023,
  title = {GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model},
  shorttitle = {GeneCompass},
  author = {Yang, Xiaodong and Liu, Guole and Feng, Guihai and Bu, Dechao and Wang, Pengfei and Jiang, Jie and Chen, Shubai and Yang, Qinmeng and Zhang, Yiyang and Man, Zhenpeng and Liang, Zhongming and Wang, Zichen and Li, Yaning and Li, Zheng and Liu, Yana and Tian, Yao and Li, Ao and Dong, Jingxi and Hu, Zhilong and Fang, Chen and Miao, Hefan and Cui, Lina and Deng, Zixu and Jiang, Haiping and Cui, Wentao and Zhang, Jiahao and Yang, Zhaohui and Li, Handong and He, Xingjian and Zhong, Liqun and Zhou, Jiaheng and Wang, Zijian and Long, Qingqing and Xu, Ping and Consortium, The X.-Compass and Wang, Hongmei and Meng, Zhen and Wang, Xuezhi and Wang, Yangang and Wang, Yong and Zhang, Shihua and Guo, Jingtao and Zhao, Yi and Zhou, Yuanchun and Li, Fei and Liu, Jing and Chen, Yiqiang and Yang, Ge and Li, Xin},
  year = {2023},
  month = sep,
  primaryclass = {New Results},
  pages = {2023.09.26.559542},
  publisher = {bioRxiv},
  doi = {10.1101/2023.09.26.559542},
  urldate = {2023-11-06},
  abstract = {Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, the traditional research paradigm primarily focuses on individual model organisms, resulting in limited collection and integration of complex features on various cell types across species. Recent breakthroughs in single-cell sequencing and advancements in deep learning techniques present an unprecedented opportunity to tackle this challenge. In this study, we developed GeneCompass, the first knowledge-informed, cross-species foundation model pre-trained on an extensive dataset of over 120 million single-cell transcriptomes from human and mouse. During pre-training, GeneCompass effectively integrates four types of biological prior knowledge to enhance the understanding of gene regulatory mechanisms in a self-supervised manner. Fine-tuning towards multiple downstream tasks, GeneCompass outperforms competing state-of-the-art models in multiple tasks on single species and unlocks new realms of cross-species biological investigation. Overall, GeneCompass marks a milestone in advancing knowledge of universal gene regulatory mechanisms and accelerating the discovery of key cell fate regulators and candidate targets for drug development.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/3WUJ2ZV7/Yang et al. - 2023 - GeneCompass Deciphering Universal Gene Regulatory.pdf}
}

@article{yangScBERTLargescalePretrained2022,
  title = {scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data},
  author = {Yang, Fan and Wang, Wenchuan and Wang, Fang and Fang, Yuan and Tang, Duyu and Huang, Junzhou and Lu, Hui and Yao, Jianhua},
  year = {2022},
  month = oct,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {10},
  pages = {852--866},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00534-z},
  urldate = {2023-11-06},
  abstract = {Annotating cell types on the basis of single-cell RNA-seq data is a prerequisite for research on disease progress and tumour microenvironments. Here we show that existing annotation methods typically suffer from a lack of curated marker gene lists, improper handling of batch effects and difficulty in leveraging the latent gene\textendash gene interaction information, impairing their generalization and robustness. We developed a pretrained deep neural network-based model, single-cell bidirectional encoder representations from transformers (scBERT), to overcome the challenges. Following BERT's approach to pretraining and fine-tuning, scBERT attains a general understanding of gene\textendash gene interactions by being pretrained on huge amounts of unlabelled scRNA-seq data; it is then transferred to the cell type annotation task of unseen and user-specific scRNA-seq data for supervised fine-tuning. Extensive and rigorous benchmark studies validated the superior performance of scBERT on cell type annotation, novel cell type discovery, robustness to batch effects and model interpretability.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {6.S898 project,Bioinformatics,Classification and taxonomy,Gene expression},
  file = {/Users/rcalef/Zotero/storage/I33Y63J6/Yang et al. - 2022 - scBERT as a large-scale pretrained deep language m.pdf}
}

@article{zhangScPretrainMultitaskSelfsupervised2022,
  title = {scPretrain: multi-task self-supervised learning for cell-type classification},
  shorttitle = {scPretrain},
  author = {Zhang, Ruiyi and Luo, Yunan and Ma, Jianzhu and Zhang, Ming and Wang, Sheng},
  editor = {Mathelier, Anthony},
  year = {2022},
  month = mar,
  journal = {Bioinformatics},
  volume = {38},
  number = {6},
  pages = {1607--1614},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/btac007},
  urldate = {2023-08-28},
  abstract = {Motivation: Rapidly generated scRNA-seq datasets enable us to understand cellular differences and the function of each individual cell at single-cell resolution. Cell-type classification, which aims at characterizing and labeling groups of cells according to their gene expression, is one of the most important steps for single-cell analysis. To facilitate the manual curation process, supervised learning methods have been used to automatically classify cells. Most of the existing supervised learning approaches only utilize annotated cells in the training step while ignoring the more abundant unannotated cells. In this article, we proposed scPretrain, a multi-task self-supervised learning approach that jointly considers annotated and unannotated cells for cell-type classification. scPretrain consists of a pre-training step and a fine-tuning step. In the pre-training step, scPretrain uses a multi-task learning framework to train a feature extraction encoder based on each dataset's pseudo-labels, where only unannotated cells are used. In the fine-tuning step, scPretrain fine-tunes this feature extraction encoder using the limited annotated cells in a new dataset.},
  langid = {english},
  keywords = {6.S898 project,pretraining,representation learning,scRNA,self-supervised learning},
  file = {/Users/rcalef/Zotero/storage/G2I2ZNCQ/Zhang et al. - 2022 - scPretrain multi-task self-supervised learning fo.pdf}
}

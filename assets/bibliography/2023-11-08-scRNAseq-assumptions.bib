@techreport{boiarskyDeepDiveSingleCell2023,
  type = {preprint},
  title = {A Deep Dive into Single-Cell RNA Sequencing Foundation Models},
  author = {Boiarsky, Rebecca and Singh, Nalini and Buendia, Alejandro and Getz, Gad and Sontag, David},
  year = {2023},
  month = oct,
  institution = {Genomics},
  doi = {10.1101/2023.10.19.563100},
  urldate = {2023-12-11},
  abstract = {Large-scale foundation models, which are pre-trained on massive, unlabeled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology. In this paper, we explore two foundation models recently developed for single-cell RNA sequencing data, scBERT and scGPT. Focusing on the fine-tuning task of cell type annotation, we explore the relative performance of pre-trained models compared to a simple baseline, L1-regularized logistic regression, including in the few-shot setting. We perform ablation studies to understand whether pretraining improves model performance and to better understand the difficulty of the pre-training task in scBERT. Finally, using scBERT as an example, we demonstrate the potential sensitivity of fine-tuning to hyperparameter settings and parameter initializations. Taken together, our results highlight the importance of rigorously testing foundation models against well established baselines, establishing challenging fine-tuning tasks on which to benchmark foundation models, and performing deep introspection into the embeddings learned by the model in order to more effectively harness these models to transform single-cell data analysis. Code is available at https://github.com/clinicalml/sc-foundation-eval.},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/RXF2LT4V/Boiarsky et al. - 2023 - A Deep Dive into Single-Cell RNA Sequencing Founda.pdf}
}

@misc{chenGeneptSimpleHardtoBeat2023,
  title = {Genept: A Simple but Hard-to-Beat Foundation Model for Genes and Cells Built from Chatgpt},
  shorttitle = {Genept},
  author = {Chen, Yiqun T. and Zou, James},
  year = {2023},
  month = oct,
  primaryclass = {New Results},
  pages = {2023.10.16.562533},
  publisher = {bioRxiv},
  doi = {10.1101/2023.10.16.562533},
  urldate = {2023-11-03},
  abstract = {There has been significant recent progress in leveraging large-scale gene expression data to develop foundation models for single-cell transcriptomes such as Geneformer [1], scGPT [2], and scBERT [3]. These models infer gene functions and interrelations from the gene expression profiles of millions of cells, which requires extensive data curation and resource-intensive training. Here, we explore a much simpler alternative by leveraging ChatGPT embeddings of genes based on literature. Our proposal, GenePT, uses NCBI text descriptions of individual genes with GPT-3.5 to generate gene embeddings. From there, GenePT generates single-cell embeddings in two ways: (i) by averaging the gene embeddings, weighted by each gene's expression level; or (ii) by creating a sentence embedding for each cell, using gene names ordered by the expression level. Without the need for dataset curation and additional pretraining, GenePT is efficient and easy to use. On many downstream tasks used to evaluate recent single-cell foundation models {\textemdash} e.g., classifying gene properties and cell types {\textemdash} GenePT achieves comparable, and often better, performance than Geneformer and other methods. GenePT demonstrates that large language model embedding of literature is a simple and effective path for biological foundation models.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/RB3JFP9K/Chen and Zou - 2023 - Genept A Simple but Hard-to-Beat Foundation Model.pdf}
}

@article{consortiumTabulaSapiensMultipleorgan2022,
  title = {The Tabula Sapiens: A multiple-organ, single-cell transcriptomic atlas of humans},
  shorttitle = {The Tabula Sapiens},
  author = {Consortium*, The Tabula Sapiens and Jones, Robert C. and Karkanias, Jim and Krasnow, Mark A. and Pisco, Angela Oliveira and Quake, Stephen R. and Salzman, Julia and Yosef, Nir and Bulthaup, Bryan and Brown, Phillip and Harper, William and Hemenez, Marisa and Ponnusamy, Ravikumar and Salehi, Ahmad and Sanagavarapu, Bhavani A. and Spallino, Eileen and Aaron, Ksenia A. and Concepcion, Waldo and Gardner, James M. and Kelly, Burnett and Neidlinger, Nikole and Wang, Zifa and Crasta, Sheela and Kolluru, Saroja and Morri, Maurizio and Pisco, Angela Oliveira and Tan, Serena Y. and Travaglini, Kyle J. and Xu, Chenling and {Alc{\'a}ntara-Hern{\'a}ndez}, Marcela and Almanzar, Nicole and Antony, Jane and Beyersdorf, Benjamin and Burhan, Deviana and Calcuttawala, Kruti and Carter, Matthew M. and Chan, Charles K. F. and Chang, Charles A. and Chang, Stephen and Colville, Alex and Crasta, Sheela and Culver, Rebecca N. and Cvijovi{\'c}, Ivana and D'Amato, Gaetano and Ezran, Camille and Galdos, Francisco X. and Gillich, Astrid and Goodyer, William R. and Hang, Yan and Hayashi, Alyssa and Houshdaran, Sahar and Huang, Xianxi and Irwin, Juan C. and Jang, SoRi and Juanico, Julia Vallve and Kershner, Aaron M. and Kim, Soochi and Kiss, Bernhard and Kolluru, Saroja and Kong, William and Kumar, Maya E. and Kuo, Angera H. and Leylek, Rebecca and Li, Baoxiang and Loeb, Gabriel B. and Lu, Wan-Jin and Mantri, Sruthi and Markovic, Maxim and McAlpine, Patrick L. and de Morree, Antoine and Morri, Maurizio and Mrouj, Karim and Mukherjee, Shravani and Muser, Tyler and Neuh{\"o}fer, Patrick and Nguyen, Thi D. and Perez, Kimberly and Phansalkar, Ragini and Pisco, Angela Oliveira and Puluca, Nazan and Qi, Zhen and Rao, Poorvi and {Raquer-McKay}, Hayley and Schaum, Nicholas and Scott, Bronwyn and Seddighzadeh, Bobak and Segal, Joe and Sen, Sushmita and Sikandar, Shaheen and Spencer, Sean P. and Steffes, Lea C. and Subramaniam, Varun R. and Swarup, Aditi and Swift, Michael and Travaglini, Kyle J. and Treuren, Will Van and Trimm, Emily and Veizades, Stefan and Vijayakumar, Sivakamasundari and Vo, Kim Chi and Vorperian, Sevahn K. and Wang, Wanxin and Weinstein, Hannah N. W. and Winkler, Juliane and Wu, Timothy T. H. and Xie, Jamie and Yung, Andrea R. and Zhang, Yue and Detweiler, Angela M. and Mekonen, Honey and Neff, Norma F. and Sit, Rene V. and Tan, Michelle and Yan, Jia and Bean, Gregory R. and Charu, Vivek and Forg{\'o}, Erna and Martin, Brock A. and Ozawa, Michael G. and Silva, Oscar and Tan, Serena Y. and Toland, Angus and Vemuri, Venkata N. P. and Afik, Shaked and Awayan, Kyle and Botvinnik, Olga Borisovna and Byrne, Ashley and Chen, Michelle and Dehghannasiri, Roozbeh and Detweiler, Angela M. and Gayoso, Adam and Granados, Alejandro A. and Li, Qiqing and Mahmoudabadi, Gita and McGeever, Aaron and de Morree, Antoine and Olivieri, Julia Eve and Park, Madeline and Pisco, Angela Oliveira and Ravikumar, Neha and Salzman, Julia and Stanley, Geoff and Swift, Michael and Tan, Michelle and Tan, Weilun and Tarashansky, Alexander J. and Vanheusden, Rohan and Vorperian, Sevahn K. and Wang, Peter and Wang, Sheng and Xing, Galen and Xu, Chenling and Yosef, Nir and {Alc{\'a}ntara-Hern{\'a}ndez}, Marcela and Antony, Jane and Chan, Charles K. F. and Chang, Charles A. and Colville, Alex and Crasta, Sheela and Culver, Rebecca and Dethlefsen, Les and Ezran, Camille and Gillich, Astrid and Hang, Yan and Ho, Po-Yi and Irwin, Juan C. and Jang, SoRi and Kershner, Aaron M. and Kong, William and Kumar, Maya E. and Kuo, Angera H. and Leylek, Rebecca and Liu, Shixuan and Loeb, Gabriel B. and Lu, Wan-Jin and Maltzman, Jonathan S. and Metzger, Ross J. and de Morree, Antoine and Neuh{\"o}fer, Patrick and Perez, Kimberly and Phansalkar, Ragini and Qi, Zhen and Rao, Poorvi and {Raquer-McKay}, Hayley and Sasagawa, Koki and Scott, Bronwyn and Sinha, Rahul and Song, Hanbing and Spencer, Sean P. and Swarup, Aditi and Swift, Michael and Travaglini, Kyle J. and Trimm, Emily and Veizades, Stefan and Vijayakumar, Sivakamasundari and Wang, Bruce and Wang, Wanxin and Winkler, Juliane and Xie, Jamie and Yung, Andrea R. and Artandi, Steven E. and Beachy, Philip A. and Clarke, Michael F. and Giudice, Linda C. and Huang, Franklin W. and Huang, Kerwyn Casey and Idoyaga, Juliana and Kim, Seung K. and Krasnow, Mark and Kuo, Christin S. and Nguyen, Patricia and Quake, Stephen R. and Rando, Thomas A. and {Red-Horse}, Kristy and Reiter, Jeremy and Relman, David A. and Sonnenburg, Justin L. and Wang, Bruce and Wu, Albert and Wu, Sean M. and {Wyss-Coray}, Tony},
  year = {2022},
  month = may,
  journal = {Science},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.abl4896},
  urldate = {2023-12-12},
  abstract = {Single-cell transcriptomics provides a molecularly defined phenotypic reference of human cell types that spans 24 human tissues and organs.},
  copyright = {Copyright {\textcopyright} 2022 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/ZXJZPCWW/Consortium et al. - 2022 - The Tabula Sapiens A multiple-organ, single-cell .pdf}
}

@misc{cuiScGPTBuildingFoundation2023,
  title = {scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI},
  shorttitle = {scGPT},
  author = {Cui, Haotian and Wang, Chloe and Maan, Hassaan and Wang, Bo},
  year = {2023},
  month = may,
  primaryclass = {New Results},
  pages = {2023.04.30.538439},
  publisher = {bioRxiv},
  doi = {10.1101/2023.04.30.538439},
  urldate = {2023-11-08},
  abstract = {Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. While texts are made up of words, cells can be characterized by genes. This analogy inspires us to explore the potential of foundation models for cell and gene biology. By leveraging the exponentially growing single-cell sequencing data, we present the first attempt to construct a single-cell foundation model through generative pre-training on over 10 million cells. We demonstrate that the generative pre-trained transformer, scGPT, effectively captures meaningful biological insights into genes and cells. Furthermore, the model can be readily finetuned to achieve state-of-the-art performance across a variety of downstream tasks, including multi-batch integration, multi-omic integration, cell-type annotation, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at https://github.com/bowang-lab/scGPT.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/2S6TRQIB/Cui et al. - 2023 - scGPT Towards Building a Foundation Model for Sin.pdf}
}

@article{furusawaZipfLawGene2003,
  title = {Zipf's law in gene expression},
  author = {Furusawa, Chikara and Kaneko, Kunihiko},
  year = {2003},
  month = feb,
  journal = {Physical Review Letters},
  volume = {90},
  number = {8},
  pages = {088102},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.90.088102},
  abstract = {Using data from gene expression databases on various organisms and tissues, including yeast, nematodes, human normal and cancer tissues, and embryonic stem cells, we found that the abundances of expressed genes exhibit a power-law distribution with an exponent close to -1; i.e., they obey Zipf's law. Furthermore, by simulations of a simple model with an intracellular reaction network, we found that Zipf's law of chemical abundance is a universal feature of cells where such a network optimizes the efficiency and faithfulness of self-reproduction. These findings provide novel insights into the nature of the organization of reaction dynamics in living cells.},
  langid = {english},
  pmid = {12633463},
  keywords = {6.S898 project,Animals,Caenorhabditis elegans,Computer Simulation,Gene Expression Regulation,Humans,{Models, Genetic},Neoplasms,Rats,Saccharomyces cerevisiae,Stem Cells},
  file = {/Users/rcalef/Zotero/storage/M4A4WZXB/Furusawa and Kaneko - 2003 - Zipf's law in gene expression.pdf}
}

@article{gunawanIntroductionRepresentationLearning2023,
  title = {An introduction to representation learning for single-cell data analysis},
  author = {Gunawan, Ihuan and Vafaee, Fatemeh and Meijering, Erik and Lock, John George},
  year = {2023},
  month = aug,
  journal = {Cell Reports Methods},
  volume = {3},
  number = {8},
  pages = {100547},
  issn = {2667-2375},
  doi = {10.1016/j.crmeth.2023.100547},
  urldate = {2023-11-06},
  abstract = {Single-cell-resolved systems biology methods, including omics- and imaging-based measurement modalities, generate a wealth of high-dimensional data characterizing the heterogeneity of cell populations. Representation learning methods are routinely used to analyze these complex, high-dimensional data by projecting them into lower-dimensional embeddings. This facilitates the interpretation and interrogation of the structures, dynamics, and regulation of cell heterogeneity. Reflecting their central role in analyzing diverse single-cell data types, a myriad of representation learning methods exist, with new approaches continually emerging. Here, we contrast general features of representation learning methods spanning statistical, manifold learning, and neural network approaches. We consider key steps involved in representation learning with single-cell data, including data pre-processing, hyperparameter optimization, downstream analysis, and biological validation. Interdependencies and contingencies linking these steps are also highlighted. This overview is intended to guide researchers in the selection, application, and optimization of representation learning strategies for current and future single-cell research applications., High-dimensional data generated by single-cell systems biology (omics) methods require powerful representation learning approaches to enable interpretation and interrogation of the structures, dynamics, and regulation of cell heterogeneity. In this perspective, Gunawan et~al. elucidate key steps involved in representation learning to guide optimized method application by researchers analyzing diverse single-cell data modalities.},
  pmcid = {PMC10475795},
  pmid = {37671013},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/NGUXKNUZ/Gunawan et al. - 2023 - An introduction to representation learning for sin.pdf}
}

@techreport{haoLargeScaleFoundation2023,
  type = {preprint},
  title = {Large Scale Foundation Model on Single-cell Transcriptomics},
  author = {Hao, Minsheng and Gong, Jing and Zeng, Xin and Liu, Chiming and Guo, Yucheng and Cheng, Xingyi and Wang, Taifeng and Ma, Jianzhu and Song, Le and Zhang, Xuegong},
  year = {2023},
  month = may,
  institution = {Bioinformatics},
  doi = {10.1101/2023.05.29.542705},
  urldate = {2023-11-06},
  abstract = {Large-scale pretrained models have become foundation models leading to breakthroughs in natural language processing and related fields. Developing foundation models in life science for deciphering the "languages" of cells and facilitating biomedical research is promising yet challenging. We developed a large-scale pretrained model scFoundation with 100M parameters for this purpose. scFoundation was trained on over 50 million human single-cell transcriptomics data, which contain high-throughput observations on the complex molecular features in all known types of cells. scFoundation is currently the largest model in terms of the size of trainable parameters, dimensionality of genes and the number of cells used in the pre-training. Experiments showed that scFoundation can serve as a foundation model for single-cell transcriptomics and achieve state-of-the-art performances in a diverse array of downstream tasks, such as gene expression enhancement, tissue drug response prediction, single-cell drug response classification, and single-cell perturbation prediction.},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/6IFWM67I/Hao et al. - 2023 - Large Scale Foundation Model on Single-cell Transc.pdf}
}

@article{hwangSinglecellRNASequencing2018,
  title = {Single-cell RNA sequencing technologies and bioinformatics pipelines},
  author = {Hwang, Byungjin and Lee, Ji Hyun and Bang, Duhee},
  year = {2018},
  month = aug,
  journal = {Experimental \& Molecular Medicine},
  volume = {50},
  number = {8},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2092-6413},
  doi = {10.1038/s12276-018-0071-8},
  urldate = {2023-11-08},
  abstract = {Rapid progress in the development of next-generation sequencing (NGS)technologies in recent years has provided many valuable insights into complexbiological systems, ranging from cancer genomics to diverse microbial communities.NGS-based technologies for genomics, transcriptomics, and epigenomics are nowincreasingly focused on the characterization of individual cells. These single-cellanalyses will allow researchers to uncover new and potentially unexpected biologicaldiscoveries relative to traditional profiling methods that assess bulk populations.Single-cell RNA sequencing (scRNA-seq), for example, can reveal complex and rarecell populations, uncover regulatory relationships between genes, and track thetrajectories of distinct cell lineages in development. In this review, we will focuson technical challenges in single-cell isolation and library preparation and oncomputational analysis pipelines available for analyzing scRNA-seq data. Furthertechnical improvements at the level of molecular and cell biology and in availablebioinformatics tools will greatly facilitate both the basic science and medicalapplications of these sequencing technologies.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {6.S898 project,Bioinformatics,Sequencing},
  file = {/Users/rcalef/Zotero/storage/N3UGZQ8I/Hwang et al. - 2018 - Single-cell RNA sequencing technologies and bioinf.pdf}
}

@article{kedzierskaAssessingLimitsZeroshot,
  title = {Assessing the limits of zero-shot foundation models in single-cell biology},
  author = {Kedzierska, Kasia Z and Crawford, Lorin and Amini, Ava P and Lu, Alex X},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/ZUGP4SWS/Kedzierska et al. - Assessing the limits of zero-shot foundation model.pdf}
}

@article{lahnemannElevenGrandChallenges2020,
  title = {Eleven grand challenges in single-cell data science},
  author = {L{\"a}hnemann, David and K{\"o}ster, Johannes and Szczurek, Ewa and McCarthy, Davis J. and Hicks, Stephanie C. and Robinson, Mark D. and Vallejos, Catalina A. and Campbell, Kieran R. and Beerenwinkel, Niko and Mahfouz, Ahmed and Pinello, Luca and Skums, Pavel and Stamatakis, Alexandros and Attolini, Camille Stephan-Otto and Aparicio, Samuel and Baaijens, Jasmijn and Balvert, Marleen and de Barbanson, Buys and Cappuccio, Antonio and Corleone, Giacomo and Dutilh, Bas E. and Florescu, Maria and Guryev, Victor and Holmer, Rens and Jahn, Katharina and Lobo, Thamar Jessurun and Keizer, Emma M. and Khatri, Indu and Kielbasa, Szymon M. and Korbel, Jan O. and Kozlov, Alexey M. and Kuo, Tzu-Hao and Lelieveldt, Boudewijn P.F. and Mandoiu, Ion I. and Marioni, John C. and Marschall, Tobias and M{\"o}lder, Felix and Niknejad, Amir and R{\k{a}}czkowska, Alicja and Reinders, Marcel and de Ridder, Jeroen and Saliba, Antoine-Emmanuel and Somarakis, Antonios and Stegle, Oliver and Theis, Fabian J. and Yang, Huan and Zelikovsky, Alex and McHardy, Alice C. and Raphael, Benjamin J. and Shah, Sohrab P. and Sch{\"o}nhuth, Alexander},
  year = {2020},
  month = feb,
  journal = {Genome Biology},
  volume = {21},
  number = {1},
  pages = {31},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-1926-6},
  urldate = {2023-11-08},
  abstract = {The recent boom in microfluidics and combinatorial indexing strategies, combined with low sequencing costs, has empowered single-cell sequencing technology. Thousands{\textemdash}or even millions{\textemdash}of cells analyzed in a single experiment amount to a data revolution in single-cell biology and pose unique data science problems. Here, we outline eleven challenges that will be central to bringing this emerging field of single-cell data science forward. For each challenge, we highlight motivating research questions, review prior work, and formulate open problems. This compendium is for established researchers, newcomers, and students alike, highlighting interesting and rewarding problems for the coming years.},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/DT2NV9J8/Lähnemann et al. - 2020 - Eleven grand challenges in single-cell data scienc.pdf}
}

@techreport{levineCell2SentenceTeachingLarge2023,
  type = {preprint},
  title = {Cell2Sentence: Teaching Large Language Models the Language of Biology},
  shorttitle = {Cell2Sentence},
  author = {Levine, Daniel and Rizvi, Syed Asad and L{\'e}vy, Sacha and Pallikkavaliyaveetil, Nazreen and Wu, Ruiming and Zheng, Zihe and Fonseca, Antonio Oliveira and Chen, Xingyu and Ghadermarzi, Sina and Dhodapkar, Rahul M. and Van Dijk, David},
  year = {2023},
  month = sep,
  institution = {Bioinformatics},
  doi = {10.1101/2023.09.11.557287},
  urldate = {2023-10-18},
  abstract = {Large language models like GPT have shown impressive performance on natural language tasks. Here, we present a novel method to directly adapt these pretrained models to a biological context, specifically single-cell transcriptomics, by representing gene expression data as text. Our Cell2Sentence approach converts each cell's gene expression profile into a sequence of gene names ordered by expression level. We show that these gene sequences, which we term "cell sentences", can be used to fine-tune causal language models like GPT-2. Critically, we find that natural language pretraining boosts model performance on cell sentence tasks. When fine-tuned on cell sentences, GPT-2 generates biologically valid cells when prompted with a cell type. Conversely, it can also accurately predict cell type labels when prompted with cell sentences. This demonstrates that language models fine-tuned using Cell2Sentence can gain a biological understanding of single-cell data, while retaining their ability to generate text. Our approach provides a simple, adaptable framework to combine natural language and transcriptomics using existing models and libraries. Our code is available at: https://github.com/vandijklab/cell2sentence-ft.},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/VR8Q6N4V/Levine et al. - 2023 - Cell2Sentence Teaching Large Language Models the .pdf}
}

@article{lopezDeepGenerativeModeling2018,
  title = {Deep generative modeling for single-cell transcriptomics},
  author = {Lopez, Romain and Regier, Jeffrey and Cole, Michael B. and Jordan, Michael I. and Yosef, Nir},
  year = {2018},
  month = dec,
  journal = {Nature Methods},
  volume = {15},
  number = {12},
  pages = {1053--1058},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-018-0229-2},
  urldate = {2023-11-08},
  abstract = {Single-cell transcriptome measurements can reveal unexplored biological diversity, but they suffer from technical noise and bias that must be modeled to account for the resulting uncertainty in downstream analyses. Here we introduce single-cell variational inference (scVI), a ready-to-use scalable framework for the probabilistic representation and analysis of gene expression in single cells ( https://github.com/YosefLab/scVI ). scVI uses stochastic optimization and deep neural networks to aggregate information across similar cells and genes and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity. We used scVI for a range of fundamental analysis tasks including batch correction, visualization, clustering, and differential expression, and achieved high accuracy for each task. scVI is a ready-to-use generative deep learning tool for large-scale single-cell RNA-seq data that enables raw data processing and a wide range of rapid and accurate downstream analyses.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {6.S898 project,Computational biology and bioinformatics,Computational models},
  file = {/Users/rcalef/Zotero/storage/NLNPACIB/Lopez et al. - 2018 - Deep generative modeling for single-cell transcrip.pdf}
}

@article{lotfollahiScGenPredictsSinglecell2019,
  title = {scGen predicts single-cell perturbation responses},
  author = {Lotfollahi, Mohammad and Wolf, F. Alexander and Theis, Fabian J.},
  year = {2019},
  month = aug,
  journal = {Nature Methods},
  volume = {16},
  number = {8},
  pages = {715--721},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0494-8},
  urldate = {2023-11-06},
  abstract = {Accurately modeling cellular response to perturbations is a central goal of computational biology. While such modeling has been based on statistical, mechanistic and machine learning models in specific settings, no generalization of predictions to phenomena absent from training data (out-of-sample) has yet been demonstrated. Here, we present scGen (https://github.com/theislab/scgen), a model combining variational autoencoders and latent space vector arithmetics for high-dimensional single-cell gene expression data. We show that scGen accurately models perturbation and infection response of cells across cell types, studies and species. In particular, we demonstrate that scGen learns cell-type and species-specific responses implying that it captures features that distinguish responding from non-responding genes and cells. With the upcoming availability of large-scale atlases of organs in a healthy state, we envision scGen to become a tool for experimental design through in silico screening of perturbation response in the context of disease and drug treatment.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {6.S898 project,Cell biology,Gene expression,Machine learning},
  file = {/Users/rcalef/Zotero/storage/X9ZLCBQ7/Lotfollahi et al. - 2019 - scGen predicts single-cell perturbation responses.pdf}
}

@article{lueckenBenchmarkingAtlaslevelData2022,
  title = {Benchmarking atlas-level data integration in single-cell genomics},
  author = {Luecken, Malte D. and B{\"u}ttner, M. and Chaichoompu, K. and Danese, A. and Interlandi, M. and Mueller, M. F. and Strobl, D. C. and Zappia, L. and Dugas, M. and {Colom{\'e}-Tatch{\'e}}, M. and Theis, Fabian J.},
  year = {2022},
  month = jan,
  journal = {Nature Methods},
  volume = {19},
  number = {1},
  pages = {41--50},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01336-8},
  urldate = {2023-11-09},
  abstract = {Single-cell atlases often include samples that span locations, laboratories and conditions, leading to complex, nested batch effects in data. Thus, joint analysis of atlas datasets requires reliable data integration. To guide integration method choice, we benchmarked 68 method and preprocessing combinations on 85 batches of gene expression, chromatin accessibility and simulation data from 23 publications, altogether representing {$>$}1.2 million cells distributed in 13 atlas-level integration tasks. We evaluated methods according to scalability, usability and their ability to remove batch effects while retaining biological variation using 14 evaluation metrics. We show that highly variable gene selection improves the performance of data integration methods, whereas scaling pushes methods to prioritize batch removal over conservation of biological variation. Overall, scANVI, Scanorama, scVI and scGen perform well, particularly on complex integration tasks, while single-cell ATAC-sequencing integration performance is strongly affected by choice of feature space. Our freely available Python module and benchmarking pipeline can identify optimal data integration methods for new data, benchmark new methods and improve method development.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {6.S898 project,Data integration,Machine learning,Software,Transcriptomics},
  file = {/Users/rcalef/Zotero/storage/ZQRLWN98/Luecken et al. - 2022 - Benchmarking atlas-level data integration in singl.pdf}
}

@article{mathysSinglecellAtlasReveals2023,
  title = {Single-cell atlas reveals correlates of high cognitive function, dementia, and resilience to Alzheimer's disease pathology},
  author = {Mathys, Hansruedi and Peng, Zhuyu and Boix, Carles A. and Victor, Matheus B. and Leary, Noelle and Babu, Sudhagar and Abdelhady, Ghada and Jiang, Xueqiao and Ng, Ayesha P. and Ghafari, Kimia and Kunisky, Alexander K. and Mantero, Julio and Galani, Kyriaki and Lohia, Vanshika N. and Fortier, Gabrielle E. and Lotfi, Yasmine and Ivey, Jason and Brown, Hannah P. and Patel, Pratham R. and Chakraborty, Nehal and Beaudway, Jacob I. and Imhoff, Elizabeth J. and Keeler, Cameron F. and McChesney, Maren M. and Patel, Haishal H. and Patel, Sahil P. and Thai, Megan T. and Bennett, David A. and Kellis, Manolis and Tsai, Li-Huei},
  year = {2023},
  month = sep,
  journal = {Cell},
  volume = {186},
  number = {20},
  pages = {4365-4385.e27},
  publisher = {Elsevier},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2023.08.039},
  urldate = {2023-11-09},
  langid = {english},
  pmid = {37774677},
  keywords = {6.S898 project,Alzheimer's disease,cognitive impairment,cognitive resilience,cohesin complex,DNA damage response,inhibitory neurons,neurodegeneration,single-cell transcriptomic atlas},
  file = {/Users/rcalef/Zotero/storage/IFBR7EZY/Mathys et al. - 2023 - Single-cell atlas reveals correlates of high cogni.pdf}
}

@misc{mcinnesUMAPUniformManifold2020,
  title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  shorttitle = {UMAP},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  number = {arXiv:1802.03426},
  eprint = {1802.03426},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.03426},
  urldate = {2023-12-12},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arxiv},
  keywords = {6.S898 project,Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/rcalef/Zotero/storage/8XQ59IUS/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf}
}

@article{panMicrofluidicsFacilitatesDevelopment2022,
  title = {Microfluidics Facilitates the Development of Single-Cell RNA Sequencing},
  author = {Pan, Yating and Cao, Wenjian and Mu, Ying and Zhu, Qiangyuan},
  year = {2022},
  month = jul,
  journal = {Biosensors},
  volume = {12},
  number = {7},
  pages = {450},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-6374},
  doi = {10.3390/bios12070450},
  urldate = {2023-12-05},
  abstract = {Single-cell RNA sequencing (scRNA-seq) technology provides a powerful tool for understanding complex biosystems at the single-cell and single-molecule level. The past decade has been a golden period for the development of single-cell sequencing, with scRNA-seq undergoing a tremendous leap in sensitivity and throughput. The application of droplet- and microwell-based microfluidics in scRNA-seq has contributed greatly to improving sequencing throughput. This review introduces the history of development and important technical factors of scRNA-seq. We mainly focus on the role of microfluidics in facilitating the development of scRNA-seq technology. To end, we discuss the future directions for scRNA-seq.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {6.S898 project,droplet,microfluidics,microwell,scRNA-seq},
  file = {/Users/rcalef/Zotero/storage/3WW3SVGB/Pan et al. - 2022 - Microfluidics Facilitates the Development of Singl.pdf}
}

@article{stuartComprehensiveIntegrationSingleCell2019,
  title = {Comprehensive Integration of Single-Cell Data},
  author = {Stuart, Tim and Butler, Andrew and Hoffman, Paul and Hafemeister, Christoph and Papalexi, Efthymia and Mauck, William M. and Hao, Yuhan and Stoeckius, Marlon and Smibert, Peter and Satija, Rahul},
  year = {2019},
  month = jun,
  journal = {Cell},
  volume = {177},
  number = {7},
  pages = {1888-1902.e21},
  publisher = {Elsevier},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2019.05.031},
  urldate = {2023-12-12},
  langid = {english},
  pmid = {31178118},
  keywords = {6.S898 project,integration,multi-modal,scATAC-seq,scRNA-seq,single cell,single-cell ATAC sequencing,single-cell RNA sequencing},
  file = {/Users/rcalef/Zotero/storage/3FZZYNS5/Stuart et al. - 2019 - Comprehensive Integration of Single-Cell Data.pdf}
}

@article{theodorisTransferLearningEnables2023,
  title = {Transfer learning enables predictions in network biology},
  author = {Theodoris, Christina V. and Xiao, Ling and Chopra, Anant and Chaffin, Mark D. and Al Sayed, Zeina R. and Hill, Matthew C. and Mantineo, Helene and Brydon, Elizabeth M. and Zeng, Zexian and Liu, X. Shirley and Ellinor, Patrick T.},
  year = {2023},
  month = jun,
  journal = {Nature},
  volume = {618},
  number = {7965},
  pages = {616--624},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06139-9},
  urldate = {2023-10-18},
  abstract = {Mapping gene networks requires large amounts of transcriptomic data to learn the connections between genes, which impedes discoveries in settings with limited data, including rare diseases and diseases affecting clinically inaccessible tissues. Recently, transfer learning has revolutionized fields such as natural language understanding1,2 and computer vision3 by leveraging deep learning models pretrained on large-scale general datasets that can then be fine-tuned towards a vast array of downstream tasks with limited task-specific data. Here, we developed a context-aware, attention-based deep learning model, Geneformer, pretrained on a large-scale corpus of about 30\,million single-cell transcriptomes to enable context-specific predictions in settings with limited data in network biology. During pretraining, Geneformer gained a fundamental understanding of network dynamics, encoding network hierarchy in the attention weights of the model in a completely self-supervised manner. Fine-tuning towards a diverse panel of downstream tasks relevant to chromatin and network dynamics using limited task-specific data demonstrated that Geneformer consistently boosted predictive accuracy. Applied to disease modelling with limited patient data, Geneformer identified candidate therapeutic targets for cardiomyopathy. Overall, Geneformer represents a pretrained deep learning model from which fine-tuning towards a broad range of downstream applications can be pursued to accelerate discovery of key network regulators and candidate therapeutic targets.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {6.S898 project,Cardiomyopathies,Computational models,Gene regulatory networks,Machine learning,Regulatory networks},
  file = {/Users/rcalef/Zotero/storage/D399P6II/Theodoris et al. - 2023 - Transfer learning enables predictions in network b.pdf}
}

@misc{virshupAnndataAnnotatedData2021,
  title = {anndata: Annotated data},
  shorttitle = {anndata},
  author = {Virshup, Isaac and Rybakov, Sergei and Theis, Fabian J. and Angerer, Philipp and Wolf, F. Alexander},
  year = {2021},
  month = dec,
  primaryclass = {New Results},
  pages = {2021.12.16.473007},
  publisher = {bioRxiv},
  doi = {10.1101/2021.12.16.473007},
  urldate = {2023-12-12},
  abstract = {anndata is a Python package for handling annotated data matrices in memory and on disk (github.com/theislab/anndata), positioned between pandas and xarray. anndata offers a broad range of computationally efficient features including, among others, sparse data support, lazy operations, and a PyTorch interface. Statement of need Generating insight from high-dimensional data matrices typically works through training models that annotate observations and variables via low-dimensional representations. In exploratory data analysis, this involves iterative training and analysis using original and learned annotations and task-associated representations. anndata offers a canonical data structure for book-keeping these, which is neither addressed by pandas (McKinney, 2010), nor xarray (Hoyer \& Hamman, 2017), nor commonly-used modeling packages like scikit-learn (Pedregosa et al., 2011).},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/U93QFEEQ/Virshup et al. - 2021 - anndata Annotated data.pdf}
}

@article{virshupScverseProjectProvides2023,
  title = {The scverse project provides a computational ecosystem for single-cell omics data analysis},
  author = {Virshup, Isaac and Bredikhin, Danila and Heumos, Lukas and Palla, Giovanni and Sturm, Gregor and Gayoso, Adam and Kats, Ilia and Koutrouli, Mikaela and Berger, Bonnie and Pe'er, Dana and Regev, Aviv and Teichmann, Sarah A. and Finotello, Francesca and Wolf, F. Alexander and Yosef, Nir and Stegle, Oliver and Theis, Fabian J.},
  year = {2023},
  month = may,
  journal = {Nature Biotechnology},
  volume = {41},
  number = {5},
  pages = {604--606},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-023-01733-8},
  urldate = {2023-12-12},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {6.S898 project,Bioinformatics,Computational platforms and environments,Machine learning,Software},
  file = {/Users/rcalef/Zotero/storage/XWUF5XLM/Virshup et al. - 2023 - The scverse project provides a computational ecosy.pdf}
}

@misc{yangGeneCompassDecipheringUniversal2023,
  title = {GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model},
  shorttitle = {GeneCompass},
  author = {Yang, Xiaodong and Liu, Guole and Feng, Guihai and Bu, Dechao and Wang, Pengfei and Jiang, Jie and Chen, Shubai and Yang, Qinmeng and Zhang, Yiyang and Man, Zhenpeng and Liang, Zhongming and Wang, Zichen and Li, Yaning and Li, Zheng and Liu, Yana and Tian, Yao and Li, Ao and Dong, Jingxi and Hu, Zhilong and Fang, Chen and Miao, Hefan and Cui, Lina and Deng, Zixu and Jiang, Haiping and Cui, Wentao and Zhang, Jiahao and Yang, Zhaohui and Li, Handong and He, Xingjian and Zhong, Liqun and Zhou, Jiaheng and Wang, Zijian and Long, Qingqing and Xu, Ping and Consortium, The X.-Compass and Wang, Hongmei and Meng, Zhen and Wang, Xuezhi and Wang, Yangang and Wang, Yong and Zhang, Shihua and Guo, Jingtao and Zhao, Yi and Zhou, Yuanchun and Li, Fei and Liu, Jing and Chen, Yiqiang and Yang, Ge and Li, Xin},
  year = {2023},
  month = sep,
  primaryclass = {New Results},
  pages = {2023.09.26.559542},
  publisher = {bioRxiv},
  doi = {10.1101/2023.09.26.559542},
  urldate = {2023-11-06},
  abstract = {Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, the traditional research paradigm primarily focuses on individual model organisms, resulting in limited collection and integration of complex features on various cell types across species. Recent breakthroughs in single-cell sequencing and advancements in deep learning techniques present an unprecedented opportunity to tackle this challenge. In this study, we developed GeneCompass, the first knowledge-informed, cross-species foundation model pre-trained on an extensive dataset of over 120 million single-cell transcriptomes from human and mouse. During pre-training, GeneCompass effectively integrates four types of biological prior knowledge to enhance the understanding of gene regulatory mechanisms in a self-supervised manner. Fine-tuning towards multiple downstream tasks, GeneCompass outperforms competing state-of-the-art models in multiple tasks on single species and unlocks new realms of cross-species biological investigation. Overall, GeneCompass marks a milestone in advancing knowledge of universal gene regulatory mechanisms and accelerating the discovery of key cell fate regulators and candidate targets for drug development.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  keywords = {6.S898 project},
  file = {/Users/rcalef/Zotero/storage/3WUJ2ZV7/Yang et al. - 2023 - GeneCompass Deciphering Universal Gene Regulatory.pdf}
}

@article{yangScBERTLargescalePretrained2022,
  title = {scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data},
  author = {Yang, Fan and Wang, Wenchuan and Wang, Fang and Fang, Yuan and Tang, Duyu and Huang, Junzhou and Lu, Hui and Yao, Jianhua},
  year = {2022},
  month = oct,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {10},
  pages = {852--866},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00534-z},
  urldate = {2023-11-06},
  abstract = {Annotating cell types on the basis of single-cell RNA-seq data is a prerequisite for research on disease progress and tumour microenvironments. Here we show that existing annotation methods typically suffer from a lack of curated marker gene lists, improper handling of batch effects and difficulty in leveraging the latent gene{\textendash}gene interaction information, impairing their generalization and robustness. We developed a pretrained deep neural network-based model, single-cell bidirectional encoder representations from transformers (scBERT), to overcome the challenges. Following BERT's approach to pretraining and fine-tuning, scBERT attains a general understanding of gene{\textendash}gene interactions by being pretrained on huge amounts of unlabelled scRNA-seq data; it is then transferred to the cell type annotation task of unseen and user-specific scRNA-seq data for supervised fine-tuning. Extensive and rigorous benchmark studies validated the superior performance of scBERT on cell type annotation, novel cell type discovery, robustness to batch effects and model interpretability.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {6.S898 project,Bioinformatics,Classification and taxonomy,Gene expression},
  file = {/Users/rcalef/Zotero/storage/I33Y63J6/Yang et al. - 2022 - scBERT as a large-scale pretrained deep language m.pdf}
}

@article{zhangScPretrainMultitaskSelfsupervised2022,
  title = {scPretrain: multi-task self-supervised learning for cell-type classification},
  shorttitle = {scPretrain},
  author = {Zhang, Ruiyi and Luo, Yunan and Ma, Jianzhu and Zhang, Ming and Wang, Sheng},
  editor = {Mathelier, Anthony},
  year = {2022},
  month = mar,
  journal = {Bioinformatics},
  volume = {38},
  number = {6},
  pages = {1607--1614},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/btac007},
  urldate = {2023-08-28},
  abstract = {Motivation: Rapidly generated scRNA-seq datasets enable us to understand cellular differences and the function of each individual cell at single-cell resolution. Cell-type classification, which aims at characterizing and labeling groups of cells according to their gene expression, is one of the most important steps for single-cell analysis. To facilitate the manual curation process, supervised learning methods have been used to automatically classify cells. Most of the existing supervised learning approaches only utilize annotated cells in the training step while ignoring the more abundant unannotated cells. In this article, we proposed scPretrain, a multi-task self-supervised learning approach that jointly considers annotated and unannotated cells for cell-type classification. scPretrain consists of a pre-training step and a fine-tuning step. In the pre-training step, scPretrain uses a multi-task learning framework to train a feature extraction encoder based on each dataset's pseudo-labels, where only unannotated cells are used. In the fine-tuning step, scPretrain fine-tunes this feature extraction encoder using the limited annotated cells in a new dataset.},
  langid = {english},
  keywords = {6.S898 project,pretraining,representation learning,scRNA,self-supervised learning},
  file = {/Users/rcalef/Zotero/storage/G2I2ZNCQ/Zhang et al. - 2022 - scPretrain multi-task self-supervised learning fo.pdf}
}

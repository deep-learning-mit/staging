@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@misc{cunningham2023sparse,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{marks2023geometry,
      title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets}, 
      author={Samuel Marks and Max Tegmark},
      year={2023},
      eprint={2310.06824},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{Bansal2021stitching,
Author = {Yamini Bansal and Preetum Nakkiran and Boaz Barak},
Title = {Revisiting Model Stitching to Compare Neural Representations},
Year = {2021},
Eprint = {arXiv:2106.07682},
}

@ARTICLE{Elhage2022-wh,
  title         = "Toy models of superposition",
  author        = "Elhage, Nelson and Hume, Tristan and Olsson, Catherine and
                   Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and
                   Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and
                   Chen, Carol and Grosse, Roger and McCandlish, Sam and
                   Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and
                   Olah, Christopher",
  abstract      = "Neural networks often pack many unrelated concepts into a
                   single neuron - a puzzling phenomenon known as
                   'polysemanticity' which makes interpretability much more
                   challenging. This paper provides a toy model where
                   polysemanticity can be fully understood, arising as a result
                   of models storing additional sparse features in
                   ``superposition.'' We demonstrate the existence of a phase
                   change, a surprising connection to the geometry of uniform
                   polytopes, and evidence of a link to adversarial examples.
                   We also discuss potential implications for mechanistic
                   interpretability.",
  month         =  sep,
  year          =  2022,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2209.10652"
}


@ARTICLE{Cunningham2023-co,
  title         = "Sparse autoencoders find highly interpretable features in
                   language models",
  author        = "Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and
                   Huben, Robert and Sharkey, Lee",
  abstract      = "One of the roadblocks to a better understanding of neural
                   networks' internals is
                   \textbackslashtextit\{polysemanticity\}, where neurons
                   appear to activate in multiple, semantically distinct
                   contexts. Polysemanticity prevents us from identifying
                   concise, human-understandable explanations for what neural
                   networks are doing internally. One hypothesised cause of
                   polysemanticity is \textbackslashtextit\{superposition\},
                   where neural networks represent more features than they have
                   neurons by assigning features to an overcomplete set of
                   directions in activation space, rather than to individual
                   neurons. Here, we attempt to identify those directions,
                   using sparse autoencoders to reconstruct the internal
                   activations of a language model. These autoencoders learn
                   sets of sparsely activating features that are more
                   interpretable and monosemantic than directions identified by
                   alternative approaches, where interpretability is measured
                   by automated methods. Moreover, we show that with our
                   learned set of features, we can pinpoint the features that
                   are causally responsible for counterfactual behaviour on the
                   indirect object identification task
                   \textbackslashcitep\{wang2022interpretability\} to a finer
                   degree than previous decompositions. This work indicates
                   that it is possible to resolve superposition in language
                   models using a scalable, unsupervised method. Our method may
                   serve as a foundation for future mechanistic
                   interpretability work, which we hope will enable greater
                   model transparency and steerability.",
  month         =  sep,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2309.08600"
}


@ARTICLE{Cunningham2023-wy,
  title         = "Sparse autoencoders find highly interpretable features in
                   language models",
  author        = "Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and
                   Huben, Robert and Sharkey, Lee",
  abstract      = "One of the roadblocks to a better understanding of neural
                   networks' internals is
                   \textbackslashtextit\{polysemanticity\}, where neurons
                   appear to activate in multiple, semantically distinct
                   contexts. Polysemanticity prevents us from identifying
                   concise, human-understandable explanations for what neural
                   networks are doing internally. One hypothesised cause of
                   polysemanticity is \textbackslashtextit\{superposition\},
                   where neural networks represent more features than they have
                   neurons by assigning features to an overcomplete set of
                   directions in activation space, rather than to individual
                   neurons. Here, we attempt to identify those directions,
                   using sparse autoencoders to reconstruct the internal
                   activations of a language model. These autoencoders learn
                   sets of sparsely activating features that are more
                   interpretable and monosemantic than directions identified by
                   alternative approaches, where interpretability is measured
                   by automated methods. Moreover, we show that with our
                   learned set of features, we can pinpoint the features that
                   are causally responsible for counterfactual behaviour on the
                   indirect object identification task
                   \textbackslashcitep\{wang2022interpretability\} to a finer
                   degree than previous decompositions. This work indicates
                   that it is possible to resolve superposition in language
                   models using a scalable, unsupervised method. Our method may
                   serve as a foundation for future mechanistic
                   interpretability work, which we hope will enable greater
                   model transparency and steerability.",
  month         =  sep,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2309.08600"
}

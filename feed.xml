<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://deep-learning-mit.github.io/staging/feed.xml" rel="self" type="application/atom+xml"/><link href="https://deep-learning-mit.github.io/staging/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-02T23:05:43+00:00</updated><id>https://deep-learning-mit.github.io/staging/feed.xml</id><title type="html">6.S898 Deep Learning Blogs 2023</title><subtitle>Staging website for the 2023 ICLR Blogposts track </subtitle><entry><title type="html">Investigating Vision Transformer-Based Models for Closure Modeling of Fluid Dynamical Systems</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/Investigating-neural-operator-models-for-closure-modeling-of-dynamical-systems/" rel="alternate" type="text/html" title="Investigating Vision Transformer-Based Models for Closure Modeling of Fluid Dynamical Systems"/><published>2023-12-19T00:00:00+00:00</published><updated>2023-12-19T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/Investigating-neural-operator-models-for-closure-modeling-of-dynamical-systems</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/Investigating-neural-operator-models-for-closure-modeling-of-dynamical-systems/"><![CDATA[<h1 id="motivation-and-background">Motivation and Background</h1> <p>Over the past decade, deep learning models have increasingly been used for modeling time series data for fluid dynamical systems. One of the most recent applications is in forecasting weather <d-cite key="schultz2021can"></d-cite> with deep learning models being developed by tech giants including NVIDIA <d-cite key="pathak2022fourcastnet"></d-cite> and Google <d-cite key="lam2022graphcast"></d-cite> with reasonable prediction accuracy compared to conventional numerical weather prediction. While these models completely replace traditional numerical weather models with deep neural networks (i.e., “surrogate modeling”), in general, deep neural models can also be used to augment existing numerical solvers and methods <d-cite key="lino2023current"></d-cite>.</p> <p>Training deep neural models to completely replace numerical solvers requires a lot of data, which might not be available due to constraints with sensor and satellite usage associated with collecting ocean and weather data. Additionally, these surrogate models are completely data-driven and could lead to non-physical predictions (lack of volume preservation, and non-conservation of physical laws) if these needs are not explicitly attended to during training <d-cite key="lino2023current"></d-cite>. A huge advantage of these models is their very low computational cost during inference compared to using numerical solvers <d-cite key="pathak2022fourcastnet"></d-cite>. Another approach is to use closure models that augment low fidelity (low resolution) numerical simulations with a neural network (i.e., a closure term) to predict high fidelity (high resolution) forecasts <d-cite key="gupta2021neural"></d-cite>. This approach could lead to some conservation of physical laws since it builds upon conventional numerical solvers that obey physical equations like PDEs, with a lower computational cost compared to directly running high-fidelity numerical simulations.</p> <p>In closure modeling, we are interested in solving the following problem: Here, we describe the case of closure modeling with loss of accuracy due to low numerical resolution which leads to loss of sub-grid scale processes, and sometimes even truncation and discretization errors. But there could also be closure due to missing or unknown physics, incorrect parameters, etc. <d-cite key="gupta2021neural"></d-cite>.</p> <p>Consider a low-fidelity model (low resolution):</p> <div id="eq: 1"> $$ \frac{du_{LF}(t)}{dt} = f(u_{LF}(t)) \tag{1} $$ </div> <p>and the high fidelity (high resolution) equivalent model:</p> <div id="eq: 2"> $$ \frac{du_{HF}(t)}{dt} = f(u_{HF}(t)) \tag{2} $$ </div> <p>The purpose of the closure in this context is to augment the low fidelity model with a neural closure model $NN(u_{LF}(t))$, such that:</p> <div id="eq: 3"> $$ u_{HF}(t+1) = \int_{t}^{t+1} \bigg(\frac{d u_{LF}}{dt} + NN(u_{LF}(t)) \bigg) dt \tag{3} $$ </div> <p>Previous works for neural closure models have used neural ODEs and neural DDEs <d-cite key="gupta2023generalized"></d-cite>. However, these methods require coding an adjoint equation, which could be difficult for complex 2D problems. Another approach has been to use local CNNs for turbulent models <d-cite key="srinivasan2023turbulence"></d-cite>. More recently, vision transformers have been shown to be very successful for image recognition <d-cite key="dosovitskiy2020image"></d-cite>, and have also been adapted to predict the mapping between two infinite-dimensional function spaces using Neural Operators <d-cite key="guibas2021adaptive"></d-cite> (more details in the following sections). These adaptive neural operators have also been used for predicting fluid flows like flow past a cylinder using experimental data for training <d-cite key="renn2023forecasting"></d-cite>. However, to the best of our knowledge, the usage of vision transformers for closure modeling of 2D fluid flow fields has not been explored.</p> <p>Another issue with using neural networks for solving fluid dynamics PDEs is unstable recursive predictions, due to the exponential growth of accumulated errors across time. Some papers have explored ways to limit this exponential error growth by using custom loss functions <d-cite key="kim2019deep"></d-cite>, and adding additional spectral networks to limit error growth <d-cite key="lippe2023pde"></d-cite>. In this project, we attempt to achieve stable long roll-outs by first training models for one-time-step predictions and then fine-tuning for long roll-out, inspired by <d-cite key="pathak2022fourcastnet"></d-cite>.</p> <h1 id="methods-and-experiments">Methods and Experiments</h1> <h2 id="test-case-setup">Test case setup</h2> <p>In this project, we develop and investigate methods to augment low-fidelity (low-resolution) numerical simulations of flow past a cylinder with deep neural networks and compare them with high-fidelity (high-resolution) numerical simulations. The neural closure aims to learn the missing subgrid-scale processes and truncation and discretization errors in the low-fidelity simulation and augment it to match the high-fidelity simulation <d-cite key="gupta2021neural"></d-cite> <d-cite key="zanna2020data"></d-cite></p> <h3 id="data-generation">Data generation</h3> <p>We use data generated from numerical simulations for two-dimensional flow past a cylinder <d-cite key="cohen2004fluid"></d-cite>. We employ the MIT-MSEAS 2.29 Finite Volume framework <d-cite key="ueckermann20122"></d-cite> to solve the Navier Stokes equations at different numerical resolutions and fidelities. Figure <a href="#fig:fpc_setup"><span>Figure 1</span></a> shows the geometry, inlet conditions, and boundary conditions used for the numerical simulations. The test set-up consists of a rectangular geometry of length 15m and height 5m. A cylinder is placed 2m upstream with a diameter of 1m. The inlet velocity is set to 2m/s horizontal velocity and no vertical velocity. The boundary conditions are set to outflow conditions on the right boundary and free slip on the top and bottom boundaries. We use Reynolds’s number of $Re=200$. The Reynolds’s number is a key parameter that affects the regime of solutions observed, at $Re\geq 200$, complex periodic patterns can be observed known as wakes, eddy shedding, or Karma vortex streets <d-cite key="cohen2004fluid"></d-cite>. It is in this complex regime that we are interested in applying our deep learning methods.</p> <div id="fig:fpc_setup"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup.png" class="fig:fpc_setup" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 1: Schematic of the flow past a cylinder setup used to generate data </div> <p>To generate the high-fidelity (high-resolution) data we use a resolution of 200x600. For the low-fidelity (low-resolution) data, we use a resolution of 50x150. The MIT-MSEAS 2.29 numerical simulation uses second-order finite volumes and a time-step of 0.000625s. The diffusion operator was discretized using central boundary fluxes. For advection, we use a total variation diminishing (TVD) scheme with a monotonized central (MC) symmetric flux limiter. For numerical time integration, we use a rotational incremental pressure correction scheme. The reader is directed to <d-cite key="ueckermann20122"></d-cite> for more details on the numerical schemes. We simulated the flow for a total of 30s, and the data is saved once every 0.03125s, for a total of 9600 snapshots. We observed fully developed eddies after 15s. An eddy sheds approximately every 2s, so we find a shedding period of about 4s.</p> <h3 id="data-usage-for-deep-learning">Data usage for deep learning</h3> <p>We save the u and v velocity fields, as well as the time derivatives (which act as the numerical approximation of the RHS terms (dynamics terms) of the PDEs being solved).</p> <p>Thus we save a tensor of size $9600 \times 4 \times \text{num. horizontal cells} \times \text{num. vertical cells} $, for each numerical resolution, where 9600 is the total number of time steps saved, and 4 indicates 4 channels for the 2 velocities and 2 time derivatives (horizontal: u and vertical: v). To maintain the same grid size for deep learning, we down-sample the high-fidelity data using linear interpolation to have the same size as the low-fidelity data.</p> <p>For deep learning, we neglect the first 4800 snapshots as spin-ups to reach a fully developed flow state. Then we use the next 3600 snapshots for training, 600 snapshots for validation, and 600 snapshots for testing.</p> <p>We created a custom data class with a data loader to feed randomized sequences of required sequence lengths from the training set for training. For inference, we feed the snapshots one by one from the test data set.</p> <h2 id="deep-learning-model">Deep learning model</h2> <p>For this project, we initially tried to explore architectures inspired by Fourier neural operators <d-cite key="li2020fourier"></d-cite>. These operators learn the relationship between two infinite dimensional spaces, by using kernels in the Fourier space instead of physical space. Since multiplications in Fourier space are akin to convolutions in physical space, these models can learn powerful infinite dimensional mapping at reduced computational cost. In particular, we were interested in Adaptive Fourier Neural Operators <d-cite key="guibas2021adaptive"></d-cite>, which build upon vision transformers. Like vision transformers, these models first split the input image into patches and tokens with positional embedding. However, instead of relying on QKV (Query, Key, Value) attention mechanisms, they perform a 2D spatial Fourier transform of the tokenized representation and use MLPs (which act as convolution, since multiplication in Fourier space is convolution in physical space) to learn the mapping between input and output images. Since these models don’t involve self-attention, they have significantly fewer parameters compared to vision transformers and can even outperform them in certain tasks. Figure <a href="#fig:afno"><span>Figure 2</span></a> shows a schematic comparing a regular vision transformer with AFNO. However, we were not able to further explore this architecture due to time constraints.</p> <div id="fig:afno"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer.png" class="fig:afno" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2: Schematic of the difference between a regular QKV attention based vision transformer and an AFNO layer </div> <p>Instead, we focus on utilizing vision transformer-based architectures for our closure modeling task. Although they have a higher number of parameters than AFNO, vision transformers rely on self-attention that can learn long-range physical dependencies, and are hence well suited to our task.</p> <h3 id="single-time-step-predictions">Single time-step predictions</h3> <p>We first focus on a framework that just predicts output for the next time step, i.e., single time-step predictions. This is a simpler task than recursive or long roll-out predictions which we will discuss in the following sections.</p> <p><a href="#fig:vit_model_single"><span>Figure 3</span></a> contains a flowchart of how our Vision Transformer-based closure model “ViTClosure()” works. In the first part of the architecture, a vision transformer-based architecture takes in a low fidelity velocity field at time t as input and returns the closure term (which is similar to a time derivative field) at time t+1.</p> <ol> <li>First, a 2D CNN transforms the input low fidelity 2D velocity field at time $t$ into patches</li> <li>Next, a layer adds positional embedding and creates tokens. In this project, we use learned absolute positional embeddings.</li> <li>Next, the tokens are passed through a vision transformer with many attention layers with multi-headed attention heads.</li> <li>Next, we have a layer that applies layer norm to the final output of the vision transformer. Here the output would be of size $\text{Batch size} \times \text{Num. of tokens} \times \text{Hidden dimension}$. Since we need an output field of the same dimension as the input field, we use an MLP for this transformation. We call this output closure term $NN(u_{LF}(t))$.</li> <li>Next comes a numerical integration step which combines the low fidelity numerical solver (which provides $\frac{d u_{LF}}{dt}$) with the neural closure term $NN(u_{LF}(t))$, to predict the high-fidelity field $u_{HF}(t+1)$, shown in <a href="#eq: num_int"><span>Eq. (4)</span></a>.</li> </ol> <div id="eq: num_int"> $$ u_{HF}(t+1) = \int_{t}^{t+1} \bigg(\frac{d u_{LF}}{dt} + NN(u_{LF}(t)) \bigg) dt \tag{4} $$ </div> <div id="fig:vit_model_single"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single.png" class="fig:vit_model_single" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 3: Flow chart of the model used for closure modeling with single time step predictions. Please see text description for more details </div> <h3 id="loss-functions">Loss functions</h3> <p>Finally, we compare the predicted high-fidelity field $u_{HF}(t+1)$ and neural closure term $NN(u_{LF}(t))$ with the ground truth simulations. Similar loss functions have been used in neural closure models based on neural-ODEs <d-cite key="gupta2021neural"></d-cite>. In this work, we use a weighting factor that is a hyperparameter that can be tuned to weigh either the state errors or closure term errors depending on the application. The loss function is defined in <a href="#eq: loss_func"><span>Eq. (5)</span></a>.</p> <div id="eq: loss_func"> $$ \mathcal{L}_{\text{weighted}}(.;\lambda) = \text{MSE}\bigg(u_{HF}(t+1),u_{HF_{true}}(t+1)\bigg) + \lambda \text{MSE}\bigg(NN(u_{LF}(t)),\text{True closure}(u_{LF}(t))\bigg) \tag{5} $$ </div> <p>Then, we use backpropagation with the Adam optimizer and a cosine learning rate to optimize the model parameters.</p> <h3 id="fine-tuning-for-recursive-predictions">Fine tuning for recursive predictions</h3> <p>Now that we have a model trained for single time-step predictions, we move on to multiple time-step predictions or long roll-out predictions. Achieving long roll-out predictions is hard due to the exponential accumulation of errors during recursive predictions. Hence a small error at timestep t+1 could lead to huge errors after a few more timesteps. <d-cite key="lippe2023pde"></d-cite> have recently worked on achieving stable long roll-outs using spectral data augmentations. In this project, we try a different approach by using fine-tuning, and using a loss function inspired by <d-cite key="kim2019deep"></d-cite>. The flowchart of the fine-tuning process is shown in <a href="#fig:vit_model_single"><span>Figure 3</span></a>. The description is as follows:</p> <ol> <li>We first initialize the model using the best-performing model of the single-time step prediction, i.e., the best ViT-closure() model.</li> <li>Next, we recursively use the ViT-closure() model, and feed the output at time t+i as input to the model at time t+i+1.</li> <li>Finally, we average all the losses across time <a href="#eq: loss_func_rec"><span>Eq. (6)</span></a>. and backpropagate through time.</li> </ol> <div id="eq: loss_func_rec"> $$ \mathcal{L}_{\text{recursive}}=\frac{1}{n}\sum_{t=1}^n \mathcal{L}_{\text{weighted}}(.;t,\lambda) \tag{6} $$ </div> <p>where ‘n’ is a hyperparameter that determines how many times the model is propagated recursively during fine-tuning.</p> <div id="fig:vit_fine_tune"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune.png" class="fig:vit_fine_tune" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 4: Flow chart of the fine-tuning step used to train the closure model to predict recursively for long roll-outs. Please see the text description for more details </div> <h1 id="results-and-analysis">Results and Analysis</h1> <p>We ran many experiments for the flow past a cylinder set-up with high-fidelity (high-resolution) data of resolution 200x600 and low-fidelity (low-resolution) data of resolution 50x150. The high-fidelity run was downsampled as described previously to achieve the same grid size as the low-fidelity data for comparison. We used one NVIDIA RTX A6000 GPU for all the training and inference runs.</p> <h2 id="single-time-step-predictions-1">Single time-step predictions</h2> <p>First, we train the single time-step prediction model using the procedure described previously. We tried many runs (~ 20) by varying the key hyperparameters such as the global batch size between 1 and 16, the embedding dimension between 64 and 256, the number of attention layers between 3 and 9, the number of attention heads between 5 and 10, the patch size between 5 and 10, and the weightage in the loss function $\lambda$ between 0 and 1.</p> <p>Increasing the batch size led to faster training, but higher GPU memory requirements. With a batch size of 1, it took around 8 minutes of wall-clock time for one epoch of training. We observed the best training results with a loss function weightage $\lambda$ of close to 0, this may be because we ran the numerical solver offline to obtain the low-fidelity derivatives and ground truth closure, which is not as accurate as obtaining these values online during training. The other hyperparameters decreased training errors but the validation error after 50 epochs of training was around 0.03 m/s compared to the average velocity field of 2 m/s (so about 1.5\% relative error). Regularization and avoiding overfitting of the model needs to be investigated further in future work.</p> <p>For the GIFs below, we use the best-trained model, which was trained using a batch size of 16, embedding dimension of 128, number of attention layers of 6, number of attention heads of 10, patch size of 5, and weightage in the loss function $\lambda$ of 0.05.</p> <p>Using this model, we can visualize the attention layers, to identify which features have been most useful for closure modeling. <a href="#fig:attention_patch"><span>Figure 5</span></a> shows the attention map of multiple patches on the low-fidelity u velocity input at the same time step. We can observe that the most important feature seems to be the phase (whether the eddy is facing upward or downward) of the eddy shed right at the cylinder, and the other eddies downstream. We can also see that there is very little attention near the inlet and top and bottom boundaries since those values are set as inputs to the simulation.</p> <div id="fig:attention_patch"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif" class="fig:attention_patch" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 5: GIF shows the attention map of multiple patches on the low-fidelity u velocity input at the same time step </div> <p><a href="#fig:attention_patch"><span>Figure 6</span></a> shows the attention map of a single patch on the low-fidelity u velocity input at different time steps. We can again observe that the attention map follows the eddy shedding at all times. These two plots indicate that the model can identify that the eddies are the most important features, and the inlet and boundaries are not that critical for predicting the flow field. However, this may not be true if we are attempting to do closure modeling between simulations with different inlet and boundary conditions, which can be further investigated in future work.</p> <div id="fig:attention_time"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif" class="fig:attention_time" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 6: GIF shows the attention map of a single patch on the low-fidelity u velocity input at different time steps </div> <p>Now that we know that our model focuses on the most important features for closure modeling, we can compare the field plots of the high-fidelity predictions using the model.</p> <p><a href="#fig:u_pred"><span>Figure 7</span></a> and <a href="#fig:v_pred"><span>Figure 8</span></a> show the comparison of the neural closure model predictions, and predictions with just low-resolution simulations. We can observe that the neural closure model prediction (3rd row) performs way better than the low-fidelity simulations (2nd row). The low-fidelity simulation is out of phase compared to the high-resolution ground truth, but our neural closure model is able to augment the low-fidelity simulation and learn the true phase and missing sub-grid scale processes!</p> <div id="fig:u_pred"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif" class="fig:u_pred" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 7: GIF shows comparison of low fidelity prediction and augmented neural closure single step predictions for u velocity for testing times </div> <div id="fig:v_pred"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif" class="fig:v_pred" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 8: GIF shows comparison of low fidelity prediction and augmented neural closure single step predictions for v velocity for testing times </div> <h2 id="fine-tuning-for-recursive-predictions-1">Fine tuning for recursive predictions</h2> <p>Next, we evaluate the capabilities of the best model from the single time-step predictions for recursive predictions. We set the initial conditions to be the first snapshot in the testing data set and perform recursive predictions as described before for 50 time steps. The resulting predictions and error fields are shown in the GIF in <a href="#fig:rec_pred_single"><span>Figure 9</span></a>. We can see that the errors grow exponentially even before a few recursive predictions, and the model is incapable of any accurate long-term predictions. The predictions quickly become out of phase compared to the ground truth.</p> <div id="fig:rec_pred_single"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif" class="fig:rec_pred_single" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 9: GIF shows the result of recursive predictions by the best single time-step prediction model </div> <p>Next, we initialize a new model using the weights from the best single time-step predictions and perform fine-tuning as described previously.</p> <p>We explore multiple values of $n$, which is a hyperparameter that controls how long the recursive sequence lengths used for fine-tuning are. We found the ideal value of ‘n’ to be $5$ timesteps. Using $n&lt;5$ led to inaccurate recursive predictions, using $n&gt;5$ did not lead to much improvement in accuracy but increased training costs due to requirements to backpropagate through long-time sequences.</p> <p>The resulting predictions and error fields after fine-tuning are shown in the GIF in <a href="#fig:rec_pred_multiple"><span>Figure 10</span></a>. We can observe that the model predictions remain in phase with the ground truth, and the error fields are low up to 50 timesteps. We can see that even though we only fine-tuned with sequence lengths of 5, the resulting model is stable for long roll-outs of up to 50-time steps. However, this could be because eddy shedding in flow past a cylinder is somewhat periodic, which aids in some generalization.</p> <div id="fig:rec_pred_multiple"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif" class="fig:rec_pred_multiple" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 10: GIF shows the result of recursive predictions after fine-tuning </div> <p>Finally, to show the effectiveness of fine-tuning, we compare the weighted loss of the original best single-time step model and the resulting best-fine-tuned model for long roll-out predictions of 50-time steps in <a href="#fig:loss_rec_no_rec"><span>Figure 11</span></a>. We can see that the original model sees exponential growth of errors up to 2 orders of magnitude higher in just 10 recursions while the fine-tuned model is stable up to 50 recursions. Thus, we can see that our fine tuning mechanism allows for stable roll outs well beyond the training sequence length.</p> <div id="fig:loss_rec_no_rec"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec-1400.webp"/> <img src="/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec.png" class="fig:rec_pred_multiple" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 11: Comparison of the weighted loss of the original best single-time step model and the resulting best-fine-tuned model for long roll-out predictions of 50-time steps </div> <h1 id="conclusions-and-future-work">Conclusions and Future Work</h1> <p>In this project, our key contributions have been</p> <ol> <li>Extending vision transformer-based models for neural closure modeling for an idealized 2D flow field (eddy shedding from flow past a cylinder). We observed a reduction in error fields by up to 95\% using neural closure augmentations, compared to just using the low-fidelity output.</li> <li>Developing a fine-tuning procedure to achieve long recursive predictions, i.e., we have achieved stable long roll-outs for ~50 timesteps by training on just ~5-time step sequences.</li> </ol> <p>Some of the limitations of our model and future work are:</p> <ol> <li> <p>Scalability and usage of large amounts of VRAM for training: Training with a batch size of 1 takes up 7 GB of VRAM for the 50x150 field. Training with parallel GPUs or other more efficient transformer token mixing models like Adaptive Fourier Neural Operators (AFNOs) needs to be explored to increase the batch size or number of grid points.</p> </li> <li> <p>Numerical integrator inside closure model: We use a forward Euler numerical solver for the numerical integrator during closure modeling, however more stable and accurate solvers like DOPRI5 or RK4 need to be investigated.</p> </li> <li> <p>Offline training: Right now, our model uses offline data generated by a numerical solver. For complete accuracy, we would need to integrate the numerical solver (written in MATLAB or FORTRAN) into the training code and allow it to compute the low-fidelity derivatives online during training.</p> </li> <li> <p>Effect of periodicity: The eddy shedding in the flow past a cylinder is somewhat periodic which may aid the deep learning model during training. The model needs to be tested on more realistic flows like gyre flow, or real ocean velocity fields, to test its true capabilities.</p> </li> </ol>]]></content><author><name>Anantha Narayanan Suresh Babu</name></author><summary type="html"><![CDATA[Project Report for 6.s898 Deep Learning (Fall 2023)]]></summary></entry><entry><title type="html">Injecting Node Information via Embedding Initializations</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/SmartEmbeddingInitializations/" rel="alternate" type="text/html" title="Injecting Node Information via Embedding Initializations"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/SmartEmbeddingInitializations</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/SmartEmbeddingInitializations/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Graph Neural Networks (GNNs) have emerged as a transformative tool in machine learning, with the ability to capture the complex structures and relationships inherent in data. In molecular property prediction, for example, GNNs are great at encoding the atomic structure and intermolecular forces into high-dimensional embeddings, leading to more accurate predictions of chemical properties and drug efficacy. GNNs have also be used in traffic time prediction problems, physics simulations and social media analysis applications. Through message-passing and updating, GNNs are capable of learning embeddings that encode informations of node neighbors and long-distance complex connections – that we, as humans, may not be able to make. The quality of the embeddings is not only important for the accuracy of the task the GNN is trained on, but quality node embeddings can be used through transfer learning – enabling models trained on one task to adapt and excel in another. The importance of good embeddings in GNNs is why we want to look closer at embedding initializations and if we can inject additional information – not present in the graph – to result in better learned embeddings after training.</p> <p>Possible applications of initial embedding initializations could help in the field of drug discovery. For GNNs used for protein retrieval trained on a biomedical knowledge graph, using ESM embeddings for the proteins could add structure information that is not previously encoded in the graph entities.</p> <h3 id="project-outline">Project Outline</h3> <p>We will explore the question can additional node information be injected into the model by using intentional embedding initializations rather than random initializations? Furthermore, are the learned embeddings better representations of the nodes? To answer this question we will follow the steps outlined below:</p> <ol> <li>We will download a precision medicine knowledge graph that and use a GNN, TxGNN, that is implemented for disease-drug link prediction on a biomedical knowledge graph as a baseline model.</li> <li>We will modify the GNN for protein-molecular function link prediction.</li> <li>Generate and download ESM embeddings for each protein</li> <li>Pretrain and finetune two models – one using random protein node initialization and one using ESM embeddings for protein node initialization. We must pretrain our own models, rather than use the already pretrained model, since we are focusing on how different node initializations impact the predictive power.</li> <li>Evaluate both models</li> <li>Visualize latent spaces before pretrain, after pretraining and after finetuning</li> </ol> <h2 id="related-work--motivation">Related Work &amp; Motivation</h2> <p>In reviewing the literature, we found several papers which reference the possibility of improved performance through a more informed initialization process. As discussed by Li et al., the initialization methods used for GNNs, such as Xavier random initialization were originally designed for CNNs and FNNs. In that setting, the Xavier approach helped to avoid vanishing gradients and maintain a constant information flux. However, Li et al. point out that by leveraging the structure of the graph, we can likely do better than the random intializations used previously <d-cite key="Li2023"></d-cite>.</p> <p>In the paper detailing TxGNN, Huang et al. present promising results on their ability to predict drug repurposing opportunities using a GNN <d-cite key="Huang2023"></d-cite>. However, in their work they considered only the Xavier random initializations for weight matrices and node embeddings. This left open the idea of initializing the graph using more sophisticated methods.</p> <p>Previous work by Cui et al. has explored the power of artificial node initializations, finding that encoding structural and positional information in the node initializations can have profound effect on the ability of a GNN to accurately predict features based on the graph. They provide a basis for our investigation by showing the effect that initializations can have on the results, if done correctly. We seek to build on this work by testing the effect of injecting related, but not exactly equivalent information through the node initializations <d-cite key="Cui2021"></d-cite>.</p> <p>Not only did we see an opportunity to try a different initialization method, but this problem also lent itself well to data-informed initializations. The molecules in TxGNN have a wealth of knowledge about them which is not represented in the linkages in the graph, some of which is represented in the ESM embeddings of the molecules. Thus, we thought that by supplying these embeddings to the GNN, we might be able to leverage the additional data to make better predictions.</p> <h2 id="data">Data</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/PrimeKG-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/PrimeKG-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/PrimeKG-1400.webp"/> <img src="/staging/assets/img/PrimeKG.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Precision Medicine Knowledge Graph. Figure credit: <i>Building a knowledge graph to enable precision medicine</i> (Chandak, Huang, Zitnik 2023). </div> <p>We used a precision medicine knowledge graph (PrimeKG) constructed by Marinka Zitnik’s group at Harvard <d-cite key="Chandak2023"></d-cite>. PrimeKG compiles data from knowledge bases that coverage a broad variety of biomedical information including human disease, drug-protein interactions, genes and proteins with their associated biological processes, functions and cellular component, etc. PrimeKG contains 10 different node types – shown above – and 29 different types of undirected edges. There are over 120,000 nodes in total and over 8 million edges. What PrimeKG lacks, importantly, is any nodes or encodings of structural, molecular or sequenctial information for entity nodes such as proteins and drugs. The node types of interest for our model are proteins, extracted from NCBI, and molecular function Gene Ontology (GO) annotations <d-cite key="Gene_Ontology_Consortium2021-uk"></d-cite>. We will be predicting links between these two node types.</p> <p>The other data used were ESM embeddings for proteins in PrimeKG. ESM embeddings, or Evolutionary Scale Modeling embeddings, are high-dimensional vector representations of proteins, derived from advanced machine learning models developed by Meta trained on large datasets of protein sequences. These embeddings capture the intricate structural and functional characteristics of proteins, reflecting evolutionary relationships and biochemical properties that are crucial for various biological and computational applications <d-cite key="Lin2022-esm2"></d-cite>. The reason we were interested in using ESM embeddings, rather than embeddings from other protein foundation models, was that structural information was not already captured in PrimeKG, as previously mentioned. To obtain the ESM embeddings, first we downloaded the amino acid sequence for each protein from NCBI using Entrez. Then, using these sequences as input to Facebook’s ESM2 model, we extracted the corresponding embedding.</p> <h2 id="gnn">GNN</h2> <p>The model we used as a baseline is TxGNN, a graph neural network trained on PrimeKG used to make therapeutic drug predictions for diseases <d-cite key="Huang2023"></d-cite>. The GNN has two training phases. First, pretraining where the GNN finds biologically meaningful embeddings for all nodes in the knowledge graph, and therefore the objective is all link prediction. The second phase is to finetune the GNN, using self-supervised learning, to be able to predict drugs for diseases. Therefore, the objective for finetuning is to optimize contraindication and indication link prediction – the two types of links between diseases and drugs. We modified the training code for the finetuning phase, to train and validate on protein-molecular function links instead.</p> <h3 id="architecture">Architecture</h3> <p>The GNN has two linear layers with parameters n_input, n_hidden, and n_output. For all our models n_input is 1280, restricted by the length of ESM embeddings. We play around with different dimensions for the hidden and output layers. Leaky ReLU activation is used after the first layer.</p> <h3 id="training">Training</h3> <p>The first step of the training phase is <strong>node embedding initialization</strong>. The default, which is our random control, is to initialize all nodes using Xavier uniform initialization <d-cite key="pmlr-v9-glorot10a"></d-cite>. Models referred to as <em>random</em> from here on out are referring to using Xavier uniform initialization. For our experimental model, we initialized the protein nodes using the ESM embeddings we obtained earlier. All other node types were still initialized with Xavier uniform initialization. Note that we reinitialized nodes between pretraining and finetuning.</p> <p>During the training phase, the GNN uses a standard message-passing algorithm to update and optimize the node embeddings. There is a relation-type specific weight matrix (for each of the 29 relation types) used to calculate relation-type specific messages. The message for one relation to the some node $i$ is calculated using this equation: \begin{equation} m_{r, i}^{(l)} = W_{r, M}^{(l)} h_i^{(l-1)} \end{equation}</p> <p>For each node $v_i$, we aggregate incoming messages from neighboring nodes for each relation-type $r$, denoted as $N_r(i)$. This is done by taking the average of these messages: \begin{equation} m_{g_r, i}^{(l)} = \frac{1}{|N_r(i)|} \sum_{j \in N_r(i)} m_{r, j}^{(l)} \end{equation}</p> <p>The new node embedding is then updated by combining the node embedding from the last layer and the aggregated messages from all relations: \begin{equation} h_i^{(l)} = h_i^{(l-1)} + \sum_{r \in TR} m_{g_r, i}^{(l)} \end{equation}</p> <p>Finally, DistMult <d-cite key="Yang2014-zb"></d-cite> is used to calculate link prediction between two nodes using their respective embeddings.</p> <h2 id="model-evaluation">Model Evaluation</h2> <p>We fixed all parameters and hyperparameters, and trained two models – one using random initializations and one using ESM embeddings. We pretrained for 3 epochs with a learning rate of $1e-3$ and a batch size of 1024. We then finetuned for 150 epochs with a learning rate of $5e-4$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/Figure2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/Figure2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/Figure2-1400.webp"/> <img src="/staging/assets/img/Figure2.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These results are promising and using ESM embeddings to initialize the protein node representations slightly improves the model. The ESM model has a final testing loss of 0.3915, whereas the random model has a final testing loss of 0.4151. However, the difference between the models is slim and may not be significant, especially looking at the similarities in the pretraining, training and validation loss curves. Later, we will look more in depth about how the embedding spaces vary between the 2 models which has the potential to yield more interesting results.</p> <h3 id="testing-varying-hidden-and-output-layer-dimensions">Testing varying hidden and output layer dimensions</h3> <p>We wanted to see the impact changing the hidden and output layer dimensions would have on model performance. We tested 3 models, with parameters detailed in Table 1. All models outside of this experiment, unless otherwise specified, have the same parameters as Model 1.</p> <table> <thead> <tr> <th> </th> <th>Input Dimensions</th> <th>Hidden Layer Dim.</th> <th>Output Layer Dim.</th> </tr> </thead> <tbody> <tr> <td>Model 1</td> <td>1280</td> <td>1280</td> <td>1280</td> </tr> <tr> <td>Model 2</td> <td>1280</td> <td>512</td> <td>512</td> </tr> <tr> <td>Model 3</td> <td>1280</td> <td>512</td> <td>128</td> </tr> </tbody> </table> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/Testing_output_dim-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/Testing_output_dim-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/Testing_output_dim-1400.webp"/> <img src="/staging/assets/img/Testing_output_dim.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can see from the testing loss that when just comparing ESM initialized model, testing loss increases as the output layer decreases. The same trend holds true between random initialized models. We can also see that when comparing ESM and random models for the same layer dimensions, ESM always slightly outperforms the random model.</p> <h2 id="latent-space-visualizations">Latent Space Visualizations</h2> <p>In the fast-evolving world of deep learning, the analysis of model latent spaces has emerged as an interesting area of study, especially to get a better understanding of how models are achieving their tasks. These spaces are important to understanding how complex models like GNNs perceive and process the intricate relationships and structures inherent in graph data. GNNs can learn powerful representations that capture both node-level and graph-level features. By analyzing the latent spaces of GNNs, we can get insights into how these models prioritize various patterns and connections within the data. The following analyses visualize the latent spaces our models, clustered and colored in different ways, to get a deeper understanding of how the ESM initialized embeddings are effecting the GNN.</p> <p>We first were curious whether, after training our model, the final embeddings retained structural information about the proteins. To do this, we first clustered the original ESM embeddings using K-Means clustering. Next, we visualized the embedding space of the original ESM embeddings, the final embeddings from the ESM model and the final embeddings from the random model using t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction. From the t-SNE plot of original ESM embeddings, we can clearly see the clusters from K-Means which serves as a verification of our clustering technique.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/init_cinit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/init_cinit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/init_cinit-1400.webp"/> <img src="/staging/assets/img/init_cinit.jpeg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Looking at the embedding space for the ESM and random models, colored by ESM clusters, we note that most of the ESM structural information (used to create the clusters) seems to be forgotten during the training process, as evidenced by the mostly random assortment of colors present in the t-SNE plot. We note that some clusters do remain, for example cluster 12 (light sage green on the right side of the ESM initialized plots) is still clustering in the final embeddings (top middle cluster). However, the most prominent ones appear in both the ESM initialized and random initialized data, meaning that the ESM embedding did encode some function, but the model using random initialized embeddings was able to capture that relation as well.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/cluster_init-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/cluster_init-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/cluster_init-1400.webp"/> <img src="/staging/assets/img/cluster_init.jpeg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Given that the final embedding space for the ESM model didn’t seem to retain much of the information for the ESM embedding initialization, we were curious whether the ESM and random embeddings converged to a similar space. To test this theory, we clustered the final ESM model embeddings and subsequently visualized the final embeddings of the ESM and random models using t-SNE and colored by those clusters.</p> <p>If the two models converged to similar embedding spaces, we’d expect to see that clusters found in one embedding space would also be found in the other. This is the case, as seen in the two plots below. Both plots are colored based on a clustering of the final embeddings generated by the ESM initialized network, and they share many of the same structures, indicating that the two networks were able to pick up on mostly the same features in the underlying information. Both models converged to a similar embedding space different initialization methods.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/cluster_esm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/cluster_esm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/cluster_esm-1400.webp"/> <img src="/staging/assets/img/cluster_esm.jpeg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="testing-varying-hidden-and-output-layer-dimensions-1">Testing varying hidden and output layer dimensions</h3> <p>As mentioned earlier, we tested different dimensions for the hidden and output layers to see whether more and less output dimensions would retain the original ESM embedding information.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/dimensions-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/dimensions-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/dimensions-1400.webp"/> <img src="/staging/assets/img/dimensions.jpeg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Although there are more distinct clusters on the t-SNE plots as the number of output dimensions increases, these clusters are not the same as the clusters from the original ESM embeddings (seen by the randomly colored dots). Therefore, neither of these 3 models retained the structural information provided by initializing with ESM embeddings. It does not seem that decreasing output and hidden layer dimensions improves the model performance or latent space of our GNN.</p> <h3 id="clustering-by-molecular-function-labels">Clustering by molecular function labels</h3> <p>Because our model’s task was to predict links between protein and molecular function nodes, we were curious to see if the final embeddings for the protein nodes would cluster well on the function labels. However, this wasn’t as straight forward as having 1 molecular function label for each protein node, because each protein may be linked to multiple molecular functions. One protein may have multiple molecular function Gene Ontology (GO) annotations because the GO database uses a hierarchical system to categorize functions, where broader functions encompass more specific ones. A protein can be involved in several distinct biochemical activities, each represented by its own GO term, reflecting the diverse roles a protein can play in the cell. Instead of a single label, we extracted a molecular function profile, $v_i$, for each protein where $v_i[j] = 1$ if a link exists between protein $i$ and function $j$. We then had a sparse matrix, $V^{i \times j}$. Before clustering, we performed dimensionality reduction using truncated SVD which is optimal for sparse matrices. Finally, we performed K-Means clustering.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/cluster_func-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/cluster_func-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/cluster_func-1400.webp"/> <img src="/staging/assets/img/cluster_func.jpeg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Looking at the t-SNE plots, there is no apparent clustering by molecular function profiles in the final embedding spaces for either the ESM model or the randomly initialized model. There are multiple possible explanations for this. One explanation is that the actual objective is to prediction each singular link between a protein and a function node, not to predict do well at predict all function nodes linked to a protein at once. On top of that our GNN uses self-supervised learning, therefore the molecular function profiles are not true labels used during training.</p> <p>The second plausible explanation has to do once again with the hierarchical nature of molecular function GO annotations. Because the molecular function nodes have random indices when stored in PrimeKG, it is not apparent that molecular function that have the same parent function are close to each other, or their parent function in the molecular function profiles. Therefore, when performing truncated SVD and subsequently k-means clustering, the similar functions may not be clustered together if their indices are far apart. Further analysis could be done to reorder the molecular function nodes and then conduct hierarchical clustering, instead than k-means. These possible clusters may then be found in the final latent spaces for the two models.</p> <h2 id="discussion">Discussion</h2> <p>In this post, we have modified and fine-tuned a Graph Neural Network, TxGNN originally designed for drug-repurposing prediction, for protein function prediction with a variety of initializations of the node embeddings. We observed that while much of the information in the initialization is forgotten during the training process, a small amount is retained, leading to slightly better performance on the test set in the final network. This provides a potential avenue for further study, investigating the overall effects of informed initialization techniques on GNN performance. Some of this investigation is discussed in Li et al. <d-cite key="Li2023"></d-cite>, where they experiment with weight matrix initializations and propose a new paradigm for determining weight initializaiotns, but there is certainly more investigation to be done.</p>]]></content><author><name>Emma Tysinger</name></author><summary type="html"><![CDATA[Graph Neural Networks (GNNs) have revolutionized our approach to complex data structures, enabling a deeper understanding of relationships and patterns that traditional neural networks might miss. This project looks into the potential of embedding initializations in GNNs, particularly in the context of molecular function prediction and protein retrieval tasks. By investigating the effect of intentional, information-rich initializations versus random initializations, we aim to enhance the learning efficiency and accuracy of GNNs in these domains. Our study focuses on a precision medicine knowledge graph (PrimeKG) and employs TxGNN, a GNN model initially designed for disease-drug link prediction, repurposed for protein-molecular function link prediction. We explore the impact of using ESM embeddings for protein nodes, hypothesizing that these embeddings could provide structural information not explicitly present in the graph data. Through comparisons of the latent spaces and performances, we look to see the effectiveness of these embeddings in improving the model's predictive powe of protein function.]]></summary></entry><entry><title type="html">Ensemble Learning for Mitigating Double Descent</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/double_descent/" rel="alternate" type="text/html" title="Ensemble Learning for Mitigating Double Descent"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/double_descent</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/double_descent/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>We outline the fundamental ‘bias-variance tradeoff’ concept in machine learning, as well as how the double descent phenomenon counterintuitively bucks this trend for models with levels of parameterization at or beyond the number of data points in a training set. We present a novel investigation of the mitigation of the double descent phenomenon by coupling overparameterized neural networks with each other as well as various weak learners. Our findings demonstrate that coupling neural models results in decreased loss during the variance-induced jump in loss before the interpolation threshold, as well as a considerable improvement in model performance well past this threshold. Machine learning practitioners may also find useful the additional dimension of parallelization allowed through ensemble training when invoking double descent.</p> <hr/> <h2 id="motivation">Motivation</h2> <p>There are many important considerations that machine learning scientists and engineers must consider when developing a model. How long should I train a model for? What features and data should I focus on? What exactly is an appropriate model size? This last question is a particularly interesting one, as there is a bit of contention regarding the correct answer between different schools of thought. A classical statistician may argue that, at a certain point, larger models begin to hurt our ability to generalize. By adding more and more parameters, we may end up overfitting to the training data, resulting in a model that poorly generalizes on new samples. On the other hand, a modern machine learning scientist may contest that a bigger model is always better. If the true function relating an input and output is conveyed by a simple function, in reality, neither of these ideas are completely correct in practice, and empirical findings demonstrate some combination of these philosophies. This brings us to the concept known as <em>double descent</em>. Double descent is the phenomenon where, as a model’s size is increased, test loss increases after reaching a minimum, then eventually decreases again, potentially to a new global minimum. This often happens in the region where training loss becomes zero (or whatever the ’perfect’ loss score may be), which can be interpreted as the model ’memorizing’ the training data given to it. Miraculously, however, the model is not only memorizing the training data, but learning to generalize as well, as is indicated by the decreasing test loss.</p> <p>The question of ’how big should my model be?’ is key to the studies of machine learning practitioners. While many over-parameterized models can achieve lower test losses than the initial test loss minimum, it is fair to ask if the additional time, computing resources, and electricity used make the additional performance worth it. To study this question in a novel way, we propose incorporating <em>ensemble learning</em>.</p> <p>Ensemble learning is the practice of using several machine learning models in conjunction to potentially achieve even greater accuracy on test datasets than any of the individual models. Ensemble learning is quite popular for classification tasks due to this reduced error empirically found on many datasets. To our knowledge, there is not much literature on how double descent is affected by ensemble learning versus how the phenomenon arises for any individual model.</p> <p>We are effectively studying two different <em>types</em> of model complexity: one that incorporates higher levels of parameterization for an individual model, and one that uses several models in conjunction with each other. We demonstrate how ensemble learning affects the onset of the double descent phenomenon. By creating an ensemble that includes (or is fully comprised of) overparameterized neural networks, which can take extreme amounts of time and resources to generate, with overparameterized machine learning models, we will show the changes in the loss curve, specifically noting the changes in the regions where double descent is invoked. We hope that the results we have found can potentially be used by machine learning researchers and engineers to build more effective models.</p> <hr/> <h2 id="related-work">Related Work</h2> <p>One of the first papers discussing double descent was ’Reconciling modern machine- learning practice and the classical bias–variance trade-off’ by Belkin et al. <d-cite key="belkin2019reconciling"></d-cite>. This paper challenged the traditional idea of the ’bias-variance tradeoff’, a fundamental concept in machine learning that describes the tension between two types of model error: bias and variance. Bias is the error between the expected prediction of the model and the true output value, introduced by approximating a real-world quantity with a model, which may overisimplify the true problem at hand. Variance refers to the error due to a model’s sensitivity to small fluctuations in the training dataset. Overfitted models may have high variance, as they may model random noise in the data as well.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/biasvariance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/biasvariance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/biasvariance-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/biasvariance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="The Double Descent Curve" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Bias-Variance Tradeoff <d-cite key="cornell"></d-cite> </div> <p>In short, classical statistical learning argues that there is some optimal level of parameterization of a model, where it is neither underparameterized nor overparameterized, that minimizes the total error between bias and variance. However, Belkin’s paper finds that, empirically, the tension between bias and variance no longer becomes a tradeoff after a certain level of overparamaterization. They showed that after the interpolation threshold (beyond where the model fits perfectly to the training data), test error eventually began to decrease again, even going below the error deemed optimal by the bias-variance minimum.</p> <p>Nakkiran et al.’s ’Deep Double Descent: Where Bigger Models and More Data Hurt’ <d-cite key="nakkiran2021deep"></d-cite> expanded these findings to the realm of <em>deep</em> learning. In this work, double descent is shown to occur for both large models and large datasets. Additionally, this paper demonstrates that, counterintuitively, adding more data at a certain point actually worsened the performance of sufficiently large models. Specifically, this occurred at and close to the interpolation threshold for neural models. This paper’s results can be seen here:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/openai-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/openai-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/openai-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/openai.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="The Double Descent Curve" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Double Descent Curve <d-cite key="nakkiran2021deep"></d-cite> </div> <p>For the region between the first and second loss minima, model performance can suffer greatly, despite the increased computational time and resources used to generate such models. While this region of the test loss curve is typically not a level of parameterization that one would use in practice, understanding such loss curve behavior can help practitioners for several reasons. For one, this degraded phase of performance can be crucial for tweaking model architecture and adjusting training strategies. This is key to discovering if one’s model is robust and adaptable to various other datasets and tasks. This highlights the need for a new understanding for model selection in order to effectively generalize to testing datasets better, mitigating decreases in model performance and invoking a second loss minimum quickly.</p> <p>In the classic paper ’Bagging Predictors’, Breiman describes the concept of combining the decisions of multiple models to improve classification ability <d-cite key="breiman1996bagging"></d-cite>. Empirically, this bootstrap aggregating, or ’bagging’ technique, reduced variance and improved accuracy, outperforming the single predictors that comprised the ensemble model. We present a novel combination of the findings of this paper with the double descent phenomenon. Effectively, by increasing model complexity via overparameterization and ensemble learning, we aim to study if this combination can mitigate loss increases and invoke a second loss minimum with smaller models.</p> <hr/> <h2 id="setup">Setup</h2> <h3 id="computing-resources-and-software">Computing Resources and Software</h3> <p>We have implemented this project using CUDA and the free version of Google Colab, with additional computing units for more costly experiments. To train and test these models, we use various machine learning packages in Python, namely Scikit-learn, PyTorch and Tensorflow. Additional software commonly used for machine learning projects, such as numpy, tensorboard and matplotlib, was also utilized.</p> <p>All plots have been produced by us, unless otherwise specified. Note that all tensorboard plots have $0.25$ smoothing applied, except for the Soft-Voting Ensemble, which has $0.6$ smoothing applied (though this won’t make much of a difference as will soon be seen). The non-smoothed plot can be seen traced in light-blue in all provided plots.</p> <h3 id="data">Data</h3> <p>We use the MNIST dataset for this report <d-cite key="deng2012mnist"></d-cite>. MNIST is a popular dataset used for image classification, where each sample image is a $28$ by $28$ grayscale image of a written integer between $0$ and $9$, inclusive. Each image comes with the true label of the image’s integer. This data is publicly available for experimentation, and our use of it does not pose any ethical or copyright concerns.</p> <p>For this project, we use the MNIST dataset to unearth the double descent phenomenon. We experiment with a variety of models, as well as an ensemble of them: decision trees, AdaBoost trees, L2-Boost trees, random forests, logistic regression, and small neural networks. We choose these models because of their ability to be used for classification tasks, and more complicated models run the risk of exceeding Google Colab’s limitations, especially when we overparameterize these models to invoke double descent.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/mnist-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/mnist-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/mnist-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/mnist.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="MNIST Sample Data" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sample MNIST Data <d-cite key="deng2012mnist"></d-cite> </div> <hr/> <h2 id="models">Models</h2> <h3 id="decision-trees">Decision Trees</h3> <p>Decision trees are a machine learning model used for classification tasks. This model resembles a tree, splitting the data at branches, culminating in a prediction at the leaves of the tree.</p> <p>To invoke overparameterization for decision trees, we can start with a tree of depth 2, and increase the number of maximum leaves of the model until the loss plateaus. Then, keeping this new number of max leaves in our decision tree, we continually increase the maximum depth of the tree until the loss once again stops decreasing. Lastly, keep both the maximum leaves and depth at their plateau levels while increasing the max features. The results of this are plotted below. Notice how varying the number of maximum leaves has minimal effect on the loss, and how increasing the maximum depth causes the most dramatic decrease. However, fluctuations on the maximum depth at this point do not have a major effect, whereas varying the number of features causes another slight, yet consistent, fall in classification loss.</p> <p>Notice that the loss curve is more or less linear in the number of parameters (with some having much more effect than others), and so there is little evidence of double descent for this model.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Decision Tree Overparameterization </div> <h3 id="adaboost-tree">AdaBoost Tree</h3> <p>Adaptive Boosting (AdaBoost) itself is an ensemble model used for robust classification. Freund et al.’s paper ‘A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting’ first introduced the algorithm <d-cite key="freund1997decision"></d-cite>. On a high level, this paper describes how boosting is especially effective when sequentially combining weak learners that are moderately inaccurate (in this case, these are decision trees) to create a strong learner. We study the loss curve of the AdaBoost model as we first increase the number of boosting trees which form a forest, then increase the number of forests across which we average results, after adding additional trees fails to significantly increase model performance. Each tree is constrained to have no more than 10 leaves.</p> <p>Notice that the loss curve is more or less linear in the number of parameters, and the double-U shape doesn’t seem to make its presence known.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> AdaBoost Overparameterization </div> <h3 id="l2-boost-tree">L2-Boost Tree</h3> <p>L2 Boosting is quite similar to the AdaBoost model, except for L2 Boosting, as models are built sequentially, each new model in the boosting algorithm aims to minimize the L2 loss<d-cite key="article"></d-cite><d-cite key="10.1214/aos/1013203451"></d-cite>. Like before, we first increase the number of boosting trees which form a forest in the L2-Boost model, then the number of forests across which we average using majority voting. The results can be seen below. Each tree is constrained to have no more than 10 leaves.</p> <p>Notice how the classification loss begins to fall, then rises up again, then falls once more when we average across more forests to lower minimums than before. This result was consistent across multiple runs of this experiment, suggesting that double descent is real for L2-Boosted Tree Ensembles.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> L2-Boost Overparameterization </div> <p>The behavior of the loss once we add more models agrees with general intuition regarding ensembling, but the appearance of double descent as we increase the total number of parameters is still quite interesting to see. L2-Boost is a relatively inexpensive model and ensembling a large number of trees is still quite fast, suggesting that overparameterization could be the way to go in this case.</p> <h3 id="random-forest">Random Forest</h3> <p>Random Forest is another popular ensemble model. As the name implies, it is a collection of decision trees with randomly selected features, and, like the singular decision tree, this model is used for classification tasks.</p> <p>We initialize random forest with a small number of maximum leaves allowed in each tree, and increase the max leaves until we see the loss plateau as we continually add more. After this, we begin increasing the number of trees in our forest until the loss plateaus once again.</p> <p>While Belkin et al. lists random forest as a model exhibiting double descent, this claim has been recently disputed, namely by Buschjager et al, which suggests that there is no true double descent with the random forest model <d-cite key="randomforest"></d-cite>. Instead, they suggest that random forest does not overfit in the classical sense, and argue that its curve instead has a single descent. By looking at our results, there is little evidence for the double-U curve, but we will refrain from drawing a decisive conclusion.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Random Forest Overparameterization </div> <p>Despite this, for our ensemble model, we aim to see if the addition of this overparameterized learner to the neural network’s decision making is able to improve ensemble performance.</p> <h3 id="logistic-regression">Logistic Regression</h3> <p>Logistic regression is a classic model used for estimating the probability a sample belongs to various classes. We induce overfitting in logistic regression through two methods.</p> <p>First, we continually increase the ‘C’ parameter, indicating the inverse strength of regularization applied to the regression, as shown below. Notice that the loss decreases to a minimum before it starts slowly rising again, indicating that overfitting through fluctuations in ‘C’ may not actually lead to double descent, as would be expected from classical theory.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Logistic Regression Overparameterization (Regularization Based) </div> <p>Second, we try inducing double descent by varying the ratio of the number of features over the amount of data. We gradually reduce this ratio using the intuition developed by Deng et al. in order to induce overfitting <d-cite key="logistic"></d-cite>, since the data becomes more and more separable as the number of features increases relative to the number of data samples.</p> <p>To do this, we test varying across the number of training samples instead of varying the number of features used for training. This eventually leads to 0 training error, but causes testing error to blow up, suggesting that some significant amount of training data is still needed to witness the desired behavior, consistent with both statistical and machine learning theory.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Logistic Regression Overparameterization (Feature-Data Ratio Based) </div> <p>An interesting setup for future experiments would be simultaneously increasing the amount of training samples and the number of polynomial features given to the logistic regression, while increasing the feature-data ratio each time we reparameterize or redefine the dataset.</p> <h3 id="neural-networks">Neural Networks</h3> <p>We use a Multilayer Perceptron as our main model for the ensemble. Our deep learning model is a relatively small one, with variable width in the hidden layer. By increasing this width, we eventually achieve perfect training loss.</p> <p>We define the general architecture of the neural network used in this report as follows:</p> <h4 id="network-layer">Network Layer</h4> <p>Let the input data be an $m$ by $m$ pixel image from the MNIST dataset, which can be processed as an $m$ by $m$ matrix, where entry $(i,j)$ is an integer between $0$ and $255$ (inclusive) representing the grayscale color of the pixel. Note that $m=28$ for MNIST, though for generality, we use $ m $ in this network definition. A value of $0$ represents a black pixel, $255$ is a white pixel, and values between these are varying shades of gray. We first flatten this structure into a $d = m^2 $ by 1 vector, such that the entry $ (i,j) $ of the matrix becomes the $ j + 28*i$-th entry of the vector, using zero-indexing. We use this vector as the input of our neural network.</p> <p>Set $H$ as the hidden layer width, which in our project will be varied in different tests. Let $ W^1 $ be an $ d \times H$ matrix, where $ W^1_{ij}$ is the weight of input $i$ applied to node $j$, and let $W^1_0$ be an $H \times 1$ column vector representing the biases added to the weighted input. For an input $X$, we define the <em>pre-activation</em> to be an $H \times 1$ vector represented by $Z = {W^1}^T X + W^1_0$.</p> <p>We then pass this linearly transformed vector to the ReLU activation function, defined such that</p> \[\begin{equation*} \text{ReLU}(x)=\begin{cases} x \quad &amp;\text{if} \, x &gt; 0 \\ 0 \quad &amp;\text{if} \, x \leq 0 \\ \end{cases} \end{equation*}\] <p>We use this choice of activation function due to the well-known theorem of universal approximation. This theorem states that a feedforward network with at least one single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $ \mathbb{R}^{m^2} $ if the ReLU activation function is used <d-cite key="hornik1991approximation"></d-cite>. Applying an activation function ReLU to each element of $Z $, the layer finally outputs</p> \[A = \text{ReLU}(Z) = \text{ReLU}(W^T X + W_0)\] <p>Next, we will input $A$ into a second hidden layer of the neural network. Let $K$ be the number of classes that the data can possibly belong to. Again, $K = 10$ for MNIST, though we will use $K$ for generality. Then let $W^2$ be an $H$ by $K$ matrix, where $W^2_{ij}$ is the weight of input $i$ applied to node $j$, and let $W^2_0$ be a $K \times 1$ column vector representing the biases added to the weighted input. For input $A$, define a second pre-activation to be a $K \times 1$ vector represented by $B = {W^2}^T A + W^2_0$.</p> <p>This will yield a $K \times 1$ vector representing the logits of the input image, with which we’ll be able to take Cross Entropy Loss or compute its probability of belonging to any of the $K$ classes.</p> <h4 id="training">Training</h4> <p>Let class $i $ be the true classification for a data point. We have that $y_i = 1$, and for all $j \neq i$, $y_j = 0$. Furthermore, let $\hat{y_i}$ be the generated probability that the sample belongs to class $i$. The categorical cross-entropy loss is then defined as follows:</p> \[\mathcal{L}_{CCE} (y_i, \hat{y_i}) = - \sum_{i=0}^{9} y_i \log (\hat{y_i})\] <p>From this computed loss, we use backpropagation and stochastic gradient descent (SGD) with learning rate $\eta = 0.1$ and $momentum = 0.95$ to optimize model weights. We run experiments on a dataset with $n = 4000$ subsamples that train over $100$, $500$, and $2000$ epochs using Belkin et al.’s approach to training <d-cite key="belkin2019reconciling"></d-cite>. Up to interpolation, we train until we reach zero classification error on the training data, or until we have finished all of the epochs, whichever comes first. After we have reached the interpolation thereshold, we train until we have gone through all of the epochs. Note that to get their results, Belkin et al. trained over $6000$ epochs, which proved to be prohibitively expensive given our resources. Instead, we chose to train over a variety of smaller maximum epoch sizes to illustrate the double descent curve taking clearer shape, with the $2000$ epoch run being the most indicative of this phenomena. Below are the results of the trained and tested neural networks. Notice that interpolation consistently happens when the number of parameters is roughly equal to $n\times K$ (i.e. Parameter Count / 1000 $= 40$), and the test loss starts consistently getting lower and lower as we add more and more parameters beyond this threshold. Double descent is real!</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="MLP 100 Epoch Training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="MLP 100 Epoch Testing" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MLP 100 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 </div> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="MLP 500 Epoch Training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="MLP 500 Epoch Testing" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MLP 500 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 </div> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="MLP 2000 Epoch Training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-15 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="MLP 2000 Epoch Testing" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MLP 2000 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 </div> <p>For the sake of brevity, we avoid including plots for train/test classification loss for the MLPs. However, it is worth noting that train classification loss eventually reaches 0 in all experiments, whereas test loss eventually becomes $\sim$ 0.08 or smaller.</p> <p>Throughout each experiment, we vary across the number of total parameters of the model. For a network with $H$ hidden units, the total number of parameters is equal to $(d+1)\times H + (H + 1)\times K$, and so we choose $H$ accordingly each time we reparametrize.</p> <p>Note that we also incorporated a weight reuse scheme for models in the underparametrized regime to cut on training time, similarly to the approach in Belkin et al <d-cite key="belkin2019reconciling"></d-cite>.</p> <p>Additionally, even though the individual MLPs are small, training several of them sequentially for a relatively large number of epochs can take a very long time. To help reduce the time it takes to complete experiments, we also try adapting the Parameter Count Generation Algorithm provided in John Abascal’s blog <d-cite key="testbed"></d-cite> to intelligently select the next level of parameterization given the performance of the previous one. This algorithm was designed to most clearly showcase the existence of the double descent curve by fitting a third degree polynomial (since that is roughly what we expect the double descent curve to look like) to the model capacity vs. test loss graph, and choosing the next parameter by examining the first derivative of the polynomial fit to the data. More detail is provided in his blog.</p> <p>This algorithm proved helpful for empirically confirming the existence and validity of the interpolation threshold. However, after a few tests with the algorithm, we chose to complete most of the experiments using a pre-specified list of parameters which were able to consistently capture the double descent phenomena in detail.</p> <hr/> <h2 id="ensemble-learning">Ensemble Learning</h2> <p>We experimented with two different types of ensembles. The first ensemble is what we call the ‘weak-learner’ ensemble, which is the model that incorporates the multi-layer perceptron supported by L2-Boost tree ensembles, random forests, decision trees and logistic regression. Note that we ultimately did not use AdaBoost in this ensemble because we believed this was too similar to the included L2-Boost model in both architecture and performance.</p> <p>The second ensemble is the ‘multi-layer perceptron’ ensemble, which includes 5 MLPs.</p> <h3 id="weak-learner-ensemble">Weak-Learner Ensemble</h3> <p>We use bootstrap aggregating, or ‘bagging’, to formulate our ensemble of these five models . Effectively, each model is given a certain number of ‘votes’ on what that model believes is the correct classification for any given MNIST sample image. We then experimented with two approaches to voting: hard voting and soft voting.</p> <p>In hard voting, the classification with the most total votes is then used as the ensemble’s overall output. In the event of a tie, the neural network’s prediction is chosen. Using this voting scheme, we train the MLP independently of the other models in the ensemble, using the same scheme as described previously.</p> <p>In soft voting, the weighted average of the predicted class probabilities of each model is used as the predicted class probabilities of the ensemble. We utilize this prediction when training the MLP, and use negative log likelihood loss instead of cross entropy loss, since taking the softmax of probabilities is not necessary. This way, we can incorporate the predictions of the whole ensemble into the training of the MLP. Since the ensemble now outputs a vector of class probabilities, the one with the highest probability will be used as the soft voting ensemble’s prediction.</p> <p>Since we want a neural model to be the basis of our ensemble, we vary the number of votes assigned to the neural network while keeping the number of votes for other models fixed to 1. With four supplementary models in addition to the neural network, giving the neural network 4 or more votes is not necessary, since this ensemble would always output the same results as the neural network. Because of this, we study the loss curve when giving the neural network 1, 2, and 3 votes. Note that decimal value votes for the neural network are not sensible (at least in the hard-voting scheme), since it can be proved that all potential voting scenarios are encapsulated into the three voting levels we have chosen.</p> <p>Another important aspect of our ensemble is that the ‘weak’ classifiers do not vary in parameterization; only the MLP does. Refitting all the weak classifiers across epochs and MLP parameterizations took much longer than expected, perhaps due to incompatibilities between sklearn and GPUs, and completing the experiments using this approach was unfortunately unfeasible. Hence, all ‘weak’ classifiers have fixed architectures, chosen such that each one has low test error but is not at the highest level of parameterization according to the previous discussion, and only the MLP varies.</p> <h3 id="multi-layer-perceptron-ensemble">Multi-Layer Perceptron Ensemble</h3> <p>The Multi-Layer Perceptron Ensemble uses 5 identically initialized MLPs which are trained in parallel using Pytorch’s autovectorization capabilities. Since they are defined in the same way and trained simultaneously using the MLP training scheme discussed above, each receives equal weight when it comes to taking an averaged prediction. However, unlike the bagging method used for the Weak-Learner Ensemble, we take advantage of the identical architectures of the models and the numerical stability provided by this, and generate ensemble predictions by averaging the logits of all five learners and using those values as the logits of the ensemble. Again, we experiment using 100 and 500 epochs to see how the behavior evolves across increasing number of epochs, but we omit training over 2000 epochs due to excessive computational costs. An experiment for the future would be training over a very large number of epochs for even greater ensemble sizes to see how results vary across time.</p> <p>There has been discussion in the past of whether to average the raw logits or the softmax-transformed probabilities. The main concern raised over averaging across raw logits is that the outputted values can vary greatly in magnitude across models (and therefore overconfident models can potentially overshadow all other models when taking the prediction), but, empirically, that doesn’t seem to be a problem here. Tassi et al. provide some intuition in “The Impact of Averaging Logits Over Probabilities on Ensembles of Neural Networks” <d-cite key="Tassi2022TheIO"></d-cite>, suggesting that different approaches to ensembling should be taken depending on the architecture and levels of confidence of the models. For general safety, they recommend averaging the probabilities, but for the purposes of our task, averaging the logits suffices.</p> <hr/> <h2 id="results-and-discussion">Results and Discussion</h2> <p>Contrary to our expectations, the Weak Learner Ensemble performs much worse than even the individual models on MNIST classification. Although our focus is on double descent and not on the strong predictive power of ensembles, the latter is needed to observe the former, or at least discuss it at an interesting level.</p> <p>Initially, we tried applying the soft-voting scheme for the Weak Learner Ensemble, but the reported results are unexpectedly poor, yielding very high classification loss, especially when compared to the results of each model taken individually. This may be because each ‘weak’ learner has high confidence in its predicted class, whereas the MLP may be more evenly split between different classes, which would result in the weak classifiers winning more often, even if the MLP has higher weight in the prediction. The plot of the negative log likelihood loss for both training and testing is also hard to understand, but it is clear the ensemble has a very hard time improving, even as more parameters are added. We only include the results for the ensemble that allocates 3 votes to the MLP, but note that these are the best loss curves we were able to produce given this method.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Soft-Voting Weak-Learner Ensemble 100 Epoch Training/Testing Negative Log Likelihood Loss Over MLP Parameter Count / 1000. MLP given 3 votes. </div> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Soft-Voting Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 3 votes. </div> <p>We then tried the weak-learner approach again with hard-voting, and let the MLP independently train using the unmodified MLP training scheme mentioned previously. However, as opposed to halting training when MLP classifications loss first hits 0, we only halt training when <em>ensemble</em> classification first hits 0.</p> <p>We found that while classification loss had certainly gone down when compared to the soft-voting scheme (with even just <em>one</em> vote!), the ensemble still severely underperformed when compared to each of the individual models used. As seen in the plots, the classification loss starts to improve once the MLP gets more and more votes, agreeing with intuition that, eventually, the MLP has the veto right. As opposed to the soft-voting scheme, all classifiers now have a contribution that is proportional to their voting weight, which mitigates the previous problem of some models having much higher confidence than others. However, we believe the poor results can be attributed to the models we used for ensembling. Indeed, a significant number of models are regular, boosted or ensembled (or all) versions of decision trees, which means there is a significant chance that they make similar mistakes on similar data points. Looking at the plots for overparameterized decision trees and L2-Boost ensembles, we see that train error never quite reaches 0 for any of them. Since the train loss seems to pleateau for our models as well, this may prove why. In the cases of 1 or 2 votes, this can lead to consistently poor predictions, especially since the models are not reparameterized across the experiment. For 3 votes, this phenomenon is less significant, as the ensemble slowly begins to reach the testing performance of the individual models.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 1 vote. </div> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 2 votes. </div> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 3 votes. </div> <p>Further work could be done on the Weak-Learner Ensemble, focusing on better model selection and concurrent reparameterization across all models. Given the limited time and compute resources at our disposal, we leave this problem open for now.</p> <p>All hope is not lost, however. Seeing the poor performance of the Weak-Learner Ensemble given the significantly better performance of individual models, one could be discouraged from attempting to use ensembling to mitigate double descent, since it may not even be observable in such settings. However, we saw double descent in L2-Boost ensembles and, arguably, in random forests, and so we pushed onward. All other ensemble methods used multiple copies of the same model, and so we decided to experiment with a small ensemble of MLPs, to see how they would behave.</p> <p>This was feasible for 100 and 500 epochs only, but the obtained results shed light on how ensembling could in fact mitigate double descent. The phenomenon is not quite as observable in the 100 epoch case (one explanation could be that the train loss has not converged yet), but it becomes quite clear when looking at the 500 epoch ensemble and comparing it with the original 500 epoch MLP. Double descent is still very easy to see, ocuring at the same threshold as before. This makes sense, since the MLPs have all reached interpolation, which should increase test loss for all, and then start going down as we overparametrize more and more. However, the main result is that the increase once we reach interpolation is <em>much</em> lower than before. Indeed, the ensemble sees a jump from $\sim$ 0.35 to around $\sim$ 0.4 at the highest, whereas the individual MLP sees a jump from $\sim$ 0.36 to around $\sim$ 0.52. Another important result is that the loss as we overparameterize becomes <em>significantly</em> lower in the ensemble model than in the individual MLP.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MLP Ensemble 100 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 </div> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test-1400.webp"/> <img src="/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MLP Ensemble 500 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 </div> <p>While we weren’t able to fully get rid of the double descent curve by ensembling multiple MLPs, the fact that it became flatter and the loss past the interpolation threshold started to become smaller is quite exciting, as it suggests that, potentially, large ensembles of MLPs may not noticeably suffer from double descent at all, and yield better overall predictions than individual models can. One notable advantage to this ensemble method is the ability to further parallelize one’s training of overparameterized neural networks. These models can take extreme lengths of time to train, and besides increasing the computational allocation used, practitioners may use data, model, or processor parallelism in order to reduce this time. The ensemble neural networks we use are independently generated, meaning that they can be vectorized or trained on different GPU cores without issue. This could be a valid alternative to training for more epochs for reducing model error past the interpolation threshold. More work investigating the effect of neural network ensembling on double descent, especially on models trained over many epochs, would be very exciting and potentially shed even more light on the possible advantages of overparameterization.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>We discussed the existence of double descent for some simple and classical models, observing the effects of varying across levels of parameterization and noting that single descent can sometimes be mistaken for double descent, and proposed the use of various ensembles to mitigate the effects of double descent.</p> <p>Ensembles consisting solely of neural networks resulted in a considerable boost in performance past the individual model interpolation threshold, and in a flatter curve when compared to individual models. However, pairing the neural network with weak learners in an ensemble voting system decreased testing performance, though this adverse effect waned as the neural network received proportionally more votes. Machine learning engineers that intend to intentionally overparameterize their models may take advantage of not only the ensemble approach’s increased performance and significantly more reliable results, but the enhanced parallelization and vectorization capabilities offered by the proposed method.</p> <hr/> <h2 id="future-work">Future Work</h2> <p>This project was implemented using Google Colab, which proved to be restrictive for adopting more complex models. A key part of the double descent phenomenon is overparameterization, which happens across multiple full training loops, and so complex models that are additionally overparameterized will require more powerful computing resources beyond what we used. For example, a model which takes 10 hours to complete a single training loop will take multiple days to train before being able to plot results and observe double descent. Even for models that take around 10 to 15 minutes to train, such as the 500 epoch MLP we explored throughout our project, a full experiment that showcases the double descent curve in detail can take upwards of 5 hours. Furthermore, additional computing power can allow for this project to be expanded to more complicated datasets and tasks. MNIST classification is computationally inexpensive, though invoking double descent in more complex tasks such as text generation in natural language processing was not feasible using Google Colab. Future projects that follow this work should keep computational limitations in mind when choosing models and datasets.</p> <p>In addition to the future work suggested throughout our project, we propose a final approach that we believe is worth exploring further. During the planning process of this project, we discussed using a more rigorous voting system than what is traditionally found in ensemble model projects. Effectively, each model would have a weight associated with how much influence its output should have on the overall ensemble output. For $n$ models, each model could start with, say, a weight of $1/n$. Then, after producing each model’s vector output, the categorical cross-entropy loss with respect the true output could be computed, and the weights of each model could be updated such that each model has its weight decreased by some amount proportional to the calculated loss. Then, these weights could be normalized using the softmax function. This would be repeated for each level of parameterization. Due to resource constraints and the limitations of sklearn to the CPU, learning both the model weights and ensemble weights at each level of ensemble parameterization was not feasible given the size of the models we built and the classifiers we chose to use, as well as the number of epochs we trained over. Future studies may wish to implement this method, however, to produce a more robust ensemble for classification.</p> <hr/> <h2 id="reproducibility-statement">Reproducibility Statement</h2> <p>To ensure reproducibility, we have included the codebase used for this project, as well as the above description of our data, models, and methods <d-cite key="colab"></d-cite>. Note that the Colab notebook that we have worked in is currently very messy and sometimes incomplete due to faults in Google’s autosaving feature, but we plan to clean it up and have it available for easy future experimentation.</p>]]></content><author><name>Mohit Dighamber</name></author><summary type="html"><![CDATA[Exploring when and why Double Descent occurs, and how to mitigate it through Ensemble Learning.]]></summary></entry><entry><title type="html">Autoen-chorder: Predicting Musical Success With Neural Nets</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/foley-to-video/" rel="alternate" type="text/html" title="Autoen-chorder: Predicting Musical Success With Neural Nets"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/foley-to-video</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/foley-to-video/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Our aim is to use deep learning (the crux of 6.s898) to help musicians and their sponsors (for example: agents, record labels, and investors) identify whether songs will resonate with listeners. Solving this problem would enable established artists to release more impactful music, and spur new musicians to break into a competitive market.</p> <p>We first begin by establishing what our success metric is. For the purposes of this project, we will use the concept of song “popularity” as the metric we want to predict, and we source our popularity data from the SpotGenTrack Popularity Dataset. This dataset leverages Spotify’s Popularity Index, which is a relative rank measured against other songs’ popularities. It is a function of recent stream count, save rate, playlist appearance count, skip rate, share rate, and more.</p> <p>There already exist a few models to help us solve this problem. However, these models make use of metadata, such as artist name, year of release, and genre. We believe that these models – while interesting – are insufficient to be actionable, particularly for up-and-coming musicians who may be innovating new music genres, or who may not yet have a strong name. Specifically, metadata like Artist Name are both highly-weighted (for example, even Taylor Swift’s least popular song will be a relative superhit) and unchangeable (we cannot suggest that artists change their identity to Beyonce). Additionally, features like Genre are imprecise, and can quickly become outdated as new subgenres and crossover genres are developed.</p> <p>To address this gap and become more actionable to musicians, we aimed to create a new model that can achieve near-parity with metadata-based models without leveraging any metadata. By combining multiple audio-feature models, we not only achieved comparable results to metadata-based models, we actually outperformed metadata-based models on more than half our testing data.</p> <h2 id="previous-works">Previous Works</h2> <p>The most prominent existing model is HitMusicNet (heretofore referred to as “HMN”). The HMN model predicts popularity based on lyric data from Genius.com (syllables, words, etc.), high-level audio features from SpotGenTrack (e.g. acousticness, key, tempo, speechiness), low-level audio features from SpotGenTrack (audio preprocessing, such as spectral analyses), and metadata from SpotGenTrack (e.g. artist name, year of release, genre). A feature vector is created with this information, and said vector is fed as the input into an Autoencoder network to compress the features, followed by a neural network to obtain the predicted popularity.</p> <p>HitMusicNet has two different objectives: Regression and classification. For this project, we will focus only on regression since it will allow us to visualize differences between our model and HMN with higher granularity. We replicated the code from the paper in PyTorch, using the same functions and data and calculated metrics to make sure our implementation is correctly replicating the paper. We see a slight discrepancy in the errors, likely due to the test/train split during the paper’s training. Altogether, we can still consider our replicated model as valid as the metrics are within reasonable range to the reported metrics. Additionally, we added the R-squared metric as an additional metric to ensure our model fits the data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-12-Comparison-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-12-Comparison-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-12-Comparison-1400.webp"/> <img src="/staging/assets/img/2023-12-12-Comparison.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A second model, VGGish, is a pretrained convolutional neural network trained on YouTube-100M (a database with 100 million YouTube videos). This network is a representation learning network widely used in established papers. This network takes in a wav file and processes it on 0.96-second windows, and calculates 128 embeddings per window. This means that the resulting tensor from VGGish will be 2 dimensional for a single file, and 3 dimensional for a batch of files.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-12-HMN-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-12-HMN-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-12-HMN-1400.webp"/> <img src="/staging/assets/img/2023-12-12-HMN.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-12-Autoencoder-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-12-Autoencoder-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-12-Autoencoder-1400.webp"/> <img src="/staging/assets/img/2023-12-12-Autoencoder.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hypothesis">Hypothesis</h2> <p>HMN has a tendency to be heavily-indexed on metadata features and lyrics. Data such as artist name heavily bias the model’s popularity predictions in favor of big-name artists. Lyrics information can make the model biased to predicting instrumental music as less likely to be successful. While this may be representative of how the industry works, it makes HMN much less actionable for musicians trying to assess their chance of success with the market.</p> <p>We believe that audio-only features – such as temporal information (i.e. the structure of the song and information about previous sections) and repetition – can alone be fairly successful in determining a song’s popularity. Thus, we chose to use just audio data, as well as temporal data, to predict popularity.</p> <p>We hypothesize that combining the audio-only features of HMN with VGGish’s audio representation will yield superior outcomes to HMN’s audio-only features alone. We also hope that our new model can compete with the full HMN model (i.e. audio features and metadata combined).</p> <h2 id="data-preprocessing">Data Preprocessing</h2> <p>Given our hypothesis, we need to extract the low-level features from our signal and map each row to its corresponding audio file to be fed into VGGish. We used Spotify’s API to obtain the raw audio files to be processed, and then ran them through the VGGish network. We performed the same preprocessing as the one done in the MusicHitNet paper.</p> <h3 id="file-length-limitation">File length limitation</h3> <p>Unfourtunately, Spotify only allows the download of 30s previews of songs.</p> <h3 id="memory-limitation">Memory limitation</h3> <p>Audio files are heavy, and the longer they are, the heavier. We should ideally process all 95,000 songs’ full length, but given Spotify’s API’s request limit, and the size of the files, we were only able to obtain 10,000 30s snippets. This still resulted in roughly 14.5 GB of data. Processing the whole dataset would not only require roughly 140 GBs of data, but the Spotify API’s limits will likely be exceeded, and our colab notebook will likely run out of memory.</p> <h3 id="downsampling-and-splitting">Downsampling and splitting</h3> <p>Given the considerations above, we decided to use 10,000 songs for our model’s development, splitting the data into 8,500 samples for training and 1,500 for validation. Given that this is roughly 10% of the original data, we expect the model’s performance to be below the reported metrics.</p> <h2 id="baselines">Baselines</h2> <p>The metrics obtained when replicating the HMN network serve as a reasonable parameter to verify our model’s accuracy. As mentioned above, the model’s performance is expected to be below the paper’s reported metrics. To understand the range, we retrained a network with the same shape as the paper’s using the 10,000 samples in the same train/test split we will feed to our new network. The resulting metrics for this experiment can be seen in Table 2.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/img5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/img5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/img5-1400.webp"/> <img src="/staging/assets/img/img5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Training a model that results in similar metrics would be ideal, but realistically, as we will only be using low-level data, we expect the metrics to be lower than the values in Table 2. To ensure that our trained model isn’t just predicting noise, we use a baseline comparison, comparing against a random normal distribution with mean μ=40.02 and σ=16.79.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/img6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/img6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/img6-1400.webp"/> <img src="/staging/assets/img/img6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As seen in table 3, the baseline intuitively would not appear to be too far from the trained HMN model in terms of MSE and MAE. When looking at the r-squared, the random model has a negative value, while the trained HMN netw ork results with a much higher 0.5616 value. To deem a model as successful, we will compare it against both sets of metrics.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/img1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/img1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/img1-1400.webp"/> <img src="/staging/assets/img/img1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/img2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/img2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/img2-1400.webp"/> <img src="/staging/assets/img/img2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="alternate-models">Alternate Models</h2> <ul> <li>Single autoencoder. Our first iteration to solve this problem consisted of using a single autoencoder to find representations with data coming from VGGish and SpotGetTrack low level features, and then running that through a feed-forward network similar to the one used in HMN. Since the output of VGGish is a tensor of shape (batch_size, n_windows, n_features) and the output of SpotGenTrack is (batch_size, 207), we concluded there was no simple way to combine the two data sources without losing temporal information.</li> <li>RNN. Our second iteration consisted of running the data coming from SpotGenTrack Low-Level through an autoencoder in the same way HMN does it. After this initial train gives us a compressed representation of the data from SpotGenTrack Low-Level, we train two subsequent networks: First an LSTM RNN which transforms data into (batch_size, 20), then we add the compressed representation from SpotGenTrack Low-Level and run that through a feedforward network. This model yielded a performance below the baseline.</li> <li>HMN+VGGish: This model consists of taking the full SpotGenTrack data, passing it through the regular autoencoder defined in HMN, and add it to the output coming from VGGish. This model, while resulting in promising results, still yielded worse performance than HMN on its own, so our team decided to explore alternatives.</li> <li>LossNet. Our third exploration consisted of training a model that uses VGGish’s outputs to try and predict losses from HMN. In essence, we are trying to use VGGish Representation to capture information that HMN consistently is unable to. This approach has parallels with Adversarial Networks, in that one model is being trained on the losses of another model. However, this approach is more cooperative than adversarial, since the result of the two models is not zero-sum. This approach led to a dead-end with surprising results.</li> </ul> <h2 id="final-architecture">Final Architecture</h2> <p>Our final iteration consists of a model with two autoencoders: One for data from SpotGenTrack low level features, the second for the representation obtained using the VGGish model. The slight difference between these two models is that the VGGish autoencoder has additional LSTM layers at the start of the encoder, and at the end of the decoder. The output from these two autoencoders is then added together and passed through a feed-forward network. This architecture can be seen in Figure 4.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-12-12-FinalArch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-12-12-FinalArch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-12-12-FinalArch-1400.webp"/> <img src="/staging/assets/img/2023-12-12-FinalArch.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="padding-and-packing">Padding and Packing</h3> <p>None of the audio files coming from Spotify previews are more than 30s in duration, but some are in fact shorter than others. To solve this issue, and also to be able to feed our model whichever sized data we require, we use pytorch’s packing functionality. Packing allows us to process sequential data with different sizes, so that only the relevant information is passed through the LSTM. Conversely, padding allows us to add zeros at the end of sequences so that all samples have the same size. This is required to store data in tensors.</p> <h3 id="hyperparameters">Hyperparameters</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/img3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/img3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/img3-1400.webp"/> <img src="/staging/assets/img/img3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="additional-model-considerations">Additional Model Considerations</h3> <p>The original HMN model compiles 228 features into 45 representations for the feed-forward network. We want our model’s feed-forward network to have a similar number of inputs as the given architecture, therefore we compress the data in the encoder of both autoencoders to 20 features, so that when added together, they result in 40 total features.</p> <p>Additionally, as can be seen in figure 3.2, the target’s distribution is condensed at a central point, and distributed in a Gaussian shape. To help our model accurately predict the shape of the results, we use multiply the losses by a weighting factor. This multiplication is important to make our model more likely to predict outliers. The equation is the following:</p> <p>\begin{equation} \frac{1}{N} \sum_{i=1}^{N} \exp\left(\left(\frac_{i} - \text)}}{\alpha \cdot \text}\right)^2 \cdot \frac{1}{\beta}\right) \end{equation}</p> <p>Our feed-forward network was suffering of vanishing gradients during training. To attempt to avoid this, we initialized all linear layers with a weight distributed by Xavier uniform, and a constant bias of 0.1.</p> <h3 id="finding-the-best-model">Finding the Best Model</h3> <p>In order to find the best model, we modified plenty of parameters and hyperparameters. We first found the optimal autoencoder models (seen on table 4), and then we proceeded to run several loops over our linear layer to obtain the model with lowest errors. The parameters modified were the following:</p> <ul> <li>Learning rate: (0.001, 0.0001, 0.0002, 0.02, 0.0005)</li> <li>Weight decays: (0, 0.0001, 0.0002)</li> <li>Batch sizes: (200, 100, 256, 277)</li> <li>Means (for weights calculation): 0.33, 0.34, 0.35, 0.37, 0.38, 0.40, 0.42, 0.45)</li> <li>Alphas (for weights calculation): (1.8, 2.0, 2.1, 2.2)</li> <li>Betas (for weights calculation): (1.8, 2.0, 2.2)</li> <li>Number of linear layers: (7, 9, 12)</li> </ul> <p>The combination that resulted in the optimal model was the following:</p> <ul> <li>Weight decays: 0</li> <li>Batch sizes: 200</li> <li>Means (for weights calculation): 0.36</li> <li>Alphas (for weights calculation): 2.0</li> <li>Betas (for weights calculation): 2.0</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/img4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/img4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/img4-1400.webp"/> <img src="/staging/assets/img/img4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Table 5 shows the best-performing models obtained after experimentation. MAE, MSE and r-squared were calculated using the testing data, i.e. Data not used in training. Looking at the data in tables 2 and 3, we see that our model shows a significant improvement above the random baseline, with a reasonable r-squared and MSE. Reduction in the MAE remains challenging, but still we see a significant improvement from the random baseline.</p> <p>Furthermore, we analyzed the testing data, and found that in 919 of the 1,500 songs (61.2%) of the songs, our model did better than HitMusicNet. Upon further analysis, we found that our model did a better job predicting the popularity of songs with popularities ranged [0.22-0.55], while HMN does a better job at predicting outliers (songs with &lt;0.2 or &gt;0.6 of popularity).</p> <h2 id="conclusions-and-next-steps">Conclusions and Next Steps</h2> <h3 id="data-exploration">Data Exploration</h3> <p>Given Spotify’s ubiquity and analytics excellence, its Popularity Index is a good proxy for relative song popularity. But there are concerns around using data from a single platform (Spotify) and from a single channel (digital streaming). Given this concern, we would like to explore other methods of calibrating a track’s popularity (for example, Billboard and Discogs API). We can aggregate popularities into a single output, or can train each model on multiple outputs of various popularity scores.</p> <p>Currently, our data consists of 30s audio clips. The average new song length is around 3min 17s, meaning that our models’ inputs cover around 15% of the song. This can cause the model to miss information critical to song likeability, such as the intro, chorus, or bridge. We would like to make our dataset more complete by using full songs as inputs. Furthermore, we’re using only 10,000 data points, which can also be affecting our training efficiency, especially our ability to detect outliers, which we have found to be a key issue with our model. Ideally, we would like to train our models on all 95k songs in SpotGenTrack.</p> <h3 id="architectures">Architectures</h3> <p>Many more architectures can further be explored to predict song popularity. We found VGGish with an LSTM to be an efficient “boosting” algorithm, which contributed to the model in a less significant way that SpotGenTrack, but still allowed our model to increase its performance. Similarly, the use of transformer architectures can help improve the performance of our model.</p> <p>In this study, we explored and evaluated our model against the HitMusicNet’s regression algorithm. In further studies, it could be beneficial to explore the classification algorithm, as we have seen very promising results in the prediction of songs along a certain range.</p> <p>We used the VGGish model purely on inference since we required to train the autoencoder and then the feed-forward network. Future studies can include architectures such that the VGGish model is trained in series with the feedforward network, and fine-tuned to predict popularity. We could also look at alternate representation models that are perhaps better suited or supply a more apt representation for our task than VGGish.</p> <p>In conclusion, the use of low-level features to predict popularity can have several real-world advantages. The proposed model is able to predict a song’s popularity to a fair degree without the need for high-level features. Emerging artists can use these parameters to determine the possible success of their songs. Music labels can use this algorithm to predict an artist’s possible popularity. Platforms such as Spotify can also take advantage of this model in order to tackle recommendations and boost emerging artists.</p> <h2 id="bibliography">Bibliography</h2> <ul> <li>D. Martín-Gutiérrez, G. Hernández Peñaloza, A. Belmonte-Hernández and F. Álvarez García, “A Multimodal End-to-End Deep Learning Architecture for Music Popularity Prediction,” in IEEE Access, vol. 8, pp. 39361-39374, 2020, doi: 10.1109/ACCESS.2020.2976033.</li> <li>Ding, Yiwei, and Alexander Lerch. “Audio embeddings as teachers for music classification.” arXiv preprint arXiv:2306.17424 (2023).</li> <li>D. Martín-Gutiérrez, “HitMusicNet” in https://github.com/dmgutierrez/hitmusicnet.</li> <li>Koutini, Khaled, et al. “Efficient training of audio transformers with patchout.” arXiv preprint arXiv:2110.05069 (2021).</li> <li>P. Nandi, “Recurrent Neural Nets for Audio Classification” in https://towardsdatascience.com/recurrent-neural-nets-for-audio-classification-81cb62327990.</li> <li>Wu, Rick, “VGGish Tensorflow to PyTorch” in https://github.com/tcvrick/audioset-vggish-tensorflow-to-pytorch.</li> <li>Wu, Yiming. (2023). Self-Supervised Disentanglement of Harmonic and Rhythmic Features in Music Audio Signals.</li> <li>S. Shahane, “Spotify and Genius Track Dataset” in https://www.kaggle.com/datasets/saurabhshahane/spotgen-music-dataset/data.</li> </ul>]]></content><author><name>Esteban Ramirez Echavarria</name></author><summary type="html"><![CDATA[In this blog, we discuss deep learning methods and results of predicting song popularity from audio features.]]></summary></entry><entry><title type="html">Reasoning with Maps: Assessing Spatial Comprehension on Maps in Pre-trained Models</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/mapreason/" rel="alternate" type="text/html" title="Reasoning with Maps: Assessing Spatial Comprehension on Maps in Pre-trained Models"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/mapreason</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/mapreason/"><![CDATA[<h1 id="motivation">Motivation</h1> <p>Humans possess a remarkable ability to intuitively understand and make sense of maps, demonstrating a fundamental capacity for spatial reasoning, even without specific domain knowledge. To illustrate this, consider the following question: Do these two maps represent the same location?</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/85_301-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/85_301-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/85_301-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/85_301.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/85_302-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/85_302-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/85_302-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/85_302.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Answering this query necessitates <strong>coregistration</strong>, the ability to align two maps by overlaying their significant landmarks or key features. Moreover, humans can go beyond mere alignment; they can tackle complex inquiries that demand aligning maps, extracting pertinent data from each, and integrating this information to provide answers.</p> <p>Maps reasoning is a fundamental skill with important applications in domains such as navigation and geographic analysis. For example, pilots need to be able to reference and understand multiple kinds of FAA charts as a core prerequisite for many aviation-related tasks. Further, making inferences on historical maps that lack digitized versions relies on human capabilities for reasoning on maps and is crucial for various fields such as geology or archeology. Machine learning models that can match human visual map understanding hold substantial promise in these applications. Additionally, such models have the potential to enhance accessibility by providing alternative modalities for individuals with visual impairments to comprehend and extract spatial information from maps.</p> <p>Our work aims to tackle the following question: To what degree do contemporary state-of-the-art (SOTA) machine learning models, pre-trained on vast datasets comprising millions or even billions of images, possess the capacity for spatial reasoning and do they reach the human level? We will do this specifically by focusing on the task of coregistration.</p> <p>We propose a map reasoning dataset which we believe is a suitable initial benchmark to test the capabilities of multimodal models on coregistration; The example given above about coregistration possibly cannot be answered directly using prior knowledge a Large Language Model (LLM) might have while ignoring the vision modality. Moreover, the complexity of the task can be increased and controlled, leading to a rigorous evaluation of the model’s ability to comprehend and synthesize information across textual and visual modalities.</p> <h2 id="literature-review-and-the-gap-in-previous-literature">Literature review and the gap in previous literature</h2> <p><strong>Multimodality:</strong> There are countless significant recent advances in Large Language Models (LLMs) achieved by models such as Meta’s Llama 2<d-cite key="llama"></d-cite>, OpenAI’s GPT 3<d-cite key="gpt3"></d-cite>, Google’s Palm 2<d-cite key="palm2"></d-cite>, WizardLM<d-cite key="wizardlm"></d-cite>, and countless others. These models have successfully achieved or surpassed human-level performances on numerous natural language processing tasks (such as Retro-Reader<d-cite key="retro_reader"></d-cite> on the SQuAD2.0<d-cite key="squad2"></d-cite> benchmark, ST-MOE<d-cite key="st_moe"></d-cite> on the SuperGLUE<d-cite key="super_glue"></d-cite> benchmark, and many other benchmarks). LLMs have achieved a surprisingly high level of knowledge about text by being able to achieve very high zero-shot scores on many NLP tasks demonstrating their understanding and versatility in Language as opposed to non-LLM NLP models that are usually trained to accomplish a specific task and do not generalize beyond that task.</p> <p>Recently there has been a massive push towards integrating other modalities into LLMs, most notably vision. Models such as Google’s Gemini<d-cite key="gemini"></d-cite>, OpenAI’s GPT 4<d-cite key="gpt4"></d-cite>, VisionLLM<d-cite key="vision_llm"></d-cite>, and many others all focus on scaling the capabilities achieved by LLMs in the NLP domain to the vision domain. The language modality LLMs are trained on consists mostly of written text recorded online that follows a grammatical structure from a human language (English, etc.) or a computer language (Python, HTML, Latex, etc). However, this is in stark contrast to the vision modality which can consist of categorically different representations consisting of anything from photographs of people in the park, to a representation of the Silicon Atom from a computer simulation, to an infographic about the importance of drinking enough liters of water per day. This difference between the distribution of the entirety of text modality and the vision modality hints that much more careful attention needs to be placed on compiling unique and diverse datasets that aim at sufficiently representing the distribution of the vision modality.</p> <p><strong>Vision-Language Benchmarks:</strong> There were significant strides made in the past years in developing benchmarks and datasets for LVLMs which are composed of questions that require both Language and Vision to successfully answer. However, there are very few datasets that include or focus on maps as part of the benchmark. LVLM-eHub<d-cite key="lvlm_ehub"></d-cite> compiles numerous benchmarks for LVLMs such as benchmarking object and scene recognition using ImageNet1K, GVT, and many other datasets, or benchmarking visual common sense using ImageNetVC and the visual commonsense reasoning datasets. They also include a Visual Reasoning benchmark, however, it focuses mostly on real-world photographs (such as the Visual Spatial Reasoning<d-cite key="visual_reasoning"></d-cite> dataset or the VisDial<d-cite key="visual_dialog"></d-cite> Dataset that is based on images from COCO<d-cite key="coco"></d-cite>). However, none of these datasets place focus or sufficiently contain images of maps or diagrams while they place much more focus on real-world images.</p> <p>A frequently used dataset for evaluating LVLMs, which is also included in the Visual Reasoning benchmark, is the ScienceQA<d-cite key="science_qa"></d-cite> dataset which includes more than 20 thousand multimodal multiple-choice questions across 127 categories including a category for maps. However, examining the narrow slice of questions with images in the geography/map category shows that many of them do not necessitate a vision component and can be answered solely based on the textual question (e.g. “Which of these states is farthest north?” with four different states as a multiple choice question), or provide an entirely unused visual representation (e.g. providing a map of the United States with no text alongside the question “What is the capital of New York?”). Out of the questions that rely on the visual component to correctly answer, many of them require relatively little understanding of maps such as asking “Which country is highlighted?” which only requires visual matching of the highlighted section with typical shapes of countries or continents. Additionally, recent papers such as LLama-adapter<d-cite key="llama_adapter"></d-cite> have demonstrated that it’s possible to achieve a high accuracy of 78.3% on ScienceQA using an unimodal text-only Large Language Model. Thus, although ScienceQA does have a subsection dedicated to maps, it does not seem sufficiently capable of testing the capabilities of LVLMs to reason and understand maps.</p> <p>An area closely related to maps that do have a relatively higher degree of focus is the capability of models to parse and reason about diagrams, figures, and plots. Datasets on this topic include the ACL-FIG<d-cite key="acl_fig"></d-cite> which involves classifying and labeling scientific figures, InfographicVQA<d-cite key="info_vqa"></d-cite> which requires reasoning over data visualizations in infographics, ChartQA<d-cite key="chart_qa"></d-cite> which requires reasoning over charts, and many other datasets that focus on figures are plots. Models have been developed to specifically tackle this challenge, such as Google’s DEPLOT<d-cite key="deplot"></d-cite> which is capable of reasoning over charts and plots by translating them to text and then using an LLM as a reasoning engine on top of the outputted text. However, charts and plots are still significantly different from maps, as the plots these datasets usually contain are simple line charts and bar graphs that can be translated into a table or textual format in a relatively lossless manner, while it is difficult or impossible to perfectly transform a sufficiently detailed map to a textual format without losing information. This illustrates the inherent complexities associated with processing maps meant to depict dense information which requires direct reasoning on the vision modality as opposed to charts and plots which present data in a simple manner.</p> <p><strong>Maps Reasoning:</strong> Huge strides have been made in specific tasks related to maps, such as image-to-map<d-cite key="image_to_map"></d-cite> conversion and map segmentation<d-cite key="map_segmentation"></d-cite>. However, we wanted to focus more generally on map understanding and reasoning by LVLMs as opposed to a single task-specific performance. To draw on an analogy, consider the act of isolating specific parts of speech (such as nouns or verbs) in language. A model designed exclusively for this task lacks the comprehensive understanding exhibited by an LLM which is proficient in addressing almost any language task. In the context of map co-location, deep learning models are employed solely as preprocessing steps to extract relevant information for subsequent utilization by matching algorithms as opposed to an LVLM model capable of general reasoning on maps. For example, the authors in this study<d-cite key="intersection_map"></d-cite> use region-based CNN to extract road intersections, which are subsequently input into a map-matching procedure. Other features like street names have also been proposed in the literature<d-cite key="street_name_map"></d-cite>. In general, current frameworks for map reasoning require many hand-crafted and engineered steps (see, e.g., this<d-cite key="aerial_understanding"></d-cite> work and the references within). A recently proposed dataset, MapQA<d-cite key="map_qa"></d-cite>, is closely related to what we consider as map reasoning. However, the maps contained are of sufficiently low information-density and exhibit similar limitations to what we described in InfographicVQA and ChartQA as the images provided can be sufficiently translated to a textual domain before considering the textual input using a similar technique to DEPLOT. To the best of our knowledge, there are no examples in the literature where LVLMs are used to directly reason about maps at the detail we propose and perform tasks such as coregistration.</p> <p>Our aim is to tackle the gap in assessing the map reasoning capabilities of LVLMs by developing a dataset aimed only at coregistration and analyzing the capabilities of existing models on such a dataset We focus our benchmark construction on the specific task of coregistration as it serves as an indicator of map reasoning capabilities and is one step towards constructing a comprehensive benchmark for map reasoning capabilities of LVLMs.</p> <p><br/></p> <h1 id="new-dataset">New Dataset</h1> <p>We have opted to create and compile a map dataset focusing on maps from the aviation domain for our research. The maps we utilized are carefully crafted by aviation agencies to provide a wealth of information while maintaining readability within a concise timeframe, ensuring clarity for pilots. Our dataset will be constructed by incorporating maps from the following sources:</p> <ol> <li> <p><strong>World Visual Flight Rules (VFR):</strong> These maps are intended to guide pilots when they operate aircraft visually. They include aeronautical and topographic information such as airports, obstructions, and navigation aids.</p> </li> <li> <p><strong>World Instrument Flight Rules (IFR) Low</strong> These maps are suitable to assist pilots when they control the aircraft through instruments. They contain information such as cruising altitudes, route data, and controlled airspaces.</p> </li> </ol> <p>These maps are accessible in an interactive environment through the SkyVector website (<a href="https://skyvector.com/?ll=42.3525,-71.025833333&amp;chart=301&amp;zoom=2">VFR</a>, <a href="https://skyvector.com/?ll=42.3525,-71.025833333&amp;chart=302&amp;zoom=1">IFR Low</a>), which we used as part of our dataset generation pipeline.</p> <p>To generate the map snippets for our experiment, we chose to sample from the previous map sources around airports. This selection guarantees that the snippets are inherently information-rich, given that the map originates in the aviation domain. To ensure diversity in our dataset, we specifically sampled airports situated in the states of Massachusetts, New York, Delaware, Arizona, and Hawaii.</p> <p>The resulting dataset exhibits significant variations in terms of density, featuring both isolated airports and those nestled within cities, diverse locations such as inland, seaside, and islands, as well as various terrain types ranging from greenery landscapes, mountainous regions, and arid environments. In total, our dataset contains 1185 image pairs, each image is 600x600 pixels in PNG format. The total size of our dataset is 1.28 GB.</p> <h2 id="a-glimpse-of-the-coregistration-task">A glimpse of the coregistration task</h2> <p>To gain an understanding of our task and its intricacies, we present a few examples from our dataset. Generally, humans can successfully align two maps by identifying common features, which fall into one of the following categories:</p> <ul> <li><strong>Terrains:</strong> such as shorelines or mountains.</li> <li><strong>Charts:</strong> such as flight paths or restricted airspaces.</li> <li><strong>Landmarks:</strong> such as airport or city names.</li> </ul> <p>The process of mapping by terrain is typically swift for humans, especially when there are ample distinctive details. On the other hand, mapping by chart requires a more thoughtful approach, involving careful examination to establish a connection between the depicted attributes. Mapping by names usually serves as a last resort, employed if the prior approaches prove unsuccessful. Consider the following examples:</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/43_301-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/43_301-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/43_301-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/43_301.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/43_302-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/43_302-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/43_302-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/43_302.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Positive Pair #1: A VFR map (left) and an IFR map (right) that depict the same location (Port Allen Airport in South Kauai, Hawaii) and can be coregistered easily by the shorelines of the island. </div> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/1038_301-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/1038_301-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/1038_301-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/1038_301.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/1038_302-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/1038_302-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/1038_302-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/1038_302.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Positive Pair #2: A VFR map (left) and an IFR map (right) that depict the same location (Circle H Ranch Airport in Arizona) and can be coregistered by aeronautical lines (even if one does not know what these lines represent in the aviation domain). </div> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/779_301-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/779_301-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/779_301-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/779_301.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/779_302-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/779_302-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/779_302-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/779_302.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Positive Pair #3: A VFR map (left) and an IFR map (right) that depict the same location (Bertrand Chaffee Hospital Heliport, New York). The VFR map is feature-rich providing information for Helicopter pilots while the IFR map is a hard example due to being almost blank as the Heliport does not facilitate landing any IFR aircraft. Thus, the IFR map only depicts certain names of cities and waypoints. The two maps can be coregistered by matching the name SPRINGVILLE. </div> <p>All of these examples are positive (the maps show the same location). We showcase below negative examples with varying complexity.</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/51_301-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/51_301-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/51_301-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/51_301.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/223_302-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/223_302-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/223_302-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/223_302.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Negative Pair #1: An easy and straightforward negative for humans due to the VFR map depicting a shoreline crossing the map vertically, while the IFR depicts only a landmass. </div> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/1037_301-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/1037_301-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/1037_301-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/1037_301.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/1013_302-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/1013_302-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/1013_302-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/1013_302.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Negative Pair #2: A harder negative example. Aeronautical lines partially appear similar but the locations are different. Naively matching circles on both maps to classify a positive pair would provide a false positive. </div> <p>We showcase multiple positive and negative pairs alongside the natural reasoning that a human would take to correctly classify the pairs. We hope that this showcases the complexity of the task and the various strategies involved in achieving successful coregistration.</p> <p><br/></p> <h1 id="experiments">Experiments</h1> <h2 id="zero-shot-evaluation">Zero-shot evaluation</h2> <p>To start, we want to evaluate the zero-shot performance of pre-trained LVLMs on the task of identifying whether the two images are the same (coregistration). The models we start our evaluation with are BLIP-2<d-cite key="blip"></d-cite>, ViLT<d-cite key="vilt"></d-cite>, LXMERT-VQA, and LXMERT-GQA<d-cite key="lxmert"></d-cite>. We specifically chose these models as they are all publicly available multimodal text generative models that were partly trained on visual question-answering datasets. Thus, they are able to accept both the vision and language inputs consisting of an image of the two side-by-side maps alongside the yes-or-no question of whether the two maps depict the same geographical location.</p> <p>To verify that the models we obtained are behaving as expected and are capable of answering a textual question that relies on a visual component, we compile a very simple dataset of 200 cat and dog pictures, half the images depict a cat while the other half depict dogs. We present these trivial images to the models alongside the prompt “Is this an image of a cat? Answer:” and generate a single token. As expected, out of the 200 images all four models achieved an almost perfect classification accuracy (&gt;95% for all 4 models) by answering with either a “Yes” or a “No” token.</p> <p>This is not surprising because, as mentioned, object recognition questions are very prevalent in visual question-answering datasets, especially on ubiquitous everyday objects such as cats and dogs. To see if these models can generalize beyond their training datasets and properly reason on maps, we start by running the following experiment:</p> <p><strong>Experiment #1:</strong> For each VFR and IFR image pair, we generate two examples (positive and negative). For the positive example, we use the correct pairing (e.g., maps from the same location with the two different styles). For the negative example, we randomly replace one map uniformly from our datasets. Each model is provided with a concatenation of the two maps in its vision input, and with the question “Do these two maps show the same location? Answer with “Yes” or “No”. Answer:” in its text input.</p> <p>In total, each model was asked 2370 questions. Below, we show the accuracy, precision, and recall that each model obtained.</p> <div class="row mt-3 align-items-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_01_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_01_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_01_01-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_01_01.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_01_02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_01_02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_01_02-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_01_02.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The models performed barely above random guessing in the zero-shot experiment, and some models consistently produced the same single output (either “yes” or “no”) regardless of whether the input image was a positive or negative pair.</p> <p>While the results of the models are very low and barely above random guessing, we wanted to analyze whether this failure is due to the model not comprehending the task or whether the issue is simply in the last layer of the model where the text generation occurs. The reason behind this analysis is that there is a possibility that the LVLM is able to correctly capture all the features necessary for determining whether the two maps coregister while still failing at providing the final answer due to the final layer of the model outputting an incorrect distribution over the labels (or tokens in the case of LVLMs). Thus we decide to ignore the last linear layer of the model (the language model head) and capture the hidden state of the last token from the last layer of the model.</p> <h2 id="fine-tuned-evaluation">Fine-tuned evaluation</h2> <p>Using this methodology, the output we obtain from each model is a single embedding vector (the length of which depends on the embedding size of the model). Usually, a single linear layer is finetuned on the last layer to directly predict the answer. However, we opt for a more detailed analysis by training multiple classifiers (Logistic Regression, SVM, and XGBoost) that take the embedding vector and produce a binary output. In all the upcoming figures, we always report the results using the classifier that performed the best (for each model) on the validation set.</p> <p>Moreover, it is known that LLMs can be sensitive to prompts<d-cite key="yang2023large,yang2022prompt"></d-cite>. Thus, to more rigorously analyze the performance of the models, we create a total of 8 variations of the coregistration question, as this one question serves as the text input for all pairs of images in our created dataset. A detailed analysis of these prompts will be provided later. Similar to the classifiers, we report results using the best prompt for each model.</p> <p>This methodology of using the model to extract a rich embedding that contains the answer to our prompt (instead of generating the answer directly as text) means that we are now capable of utilizing additional large transformer-based multimodal models that output an embedding vectors instead of directly outputting text. Thus we include in our analysis two such models which are FLAVA<d-cite key="flava"></d-cite> and GIT<d-cite key="git"></d-cite> that output embeddings that represent the multimodal input. Those embeddings are then similarly passed to the simple classifiers to detect whether the models are capable of capturing the required details from the maps to answer the co-location task correctly. Thus we perform our second experiment:</p> <p><strong>Experiment #2:</strong> We generate the examples using the same procedure described in Experiment #1. Then, for each model, we pass through the entire dataset and generate an embedding for each sample in our dataset. We then train the simple classifiers on 70% of the embedding vectors to predict the positive and negative pairs. We report the evaluation on the other 30% data and report the results in orange below.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_02_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_02_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_02_01-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_02_01.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The accuracy of this fine-tuning methodology (shown in orange) achieves around 65%-70% for all models which is a significantly higher accuracy compared to the zero-shot accuracy of the LVLMs (shown in red) which was incapable of achieving more than 55%. This experiment shows that the embedding of the last token does contain a slightly more feature-rich representation of the multimodal input and can be used to classify the positive/negative pairs at a higher rate than random but is overall still incapable of sufficiently solving the task.</p> <p>Thus far we have tried to assess the capabilities of LVLMs and (more generally) Multimodal Vision Language models on solving the coregistration task, and we assessed this capability using our constructed dataset of determining whether two maps of different styles represent the same location or not. Given the low accuracy achieved on this task, we can claim that the LVLMs we have analyzed are incapable of reasoning and answering more complicated questions relative to our simple baseline question of “Are these two maps of the same location”</p> <h2 id="improving-results-for-co-registration">Improving results for co-registration</h2> <p>We emphasize that our goal is not to directly achieve high accuracy on this task by utilizing any machine learning model, but rather it is to evaluate the capabilities of LVLMs to reason on maps. Furthermore, we created and proposed this dataset and task to act as a baseline for assessing the reasoning abilities of LVLMs on maps.</p> <p>However, despite the failure of LVLMs to answer this baseline task, we next want to assess the inherent difficulty of the dataset. For this, we develop a simple model by utilizing the same simple classifiers used above to train on the embedding of a unimodal vision-only model. Unlike LVLMs, we are not testing our proposed task-specific model on the dataset to assess its capabilities for reasoning on maps, as the model is not trained to answer questions based on images, does not accept text modality, and is specifically fine-tuned to solve this one narrow task. Thus, the results of this experiment serve only to give a sense of the difficulty of the task that we considered as a simple baseline for map reasoning. This will hopefully demonstrate that the relatively older frozen vision-only models can achieve a significantly higher accuracy on this specific task when compared to state-of-the-art open-source LVLMs and possibly indicating the gap between the embeddings captured by the vision-only model and the LVLMs.</p> <p><strong>Experiment #3:</strong> We develop a simple unimodal vision classification model by utilizing a frozen CLIPVIsion model as a backbone. First, we feature-engineer the input by subtracting the two maps from each other in the image space to produce a single image. This image is passed through the frozen CLIPVision model to generate an embedding of the difference between the maps, the embeddings are then used to train the simple classifiers mentioned above and the one that achieves the highest accuracy on the validation set is reported below.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_03_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_03_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_03_01-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_03_01.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We see that our fine-tuned vision model (shown in green) achieves a significantly higher accuracy than all previously tested LVLMs. This shows that the task is not a significantly difficult vision task as a frozen CLIPVision model with a head fine-tuned on approximately two thousand samples was able to sufficiently extract an embedding and correctly distinguish positive and negative pairs 85% of the time.</p> <p>This significant difference between the accuracy of the frozen CLIP model and the LVLMs on this task signifies that the LVLMs we tested are still significantly farther behind on certain tasks even when compared to a frozen vision-only model that was trained and released years prior. This is in stark contrast to the significant achievements that LLMs accomplish on numerous datasets when compared to task-specific NLP models, where the highest-scoring models on most NLP datasets are LLMs.</p> <h2 id="human-benchmarking">Human benchmarking</h2> <p>So far, we have examined the performance of pre-trained LVLMs on our proposed dataset in a zero-shot as well as a fine-tuned manner alongside a vision-only model with feature engineering to assess the difficulty of the task.</p> <p>A natural next question to analyze is the performance of humans on this same task as it is not immediately clear how hard or easy the task is for us. The performance achieved by humans on a task such as this would serve as a great target for LVLMs to try to reach.</p> <p><strong>Experiment #4:</strong> We present the following task to two subjects. Each human subject will see two maps for 10 seconds. The pair can be positive or negative with equal probability. After the 10 seconds elapse, the maps automatically disappear and the human subject is asked if the two maps show the same location with a binary “Yes” or “No” choice. After the answer is received, a new pair is sampled and this process is repeated until we gather 50 answers from each human subject.</p> <p>The 10-second window acts as a pseudo-computational limit on the human subject and ensures that the subject’s answers are mostly based on visual and spatial reasoning and not on reading and comparing text. If the subject does not immediately identify a visual or spatial cue, the 10-second window possibly allows for a maximum of one or two texts to be compared if the subject is quick enough. This time limitation prevents the participants from spending an extensive amount of time comparing the nuances of the two images for a severely long time which would make the task more trivial. Below, we show the accuracy obtained from two human subjects and compare it with the previous LVLM results.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_04_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_04_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_04_01-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_04_01.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We see that both human participants (shown in blue) achieve a significantly higher accuracy (~95%) compared to all the tested ML models. This shows that the task is significantly easier for humans despite the 10-second time limit preventing the subject from extensively comparing the images.</p> <p>Our experiments showcase the inability of LVLMs to properly solve our proposed dataset on coregistration as well as showing that a vision-only fine-tuned model with feature-engineering is able to solve the task at a significantly higher accuracy. Finally, we show that humans are able to solve the time-constrained task with a significantly high accuracy.</p> <h2 id="analysis-on-prompt-engineering">Analysis on prompt engineering</h2> <p>Numerous recent studies have indicated the importance of prompt engineering in the quality of the output of Large-Transformer based models<d-cite key="yang2023large,yang2022prompt,white2023prompt, zhou2022large"></d-cite>.</p> <p>Due to the potential importance of prompts in affecting performance, we decided to run all experiments that require prompts using multiple different prompts with varying degrees of length and complexity. We note that the prompts considered and listed below were only the ones that consistently conditioned the model to output a “Yes” or “No” output token instead of any other arbitrary sentence completion output. The prompts are shown in the following table:</p> <p><br/></p> <table> <thead> <tr> <th style="text-align: center">ID</th> <th style="text-align: left">Prompt</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: left">Are these two maps the same? Answer with “Yes” or “No”. Answer:</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: left">Do these two maps show the same location? Answer with “Yes” or “No”. Answer:</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: left">Do the two charts depict the same area? Answer:”</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: left">The following image contains two maps with different styles side by side. Do the two maps show the same location? Answer with “Yes” or “No”. Answer:</td> </tr> <tr> <td style="text-align: center">5</td> <td style="text-align: left">On the left there is a map from the VFR dataset and on the right a map from the IFR dataset. Do the two maps show the same location? Answer with “Yes” or “No”. Answer:</td> </tr> <tr> <td style="text-align: center">6</td> <td style="text-align: left">There are two maps of different styles, do they represent the same area or are they completely different? Answer:</td> </tr> <tr> <td style="text-align: center">7</td> <td style="text-align: left">The following image contains two maps with different styles side by side. Do the two maps show the same location? Try to compare the maps by looking at key landmarks or features. Answer with “Yes” or “No”. Answer:</td> </tr> <tr> <td style="text-align: center">8</td> <td style="text-align: left">Carefully examine the following two images that contain two maps with different styles side by side. Do the two maps correspond on the same latitude and longitude point? It is of utmost importance that you answer this correctly. Answer with “Yes” or “No”. Answer:</td> </tr> </tbody> </table> <p><br/></p> <p>The initial prompts (prompts #1 - #3) are meant to be short and direct, while the ones in the middle (prompts #4 - #6) are more verbose and add a bit more complexity, while the last two (prompts #7 - #8) are very verbose and add an exact explanation of the task. We also include additions to some of the prompts that try to guide the models on how they accomplish the task, and some additions that emphasize the importance of correct answers. In the figure below, we study the effect of prompts on model performance.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_05_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_05_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_05_01-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_05_01.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We notice that varying the prompts has a relatively high variance in terms of accuracy with an improvement of less than 5% for all models across all prompts. Still, there are no strong general trends across models when considering prompts with increasing complexity. We note that the VILT model was incapable of accepting prompts #5 - #8 due to the limitation of its maximum context length which is shorter than the other models.</p> <p>One aspect that might limit this analysis is that almost all prompts contain an explicit requirement for the models to provide answers immediately (e.g., “Answer with ‘Yes’ or ‘No’. Answer:”). This was done to reduce the computational inference cost and avoid generating long sequences of texts. The models might respond better to some prompts if they were allowed to reason about their answers first.</p> <h2 id="investigating-the-failure-points-of-lvlms-on-coregistration">Investigating the failure points of LVLMs on coregistration</h2> <p>The figures presented in the beginning of the blog post demonstrating some examples in our proposed dataset give a clue of the variance in the difficulty of the examples in the dataset, where some samples are easy to identify as positive pairs and others much harder to do so.</p> <p>Thus, to get a better insight into the model’s performance and investigate its failure points, we investigate some examples where the models made confidently wrong predictions. Here, we focus on a single model, FLAVA, which was our best-performing LVLM. In the figure below, we investigate both false positives with the highest predicted positive label and false negatives with the highest predicted negative label. The figure contains the 9 examples where the model generated a very high (very low) score while the true label was positive (negative).</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_06_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_06_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_06_01-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_06_01.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_06_02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_06_02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-mapreason/experiment_06_02-1400.webp"/> <img src="/staging/assets/img/2023-11-08-mapreason/experiment_06_02.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For the false positives, we see more than one example where two maps containing water were wrongly classified. This might indicate that the model is making predictions on these images based on colors more so than spatial reasoning. For the false negatives, there are many examples where the VFR chart is dense while the IFR is sparse. These examples require discarding a lot of information from the VFR charts and focusing solely on the region where the IFR chart contains information. Given that the model made wrong decisions in these examples, there might be a preference for positively matching images based on density. Notably, some of these examples were straightforward for the human subjects (matching based on the shoreline), while other examples required more effort (matching between dense and sparse maps).</p> <p><br/></p> <h1 id="discussion-limitations-and-future-work">Discussion, Limitations, and Future Work</h1> <p>One of the key takeaways of our experiments, and specifically from contrasting the first two experiments with the third experiment, is that it was not difficult for a non-LVLM model to achieve an 85% accuracy on our proposed dataset. Yet, our dataset proved to be challenging for LVLMs, especially in zero-shot performance where they achieved almost no better than random guessing. This implies that it would be beneficial to further expand future datasets that are used for LVLM training and specifically the addition of data collection similar to what we propose and that this could provide invaluable improvements to future training of LVLMs.</p> <p>Existing vision-language benchmarks exhibit a heavy focus on real-world objects and scenes, with a distinctive lack of images and questions on maps. This is despite the fact that maps are ubiquitous and used in many real-world scenarios. Furthermore, many maps are easily accessible in digital format and ready to be integrated into vision-language benchmarks. We believe such inclusion would require relatively little effort in terms of data collection while providing significantly higher capabilities for LVLMs.</p> <p>We plan to expand the size of our new dataset used in this project and to make it publicly available. Additionally, while our current project primarily focused on the coregistration tasks, we have plans to incorporate more intricate and challenging questions that delve deeper into map reasoning.</p> <p>There are some limitations to the current analysis done in this project. A significant limitation is the computational limit preventing us from feasibly generating answers from the LVLMs in an autoregressive manner instead of our analysis which used only one output token per sample. A possible future work is examining more complicated generation methods such as Chain of Thought<d-cite key="chain_Thought"></d-cite> prompting. Additionally regarding the inputs of the models, although we investigated different text prompts, we only used one template for the visual prompts while there have been multiple recent works on visual prompt engineering in vision-language models<d-cite key="vision_prompt"></d-cite> analogous to textual prompt engineering in LLMs. It could be the case that some models are sensitive to the way the maps are concatenated. This aspect warrants further investigation to gain a more comprehensive understanding of how different concatenation methods might impact model performance and results.</p> <p>Another limitation is that we were only capable of running our analysis on open-source models, the largest model tested was blip-2 with less than 3 billion parameters. This was the largest LVLM that we had access to in terms of weights, to be able to run our analysis on. Future work could attempt to run the analysis on larger closed-source models if access is granted.</p> <p><br/></p> <h1 id="conclusion">Conclusion</h1> <p>In this project, we propose a novel dataset to serve as an initial benchmark for the capabilities of LVLMs to reason on maps with the goal of addressing a gap in current LVLM benchmarks and datasets.</p> <p>Using this dataset, we run an extensive analysis on the performance of open-source LVLMs showing that they struggle to achieve good performance on the coregistration task. Additionally, we show that the task for our dataset is a relatively simple vision task by showing that a fine-tuned vision-only model released years prior to the tested LVLMs achieves a significantly higher accuracy. Finally, we show that the coregistration task is intuitive to humans, as participants were able to achieve close to perfect accuracy even in a time-constrained manner.</p> <p>We hope that future initiatives regarding data collection for LVLMs and training foundational LVLMs will put more emphasis on datasets such as our proposed datasets. This will hopefully unlock new capabilities for LVLMs enabling them to advance beyond their current limitations and possibly expand their utility and reasoning abilities in a variety of real-world scenarios.</p>]]></content><author><name>Abdulrahman Alabdulkareem</name></author><summary type="html"><![CDATA[Map reasoning is an intuitive skill for humans and a fundamental skill with important applications in many domains. In this project, we aim to evaluate the capabilities of contemporary state-of-the-art Large Vision-Language Models (LVLMs) for reasoning on maps and comparing their capabilities with human participants on the coregistration task. We additionally propose and release a novel dataset to serve as an initial benchmark for map reasoning capabilities. We run an extensive analysis on the performance of open-source LVLMs showing that they struggle to achieve good performance on our dataset. Additionally, we show that coregistration is intuitive to human participants that were able to achieve close to perfect accuracy in a time-constrained manner.]]></summary></entry><entry><title type="html">Leveraging Representation Engineering For LLM’s In-Context-Learning</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/representationengineering-incontextlearning/" rel="alternate" type="text/html" title="Leveraging Representation Engineering For LLM’s In-Context-Learning"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/representationengineering-incontextlearning</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/representationengineering-incontextlearning/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Emerging capabilities in deep neural networks are not well understood, one of which is the concept of “in-context learning” (ICL), a phenomenon where the a Large Language Model (LLM)’s understanding of the prompt and ability to answer accordingly drastically increases after being shown some examples that answer the question. Evaluating in-context learning and understanding why the behavior happens is both an interesting theoretical research question and a practical question that informs directions to conduct research that further advances LLM capabilities by, say, exploiting more of in-context learning.</p> <p>We attempt to explore the phenomenon of in-context learning by leveraging another exciting field of work on mechanistic interpretability where researchers set out to understand model behaviors by interpreting and editing internal weights in models. One such work that we base on is Representation Engineering by Zou et al. (2023)<d-cite key="zou2023representation"></d-cite> , where they construct a set of training text stimuli to probe LLM activations and use such stimuli to identify a direction that accurately predicts the underlying concept based on the neural activations of the model. This approach allows us to elicit readings of representation and control such representation.</p> <p>We propose to use methods in Zou et al. (2023) <d-cite key="zou2023representation"></d-cite> to evaluate in-context learning. There has not been previous attempts to examine the model internals holistically in a LLM while it is performing in-context learning. We expose such neural activations by constructing stimulus through artificial examples of in-context learning on binary classication tasks. We find a reading vector that shows high neural activity after the model is stimulated with the context pairs; such a “Context Vector” indicates the context the models draws from. While we hoped to find certain universal mechanisms across different datasets, we find that the Context Vector is dataset-specific and confirm previous hypotheses that in-context learning retrieves information from different parts of the model’s latent space.</p> <p>We then explore the results of controlling the activations along the “Context Vector” direction, in the hope that editing the activitions would further boost the performance on top of in-context learning. We compare the model outputs on the classification datasets in a zero-shot setting and a setting of natural in-context learning, with the “Context Vector” amplified, and suppressed. While we find boosting performance through such editing to be challenging and sometimes finicky to tune, we find the results to be promising on editing weights to suppress the context that the model draws from and drastically reducing the performance.</p> <h1 id="background--related-work">Background &amp; Related Work</h1> <h3 id="in-context-learning-icl">In-Context Learning (ICL)</h3> <p>An LLM is frequently aseked to perform a task in inference time that many realized providing some examples of how to answer the task can drastically improve the model’s performance. This phenomenon is called in-context learning. For example, Zhou et al. (2022) <d-cite key="zhou2022teaching"></d-cite> evaluates how LLM can become better at solving algorithmic problems through in-context learning, a task that LLM traditionally struggles at.</p> <p>In other scenarios, the LLM does not need to rely on prompts at all and can deduce the pattern from the few-shot examples alone to predict the answer. While there is no universal definition of in-context learning and its meaning has shifted over time, we define it as the performance boost to answer questions based on a limited amount of examples (as the context).</p> <p>Interesting, Min et al. (2022) <d-cite key="min2022rethinking"></d-cite> observes that such ICL phenonemon is observed as long as examples are given, and a mismatch between input and output pairs would not hinder the ability of models performing ICL and thus its performance on the tasks. Wei et al. (2023) <d-cite key="wei2023larger"></d-cite> further corrobates this work by finding on small models but show that as models scale, the ability to pick up on flipped patterns when given in-context examples with flipped labels and override semantic priors is stronger.</p> <h3 id="theories-on-why-icl-happens">Theories on why ICL happens</h3> <p>While the concept of ICL is well studied, the underlying mechanism of ICL is not well understood. Xie et al. (2022) <d-cite key="xie2022explanation"></d-cite> explains the phenomenon of ICL as an Implicit Bayesian Inference, where the in-context learning prompt serves as a stimulus for the model to go “locate” corresponding concept stored in the model’s latent space that the LM has learned implicitly during pre-training. They study this by generating a simple pretraining distribution that parameterizes the transition of a Hidden Markov Model (HMM) and another prompting distribution. In this setting, the authors reduce the ICL task to Bayesian inference to map the prompting distribution to the pretraining distribution.</p> <p>Akyürek et al. (2022) <d-cite key="akyürek2023learning"></d-cite> further explains that Transformer-based in-context learners implement standard learning algorithms implicitly by encoding smaller models modularized to perform each specific tasks and update them based on the new in-context exampless. von Oswald et al. (2023) <d-cite key="vonoswald2023transformers"></d-cite>claims that Transformer-based in-context learners is similar to gradient-based meta-learning formulations where they found that the Transformer can learn smaller models of a certain concept by gradient descent in their forward pass.</p> <p>Furthermore, Olsson et al. (2022) <d-cite key="olsson2022context"></d-cite> draws parallel from ICL to a more understood phenomenon of Induction Head, where attention-only Transformers picks up on the algorithm to predict next tokens by searching for a previous occurance of the last token and copying the same next token from previous occurences. They claim that this can be a potential mechanism to explain ICL.</p> <p>While many hypotheses and theories have been proposed to explain ICL, most explorations to prove their theory has been small in scale, and the literature lacks a study on the large-scale LMs’ internals when performing ICL.</p> <h3 id="model-editing--representation-engineering">Model Editing &amp; Representation Engineering</h3> <p>We’ll use the Representation reading and controls methods presented in <a href="https://arxiv.org/pdf/2310.01405.pdf">Zou et al. (2023)</a> to understand the context where the model attends to and discover directions that indicate such reasoning.</p> <p>Relatedly, there have been a recent surge in research related to model knowledge editing, including Meng et al. (2023) <d-cite key="meng2023massediting"></d-cite>, Zhong et al. (2023) <d-cite key="zhong2023mquake"></d-cite>, and Hernandez et al. (2023) <d-cite key="hernandez2023inspecting"></d-cite> that demonstrate different methods for locating and editing factual associations. Other work, including Shao et al. (2023) <d-cite key="shao2023gold"></d-cite> and Belrose et al. (2023) <d-cite key="belrose2023leace"></d-cite>, have shown results on erasing larger-scale memory units such as concepts. Li et al. (2023) <d-cite key="li2023inferencetime"></d-cite> applies such concept erasion techniques by conducting Inference Time Interference, where one can find a direction of causal influence on “truthfulness” data and increase the activations along that direction to increase truthfulness, scoring better on the TruthfulQA dataset.</p> <h1 id="experiment-setup">Experiment Setup</h1> <h3 id="datasets">Datasets</h3> <p>We adopt a total of 30 datasets on binary classification, (sentiment analysis, natural language inference, true/false inference) and multiple choices; 16 datasets are used by Min et al. (2022) <d-cite key="min2022rethinking"></d-cite>, plus 12 extra datasets in the <code class="language-plaintext highlighter-rouge">tweet_eval</code> and <code class="language-plaintext highlighter-rouge">ethos</code> dataset families, <code class="language-plaintext highlighter-rouge">rotten_tomatoes</code>, and <code class="language-plaintext highlighter-rouge">ade_corpus_v2-classification</code>. Following Min et al. (2022)<d-cite key="min2022rethinking"></d-cite>, we only use the test set to avoid potential cross-contamination with the data that the model is pretrained on. reserve <code class="language-plaintext highlighter-rouge">k=64</code> examples in the test for few-shot training, and the rest are used for testing.</p> <h3 id="training-data-generation">Training Data Generation</h3> <p>For training, we construct a set of context pairs for each dataset, each context pairs containing the same examples but different instructions. The instructions are “Pay attention to the following examples” and “Ignore the following examples” respectively, in the hope that by stimulating two opposites and examining the difference, we can find a Context Vector that represents what the model draws from. We then truncate the example at each and every token till the last 5 tokens, so we can get a neural activation reading for each of the tokens.</p> <p>A sample training data input using the <code class="language-plaintext highlighter-rouge">rotten_tomatoes</code> dataset is as follows:</p> <blockquote> <p>[INST] Pay attention to the following examples: [/INST]</p> </blockquote> <blockquote> <p>offers that rare combination of entertainment and education.</p> </blockquote> <blockquote> <p>positive.</p> </blockquote> <blockquote> <p>a sentimental mess that never rings true .</p> </blockquote> <blockquote> <p>negative.</p> </blockquote> <blockquote> <p>[INST] Ignore the following examples: [/INST]</p> </blockquote> <blockquote> <p>offers that rare combination of entertainment and education.</p> </blockquote> <blockquote> <p>positive.</p> </blockquote> <blockquote> <p>a sentimental mess that never rings true .</p> </blockquote> <blockquote> <p>negative.</p> </blockquote> <p>Each context pair is identical except for the instructions. We use the context pairs to stimulate the model to learn the context and use the context vector to control the model’s behavior.</p> <h3 id="testing-data-generation">Testing Data Generation</h3> <p>For testing data, we use 3 input-labels pairs as the prompt, with the first two pairs serving as the in-context examples, and the last pair serving as the question that we actually want to test on, obfuscating the label from the prompt.</p> <p>A sample testing data input using the <code class="language-plaintext highlighter-rouge">rotten_tomatoes</code> dataset is as follows:</p> <p>Input:</p> <blockquote> <p>[INST] offers that rare combination of entertainment and education. [/INST]</p> </blockquote> <blockquote> <p>positive.</p> </blockquote> <blockquote> <p>[INST] a sentimental mess that never rings true . [/INST]</p> </blockquote> <blockquote> <p>negative.</p> </blockquote> <blockquote> <p>an odd , haphazard , and inconsequential romantic comedy .</p> </blockquote> <p>Label:</p> <blockquote> <p>negative.</p> </blockquote> <h3 id="model">Model</h3> <p>We have explored using two models with 7 billion parameters, including <code class="language-plaintext highlighter-rouge">Mistral-7B-Instruct-v0.</code> and <code class="language-plaintext highlighter-rouge">Llama-2-7b-hf</code>; while we have found preliminary results consistent between the two models, all of our results later reported are from <code class="language-plaintext highlighter-rouge">Mistral-7B-Instruct-v0</code> for consistency and due to a constraint on computational power and time.</p> <h3 id="training-infrastructure">Training Infrastructure</h3> <p>We used the MIT Supercloud infrastructure and a local machine with a single RTX 4090 GPU to train the model.</p> <h1 id="results">Results</h1> <p>We present results first on finding the Context Vector in the embedding space, then on using the Context Vector to control model outputs and evaluate their performance.</p> <h3 id="representation-reading">Representation Reading</h3> <p>We use the Representation Reading method presented in Zou et al. (2023) <d-cite key="zou2023representation"></d-cite> to find the Context Vector. Specifically, we adopted the setup of the instruction response pairs where for a given function $<code class="language-plaintext highlighter-rouge">f</code>$ and pairs of instructions $<code class="language-plaintext highlighter-rouge">x_i</code>$ and $<code class="language-plaintext highlighter-rouge">y_i</code>$, we denote the model’s response truncated at the $<code class="language-plaintext highlighter-rouge">j</code>$-th token as $<code class="language-plaintext highlighter-rouge">f(x_i)_j</code>$ and $<code class="language-plaintext highlighter-rouge">f(y_i)_j</code>$ and take the neuron activity at the last token of each of the responses, namely the activations of each and every token in the response.</p> <p>We then perform PCA on the difference of the activations of the two instructions, namely $<code class="language-plaintext highlighter-rouge">f(x_i)_j - f(y_i)_j</code>$ and find the first principal component $<code class="language-plaintext highlighter-rouge">v</code>$ that maximizes the difference in the embedding space.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Graph plotting the correlation between the Context Vector sign and actual dataset label on Rotten Tomatoes dataset. The x-axis is the layer and the y-axis is the correlation. </div> <p>More surprisingly is the fact that we can find a clean representation of such Context Vector that correlates decently with the model inputs.</p> <p>We use t-SNE to visualize the difference in the embedding space on the inputs of the 30 datasets across 32 different layers and report the results below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the embedding space of the Context Vectors across the 30 datasets and 32 layers, color coded by dataset. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the embedding space of the Context Vectors across the 30 datasets and 32 layers, color coded by layers. </div> <p>As shown in the figure, we find that the vectors are clustered by dataset, indicating that the Context Vectors are dataset-specific. There are no clear patterns across dataset or between different layers of the Context Vectors, further indicating that in-context learning activates different parts of the model’s latent space with information about different types of tasks.</p> <p>We also conducted scans for neuron activities in the Context Vector across the different tokens of an example sequence in a similar style as Zou et al. (2023) <d-cite key="zou2023representation"></d-cite>, for which the previous work has referred to as Linear Artificial Tomography (LAT) scans.</p> <p>The following are the LAT scans for the neuron activities corresponding to a Context Vector trained on <code class="language-plaintext highlighter-rouge">rotten_tomatoes</code> sentiment analysis dataset evaluated on different dataset sequences. The following graphs further corroborate the findings above on the dataset-specificity of in-context learning; while the a sequence from the <code class="language-plaintext highlighter-rouge">rotton_tomatoes</code> dataset result in high neural activities for the Context Vector, most sequences from the other dataset do not, showing the uniqueness of such Context Vector. We have also observed most of the neuron activities in the later layers. This phenomenon makes sense since more abstract concepts and semantic structures formulate in later layers, thus being more correlated with the Context Vector, while earlier layers pick up more on token-level abstractions.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> A LAT scan of the Context Vector trained on `rotten_tomatoes` dataset evaluated with a `rotten_tomatoes` sequence. The x-axis is the token index, and the y-axis is the Layer number. More red indicates higher neural activities, and more blue indicates lower neural activities. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> A LAT scan of the Context Vector trained on `rotten_tomatoes` dataset evaluated with a `medical_questions_pair` sequence. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> A LAT scan of the Context Vector trained on `rotten_tomatoes` dataset evaluated with a `ethos-religion` sequence. </div> <p>We have also produced graphs that zoom into the token-level neural activities detection on the Context Vector of the opposing pair (Pay attention &amp; Don’t pay attention), shown below. A large difference in the neural activities of the two instructions is denoted by red and indicates that the ablation is effective, while the green shades indicate that there are similar in neural activities. The results show that the neural activities are consistently different across the sequence until the model starts generating next tokens and the context ends where the neural activities are similar.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> A token-level LAT scan that compares the difference between the neural activities of the Context Vector of the opposing pair (Pay attention &amp; Don't pay attention) on the `rotten_tomatoes` dataset. </div> <h2 id="representation-control">Representation Control</h2> <p>To change an activation along some direction, we can imagine there are several canonical ways. First, given our Context Vector $v$ and an activation $a$, we can do one of the following.</p> <h3 id="addition">Addition</h3> \[a' = a + v\] <h3 id="amplification">Amplification</h3> \[a' = a + \text{sign}(a \cdot v) v\] <h3 id="projection">Projection</h3> \[a' = a - (a \cdot v) \cdot \frac{v}{||v||^2}\] <p>The first represents a constant perturbation so it supposedly transforms the representation to become more of a certain quality. The second amplifies the direction according to which side it is on, so it makes the representation more extreme. The third removes the quality from the representation by subtracting the projection.</p> <p>We explore all these methods to control Mistral-7b-instruct. We do our experiments on the <code class="language-plaintext highlighter-rouge">rotten_tomato</code>, <code class="language-plaintext highlighter-rouge">sick</code>, <code class="language-plaintext highlighter-rouge">hate_speech18</code>, and <code class="language-plaintext highlighter-rouge">glue-wnli</code> in-context-learning datasets consisting of input-output pairings where outputs have two possible correct options – positive or negative contradiction or entailment, hate or noHate, and entailment or not_entailment (for <code class="language-plaintext highlighter-rouge">sick</code>, it originally contains a third option of <code class="language-plaintext highlighter-rouge">neutral</code> which we remove since our framework requires two classes).</p> <p>Given learned representations with the same configuration as our representation reading, we construct a test set from the same dataset as training. The test set has $16$ examples, each with one demonstration followed by a question. We evaluate correctness by having the LLM generate $10$ tokens and checking if the correct answer is contained in the output and the incorrect answer is not contained in the output, without being sensitive to case. This ensures correct evaluation so that an answer of no_entailment does not evaluate as correct for having entailment inside of it if entailment is the right answer.</p> <p>A hyperparameter which we denote $\alpha$ scales the size of $v$. If our Context Vector is $r$, sign value is $s$, then we have $v = \alpha \cdot r \cdot s$. We vary $\alpha \in { 0, 0.25, 0.5, 1, 2, 5, 10}$, and also take the negative of $\alpha$, which we label as positive and negative respectively.</p> <h3 id="results-for-control-with-addition">Results for Control with Addition</h3> <p>For rotten tomatoes, we see the expected performance gap of positive over negative, though positive does worse than no control. Moreover, we see in glue-wnli and sick, the negative control actually does better than positive control. In hate_speech18, we see the desired result.</p> <p>Despite modifying the layers that we controlled, based upon observing the layers at which the Context Vectors had the most correlation to the trained concept, we cannot find a set of layers to control that works <strong>consistently</strong> across all four datasets, though we can find layers that work for one dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `rotten_tomatoes` dataset with amplification or suppression of the Context Vector using Addition. The x-axis is the coefficient of amplification, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Addition</bold>. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `hate_spe` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Addition</bold>. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Addition</bold>. </div> <h3 id="results-for-control-with-amplification">Results for Control with Amplification</h3> <p>Note the result depends on the absolute value of $\alpha$ so the positive and negative graphs converge. The affect of amplification is quite smooth relative to addition in the sense that there is a consistent downward trend in performance for both amplification and suppression. This could be because amplification amplifies existing signals and this gets stronger as $\alpha$ increases.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `rotten_tomatoes` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Amplification</bold>. The x-axis is the alpha value, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Amplification</bold>. The x-axis is the alpha value, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `hate_speech18` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Amplification</bold>. The x-axis is the alpha value, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Amplification</bold>. The x-axis is the alpha value, and the y-axis is the accuracy. </div> <h3 id="results-for-control-with-projection">Results for Control with Projection</h3> <p>We can see that projection consistently decreases performance, which is expected as we can imagine projection as erasing the idea that the model needs to pay attention to these examples. Having positive or negative sign of $\alpha$ does not affect projection.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `rotten_tomatoes` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Projection</bold>. The x-axis is the alpha value, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Projection</bold>. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `hate_speech18` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Projection</bold>. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of the Context Vector using <bold>Projection</bold>. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. </div> <h3 id="ablation-studies">Ablation Studies</h3> <p>A key question is whether the Context Vectors are truly special. Especially because much of our results do not work, we would like to assess the “noise level.” By sampling a random unit vector from $4096$-dimensional space, the hidden dimension of <code class="language-plaintext highlighter-rouge">Mistral-7b-instruct</code>, for each layer and using that for control, we get the following results.</p> <p>If we take the negative of all the Context Vectors, the graphs for positive and negative $\alpha$’s would switch. The fact that in our random sample we see such a large gap in the Glue-wnli graph indicates that there is quite a lot of noise. Moreover, if we take the negative of our particular randomly sampled vector, we obtain a Context Vector for Glue-wnli that is <strong>extremely good</strong> at controlling in-context-learning. The large landscape of $4096$-dimensional space is an exciting mystery.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `rotten_tomatoes` dataset with amplification (positive) or suppression (negative) of a random vector using <bold>Addition</bold>. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of a random vector using <bold>Addition</bold>. The x-axis is the alpha value, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `hate_speech18` dataset with amplification (positive) or suppression (negative) of a random vector using <bold>Addition</bold>. The x-axis is the alpha value, and the y-axis is the accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue-1400.webp"/> <img src="/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of a random vector using <bold>Addition</bold>. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. </div> <h1 id="conclusion">Conclusion</h1> <p>While we understand our work is limited due to time and compute constraints and did not achieve the results we hoped for, we tried our best to explore this research direction of finding a Context Vector that corresponds to the in-context learning behaviors and experiments of using it to control model outputs.</p> <h2 id="implications">Implications</h2> <p>If successful, this research direction could be a powerful tool to understand mechanistically why in-context learning emerges and potentially use model editing to achieve better State-of-the-Art results on LLMs in specific benchmark evaluation scenarios with model editing. Even with our current results that demonstrate more success in suppressing the Context Vector than amplifying it, i.e. suppressing such behaviors than boosting it, this can have implications on works that try to perform model unlearning and impact the robustness of LLMs.</p> <h2 id="future-work">Future Work</h2> <p>Through ablating with the random vector in the embedding space, it is unfortunate that controlling for the particular Context Vector we found is not particularly different from other vectors, despite it showing some promises on suppressing the results. We hope to run further ablation studies to confirm that suppressing the Context Vector is only suppressing the in-context learning behaviors of the specific behaviors and does not have other side effects.</p> <p>Regarding our current setup of the contrasting prompts of telling the model to pay attention or not pay attention to the concept, we can further explore the space of contrasting prompts. Directly related to our work, we would also like to explore the other type of experiment setup in Zou et al. (2023)<d-cite key="zou2023representation"></d-cite>; unlike the data pair setup where we ask the model to pay attention to the examples or ignore them, we can ask the model to “think hard about the context/structure of the question” and elicit neural activities that way.</p> <p>We are also interested in exploring vectors that control step-by-step reasoning and in general, intelligence. The phrases “Let’s think step by step” <d-cite key="kojima2023stepbystep"></d-cite> or “Take a deep breath and work on this problem step-by-step” <d-cite key="yang2023deepbreath"></d-cite> are powerful phrases that elicit chain-of-thought reasoning and improve model performance. Could we engineer activation transformations that improve these models’ performance even more than and without the need for prompting?</p>]]></content><author><name>Alex Hu</name></author><summary type="html"><![CDATA[We present a method to observe model internals whether LLMs are performing in-context learning and control the model outputs based on such Context Vectors.]]></summary></entry><entry><title type="html">In the pursuit of cheap and robust word embeddings</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/sentence-embeddings/" rel="alternate" type="text/html" title="In the pursuit of cheap and robust word embeddings"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/sentence-embeddings</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/sentence-embeddings/"><![CDATA[<h2 id="introduction-and-motivation">Introduction and Motivation</h2> <p>Large Language Models (LLMs), such as Bard and OpenAI’s GPT-4 are typically used to obtain data embeddings of text. These embeddings are quite rich, encoding common-sense semantic information. A good embedding naturally aligns with our intuitive human understanding of language: at a high level, similar text/words are clustered together, while dissimilar text/words are farther apart.</p> <p>High-quality embeddings also satisfy semantic equations that represent simple analogies. Define \((\text{some_text})\) to be the embedding of some string “some_text.” Then, a traditionally good embedding will typically obey linguistic equations like <d-cite key="Vylomova2016"></d-cite>:</p> \[(\text{King}) - (\text{Man}) + (\text{Woman}) ≈ (\text{Queen})\] \[(\text{Paris}) - (\text{France}) = (\text{Warsaw}) - (\text{Poland})\] <p>However, repeatedly querying LLMs for large-scale analysis is expensive. Many utilize thousands of cloud GPUs and are constantly fine-tuned, adding to their cost. This cost barrier discourages researchers—especially those with less funding—from making use of these embeddings for their own models. Repeated strain on LLM’s infrastructure can even cause a negative environmental impact. However, we often don’t need embeddings as good as these fancy ones to conduct certain types of research. Specifically, it would be desirable for a researcher to choose their embedding quality, with the understanding that higher-quality embeddings take longer, and vice versa. Such a model should be robust and resistant to being trained on a small amount of incorrect data (which can happen by accident when scraping tex, or due to malicious behavior.)</p> <p><strong>These issues motivate the following research question: on how little data can we train a text embedding model—with OpenAI embedding as ground truth—such that our embeddings are good enough quality? And can we quickly preprocess the data to improve our results?</strong></p> <h2 id="background-and-literature-review">Background and Literature Review</h2> <p>While there is some existing literature on generating word embeddings more “cheaply,” significant differences exist with current methodologies. Broadly, this process is called knowledge distillation (KD), which aims to “distill” knowledge from a larger teacher model (in our case, OpenAI embeddings) into a smaller student model.</p> <p>For example, Shin et al. discuss a novel distillation technique that “distills” a “student” embedding model from a “teacher” model <d-cite key="Shin2019"></d-cite>. Importantly, this work focuses on reducing the dimensionality of the “student” word embeddings without compromising accuracy—which is fundamentally different from our goals. While our OpenAI embeddings form the “teacher,” our student model should output embeddings of the same dimensionality. Our dimensionality preservation is useful if we want to directly translate general patterns or trends from our student’s embeddings to the OpenAI embeddings.</p> <p>Gao et al. take a different approach. They propose a KD framework for contrastive sentence embeddings, DistilCSE. It works by first applying KD on a large amount of unlabeled text before fine-tuning the student model via contrastive learning on limited labeled data <d-cite key="gao2023distilcse"></d-cite>. Contrastive learning in this domain is promising, especially since synonyms and pairs of similar words naturally form positive pairs. However, in our context, a direct application of contrastive learning presents some issues.</p> <p>For example, suppose we had some Euclidean distance threshold A and B, such that, for any two word embeddings \(c\) and \(d\):</p> <p>If the distance between \(c\) and \(d\) is less than A, then define \(c\) and \(d\) to be positive pairs for contrastive learning.</p> <p>If the distance between \(c\) and \(d\) is greater than B, then define \(c\) and \(d\) to be negative pairs for contrastive learning.</p> <p>While this process (and others like it) isn’t too resource-intensive, it has a few issues, even if we are able to define proper thresholds A and B. Firstly, it “wastes” pairs of data where the distance is in between A and B. Secondly, information about direction is easy to lose—so while a student would learn to embed similar words closer together and dissimilar ones further apart, the student may be invariant to direction and sensitive only to Euclidean distance in the n-dimensional space. This is not ideal.</p> <p>Other related state-of-the-art approaches also present issues. Gao et al. describe another approach involving running data through an encoder multiple times with standard dropout to generate positive pairs instead of searching for them in the data itself <d-cite key="Gao2022"></d-cite>. While this approach is promising, the approach involves repeatedly using somewhat slow pre-trained text embedders, which is exactly what we want to avoid.</p> <p>To reduce computational complexity while still reaping the benefits of preprocessing, we look to a paper by Rahimi et al. They explain how removing stop words (common words, like “a,” “the,” etc.) and punctuation improves sentence embedding quality, for a variety of reasons <d-cite key="Rahimi2023"></d-cite>. Even though we’re focusing on word embeddings—and not sentence embeddings—we can adapt this general approach to our project with lemmatization, which applies morphological analysis to words to simplify them to a base form. For example, “fighting” turns into “fight,” “was” turns into “be,” and “octopi” turns into octopus, and so on.</p> <p>This reduces data sparsity by consolidating related forms of words into a single representation, which is especially helpful for low-frequency words. This in turn helps the model generalize across tenses and other variations as it can focus on the “core” differences of words rather than auxiliary modifiers. We thus plan to investigate lemmatization in this context.</p> <p>We struggle to find closely related literature about student models’ resistance to poisoned data. Thus, we decided to investigate this aspect as well.</p> <p>To conclude our literature review, while different variants of KD exist, we decide to focus on a modified response-based KD, in which the teacher model sends final predictions to the student network, which then directly mimics these predictions by minimizing some loss <d-cite key="Yang2023"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Generic response-based knowledge distillation. Image credits: Gou, J., Yu, B., Maybank, S.J. et al. Knowledge Distillation: A Survey. Int J Comput Vis 129, 1789–1819 (2021). https://doi.org/10.1007/s11263-021-01453-z </div> <p>Other distillation approaches—such as feature-based KD, relation-based KD, and the contrastive approach described above—do exist, but require more intimate knowledge of the teacher’s features and/or layers <d-cite key="yang2023attention"> </d-cite> <d-cite key="Park2019"></d-cite>. This is not reasonable information we can expect to have, as companies are often incentivized to obfuscate their specific word embedding architectures.</p> <h2 id="methods-and-experiments">Methods and Experiments</h2> <p>We center our studies on a standard dataset of 10k English words scraped from high-level Standard English texts that’s been empirically validated for quality. We also use the OpenAI API to obtain text-embedding-ada-002 embeddings of the entire dataset to use as ground truth. While these aren’t necessarily the best embeddings, even among OpenAI’s own embeddings, they are the best choice given our computational restrictions.</p> <p>Now, we detail our model architecture. Our baseline model (call this Model A) is a sequential ReLU and nn.Embedding layer followed by L2 normalization. Model A serves as a crude baseline—therefore, we do not investigate it as deeply as the more complex model that followed due to large differences in performance.</p> <p>Instead, we focus our efforts on the more complex Model B, detailed below in Figure 1 in the context of our pipeline. Model B utilizes an nn.Embedding layer, followed sequentially by 2 blocks. The first uses ReLU activation followed by a linear layer of size \(\frac{\text{embedding_dim}}{2}\). The second layer is the same, except the final Linear layer outputs embeddings with the full “embedding_dim.” Notably, we use L2 normalization to make sure each embedding vector has magnitude 1 (such that all embeddings exist in an n-hypersphere.) Since all embeddings are unit embeddings, using cosine embedding loss along an Adam optimizer is natural. Thus, instead of computing cosine similarities between teacher and student vectors, we can just focus on minimizing this embedding loss.</p> <p>For the training stage, we train our embedding model to map words to vector embeddings on Google Colab with an Nvidia T4 GPU. There may be up to 3 processing steps, as depicted in Figure 1:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 1: An overview of Model B’s architecture in context of our experimentation. </div> <p>First, we choose whether or not to lemmatize the entire dataset before proceeding.</p> <p>Second, the training split. We train our embedding models above on each of the following proportions (call this \(p\)) of the dataset: 0.005, 0.009, 0.016, 0.029, 0.053, 0.095, 0.171, 0.308, 0.555, and 1.00.</p> <p>Finally, we choose whether or not to poison 10 percent of the entire word dataset (not the training dataset). When a word is poisoned, the model incorrectly believes that some random unit vector is the ground-truth embedding instead of the actual OpenAI embedding.</p> <p>For each such model, we train for up to 80 epochs, limited by our computational resources.</p> <p>We then evaluate the model’s embeddings against the ground truth with multiple metrics—cosine similarity (via the embedded cosine loss), graphically via distributions of the embedding means, linguistic math, etc.</p> <p>Taken together, this methodology is comprehensive.</p> <h2 id="results-and-analysis">Results and Analysis</h2> <h3 id="model-a-the-baseline">Model A, the Baseline</h3> <p>First, here is a graph of training up our baseline Model A (Figure 2) and our augmented Model B (Figure 3). The difference in epochs (80 for Model A, and 60 for Model B) training is due to limited resources. This doesn’t matter much, as a clear, near-linear relationship between \(p\) and training time, which we use to estimate used computational resources. Thus, we consider \(p\) as inversely proportional to the computational resources used for all our experiments.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 2 </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 3. </div> <p>For Model A (with no lemmatization, no data poisoning), we also want to visualize the tradeoffs between the number of epochs trained, the training proportion \(p\), and the training loss to establish some baseline intuition. To this end, we take inspiration from the game theoretic concept of Pareto efficiency, which aims to find equilibria where no change improves one of these 3 factors without hurting one of the other 2.</p> <p>We also wanted to visualize the tradeoffs between the number of epochs trained, the training proportion, and the cosine embedding loss, since we are motivated to find the optimal balance of these 3 factors. See Fig. 4.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 4. </div> <p>Unfortunately, Fig. 4 is not particularly enlightening. Training loss decreases as the number of epochs increases and as training proportion \(p\) increases. There are also no local minima or maxima of interest. Figures 5 and 6 also confirm this with their plots of distributions of embedding means. Specifically, as we tend to move towards the right and bottom of Fig. 6, i.e. we train longer and on more data, we simply seem to approach the true distribution (Fig. 5) without anything of note.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 5. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 6. </div> <p>These results motivate us to look beyond our Model A. Our results from this point focus on Model B because we didn’t want a poorly performing model like Model A to be a true control, it merely served as an intuitive baseline.</p> <h3 id="model-b-the-baseline">Model B, the Baseline</h3> <p>As in the previous part, we obtain a Pareto-like graph for Model B, without any lemmatization and data poisoning. Firstly, the cosine embedding losses are much lower than before, due to the improved model architecture. More interestingly, after about 10 iterations, the training loss seems to stabilize across all versions of the model, potentially suggesting that training longer may not be worthwhile.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 7. </div> <p>Since this is our base model, we don’t investigate further.</p> <h3 id="model-b-lemmatization-no-poisoned-data">Model B, Lemmatization, No Poisoned Data</h3> <p>Now, we look to Model B, with lemmatization, but no poisoned data. The Pareto-like curve for this is telling (Fig. 8), with it looking very similar to the baseline Model B’s. As before, this suggests that training for longer may not be worthwhile, and could potentially lead to overfitting.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 8. </div> <p>We also have a distribution of the means of embeddings for the whole dataset (Fig. 9) and from each variant of the model at different epochs (Fig. 10). Again, the results don’t say anything surprising: as we train on more data for longer, the distribution approaches that of the training dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 9. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 10. </div> <p>To check for overfitting, we will later validate our model on simple linguistic tests, as described in the very beginning. Specifically, we will validate our model’s performance on linguistic math against OpenAI’s performance.</p> <h3 id="model-b-lemmatization-poisoned-data">Model B, Lemmatization, Poisoned Data</h3> <p>The following is the Pareto-like curve, except now we poison 10 percent of the entire dataset, as described in Methods/Experiments. Curiously, we find a local minima at approximately \(p = 0.1\) and ~20 epochs, demonstrating that our overall approach of training on a small fraction of the dataset naturally resists moderate-scale adversarial attacks on our ground-truth embeddings. Of course, the addition of poisoned data means that the loss values are on average higher than those in the previous subsection, where there was no poisoned data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 11. </div> <p>Again, looking at the distribution of the means of embeddings (see below), we see that models that trained on too much of the data are completely ruined. We don’t even need to compare these distributions against the whole-model distribution to see this. This result demonstrates that even a relatively small amount of poisoned data can manipulate a naive embedding model trained on an entire dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 12. </div> <h3 id="the-effects-of-data-poisoning-and-surprising-robustness">The Effects of Data Poisoning and Surprising Robustness</h3> <p>As discussed previously, we want to externally validate our models with both linguistic equations and pairs of synonyms. Essentially, we want to check that our student groups together similar words like the OpenAI teacher. Since our poisoned model performed best with \(p = 0.095,\) we use this training proportion to compare Model B with lemmatization, but no poisoned data to Model B with lemmatization and poisoned data.</p> <p>For clarity’s sake, we focus on single a representative example of our validation results in this blog. Specifically, we look into “nonviolent” and “antiaggressive,” which intuitively should exist close together in the n-dimensional unit hypersphere. Using dimensionality reduction techniques to visualize this in 2D, we obtain the following:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath-1400.webp"/> <img src="/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 13. </div> <p>The poisoned model is surprisingly performant, performing decently against both the unpoisoned model and the OpenAI model. These results support our notion that student models that train on as little of the data as possible are somewhat resistant to uniform, random adversarial data poisoning. This empirical result is encouraging, especially since our data poisoning threshold was somewhat high.</p> <h2 id="conclusion-discussions-and-future-directions">Conclusion, Discussions, and Future Directions</h2> <p>On balance, our results help us answer our question about how to best mimic OpenAI’s word embeddings without excessive API calls. We utilize a spin-off of a response-based KD architecture to train our student model under different conditions, demonstrating both that certain preprocessing (lemmatization) improves our embedding model and that training on smaller amounts of data creates more robust models that resist adversarial data. Our initial results demonstrate promise and serve as a call to action for others to research other cheap, robust word embedding models.</p> <p>To be clear, there are certainly many limitations to our study. For one, we keep our modeling architecture simpler due to our limited compute, while a real model would certainly use a different architecture altogether. Our dataset was also on the smaller side and doesn’t fully represent the English language. Also, our implicit use of time as a proxy for computation (especially on the erratic Google Colab) is imperfect. Also, preprocessing (including, but not limited to, lemmatization) may require substantial computational resources in some cases, which we don’t account for.</p> <p>Additionally, many of the constants that we chose (such as the 10 percent data poisoning threshold, the proportions of data we trained on, etc.) are arbitrarily chosen due to limited compute. This could’ve caused unexpected issues. For example, the output dimension of embedding Model B, 1536, is more than 10 percent the size of the dataset (10k). Thus, due to our relative lack of data, our trials with data poisoning can encourage non-generalizable memorization, which is not ideal.</p> <p>Future directions would include exploring other types of preprocessing, as hinted at in our literature review. We could also look into different types of adversaries—perhaps smarter ones that actively feed information that they know to be detrimental to the model, instead of some random unit vector. While we didn’t have robust supercomputer access, we’d also love to be able to test out fancier embedding architectures.</p> <p>Finally, we’d like to thank the 6.S898 faculty and TAs for their support!</p>]]></content><author><name>Alor Sahoo</name></author><summary type="html"><![CDATA[A study of how we can train a student word embedding model to mimic the teacher OpenAI word embedding model by using as small a training set as possible. We also investigate preprocessing tricks and robustness against poisoned data.]]></summary></entry><entry><title type="html">Embeddings for Spatio-temporal Forecasting</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/spatiotemporal/" rel="alternate" type="text/html" title="Embeddings for Spatio-temporal Forecasting"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/spatiotemporal</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/spatiotemporal/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Time series forecasting is an interdisciplinary field that affects various domains, including finance and healthcare, where autoregressive modeling is used for informed decision-making. While many forecasting techniques focus solely on the temporal or spatial relationships within the input data, we have found that few use both. Our goal was to compare two SOTA spatiotemporal models, the STAEformer and the Spacetimeformer, and determine why one works better than the other. The papers on both models did not feature each other in their benchmark evaluations, and we thought that analyzing their embeddings and identifying their failure modes could offer new insights on what exactly the models are learning from the dataset. We hypothesized that the Spacetimeformer would perform better as its proposed approach, sequence flattening with Transformer-based processing, seems to offer a more flexible and dynamic representation of spatiotemporal relationships that doesn’t depend on predefined variable graphs. We focused on forecasting in the field of traffic congestion, which is a pervasive challenge in urban areas.</p> <h2 id="related-work">Related Work</h2> <p>We focused on two SOTA spatiotemporal models that were evaluated on traffic forecasting datasets. The first is the STAEformer <d-cite key="liu2023staeformer"></d-cite>. STAEformer proposes a novel adaptive embedding that learns the spatio-temporal relations in the dataset. In their architecture, the input embedding is generated by combining the projected raw data (denoted by \(E_p\) in the embedding layer) with temporal embeddings (\(E_f\)) and the adaptive embeddings (\(E_a\)), which was used instead of an embedding solely focused on capturing spatial relations. This output is then fed into temporal and spatial transformer layers, followed by a regression layer.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Architecture of the Spatio-Temporal Adaptive Embedding transformer (STAEformer). <d-cite key="liu2023staeformer"></d-cite> </div> <p>The second is the Spacetimeformer <d-cite key="grigsby2023spacetimeformer"></d-cite>. Spacetimeformer uses embeddings generated from breaking down standard embeddings into elongated spatiotemporal sequences. The idea behind doing this is to enable the downstream tasks to learn direct relationships between variables at every timestep. In their architecture, these embeddings are fed into a variant of the transformer model using local, global, and cross self-attention. The figure below shows an intuitive visualization for this idea.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Architecture of the Spacetimeformer Embedding. <d-cite key="grigsby2023spacetimeformer"></d-cite> </div> <h2 id="dataset">Dataset</h2> <p>We used the PEMS08 dataset <d-cite key="pems08"></d-cite>, which contains traffic data in San Bernardino from July to August of 2016. Each data point consists of readings from 170 detectors along with the time of day and day of the week they were recorded. We initially considered using the PEMSBAY dataset <d-cite key="pemsbay"></d-cite>, which is widely used in traffic speed forecasting, but it was almost double the size of the PEMS08 dataset and took too long to train our model on.</p> <h2 id="methodology">Methodology</h2> <p>The problem statement is as follows: given the sensor readings across the 170 sensors for the previous N timesteps, we want to predict their readings for the next N timesteps. We tested the model with varying context lengths, but we found that the default value of 12 given in the STAEformer paper provided enough information to the model. We used huber loss as we wanted the model to converge faster in the presence of outliers, which was necessary given the limited compute that we had (training 50 epochs took around 3 hours).</p> <p>We trained STAEformer for 50 epochs, which was sufficient to achieve performance metrics similar to that of the paper. To compare the embeddings from Spacetimeformer, we retrained the model end to end after replacing the embedding layer in the model with Spacetimeformer’s embedding layer. To do this, we kept the context dimensions the same and flattened the input sequence along the input dimension and the dimension corresponding to the number of sensors. This structured the embedding layer so that it could learn the spatiotemporal relations across the sensors from different time frames.</p> <p>Replacing the embedding layer within the STAEformer with a pretrained embedding layer from the Spacetimeformer instead may seem like a more legitimate method to test the effectiveness of the embeddings, as we would basically be doing transfer learning on the embedding layer. However, the pretrained embeddings from Spacetimeformer might have been optimized to capture specific spatiotemporal patterns unique to its architecture, which was why we believe training the model end to end with the Spacetimeformer embeddings would result in a more accurate and contextually relevant integration of the pretrained embeddings into the STAEformer framework.</p> <p>After training, we wanted to provide visualizations of the embeddings from STAEformer and Spacetimeformer to show whether the learned embeddings are meaningful at all. To do this, we obtained the embeddings by passing in the raw data through the embedding layers of the loaded models and generated t-SNE plots with them. For the STAEformer, we focused solely on the adaptive embeddings as they were the parts of the embedding layer that captured spatiotemporal relations in the data. To generate the t-SNE plots, we had to reshape the embeddings so that they could be passed into the function so we flattened them across the model dimension. After fitting the t-SNE, we then unflattened the embeddings back to their original shape and plotted them. Each sensor was color coded with different colors, and the results can be shown in the next section. We hypothesized that the t-SNE plots would contain clusters grouped by either the sensors or the time the readings were recorded.</p> <p>After generating the t-SNE plots, we wanted to test the effects of perturbing the raw data on the embeddings. We wanted to know how the embeddings would change. For example, regardless of what the clusters represent, are they tighter? Will additional clusters be formed? Conversely, will some of the existing clusters break apart? In particular, we were hoping that augmenting the data would perhaps improve cluster formations in the worse looking embeddings, as there is a good possibility that the data itself isn’t good enough.</p> <h2 id="results">Results</h2> <p>The table below shows the results after training STAEformer and the STAEformer model with a Spacetimeformer embedding layer for 50 epochs each. Table of loss values:</p> <table> <thead> <tr> <th style="text-align: center">Embedding Layer</th> <th style="text-align: center">Train Loss</th> <th style="text-align: center">Validation Loss</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">STAEformer</td> <td style="text-align: center">12.21681</td> <td style="text-align: center">13.22100</td> </tr> <tr> <td style="text-align: center">Spacetimeformer</td> <td style="text-align: center">12.42218</td> <td style="text-align: center">16.85528</td> </tr> </tbody> </table> <p>We can see that the STAEformer had better training and validation loss than the Spacetimeformer. While the train loss converged to similar values, the validation loss for the model using the STAEformer embedding layer was much better. So now that we know the STAEformer embedding layer seems to perform better than the Spacetimeformer embedding layer, we plotted the embeddings for both to analyze why this is the case. To do this, we passed a data point from the validation set through the embedding layer. The results are shown in the figure below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the STAEformer embeddings. </div> <p>The t-SNE plot for the STAEformer embeddings shows clearly separate clusters for most of the 170 different sensors. The shape of each cluster is a “snake-like” trajectory. Therefore, we know that the embeddings preserve some pattern-like notion across readings from a single sensor. We hypothesize that each of these trajectories represent the reading of a single sensor over time. There are a couple outliers, where the clusters are not grouped by color. One prominent example is the string of cyan, maroon, and moss points along the bottom of the plot. However, even these points have some clustering, though they may not be clustered by color.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the Spacetimeformer embeddings. </div> <p>On the other hand, the t-SNE plot for the Spacetimeformer embeddings show no clear clusters across the same sensor. The distribution resembles a normal distribution, meaning that there is little pattern preserved in the embeddings. It becomes more difficult to differentiate between data points from the same sensor across time.</p> <p>In order to further analyze the effectiveness of each embedding layer, we perturbed the training data and re-trained each model. We were expecting the clusters from the STAEformer embeddings to remain largely the same, with some of the existing clusters possibly breaking apart due to the added noise. However, we were hoping that the Spacetimeformer embeddings would show more visible clusters after the raw data was perturbed. Given the characteristics of the embeddings, one possible output we expected were clusters containing multiple colors. An example is shown in the following image.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Ideal t-SNE plot of the Spacetimer embeddings. </div> <p>This would show that the Spacetimeformer successfully learned spatial relationships across the sensors at variable timesteps. Instead of each cluster representing the embeddings for one sensor, the presence of larger clusters with multiple colors could imply that the Spacetimeformer learned spatiotemporal relations among the corresponding sensors and embedded them into a larger cluster.</p> <p>The following table shows the results after training the model with the perturbed data.</p> <table> <thead> <tr> <th style="text-align: center">Embedding Layer</th> <th style="text-align: center">Train Loss</th> <th style="text-align: center">Validation Loss</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">STAEformer (with perturbations)</td> <td style="text-align: center">13.58251</td> <td style="text-align: center">13.35917</td> </tr> <tr> <td style="text-align: center">Spacetimeformer (with perturbations)</td> <td style="text-align: center">13.42251</td> <td style="text-align: center">17.01614</td> </tr> </tbody> </table> <p>As expected, validation loss slightly increased for both models, and the STAEformer continued to have lower loss values than the model with the Spacetimeformer embedding layer.</p> <p>When we generated the t-SNEplots with the new embeddings, we obtained the following:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the perturbed STAEformer embeddings. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the perturbed Spacetimeformer embeddings. </div> <p>Both t-SNE plots for the STAEformer and Spacetimeformer embeddings look the same as when the models were trained on the original, unperturbed data. So unfortunately, the augmentation had little to no effect on the embedding layers for these two models.</p> <p>Since the t-SNE plots can be hard to parse with the human eye, we decided to focus on the embeddings for the most relevant features of the dataset and see how they compared between the Spacetimeformer and STAEformer. In parallel, this would enable us to identify the failure modes of the dataset and augment those features to see if they improve the model performance. In order to do this, we used PCA to identify the principal components. From there, we found which features help explain the most variance in the dataset and identified those as the features that had the largest impact on the learned embeddings.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Z-normalized correlation matrix between the original PEMS08 dataset and PC-space, normalized by explained variance. <d-cite key="pca"></d-cite> </div> <p>This heatmap shows the top 10 principal components and and the top 10 features that correlate with each principal component. From this heatmap, we can see that the 9th sensor in the dataset is the most relevant feature. Therefore, we can find the corresponding embedding to be the most relevant.</p> <p>Using only the 5 most relevant embeddings obtained from PCA, we re-graphed the t-SNE plots. This helped us to narrow our attention to the most important embeddings.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the top 5 STAEformer embeddings. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> t-SNE plot of the top 5 Spacetimeformer embeddings. </div> <p>As expected, the embeddings for the most relevant sensors in the STAEformer all maintain the “snake-like” trajectory. However, the embeddings for even the most relevant sensors in the Spacetimeformer are seemingly random, and have no pattern across the points.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative-1400.webp"/> <img src="/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Cumulative PCA plot of the original PEMS08 dataset. <d-cite key="pca"></d-cite> </div> <p>We found that the top 25 sensors explained 95% of the variance in the dataset, so we did a quick experiment where we augmented the rest of the 145 sensors (as opposed to the entire training dataset) to see how that affected the learned embeddings. For this augmentation, we expected the results to not improve by much since the learned embeddings for even the most relevant sensors in Spacetimeformer didn’t form visible clusters in the t-SNE plots. As expected, the results were almost identical to the ones generated from augmenting the entire dataset.</p> <h2 id="conclusion-discussion-next-steps">Conclusion, Discussion, Next Steps</h2> <p>There are a couple of reasons why we think the Spacetimeformer performed worse than the STAEformer overall. The first explanation that came to mind is that the readings across different sensors may be mostly independent from one another. The color coded t-SNE plots for the STAEformer clearly separate each sensor into its individual cluster. In this case, the Spacetimeformer would not be suited for the task as its embedding layer solely focuses on learning spatiotemporal relationships, while the STAEformer also contains an embedding layer that is solely dedicated to learning temporal relationships.</p> <p>A second, more plausible explanation deals with the embedding architecture. The difference in performance between the STAEformer and the Spacetimeformer in time series forecasting shows the importance of adaptive embeddings in capturing spatio-temporal relationships. While the STAEformer introduces adaptive embeddings to comprehend the patterns in the data, the Spacetimeformer relies on breaking down standard embeddings into elongated spatiotemporal sequences. The t-SNE plots show that the STAEformer’s adaptive embeddings generate clusters representing sensors with snake-like trajectories, providing a visualization of the model’s ability to capture spatio-temporal patterns. In contrast, the Spacetimeformer’s embeddings follow a scattered distribution, indicating challenges in identifying clusters. This suggests that the Spacetimeformer’s approach may face limitations in effectively learning the spatio-temporal relationships within the PEMS08 dataset, and potentially traffic data in general.</p> <p>Having said all this, the resilience of both the STAEformer and Spacetimeformer to perturbations in the raw data showcases the robustness of their learned representations. Despite the added augmentations, the fact that the t-SNE plots remain largely unchanged indicates the stability in the embedding layers. This may be attributed to the models’ ability to learn a generalizable representation of the spatio-temporal patterns resilient to changes in the input data, regardless of how accurate they may be. This may also be attributed due to the dataset itself. The PEMS08 dataset’s readings may already have been noisy, as it’s unlikely that the readings were recorded with perfect accuracy. We would like to explore these implications of the embeddings’ robustness in our future work.</p> <p>Another possible avenue we would like to explore is why certain sensors (such as the 9th sensor) are more relevant than others beyond just the theory. We came up with a couple hypotheses. First, it’s probable that this particular sensor is placed at important intersections, such that cars that pass this sensor are guaranteed to pass many other sensors. This would mean that there exists a way to extrapolate the readings from this sensor to the readings from other sensors. Tangentially related, it’s possible that two nodes are negatively correlated, such that the cars that pass through one node tend to not pass through another node, and the model extracts readings based on this relationship. If neither of these ideas is the case, the exact opposite concept could be true: the sensor is at a location where the speed data is very consistent, such as a highway. This allows the readings from the sensor to give very accurate predictions. The next step would be to figure out the geographical locations of the sensors and determine whether the ones we found to be the most relevant seem to be placed at important locations.</p> <p>We would also like to do some more experimentation in the future. We used a personal GPU for training (an RTX 2070), and it took a few hours to train the model for every one of our experiments which made it difficult to tune our hyperparameters. Further experiments we would like to run with more compute include running the experiments on the Spacetimeformer model architecture instead of the STAEformer architecture and replacing its embedding layer with STAEformer’s. We mentioned before that the learned embeddings may have been optimized for the model architecture it’s from. Therefore, if the resulting plots from the embeddings look similar to the ones we have generated, then we have conclusive evidence that the STAEformer input embedding does a better job of learning the spatio-temporal relations in the data.</p>]]></content><author><name>Samuel Lee</name></author><summary type="html"><![CDATA[An analysis of various embeddings methods for spatio-temporal forecasting.]]></summary></entry><entry><title type="html">Augmenting Expert Domain Image Inputs for Enhancing Visual Language Models Performance</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/vig-algorithm-flow-project-proposal/" rel="alternate" type="text/html" title="Augmenting Expert Domain Image Inputs for Enhancing Visual Language Models Performance"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/vig-algorithm-flow-project-proposal</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/vig-algorithm-flow-project-proposal/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Over the past few years, we have seen a surge in creation, adoption, and excitement around visual language models, specifically around Open AI’s CLIP model. Visual language models can bridge the gap between image and text, allowing tokenized understanding of the visual world around us. For instance, Meta released Segment Anything, a model with enhanced object detection through multimodal inputs like defined bounding boxes and text.</p> <p>After the recent surge with ChatGPT, we have begun to see advancements in the visual language model space to combine the image analysis and conversational tool. While the recent developments with Bard, GPT4-v, LLava, and many others have progressed the visual language model domain, the overall capabilities of the models are limited to the type of images provided. Most of the models have been trained and finetuned on common day objects, specializing in every-day normal tasks.</p> <p>However, theses models continue to struggle with answering images derived from an expert domain, especially scientific literature. Images from these domains can be challenging for the model, as they require common background knowledge, domain knowledge, and interpretation of the diagram.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Question: What is the path from 1 to 4? <br/><br/> GPT4-v Answer: The image you've uploaded appears to show a diagram with four numbered points, possibly representing steps or locations connected by a path... However, as an AI, I can't visually trace paths or analyze images in the way a human would... </div> </div> <p>How can we assist visual language models to improve performance in expert domains?</p> <h2 id="past-works">Past Works</h2> <p>Visual Language Models have become very popular in the recent years with their ability to connect image to text. Open Flamingo<d-cite key="openflamingo"></d-cite> is an open source model with a similar architecture to Flamingo<d-cite key="flamingo"></d-cite>: images are sent to (1) visual encoders, (2) perceived resampler, and (3) through a dense group of layers. Through few shot learning, text completion, and image analysis, Open Flamingo allows users to have a conversation involving images.</p> <p>Currently, popular visual language models, like Flamingo, utilize CLIP<d-cite key="clip"></d-cite> as the visual encoder to perform the image-to-embedding conversion. Behind the scenes, CLIP uses a Vision Transformer architecture as the encoder. However, when we analyze the embeddings output from CLIP, they do not necessarily have a clear representation to the human eye. This makes it really challenging to inject domain-specific knowledge through the embedding to help perform specific tasks.</p> <p>The project, FlowchartQA<d-cite key="flowchart"></d-cite>, contributed by creating a dataset of flowcharts and code, revealing insights into the relationships forming code. However, no research has tried to understand the way visual language models interpret graphs with nodes and edges, specifically for domain specific questions. Through various changes to text and image inputs, we can learn about the way a visual language model understands graphical structures to improve future performance of VLMs.</p> <h2 id="procedure">Procedure</h2> <h3 id="dataset-creation">Dataset Creation</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Data Augmentation Pipeline for the Inputs </div> <p>To learn more about the graphical understanding of VLMs, a dataset had to be curated to test various conditions. The original images of the flowcharts are sampled from the BizGraphQA dataset<d-cite key="bizgraphqa"></d-cite>. Due to the compute and time contraints, only a sample of ten images were utilized for the analysis. This subset of images is passed through the Segment Anything Model from Meta <d-cite key="sam"></d-cite> to extract a mask of each of the nodes. From there, OCR is performed on each node to retrieve the text and hand coloring is performed to color the image. In the end, we get dataset triples of the original image, colored image, and the list of nodes in the image.</p> <p>For example, for this image, we would have the following dataset.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Original Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Colored Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Node List </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> [Finanze Tyco Roadsafe Insurance, Greyhound Bbbytf Limited, Etablissements Lgac Lthalat Incorporated Indiana, Korte Pro-Cut Grayhawk Insurace North Carolina, Hutchins Sandvik Maryland, Nm Suc. Krsx Limited Michigan] </div> </div> <h3 id="experimentation">Experimentation</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Experimentation Pipeline for the Inputs </div> <p>Bard uses Flamingo, a visual language model, to answer queries. We will provide an input image with or without the augmentation and a question about the graph into Flamingo, as illustrated in the figure above. Each image will be paired with a question in a specific category. For this analysis, we will focus on four major types of questions to evaluate the VLM’s understanding of graph connectivity. These questions are to be asked in tandem with the original image, the colored image, and the original image paired with the list of nodes in the image. We ask the following questions:</p> <ol> <li>Based on the image, is there a cycle in the graph?</li> <li>Based on the image, what is the path from <strong>__ to __</strong>_? (The ground truth path involves nodes that only have one child node.)</li> <li>Based on the image, what is the path from <strong>__ to __</strong>_? (The ground truth path involves nodes that have multiple child nodes.)</li> <li>Based on the image, how many child/parent nodes does _____ have?</li> </ol> <p>For the same image from above, here are the questions and relevant answers:</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center">Question</th> <th style="text-align: center">Answer</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">Is there a cycle in this graph?</td> <td style="text-align: center">No</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">What is the organization hierarchy path from Etablissements Lgac Lthalat Incorporated, Indiana to Nm Suc. Krsx Limited - Michigan?</td> <td style="text-align: center">The path is Etablissements Lgac Lthalat Incorporated, Indiana to Korte Pro-Cut Grayhawk Insurance, North Carolina to Nm Suc. Krsx Limited - Michigan</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">What is the organization hierarchy path from Finanze Tyco Roadsafe Insurance to Nm Suc. Krsx Limited - Michigan?</td> <td style="text-align: center">The path is from Finanze Tyco Roadsafe Insurance to Greyhound Bbbytf Limited to Nm Suc. Krsx Limited - Michigan</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">How many child companies does Greyhound Bbbytf Limited have holdings in?</td> <td style="text-align: center">Two</td> </tr> </tbody> </table> <p>But, you must be wondering: why ask these questions specifically? Each question tests understanding of graphical elements without background understanding of the topic. This should serve as a baseline for the way that VLMs understand graphical structures and the common questions to be asked.</p> <h3 id="evaluation-metrics">Evaluation Metrics</h3> <p>To evaluate the success of our model, we will conduct both qualitative and quantitative analyses on the dataset, given that quantitative evaluation of generative models can be challenging. The control group will provide a baseline for normalizing the results.</p> <p>Qualitatively, we will perform a manual analysis of the generated outputs. By using prompts, images, and answer, we will subjectively compare the prompt, the image, and the resulting answer. Our primary goal is to assess how effectively the visual language model generates the answer based on the prompt while being constrained by the graph.</p> <p>Quantitatively, we plan to utilize an accuracy score will be employed to evaluate the percentage of questions that have been answered correctly in each metric. Incorporating these metrics will provide a more comprehensive understanding of the model’s performance and allow for better comparisons with other generative models.</p> <p>This comparative analysis will facilitate a robust evaluation of the methods employed to represent graphical information within visual language models.</p> <h2 id="results">Results</h2> <p>The following table provides the accuracy metric of the aforementioned experiment:</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center">Question 1</th> <th style="text-align: center">Question 2</th> <th style="text-align: center">Question 3</th> <th style="text-align: center">Question 4</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Original Image</td> <td style="text-align: center">.7</td> <td style="text-align: center">.4</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> </tr> <tr> <td style="text-align: center">Original Image with Node List</td> <td style="text-align: center">0</td> <td style="text-align: center">.3</td> <td style="text-align: center">0</td> <td style="text-align: center">.1</td> </tr> <tr> <td style="text-align: center">Colored Image</td> <td style="text-align: center">1</td> <td style="text-align: center">.3</td> <td style="text-align: center">0.1</td> <td style="text-align: center">0.2</td> </tr> </tbody> </table> <p>We clearly see that the Colored Image performs better than the Original Image in most of the scenarios. Also, we unfortunately see that the node list did not provide gains in performance for the VLM. Overall, the performance is quite limited when asking specific questions about path finding and children nodes. Let’s dive a bit deeper to understand why.</p> <p>Preface: In the following sections, there are tables with prompts, images, and answers from Bard for various mini explorations.</p> <h3 id="superiority-in-extracting-nodes">Superiority in Extracting Nodes</h3> <p>When first embraking on this project, I believed that the VLM would struggle to pair the text and nodes together. However, the VLM actually proved to be quite impressive with the extraction of nodes and the names associated with, making the need for a node list not quite neccesary.</p> <p>Using the input figure from above without any augmentations, I asked the VLM how many child nodes a specific node in the graph had. The VLM outputs the wrong answer, but it actually provides generated code for its reasoning. Here is a portion of the code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Define the corporate structure as a dictionary
corporate_structure = {
    "Etablissements Lgac Lthalat Incorporated, Indiana": {
        "Nm Suc. Krsx Limited - Michigan": {}
    },
    "Nm Suc. Krsx Limited - Michigan": {},
    "Krsx Incorporated, California": {
        "Nm Suc. Krsx Limited - Michigan": {}
    },
    "Krsx Incorporated, Texas": {},
    "Lthalat Holdings, Delaware": {
        "Etablissements Lgac Lthalat Incorporated, Indiana": {},
        "Krsx Incorporated, California": {},
        "Krsx Incorporated, Texas": {}
    },
    "Greyhound Bbbytf Limited": {
        "Krsx Incorporated, California": {},
        "Krsx Incorporated, Texas": {}
    }
}
</code></pre></div></div> <p>The VLM was actually quite impressive with obtaining the specific nodes in the graph, recognizing the text through OCR, and pairing it with the nodes. While all the nodes are not correct, we can see that the VLM can follow a procedure to determine this. While this is a good example of node extraction, the capability is still ambiguous.</p> <p>To poke this topic a bit more, I wanted to test out the VLM’s ability to extract the nodes if the colors are the same or different. I designed a basic figure with just nodes to test this. The same prompt was passed into Bard with the images below. The only difference between the two images is the fact that in one image, the colors of the nodes are same, and in the other image, the colors of the nodes are different. In the results below, we can clearly see that the VLM is able to perform better with the colored nodes, as the VLM is able to distinguish between different nodes.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Original Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Colored Image </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt: What are the names of the nodes in this graph? </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt: What are the names of the nodes in this graph? </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer: B1, B2, B3, B4, B5. </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer: B1, B2, B3, B4. </div> </div> <p><br/> To support this argument, we look at the attention that CLIP places on segments of the image based on a caption. We specifically use CLIP because CLIP is the visual encoder in Flamingo. While this isn’t necessarily a rigorous proof, we can see that the attention on the nodes is placed stronger in the colored graph example rather than the regular graph example.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Caption: Node B1, B2, B3, B4 </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Caption: Node B1, B2, B3, B4 </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Through the examples and tests above, we can clearly see the VLM’s ability to extract nodes, especially with a visually distinugishing factor between the nodes like color. Since the VLM can do a pretty decent job of extracting the nodes, it makes sense that providing the VLM with the node list may not allow for great improvements in performance.</p> <p>So, if the VLM can extract the nodes relatively well, why is the performance still subpar?</p> <h3 id="difficulties-with-edge-dectection">Difficulties with Edge Dectection</h3> <p>Aside from nodes, most graphs have edges, and for the questions asked in the experiments, understanding the connectivity was crucial to providing the correct answer. We actually observed that the colored graphs had answers that were closer to 100% accuracy in comparison to the regular graphs. To explore how VLMs understand the connections between nodes, I decided to ask Bard about some simple graphs to determine how it responded.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Can you please tell me what the edges in the graph are? </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Can you please tell me what the edges in the graph are? </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The edges in the graph are: a -&gt; b, b -&gt; c, c -&gt; a </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The edges in the graph are: B -&gt; A, C -&gt; A </div> </div> <p>Wow! It’s really surprising that the VLM is creating edges where there aren’t even edges? Also, the direction of the edges are wrong. Why would this be the case?</p> <p>Let’s explore a bit more about the individual arrows. Can a VLM accurately describe the arrow direction?</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Which way does the arrow point? </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Which way does the arrow point? </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The arrow in the image points to the right. </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The arrow in the image points to the left. </div> </div> <p>Well, if you take a look at the responses above, we notice that the VLM can accurately provide the arrow direction for larger arrows incomparison to smaller arrows. Since most of the graphs involve smaller arrows, we can see why the VLM could not tell the directionality of the arrow.</p> <h3 id="outside-information">Outside information</h3> <p>Beyond the graphical structure of the nodes, I found that Bard typically incorporated outside information that required background knowledge about the topic. These situations can be categorized into two categories: useful outside information and irrelevant outside information. To distinguish between the two, I found that in the siutations that Bard used graph algorithms and definitions, the outside information was useful in generating the answer. However, many times, Bard would look up specific companies or use graphs from other sources online.</p> <p>In this example, Bard was asked if there was a cycle in the graph, and Bard outputted the answer below. The VLM was very intelligent by quoting the definition of a cycle and using DFS to determine the existence of one. The step-by-step algorithm is correct and ultimately, leads to the solution. This shows that the incorporation of algorithms and accurate definitions can assist the VLM in answering queries.</p> <blockquote> <p>No, there is no cycle in the graph of the Nove 1-66Express organizational structure. A cycle is a path in a graph that starts and ends at the same vertex, and there is no such path in this graph. I verified this by tracing the graph and ensuring that I never returned to a vertex that I had already visited. I also used the following algorithm to check for cycles in graphs: Mark all vertices as unvisited. Choose a vertex and start a depth-first search (DFS). As you traverse the graph, mark each vertex as visited. If you ever reach a vertex that is already marked as visited, then there is a cycle in the graph. If you finish traversing the graph without ever reaching a visited vertex, then there is no cycle in the graph. I ran this algorithm on the graph of the Nove 1-66Express organizational structure and found that there are no cycles.</p> </blockquote> <p>However, outside information can be just as harmful as it is beneficial. Pulling from outside sources can be tricky for a VLM. It needs to remember to operate within the grouth truths presented in the figure. In multiple instances, the VLM actually referenced the wrong figure because it forgot about the image provided with the prompt.</p> <p>Therefore, the VLM needs to be fine-tuned to filter the outside information it brings in to improve the accuracy of its answers.</p> <h2 id="conclusions-and-future-steps">Conclusions and Future Steps</h2> <p>In this study, I delved into the capabilities and limitations of VLMs, like Open Flamingo, in interpreting complex graphical structures within expert domains, such as scientific literature. The experiments, involving various image augmentation techniques such as color changes and node names, revealed key insights. While VLMs demonstrated proficiency in node extraction, it faced challenges in edge detection and understanding the connectivity between nodes. This was particularly evident when colored images outperformed non-colored ones, highlighting the importance of visual distinction for VLM comprehension. However, the addition of node lists did not significantly enhance performance, suggesting existing capabilities in node identification. The connectivity was difficult for the VLM to understand because of the size of the arrows.</p> <p>The findings of this research highlight a crucial challenge for VLMs: integrating domain-specific knowledge, especially for non-standard images like scientific diagrams. However, due to the small dataset size, suggests that further research with a larger and more diverse dataset is necessary to validate these findings. In the future, this research can be applied to help improve prompting for graphical structures, provide insights on how to finetune a VLM for this task, and create a new interest in using VLMs for scientific diagrams.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post explores enhancing visual language models, particularly for expert domains like scientific literature, where standard models struggle. By integrating domain-specific knowledge and advanced image embeddings, the research aims to refine the performance of visual language models such as OpenFlamingo. Leveraging graphical structured embeddings and graph neural networks, the study tests different methods of representing images to improve the models' interpretive capabilities.]]></summary></entry><entry><title type="html">New Synthesis Approach for Personalized LLMS</title><link href="https://deep-learning-mit.github.io/staging/blog/2023/PersonalizedGeneration_w_LLMAgents/" rel="alternate" type="text/html" title="New Synthesis Approach for Personalized LLMS"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://deep-learning-mit.github.io/staging/blog/2023/PersonalizedGeneration_w_LLMAgents</id><content type="html" xml:base="https://deep-learning-mit.github.io/staging/blog/2023/PersonalizedGeneration_w_LLMAgents/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Deep learning has revolutionized the way in which humans interact with the world around them. Its growing ability to ingest vast amounts of data, automate feature extraction, and learn complex patterns and nuances among inputs have contributed to breakthroughs in healthcare, natural language processing, computer vision, and more. A particularly exciting avenue of this innovation has been in the burgeoning field of personalized text generation, which aims to produce text that resembles the style, tone, and word choice taken on by a particular user. Significant advancement in this field has the potential to create more effective forms of communication for individuals with disabilities, personalize educational content, and enhance user interactions with chatbots and virtual assistants, all contributing to a better overall user experience.</p> <p>In an effort to make the availability of personalized text generation more wide-scale, researchers have conducted several studies in the field, centering their approach to the generation of domain-specific personalized text (utilizing domain-specific features/knowledge). Notable studies conducted include <a href="https://arxiv.org/pdf/1910.03506.pdf">Towards Controllable and Personalized Review Generation</a>, which utilizes a product description and self-attentive recursive autoencoders to generate a personalized review <a href="#1">[1]</a>, <a href="https://arxiv.org/pdf/2010.01480.pdf">Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural Network</a>, which constructs a model based on a CapsGNN, and <a href="https://iopscience.iop.org/article/10.1088/1742-6596/2294/1/012015/pdf">Research on user granularity-level personalized social text generation technology</a>, which utilizes an encoder and decoder for text generation <a href="#2">[2]</a>. A lesser explored part of the field and an area that we have chosen to explore for our final project is embedding in the ability to generate personalized text across domains without domain-specific features <a href="#3">[3]</a>. Our project draws inspiration from <a href="https://arxiv.org/pdf/2308.07968.pdf">“Teach LLMs to Personalize – An Approach inspired by Writing Education”</a>, which includes a promising multi-step framework that retrieves, summarizes, ranks, and synthesizes a user’s past documents to generate a personalized version of the document at hand <a href="#4">[4]</a>.</p> <p>A critical aspect of the workflow discussed in the LLM personalization paper and an area that we believe can be improved upon using some of the methods discussed in 6.S898 this semester is the way in which the model synthesizes past documents. Throughout the paper, we will be exploring two creative approaches to synthesis that utilize vector word embeddings to pull relevant words from past documents in an effort to improve the models ability to personalize text.</p> <h1 id="literature-review">Literature Review</h1> <p>An integral part of our exploration project was experimenting with using less data and smaller models to see how performance degrades with respect to the approach discussed in the personalization for LLMs paper (no open source code attached as the project is currently being worked on by researchers at Google). Experimentation required taking an extensive look at the steps involved in the original implementation, gaining an in-depth understanding of the deep learning principles discussed, and optimizing training and compute under machine constraints to process vast amounts of real-world data.</p> <p>The problem formulation for the approach to personalized text generation discussed in the paper can be stated as the following: Given the immediate context of a current document (first k characters) written by a user and access to their past documents, can we develop a model that generates text that is similar to the text of the current document (similarity evaluated by calculating Rouge-1, Rouge-2, Rouge-L, and Bleu scores) . As mentioned earlier, the framework for answering this problem formulation involves first obtaining outputs for retrieval, ranking, summarization, and synthesis, and then feeding these distinct parts into an LLM to produce a personalized body of text (we ignore the auxiliary task of training the LLM to distinguish the owners of written documents for the purposes of this project).</p> <p>The retrieval discussed in the paper uses two methods of outputting relevant documents: sparse retrieval, which compares past documents to the current context using the popular BM25 ranking algorithm, and dense retrieval, which uses a transformer-based text-to-text model to map and compare documents in a 768 dimensional vector space. The ranking step then takes this input, orders documents based on their BM25 scores or cosine similarity when compared with the immediate context, and truncates the input to 2500 characters to only take the top documents. The summarization step then summarizes the top ranked past documents in two ways: context independent summarization, which finetunes an LLM on publicly available data and applies this model to the top ranked entries, and context dependent summarization, which uses weak labels (generated from immediate context) to generate a summary in line with the contents of the current document. A visualization of the approach to the structure can be seen below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview-1400.webp"/> <img src="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An overview of the infrastructure used to process documents and fine tune the personalized generative model. </div> <p>The paper currently explores two methodologies for synthesis: (1) context dependent synthesis, which simply uses the top 20 frequently used keywords from a user’s past documents and (2) context dependent synthesis, which uses weak labels to find text from past documents similar to the immediate context of the document to be personalized. After carefully analyzing the two methodologies, we found that by focusing on keywords the synthesized text is missing an in-depth understanding of sentence structure and semantics that are crucial to personalization.</p> <p>To enhance this step of the text generation process, we have explored several new methods of synthesis and have landed on two approaches with one utilizing the Word2Vec model and the other using GloVe. We have chosen these methods because they both use unique embedding space attributes to form important relationships between texts. Both networks use the method of creating a centroid of the current document that exists in vector space and output words from top ranked past documents that exist close to this centroid. By doing this, we are essentially selecting words (after filtering out synonyms and stopwords) that are in line with the theme of the current document, which will provide the LLM with more thematically relevant synthesized entries that should in theory generate a more personalized output.</p> <p>As an additional research consideration, we explored the effect of passing in the output from both the context independent synthesis discussed in the paper and our auxiliary method of using Word2Vec or GloVe compared to passing in just one of the methods of synthesis. The motivation for doing so came from our initial hypothesis that the combination of both methods of synthesis would enable the LLM to learn complex interactions between important words (results from context independent synthesis) and thematic words (GloVe/Word2Vec) that could lead to better personalization of the final output. A more detailed explanation of the implementations of our proposed approaches will be shown in the following section.</p> <h1 id="description-of-methods--experiments">Description of methods &amp; experiments</h1> <h2 id="the-baseline-implementation">The Baseline Implementation</h2> <p>Our methodological approach began by re-implementing the baseline model from the “Teach LLMs to Personalize” paper. We utilized two datasets mentioned in the research paper: CNN_DailyMail (<a href="https://huggingface.co/datasets/cnn_dailymail">CNN_DailyMail</a>)and Amazon Review Data for Books (<a href="https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/">Amazon_review_data</a>). To enhance efficiency of compute time, we streamlined the data by reducing its size, ensuring a quicker fine-tuning process while retaining data integrity. We also utilized the <a href="https://huggingface.co/t5-base">T5-base model</a>, a smaller model than the T5-11b model mentioned in the paper, for summarization and the personalized generation model. Furthermore, we opted to use the context-independent methods for both summarization and synthesis because the research paper results indicated that their effectiveness is closely comparable to the context-dependent methods. For fine-tuning the summarization model, we utilized a 10 percent subset of the CNN daily mail dataset (311k datapoint original size) with the AdamW optimizer (seeing AdamW is a comparable optimizer to Adafactor, which is what was used in the “Teach LLMs to Personalize” paper), ensuring a balance between efficiency of tuning and comprehensive learning. This set the foundation for our exploration of advanced text synthesis techniques by giving us a base fine tuning and data processing infrastructure. On top of this, the changes we made to the amount of data used along with utilizing a smaller T5 model allowed us to analyze whether the final evaluation results degraded significantly when making the infrastructure of fine tuning the personalized generation model more compact.</p> <h2 id="overview-of-modification--experiments">Overview of Modification &amp; Experiments</h2> <p>In our new approach for synthesis, we utilized Word2Vec and GloVe which hinges on the concept of embedding space. In this space, words are represented as vectors, capturing their semantic relationships based on their context in large text corpora. By embedding the current document and past documents (from the same user) in this space, each word is assigned a position that reflects its semantic meaning.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example-1400.webp"/> <img src="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An example of how using TSNE dimension reduction can illustrate how words are placed in embedding space. Note that dimension reduction does not always come out cleanly since word embeddings are complex and can't be easily represented in 2D space. </div> <p>The ‘centroid’ of the current document in this space is a calculated mean vector, representing the overall semantic direction of the document. Words closest to this centroid are likely to be central to the document’s theme or style. When we look for words from past documents that are closest to this centroid, we are essentially searching for words that align closely with the thematic and stylistic essence of the current document.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example-1400.webp"/> <img src="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An example of how our centroid for the current document corresponds to other words from past documents (note we used PCA dimensionality here). We also chose to display words that had very close euclidean distances to the centroid. Note our centroid from the current document highlighted the following as significant words: ['like', 'since', 'first', 'mystery', 'book'] </div> <p>This method makes sense technically because it leverages the nuanced understanding of language captured in word embeddings. By focusing on words that are semantically close to the central theme of the current document, the model can more effectively identify and use terms that are likely to be relevant and stylistically consistent for personalization, thereby potentially enhancing the generated text of the personalized generation model.</p> <h2 id="experiment--word2vec-vs-glove">Experiment / Word2Vec vs GloVe</h2> <p>Word2Vec and GloVe are both models for word embeddings, but they differ in their approach to creating these embeddings. Word2Vec, developed by Google, primarily uses local context information of words (words surrounding a given word) to generate embeddings. This results in embeddings that capture more of the syntactic and semantic relationships based on specific local contexts.</p> <p>GloVe (Global Vectors for Word Representation), on the other hand, is designed by Stanford and incorporates global matrix factorization and local context window methods. It emphasizes capturing global statistics of the corpus by considering overall word co-occurrence frequencies, essentially acting as an unsupervised learning algorithm that generates word embeddings.</p> <p>When used for synthesis in text personalization, these differences influence the nature of the embeddings. Word2Vec might be more sensitive to the specific contextual use of words in the current and past documents, potentially offering more precise thematic matches based on immediate context. GloVe, with its global perspective, might bring in a broader understanding of word use, capturing more general usage patterns and thematic relationships that extend beyond the immediate context. This could lead to a slightly different set of words being selected for personalization in the synthesis process.</p> <p>In our experiment, we adapted the structure from the “Teach LLMs” paper, incorporating our novel synthesis methods using Word2Vec and GloVe. The process involved independently fine-tuning the personalized generation model for each synthesis approach. This fine-tuning was crucial to observe how the different embedding techniques influenced the model’s performance. After implementing the new synthesis methods, we conducted a thorough evaluation to compare their effectiveness, along with the combination of the original and new synthesis approaches, with the base model. The key focus was on analyzing how the different word embeddings (and combinations of embeddings) impacted the quality and personalization of the generated text, with performance metrics providing insights into the strengths and limitations of each method.</p> <h1 id="analysis--evaluation-of-results">Analysis / Evaluation of Results</h1> <p>The evaluation metrics used in the “Teach LLMs” paper (and also what we utilized), BLEU (Bilingual Evaluation Understudy), ROUGE-1, ROUGE-2, and ROUGE-L, are standard metrics used to evaluate the quality of text which has been machine-translated or generated by machine learning models.</p> <p>BLEU Score: The BLEU score evaluates the quality of machine-translated text by comparing it with one or more reference translations. It does so at various levels, from individual words to consecutive sequences of words (n-grams), to assess precision. A higher BLEU score indicates more similarity to the reference text, often implying better translation quality. However, BLEU has limitations as it does not account for the fluency or grammaticality of the generated text.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score-1400.webp"/> <img src="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Calculations behind the BLEU score calculations. </div> <p>ROUGE Scores: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is another set of metrics for evaluating automatic summarization and machine translation. ROUGE-1 and ROUGE-2 refer to the overlap of unigrams (single words) and bigrams (two consecutive words) between the machine-generated text and a set of reference texts, respectively. ROUGE-L considers the longest common subsequence, focusing on the longest coherently matching sequence of words. ROUGE scores can consider both precision (like BLEU) and recall, providing a more rounded evaluation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score-1400.webp"/> <img src="/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Calculations behind the ROUGE-N (N-gram) score calculations; in our case N = 1, 2, or longest common subsequence. </div> <p>We can also take a look into how our models performed during the fine tuning period. Based on the progression of the training and validation loss, you can infer how well the model is learning and whether it’s overfitting (learning the training data too closely and not generalizing well) or underfitting (not learning the training data well enough).</p> <p>Comparing the performance of our models using two different synthesis approaches–our base model versus the new synthesis approach using the GloVe or Word2Vec model, and the combination of the base model and new synthesis–could result in different behaviors most likely for one particular reason:</p> <p>Quality of Embeddings: The GloVe and Word2Vec models provide a different representation for words, capturing semantic relationships in a more nuanced way than just looking at IDF scores, which could lead to varied results during fine tuning. Also, combining our original synthesis with our new synthesis can give the model more information to finetune on allowing for a more intricate understanding of the text when generating.</p> <p>The differences in BLEU and ROUGE scores between the two models can arise from how each model handles the linguistic nuances of the generated text. If the new approach with the GloVe model is better at capturing the context and meaning of the sentences, it might score higher in BLEU and ROUGE, despite potentially higher loss values.</p> <p>The variations in BLEU and ROUGE scores could also indicate how each model deals with the trade-off between precision and recall—whether it’s better at producing text that contains most of the expected content (high recall) or at avoiding extraneous content not in the reference (high precision).</p> <p>Evaluating these metrics in combination with each other, rather than in isolation, provides a more comprehensive picture of a model’s performance and areas for potential improvement.</p> <p>The following results portray the overarching BLEU, ROUGE-1, ROUGE-2, and ROUGE-L score we received for the base model, our model using the new synthesis approach, and our model using the base synthesis along with the new synthesis. We have highlighted the snippets of the generated cases that produced our highest scores which are indicative of the possibilities of improvement if we were able to utilize larger T5 models and more training data.</p> <h2 id="results">Results</h2> <p>The following table highlights the results of our evaluation of generated outputs from our baseline model versus our two new approaches (new synthesis and old synth + new synth). Althought there are cases where the max score for our new approaches are high, we believe that this is most likely the case where we generate the rest of a document that is already signficantly short. Essentially, since we don’t need to generate a diverse output of words for a longer length, our more compact t5-base model with minimal training performs very well still. <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: center">BLEU (avg)</th> <th style="text-align: center">ROUGE1 (avg)</th> <th style="text-align: center">ROUGE2 (avg)</th> <th style="text-align: center">ROUGEL (avg)</th> <th style="text-align: center">BLEU (max)</th> <th style="text-align: center">ROUGE1 (max)</th> <th style="text-align: center">ROUGE2 (max)</th> <th style="text-align: right">ROUGEL (max)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Baseline Model</td> <td style="text-align: center">08.9531</td> <td style="text-align: center">29.5847</td> <td style="text-align: center">18.6126</td> <td style="text-align: center">25.6882</td> <td style="text-align: center">49.5207</td> <td style="text-align: center">65.2174</td> <td style="text-align: center">62.2222</td> <td style="text-align: right">65.2173</td> </tr> <tr> <td style="text-align: left">New Synth (Word2Vec)</td> <td style="text-align: center">09.0722</td> <td style="text-align: center">29.3465</td> <td style="text-align: center">18.3129</td> <td style="text-align: center">25.6115</td> <td style="text-align: center">46.6638</td> <td style="text-align: center">65.9340</td> <td style="text-align: center">62.2222</td> <td style="text-align: right">65.2174</td> </tr> <tr> <td style="text-align: left">New Synth (GloVe)</td> <td style="text-align: center">10.3810</td> <td style="text-align: center">31.9870</td> <td style="text-align: center">21.1543</td> <td style="text-align: center">27.4335</td> <td style="text-align: center">50.5317</td> <td style="text-align: center">65.8537</td> <td style="text-align: center">60.1942</td> <td style="text-align: right">63.4146</td> </tr> <tr> <td style="text-align: left">New Synth (Word2Vec) + Old Synth</td> <td style="text-align: center">10.4402</td> <td style="text-align: center">31.4181</td> <td style="text-align: center">20.2349</td> <td style="text-align: center">27.7710</td> <td style="text-align: center">58.0197</td> <td style="text-align: center">64.8148</td> <td style="text-align: center">61.9048</td> <td style="text-align: right">62.7907</td> </tr> <tr> <td style="text-align: left">New Synth (GloVe) + Old Synth</td> <td style="text-align: center">08.7228</td> <td style="text-align: center">29.2284</td> <td style="text-align: center">17.1685</td> <td style="text-align: center">24.6075</td> <td style="text-align: center">49.7273</td> <td style="text-align: center">65.5462</td> <td style="text-align: center">60.9756</td> <td style="text-align: right">61.9048</td> </tr> </tbody> </table> <h1 id="conclusion--discussion-of-limitations">Conclusion / Discussion of Limitations</h1> <p>Throughout the paper, we have demonstrated the potential of embedding techniques like Word2Vec and GloVe in enhancing the personalization aspect of text generation models. Our experiments, which involved comparing these methods with traditional synthesis techniques, have shown promising results in terms of creating text that more accurately reflects the style and thematic preferences of individual users.</p> <h2 id="limitations">Limitations</h2> <p>For our exploration, we were limited to running all of our models and doing our data analysis on Google Colab in a short period of time along with having to reimplement the structure used in the “Teach LLMs to Personalize” paper since no codebase exists for it. Because of this, we had to find ways to condense our models and limit the amount of data we ingested so that we could spend less time waiting on models to run and freeing up storage and more time analyzing the output of our code. Two of the big adjustments that we made to navigate these constraints was using the t5-base model (fewer tokens than t5-11b), which we ran for a limited number of epochs, instead of the t5-11b model and using only a subset of data points from the provided Amazon Review Dataset. One of the other things that we tried to make the most advantage of our compute was quantizing our t5-base model to provide faster synthesis and summary to run on our ingested data, but we unfortunately ran into dependency issues and were unable to get this method working. However, from our analysis, we estimate that our evaluation results would have been much more in line with the paper’s results, or even surpass them, if we were able to run the t5-11b model for a larger amount of epochs and utilize more amazon review data.</p> <h2 id="next-steps">Next Steps</h2> <p>If we choose to continue this project, we want to explore ways in which we can synthesize domain-specific knowledge, along with thematic tendencies, related to the current document that can be fed into the final LLM for text generation. There are a lot of benefits of providing synthesized information to the model as it filters for the “most important/significant” words in a document and we hypothesize that this supplementary information could add an extra level of knowledge to a model that has proven to perform well in personalization.</p> <p>Also, another pathway that could be explored is integrating Agent LLMs in the initial document ranking phase to see if the procured rankings are better than the current methods set in place (RankDocBM25, RankDocDense, RankSnippet, RankDocBySnpt). We believe that utilizing LLMs that have more awareness of context over large document spaces (and even varying languages) could be benefitial to the process of developing personalized generation model.</p> <h1 id="bibliography">Bibliography</h1> <p><a id="1">[1]</a> Li, Pan, and Alexander Tuzhilin. Towards Controllable and Personalized Review Generation - arXiv.Org, arxiv.org/pdf/1910.03506.pdf. Accessed 12 Dec. 2023.</p> <p><a id="2">[2]</a> Li, Junyi, et al. Knowledge-Enhanced Personalized Review Generation with … - Arxiv.Org, arxiv.org/pdf/2010.01480.pdf. Accessed 12 Dec. 2023.</p> <p><a id="3">[3]</a> Gao, Y B, et al. “IOPscience.” Journal of Physics: Conference Series, IOP Publishing, 1 June 2022, iopscience.iop.org/article/10.1088/1742-6596/2294/1/012015.</p> <p><a id="4">[4]</a> Li, Cheng, et al. Teach LLMs to Personalize: An Approach Inspired by Writing Education - Arxiv.Org, arxiv.org/pdf/2308.07968.pdf. Accessed 12 Dec. 2023.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Output Produced From our Codebase: <a href="https://github.com/dapacica/DL_finalproject_code/blob/main/FinalProjCleanColab.ipynb">https://github.com/dapacica/DL_finalproject_code/blob/main/FinalProjCleanColab.ipynb</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Daniel Papacica</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>
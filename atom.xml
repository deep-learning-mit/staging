<?xml version="1.0" encoding="utf-8"?> <feed xmlns="http://www.w3.org/2005/Atom"> <title>6.S898 Deep Learning Blogs 2023</title> <link href="https://deep-learning-mit.github.io/staging/atom.xml" rel="self"/> <link href="https://deep-learning-mit.github.io/staging/"/> <updated>2024-05-02T23:05:43+00:00</updated> <id>https://deep-learning-mit.github.io</id> <author> <name></name> <email></email> </author> <entry> <title>Investigating Vision Transformer-Based Models for Closure Modeling of Fluid Dynamical Systems</title> <link href="https://deep-learning-mit.github.io/blog/2023/Investigating-neural-operator-models-for-closure-modeling-of-dynamical-systems/"/> <updated>2023-12-19T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Investigating-neural-operator-models-for-closure-modeling-of-dynamical-systems</id> <content type="html">&lt;h1 id=&quot;motivation-and-background&quot;&gt;Motivation and Background&lt;/h1&gt; &lt;p&gt;Over the past decade, deep learning models have increasingly been used for modeling time series data for fluid dynamical systems. One of the most recent applications is in forecasting weather &lt;d-cite key=&quot;schultz2021can&quot;&gt;&lt;/d-cite&gt; with deep learning models being developed by tech giants including NVIDIA &lt;d-cite key=&quot;pathak2022fourcastnet&quot;&gt;&lt;/d-cite&gt; and Google &lt;d-cite key=&quot;lam2022graphcast&quot;&gt;&lt;/d-cite&gt; with reasonable prediction accuracy compared to conventional numerical weather prediction. While these models completely replace traditional numerical weather models with deep neural networks (i.e., “surrogate modeling”), in general, deep neural models can also be used to augment existing numerical solvers and methods &lt;d-cite key=&quot;lino2023current&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Training deep neural models to completely replace numerical solvers requires a lot of data, which might not be available due to constraints with sensor and satellite usage associated with collecting ocean and weather data. Additionally, these surrogate models are completely data-driven and could lead to non-physical predictions (lack of volume preservation, and non-conservation of physical laws) if these needs are not explicitly attended to during training &lt;d-cite key=&quot;lino2023current&quot;&gt;&lt;/d-cite&gt;. A huge advantage of these models is their very low computational cost during inference compared to using numerical solvers &lt;d-cite key=&quot;pathak2022fourcastnet&quot;&gt;&lt;/d-cite&gt;. Another approach is to use closure models that augment low fidelity (low resolution) numerical simulations with a neural network (i.e., a closure term) to predict high fidelity (high resolution) forecasts &lt;d-cite key=&quot;gupta2021neural&quot;&gt;&lt;/d-cite&gt;. This approach could lead to some conservation of physical laws since it builds upon conventional numerical solvers that obey physical equations like PDEs, with a lower computational cost compared to directly running high-fidelity numerical simulations.&lt;/p&gt; &lt;p&gt;In closure modeling, we are interested in solving the following problem: Here, we describe the case of closure modeling with loss of accuracy due to low numerical resolution which leads to loss of sub-grid scale processes, and sometimes even truncation and discretization errors. But there could also be closure due to missing or unknown physics, incorrect parameters, etc. &lt;d-cite key=&quot;gupta2021neural&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Consider a low-fidelity model (low resolution):&lt;/p&gt; &lt;div id=&quot;eq: 1&quot;&gt; $$ \frac{du_{LF}(t)}{dt} = f(u_{LF}(t)) \tag{1} $$ &lt;/div&gt; &lt;p&gt;and the high fidelity (high resolution) equivalent model:&lt;/p&gt; &lt;div id=&quot;eq: 2&quot;&gt; $$ \frac{du_{HF}(t)}{dt} = f(u_{HF}(t)) \tag{2} $$ &lt;/div&gt; &lt;p&gt;The purpose of the closure in this context is to augment the low fidelity model with a neural closure model $NN(u_{LF}(t))$, such that:&lt;/p&gt; &lt;div id=&quot;eq: 3&quot;&gt; $$ u_{HF}(t+1) = \int_{t}^{t+1} \bigg(\frac{d u_{LF}}{dt} + NN(u_{LF}(t)) \bigg) dt \tag{3} $$ &lt;/div&gt; &lt;p&gt;Previous works for neural closure models have used neural ODEs and neural DDEs &lt;d-cite key=&quot;gupta2023generalized&quot;&gt;&lt;/d-cite&gt;. However, these methods require coding an adjoint equation, which could be difficult for complex 2D problems. Another approach has been to use local CNNs for turbulent models &lt;d-cite key=&quot;srinivasan2023turbulence&quot;&gt;&lt;/d-cite&gt;. More recently, vision transformers have been shown to be very successful for image recognition &lt;d-cite key=&quot;dosovitskiy2020image&quot;&gt;&lt;/d-cite&gt;, and have also been adapted to predict the mapping between two infinite-dimensional function spaces using Neural Operators &lt;d-cite key=&quot;guibas2021adaptive&quot;&gt;&lt;/d-cite&gt; (more details in the following sections). These adaptive neural operators have also been used for predicting fluid flows like flow past a cylinder using experimental data for training &lt;d-cite key=&quot;renn2023forecasting&quot;&gt;&lt;/d-cite&gt;. However, to the best of our knowledge, the usage of vision transformers for closure modeling of 2D fluid flow fields has not been explored.&lt;/p&gt; &lt;p&gt;Another issue with using neural networks for solving fluid dynamics PDEs is unstable recursive predictions, due to the exponential growth of accumulated errors across time. Some papers have explored ways to limit this exponential error growth by using custom loss functions &lt;d-cite key=&quot;kim2019deep&quot;&gt;&lt;/d-cite&gt;, and adding additional spectral networks to limit error growth &lt;d-cite key=&quot;lippe2023pde&quot;&gt;&lt;/d-cite&gt;. In this project, we attempt to achieve stable long roll-outs by first training models for one-time-step predictions and then fine-tuning for long roll-out, inspired by &lt;d-cite key=&quot;pathak2022fourcastnet&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h1 id=&quot;methods-and-experiments&quot;&gt;Methods and Experiments&lt;/h1&gt; &lt;h2 id=&quot;test-case-setup&quot;&gt;Test case setup&lt;/h2&gt; &lt;p&gt;In this project, we develop and investigate methods to augment low-fidelity (low-resolution) numerical simulations of flow past a cylinder with deep neural networks and compare them with high-fidelity (high-resolution) numerical simulations. The neural closure aims to learn the missing subgrid-scale processes and truncation and discretization errors in the low-fidelity simulation and augment it to match the high-fidelity simulation &lt;d-cite key=&quot;gupta2021neural&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;zanna2020data&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;h3 id=&quot;data-generation&quot;&gt;Data generation&lt;/h3&gt; &lt;p&gt;We use data generated from numerical simulations for two-dimensional flow past a cylinder &lt;d-cite key=&quot;cohen2004fluid&quot;&gt;&lt;/d-cite&gt;. We employ the MIT-MSEAS 2.29 Finite Volume framework &lt;d-cite key=&quot;ueckermann20122&quot;&gt;&lt;/d-cite&gt; to solve the Navier Stokes equations at different numerical resolutions and fidelities. Figure &lt;a href=&quot;#fig:fpc_setup&quot;&gt;&lt;span&gt;Figure 1&lt;/span&gt;&lt;/a&gt; shows the geometry, inlet conditions, and boundary conditions used for the numerical simulations. The test set-up consists of a rectangular geometry of length 15m and height 5m. A cylinder is placed 2m upstream with a diameter of 1m. The inlet velocity is set to 2m/s horizontal velocity and no vertical velocity. The boundary conditions are set to outflow conditions on the right boundary and free slip on the top and bottom boundaries. We use Reynolds’s number of $Re=200$. The Reynolds’s number is a key parameter that affects the regime of solutions observed, at $Re\geq 200$, complex periodic patterns can be observed known as wakes, eddy shedding, or Karma vortex streets &lt;d-cite key=&quot;cohen2004fluid&quot;&gt;&lt;/d-cite&gt;. It is in this complex regime that we are interested in applying our deep learning methods.&lt;/p&gt; &lt;div id=&quot;fig:fpc_setup&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/fpc_setup.png&quot; class=&quot;fig:fpc_setup&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1: Schematic of the flow past a cylinder setup used to generate data &lt;/div&gt; &lt;p&gt;To generate the high-fidelity (high-resolution) data we use a resolution of 200x600. For the low-fidelity (low-resolution) data, we use a resolution of 50x150. The MIT-MSEAS 2.29 numerical simulation uses second-order finite volumes and a time-step of 0.000625s. The diffusion operator was discretized using central boundary fluxes. For advection, we use a total variation diminishing (TVD) scheme with a monotonized central (MC) symmetric flux limiter. For numerical time integration, we use a rotational incremental pressure correction scheme. The reader is directed to &lt;d-cite key=&quot;ueckermann20122&quot;&gt;&lt;/d-cite&gt; for more details on the numerical schemes. We simulated the flow for a total of 30s, and the data is saved once every 0.03125s, for a total of 9600 snapshots. We observed fully developed eddies after 15s. An eddy sheds approximately every 2s, so we find a shedding period of about 4s.&lt;/p&gt; &lt;h3 id=&quot;data-usage-for-deep-learning&quot;&gt;Data usage for deep learning&lt;/h3&gt; &lt;p&gt;We save the u and v velocity fields, as well as the time derivatives (which act as the numerical approximation of the RHS terms (dynamics terms) of the PDEs being solved).&lt;/p&gt; &lt;p&gt;Thus we save a tensor of size $9600 \times 4 \times \text{num. horizontal cells} \times \text{num. vertical cells} $, for each numerical resolution, where 9600 is the total number of time steps saved, and 4 indicates 4 channels for the 2 velocities and 2 time derivatives (horizontal: u and vertical: v). To maintain the same grid size for deep learning, we down-sample the high-fidelity data using linear interpolation to have the same size as the low-fidelity data.&lt;/p&gt; &lt;p&gt;For deep learning, we neglect the first 4800 snapshots as spin-ups to reach a fully developed flow state. Then we use the next 3600 snapshots for training, 600 snapshots for validation, and 600 snapshots for testing.&lt;/p&gt; &lt;p&gt;We created a custom data class with a data loader to feed randomized sequences of required sequence lengths from the training set for training. For inference, we feed the snapshots one by one from the test data set.&lt;/p&gt; &lt;h2 id=&quot;deep-learning-model&quot;&gt;Deep learning model&lt;/h2&gt; &lt;p&gt;For this project, we initially tried to explore architectures inspired by Fourier neural operators &lt;d-cite key=&quot;li2020fourier&quot;&gt;&lt;/d-cite&gt;. These operators learn the relationship between two infinite dimensional spaces, by using kernels in the Fourier space instead of physical space. Since multiplications in Fourier space are akin to convolutions in physical space, these models can learn powerful infinite dimensional mapping at reduced computational cost. In particular, we were interested in Adaptive Fourier Neural Operators &lt;d-cite key=&quot;guibas2021adaptive&quot;&gt;&lt;/d-cite&gt;, which build upon vision transformers. Like vision transformers, these models first split the input image into patches and tokens with positional embedding. However, instead of relying on QKV (Query, Key, Value) attention mechanisms, they perform a 2D spatial Fourier transform of the tokenized representation and use MLPs (which act as convolution, since multiplication in Fourier space is convolution in physical space) to learn the mapping between input and output images. Since these models don’t involve self-attention, they have significantly fewer parameters compared to vision transformers and can even outperform them in certain tasks. Figure &lt;a href=&quot;#fig:afno&quot;&gt;&lt;span&gt;Figure 2&lt;/span&gt;&lt;/a&gt; shows a schematic comparing a regular vision transformer with AFNO. However, we were not able to further explore this architecture due to time constraints.&lt;/p&gt; &lt;div id=&quot;fig:afno&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/afno_layer.png&quot; class=&quot;fig:afno&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: Schematic of the difference between a regular QKV attention based vision transformer and an AFNO layer &lt;/div&gt; &lt;p&gt;Instead, we focus on utilizing vision transformer-based architectures for our closure modeling task. Although they have a higher number of parameters than AFNO, vision transformers rely on self-attention that can learn long-range physical dependencies, and are hence well suited to our task.&lt;/p&gt; &lt;h3 id=&quot;single-time-step-predictions&quot;&gt;Single time-step predictions&lt;/h3&gt; &lt;p&gt;We first focus on a framework that just predicts output for the next time step, i.e., single time-step predictions. This is a simpler task than recursive or long roll-out predictions which we will discuss in the following sections.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;#fig:vit_model_single&quot;&gt;&lt;span&gt;Figure 3&lt;/span&gt;&lt;/a&gt; contains a flowchart of how our Vision Transformer-based closure model “ViTClosure()” works. In the first part of the architecture, a vision transformer-based architecture takes in a low fidelity velocity field at time t as input and returns the closure term (which is similar to a time derivative field) at time t+1.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, a 2D CNN transforms the input low fidelity 2D velocity field at time $t$ into patches&lt;/li&gt; &lt;li&gt;Next, a layer adds positional embedding and creates tokens. In this project, we use learned absolute positional embeddings.&lt;/li&gt; &lt;li&gt;Next, the tokens are passed through a vision transformer with many attention layers with multi-headed attention heads.&lt;/li&gt; &lt;li&gt;Next, we have a layer that applies layer norm to the final output of the vision transformer. Here the output would be of size $\text{Batch size} \times \text{Num. of tokens} \times \text{Hidden dimension}$. Since we need an output field of the same dimension as the input field, we use an MLP for this transformation. We call this output closure term $NN(u_{LF}(t))$.&lt;/li&gt; &lt;li&gt;Next comes a numerical integration step which combines the low fidelity numerical solver (which provides $\frac{d u_{LF}}{dt}$) with the neural closure term $NN(u_{LF}(t))$, to predict the high-fidelity field $u_{HF}(t+1)$, shown in &lt;a href=&quot;#eq: num_int&quot;&gt;&lt;span&gt;Eq. (4)&lt;/span&gt;&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;div id=&quot;eq: num_int&quot;&gt; $$ u_{HF}(t+1) = \int_{t}^{t+1} \bigg(\frac{d u_{LF}}{dt} + NN(u_{LF}(t)) \bigg) dt \tag{4} $$ &lt;/div&gt; &lt;div id=&quot;fig:vit_model_single&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_model_single.png&quot; class=&quot;fig:vit_model_single&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: Flow chart of the model used for closure modeling with single time step predictions. Please see text description for more details &lt;/div&gt; &lt;h3 id=&quot;loss-functions&quot;&gt;Loss functions&lt;/h3&gt; &lt;p&gt;Finally, we compare the predicted high-fidelity field $u_{HF}(t+1)$ and neural closure term $NN(u_{LF}(t))$ with the ground truth simulations. Similar loss functions have been used in neural closure models based on neural-ODEs &lt;d-cite key=&quot;gupta2021neural&quot;&gt;&lt;/d-cite&gt;. In this work, we use a weighting factor that is a hyperparameter that can be tuned to weigh either the state errors or closure term errors depending on the application. The loss function is defined in &lt;a href=&quot;#eq: loss_func&quot;&gt;&lt;span&gt;Eq. (5)&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt; &lt;div id=&quot;eq: loss_func&quot;&gt; $$ \mathcal{L}_{\text{weighted}}(.;\lambda) = \text{MSE}\bigg(u_{HF}(t+1),u_{HF_{true}}(t+1)\bigg) + \lambda \text{MSE}\bigg(NN(u_{LF}(t)),\text{True closure}(u_{LF}(t))\bigg) \tag{5} $$ &lt;/div&gt; &lt;p&gt;Then, we use backpropagation with the Adam optimizer and a cosine learning rate to optimize the model parameters.&lt;/p&gt; &lt;h3 id=&quot;fine-tuning-for-recursive-predictions&quot;&gt;Fine tuning for recursive predictions&lt;/h3&gt; &lt;p&gt;Now that we have a model trained for single time-step predictions, we move on to multiple time-step predictions or long roll-out predictions. Achieving long roll-out predictions is hard due to the exponential accumulation of errors during recursive predictions. Hence a small error at timestep t+1 could lead to huge errors after a few more timesteps. &lt;d-cite key=&quot;lippe2023pde&quot;&gt;&lt;/d-cite&gt; have recently worked on achieving stable long roll-outs using spectral data augmentations. In this project, we try a different approach by using fine-tuning, and using a loss function inspired by &lt;d-cite key=&quot;kim2019deep&quot;&gt;&lt;/d-cite&gt;. The flowchart of the fine-tuning process is shown in &lt;a href=&quot;#fig:vit_model_single&quot;&gt;&lt;span&gt;Figure 3&lt;/span&gt;&lt;/a&gt;. The description is as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We first initialize the model using the best-performing model of the single-time step prediction, i.e., the best ViT-closure() model.&lt;/li&gt; &lt;li&gt;Next, we recursively use the ViT-closure() model, and feed the output at time t+i as input to the model at time t+i+1.&lt;/li&gt; &lt;li&gt;Finally, we average all the losses across time &lt;a href=&quot;#eq: loss_func_rec&quot;&gt;&lt;span&gt;Eq. (6)&lt;/span&gt;&lt;/a&gt;. and backpropagate through time.&lt;/li&gt; &lt;/ol&gt; &lt;div id=&quot;eq: loss_func_rec&quot;&gt; $$ \mathcal{L}_{\text{recursive}}=\frac{1}{n}\sum_{t=1}^n \mathcal{L}_{\text{weighted}}(.;t,\lambda) \tag{6} $$ &lt;/div&gt; &lt;p&gt;where ‘n’ is a hyperparameter that determines how many times the model is propagated recursively during fine-tuning.&lt;/p&gt; &lt;div id=&quot;fig:vit_fine_tune&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/vit_fine_tune.png&quot; class=&quot;fig:vit_fine_tune&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4: Flow chart of the fine-tuning step used to train the closure model to predict recursively for long roll-outs. Please see the text description for more details &lt;/div&gt; &lt;h1 id=&quot;results-and-analysis&quot;&gt;Results and Analysis&lt;/h1&gt; &lt;p&gt;We ran many experiments for the flow past a cylinder set-up with high-fidelity (high-resolution) data of resolution 200x600 and low-fidelity (low-resolution) data of resolution 50x150. The high-fidelity run was downsampled as described previously to achieve the same grid size as the low-fidelity data for comparison. We used one NVIDIA RTX A6000 GPU for all the training and inference runs.&lt;/p&gt; &lt;h2 id=&quot;single-time-step-predictions-1&quot;&gt;Single time-step predictions&lt;/h2&gt; &lt;p&gt;First, we train the single time-step prediction model using the procedure described previously. We tried many runs (~ 20) by varying the key hyperparameters such as the global batch size between 1 and 16, the embedding dimension between 64 and 256, the number of attention layers between 3 and 9, the number of attention heads between 5 and 10, the patch size between 5 and 10, and the weightage in the loss function $\lambda$ between 0 and 1.&lt;/p&gt; &lt;p&gt;Increasing the batch size led to faster training, but higher GPU memory requirements. With a batch size of 1, it took around 8 minutes of wall-clock time for one epoch of training. We observed the best training results with a loss function weightage $\lambda$ of close to 0, this may be because we ran the numerical solver offline to obtain the low-fidelity derivatives and ground truth closure, which is not as accurate as obtaining these values online during training. The other hyperparameters decreased training errors but the validation error after 50 epochs of training was around 0.03 m/s compared to the average velocity field of 2 m/s (so about 1.5\% relative error). Regularization and avoiding overfitting of the model needs to be investigated further in future work.&lt;/p&gt; &lt;p&gt;For the GIFs below, we use the best-trained model, which was trained using a batch size of 16, embedding dimension of 128, number of attention layers of 6, number of attention heads of 10, patch size of 5, and weightage in the loss function $\lambda$ of 0.05.&lt;/p&gt; &lt;p&gt;Using this model, we can visualize the attention layers, to identify which features have been most useful for closure modeling. &lt;a href=&quot;#fig:attention_patch&quot;&gt;&lt;span&gt;Figure 5&lt;/span&gt;&lt;/a&gt; shows the attention map of multiple patches on the low-fidelity u velocity input at the same time step. We can observe that the most important feature seems to be the phase (whether the eddy is facing upward or downward) of the eddy shed right at the cylinder, and the other eddies downstream. We can also see that there is very little attention near the inlet and top and bottom boundaries since those values are set as inputs to the simulation.&lt;/p&gt; &lt;div id=&quot;fig:attention_patch&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/Low_attention_movie.gif&quot; class=&quot;fig:attention_patch&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5: GIF shows the attention map of multiple patches on the low-fidelity u velocity input at the same time step &lt;/div&gt; &lt;p&gt;&lt;a href=&quot;#fig:attention_patch&quot;&gt;&lt;span&gt;Figure 6&lt;/span&gt;&lt;/a&gt; shows the attention map of a single patch on the low-fidelity u velocity input at different time steps. We can again observe that the attention map follows the eddy shedding at all times. These two plots indicate that the model can identify that the eddies are the most important features, and the inlet and boundaries are not that critical for predicting the flow field. However, this may not be true if we are attempting to do closure modeling between simulations with different inlet and boundary conditions, which can be further investigated in future work.&lt;/p&gt; &lt;div id=&quot;fig:attention_time&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_attention_plot_low_times_compressed.gif&quot; class=&quot;fig:attention_time&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 6: GIF shows the attention map of a single patch on the low-fidelity u velocity input at different time steps &lt;/div&gt; &lt;p&gt;Now that we know that our model focuses on the most important features for closure modeling, we can compare the field plots of the high-fidelity predictions using the model.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;#fig:u_pred&quot;&gt;&lt;span&gt;Figure 7&lt;/span&gt;&lt;/a&gt; and &lt;a href=&quot;#fig:v_pred&quot;&gt;&lt;span&gt;Figure 8&lt;/span&gt;&lt;/a&gt; show the comparison of the neural closure model predictions, and predictions with just low-resolution simulations. We can observe that the neural closure model prediction (3rd row) performs way better than the low-fidelity simulations (2nd row). The low-fidelity simulation is out of phase compared to the high-resolution ground truth, but our neural closure model is able to augment the low-fidelity simulation and learn the true phase and missing sub-grid scale processes!&lt;/p&gt; &lt;div id=&quot;fig:u_pred&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch0_compressed.gif&quot; class=&quot;fig:u_pred&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 7: GIF shows comparison of low fidelity prediction and augmented neural closure single step predictions for u velocity for testing times &lt;/div&gt; &lt;div id=&quot;fig:v_pred&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00011_ch1_compressed.gif&quot; class=&quot;fig:v_pred&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 8: GIF shows comparison of low fidelity prediction and augmented neural closure single step predictions for v velocity for testing times &lt;/div&gt; &lt;h2 id=&quot;fine-tuning-for-recursive-predictions-1&quot;&gt;Fine tuning for recursive predictions&lt;/h2&gt; &lt;p&gt;Next, we evaluate the capabilities of the best model from the single time-step predictions for recursive predictions. We set the initial conditions to be the first snapshot in the testing data set and perform recursive predictions as described before for 50 time steps. The resulting predictions and error fields are shown in the GIF in &lt;a href=&quot;#fig:rec_pred_single&quot;&gt;&lt;span&gt;Figure 9&lt;/span&gt;&lt;/a&gt;. We can see that the errors grow exponentially even before a few recursive predictions, and the model is incapable of any accurate long-term predictions. The predictions quickly become out of phase compared to the ground truth.&lt;/p&gt; &lt;div id=&quot;fig:rec_pred_single&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_original_ch0_compressed.gif&quot; class=&quot;fig:rec_pred_single&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 9: GIF shows the result of recursive predictions by the best single time-step prediction model &lt;/div&gt; &lt;p&gt;Next, we initialize a new model using the weights from the best single time-step predictions and perform fine-tuning as described previously.&lt;/p&gt; &lt;p&gt;We explore multiple values of $n$, which is a hyperparameter that controls how long the recursive sequence lengths used for fine-tuning are. We found the ideal value of ‘n’ to be $5$ timesteps. Using $n&amp;lt;5$ led to inaccurate recursive predictions, using $n&amp;gt;5$ did not lead to much improvement in accuracy but increased training costs due to requirements to backpropagate through long-time sequences.&lt;/p&gt; &lt;p&gt;The resulting predictions and error fields after fine-tuning are shown in the GIF in &lt;a href=&quot;#fig:rec_pred_multiple&quot;&gt;&lt;span&gt;Figure 10&lt;/span&gt;&lt;/a&gt;. We can observe that the model predictions remain in phase with the ground truth, and the error fields are low up to 50 timesteps. We can see that even though we only fine-tuned with sequence lengths of 5, the resulting model is stable for long roll-outs of up to 50-time steps. However, this could be because eddy shedding in flow past a cylinder is somewhat periodic, which aids in some generalization.&lt;/p&gt; &lt;div id=&quot;fig:rec_pred_multiple&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/run00001_field_plot_fine_tuned_ch0_compressed.gif&quot; class=&quot;fig:rec_pred_multiple&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 10: GIF shows the result of recursive predictions after fine-tuning &lt;/div&gt; &lt;p&gt;Finally, to show the effectiveness of fine-tuning, we compare the weighted loss of the original best single-time step model and the resulting best-fine-tuned model for long roll-out predictions of 50-time steps in &lt;a href=&quot;#fig:loss_rec_no_rec&quot;&gt;&lt;span&gt;Figure 11&lt;/span&gt;&lt;/a&gt;. We can see that the original model sees exponential growth of errors up to 2 orders of magnitude higher in just 10 recursions while the fine-tuned model is stable up to 50 recursions. Thus, we can see that our fine tuning mechanism allows for stable roll outs well beyond the training sequence length.&lt;/p&gt; &lt;div id=&quot;fig:loss_rec_no_rec&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-19-Investigating-vision-transformer-based-models-for-closure-modeling-of-fluid-dynamical-systems/loss_rec_no_rec.png&quot; class=&quot;fig:rec_pred_multiple&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 11: Comparison of the weighted loss of the original best single-time step model and the resulting best-fine-tuned model for long roll-out predictions of 50-time steps &lt;/div&gt; &lt;h1 id=&quot;conclusions-and-future-work&quot;&gt;Conclusions and Future Work&lt;/h1&gt; &lt;p&gt;In this project, our key contributions have been&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Extending vision transformer-based models for neural closure modeling for an idealized 2D flow field (eddy shedding from flow past a cylinder). We observed a reduction in error fields by up to 95\% using neural closure augmentations, compared to just using the low-fidelity output.&lt;/li&gt; &lt;li&gt;Developing a fine-tuning procedure to achieve long recursive predictions, i.e., we have achieved stable long roll-outs for ~50 timesteps by training on just ~5-time step sequences.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Some of the limitations of our model and future work are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Scalability and usage of large amounts of VRAM for training: Training with a batch size of 1 takes up 7 GB of VRAM for the 50x150 field. Training with parallel GPUs or other more efficient transformer token mixing models like Adaptive Fourier Neural Operators (AFNOs) needs to be explored to increase the batch size or number of grid points.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Numerical integrator inside closure model: We use a forward Euler numerical solver for the numerical integrator during closure modeling, however more stable and accurate solvers like DOPRI5 or RK4 need to be investigated.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Offline training: Right now, our model uses offline data generated by a numerical solver. For complete accuracy, we would need to integrate the numerical solver (written in MATLAB or FORTRAN) into the training code and allow it to compute the low-fidelity derivatives online during training.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Effect of periodicity: The eddy shedding in the flow past a cylinder is somewhat periodic which may aid the deep learning model during training. The model needs to be tested on more realistic flows like gyre flow, or real ocean velocity fields, to test its true capabilities.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; </content> </entry> <entry> <title>Are Watermarked Large Language Models More Prone to Hallucinations?</title> <link href="https://deep-learning-mit.github.io/blog/2023/watermarked-llms/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/watermarked-llms</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;As LLMs grow in capabilities, it is becoming more and more difficult to tell apart human-written from AI-generated content. Current post-hoc AI detection tools like GPTZero, which are easy to bypass and can be biased against non-native English speakers, are neither robust nor fair. Watermarking schemes suggest a more secure and unbiased method of detecting LLM-generated content, but introduce potential quality degradation. In this blog post, I investigate whether watermarked LLMs are more likely to “hallucinate,” or make up facts, because of limitations imposed by the watermarking scheme. I formulate a nuanced research question, explain assumptions made and my experimental setup, present an analysis of my results, and present next steps. Overall, although I do not obtain statistically significant results, I do provide statistical evidence that hallucinations in watermarked LLMs are worth studying, with interpretable qualitative results that I explain with fundamental ML concepts.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;h3 id=&quot;the-need-for-ai-detection-algorithms&quot;&gt;The need for AI detection algorithms&lt;/h3&gt; &lt;p&gt;Deepfakes. AI-assisted academic plagiarism. Bots on social media spreading fake news. These are just a couple of the real-world problems brought about by the recent advancement in large language model capabilities that make it easy for malicious users to spread misinformation, while making it hard for social media platforms or governments to detect their AI origin. Thus, detecting AI-generated content in the wild is becoming one of the hottest research fields in AI. In fact, the White House recently commissioned an executive order &lt;d-cite key=&quot;biden&quot;&gt;&lt;/d-cite&gt; on AI to direct action towards detecting AI-generated content and authenticating official content. But as AI models get more and more powerful, the question arises: will it even be possible to tell apart truth from disinformation?&lt;/p&gt; &lt;p&gt;Some signs have already appeared that point to the answer being “no.” When ChatGPT first released to the public, coding Q&amp;amp;A site StackOverflow temporarily banned &lt;d-cite key=&quot;stack-overflow&quot;&gt;&lt;/d-cite&gt; answers generated by ChatGPT, because it was so easy to use ChatGPT to generate answers that seemed to be correct but were wrong on closer inspection. Perhaps you’ve experienced the following: you’re browsing Reddit or an online forum and reading a user’s reply, thinking that it’s a well-composed answer, only to realize that the structure of the reply is strikingly similar to how ChatGPT sounds.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-post-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-post-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-post-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-post.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt;A Reddit post pointing out a user who writes all their comments using ChatGPT.&lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-comment-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-comment-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-comment-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/reddit-comment.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt;An unexpected comment replying to the Reddit post above.&lt;/div&gt; &lt;p&gt;Some AI detection tools, such as GPTZero &lt;d-cite key=&quot;gpt-zero&quot;&gt;&lt;/d-cite&gt;, claim to solve this problem by detecting text generated by large language models (LLMs). However, from playing around with GPTZero, it’s not hard to prompt ChatGPT to generate text that bypasses GPTZero’s filters, or to manually paraphrase a few sentences while preserving the content’s general meaning. This is due to GPTZero’s simplistic detection method, which is just to predict if a piece of text is AI-generated by analyzing its perplexity—a measure of “randomness.” In addition to being inaccurate, AI detection tools like GPTZero frequently misclassify &lt;d-cite key=&quot;bias&quot;&gt;&lt;/d-cite&gt; text written by non-native English speakers as AI generated, bringing up issues of fairness and biases.&lt;/p&gt; &lt;p&gt;So is it all doomed? Will we reach a state of the world in which people can’t trust anything they see on the internet to be human-generated?&lt;/p&gt; &lt;p&gt;Not quite. New watermarking algorithms make it possible to trace back any text generated by specifically-watermarked LLMs with high accuracy and with low false-positive rates, and with considerable amount of effort required to modify the output of such an LLM without also degrading the quality of the output.&lt;/p&gt; &lt;h3 id=&quot;so-what-is-watermarking&quot;&gt;So what is watermarking?&lt;/h3&gt; &lt;p&gt;Watermarking, in the context of LLMs, is the process of modifying an LLMs generation process such that signals are embedded into generated text that are invisible to humans but algorithmically detectable. The key difference between watermarking and post-hoc detection algorithms like GPTZero is that post-hoc detectors rely on text outputted by LLMs to sound “artificial,” and as LLM capabilities grow, this is unlikely to hold. On the other hand, watermarking schemes work regardless of the capabilities of the underlying LLM, which make them more robust to advancements in AI. The watermarking scheme designed in &lt;em&gt;A Watermark for Large Language Models&lt;/em&gt; (Kirchenbauer, Geiping et al.) &lt;d-cite key=&quot;watermark-for-llms&quot;&gt;&lt;/d-cite&gt; is specially designed to have negligible impact on text quality and work with a publicly accessible detection algorithm, so that anyone can verify if a piece of text is generated by a particular LLM. The watermarking scheme works by selecting a pool of “green” tokens before text generation and softly preferring to sample from the pool of “green” tokens during text generation. Then, the detection algorithm checks if a piece of text contains a higher proportion of “green” tokens than expected, and if the result is statistically significant, determines that the text was generated by an LLM.&lt;/p&gt; &lt;p&gt;The existence of an undetectable, unbreakable, and accurate watermarking scheme would be incredible! By watermarking any LLM before its release, any text generated by the LLM would contain statistical signals that prove its AI origin, making it difficult for adversaries to pass off LLM-generated content as human-generated. Furthermore, because watermarking schemes rely on detecting signals associated with each LLM’s watermarking process and not by analyzing the perplexity of text, human-generated content would rarely be flagged as AI-generated. Unfortunately, the recent paper &lt;em&gt;Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models&lt;/em&gt; (Zhang et al.) &lt;d-cite key=&quot;watermark-impossibility&quot;&gt;&lt;/d-cite&gt; proves that under natural assumptions, it is impossible to achieve watermarking schemes that can resist all attacks by a computationally-bounded attacker. The key assumption, which is the existence of a quality oracle—an algorithm that can differentiate between higher and lower quality responses—is easily satisfied by using any LLM that is capable of verifying quality. The authors of the paper implement a general attack on any watermarked LLM by perturbing the output of an LLM an arbitrary number of times, and using the quality oracle to ensure the perturbation does not cause the text to degrade. After enough perturbations, the text is unlikely to contain the statistical signals embedded in the original output, and the attacker evades detection.&lt;/p&gt; &lt;p&gt;So if an attacker is willing to spend lots of time and effort, they can break any watermarking scheme. Still, maybe this barrier is enough to deter most attackers. Then, why wouldn’t we watermark every LLM released to the public?&lt;/p&gt; &lt;h3 id=&quot;quality-degradation-in-watermarked-llms&quot;&gt;Quality degradation in watermarked LLMs&lt;/h3&gt; &lt;p&gt;The truth is, because watermarking schemes force a LLM to preferentially sample from a pool of “green” tokens, the quality of the output of watermarked LLMs may decrease. To understand the intuition behind this, here’s a short clip from “Word Sneak with Steve Carell”: &lt;a href=&quot;https://youtu.be/9nBBgD0q6rA?feature=shared&amp;amp;t=107&quot;&gt;link&lt;/a&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/word-sneak-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/word-sneak-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/word-sneak-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/word-sneak.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;“We weren’t dressed properly for &lt;strong&gt;moose&lt;/strong&gt;-watching or &lt;strong&gt;zucchini&lt;/strong&gt;-finding… I for one had the wrong kind of &lt;strong&gt;pantaloons&lt;/strong&gt; on.”&lt;/p&gt; &lt;p&gt;Steve and Jimmy were given cards with random words and had to work them into a casual conversation. Similarly, one can imagine an LLM generating odd-sounding sentences in order to adhere to a watermarking scheme.&lt;/p&gt; &lt;p&gt;The effects of quality degradation are amplified the smaller the space of high-quality outputs is. For example, the prompts “What is 12 times 6?” or “What is the first section of the U.S. Constitution?” have only one accepted answer, forcing a watermarked LLM to either give up on watermarking the output or &lt;em&gt;hallucinate incorrect answers&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The latter bad outcome is the one I will investigate further in this blog post: Are watermarked LLMs more prone to hallucinations? In particular, I investigate if there are tradeoffs between quality of outputs and watermark security. Lastly, I perform a qualitative analysis of watermarked outputs, and explain any interpretable trends caused by the watermarking scheme.&lt;/p&gt; &lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt; &lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt; &lt;p&gt;I investigate my hypothesis by experimenting with unwatermarked and watermarked LLMs. I outline my experiment here: first, I ask an unwatermarked LLM to generate biographies for 100 famous people. I ask an evaluation oracle, aka GPT 3.5, to count the number of mistakes in each generated biography. This serves as my control group. Then, I create three experimental groups, each of which correspond to a watermarked LLM with varying degrees of watermarking security. I ask GPT 3.5 to count the number of mistakes by each of the watermarked LLMs, and perform statistical Z-tests to conclude whether or not watermarked LLMs are more likely to hallucinate.&lt;/p&gt; &lt;p&gt;I now walk through the steps of my experiment in more depth, with commentary on any decisions or tradeoffs I made in the process. Hopefully anyone reading this can follow what I did to replicate, or even build upon, my results!&lt;/p&gt; &lt;p&gt;My coding environment was Google Colab Pro, and its V100 GPU was sufficient to run all my code—a complete runthrough of my final Jupyter notebook would take a bit over an hour. The watermarking scheme I sought to replicate can be applied to any LLM where the watermark has access to the last layer of logits, so I looked into a variety of open-source LLMs. Ultimately, I decided on OPT (1.3 billion parameters) &lt;d-cite key=&quot;opt&quot;&gt;&lt;/d-cite&gt;, because its small model size allowed me to experiment with different parameters more efficiently, with faster inference times. Other open-source LLMs I considered were Mistral, Llama, and Roberta.&lt;/p&gt; &lt;p&gt;For my experiment, I needed a dataset of biographies of famous people. Unfortunately, I couldn’t find one publicly available after a few hours of searching, so I did the next best thing: I made my own. Using a list of 100 famous peoples’ biographies I found on a website &lt;d-cite key=&quot;famous-people&quot;&gt;&lt;/d-cite&gt;, I copy-pasted each of their Wikipedia biographies into a CSV.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/bios-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/bios-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/bios-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/bios.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Lastly, I needed an evaluation oracle to count up the number of factual mistakes in each generated biography. I decided to make a tradeoff between accuracy and efficiency by letting ChatGPT do the work for me instead of manually cross-checking sample biographies with their Wikipedia biographies. After a bit of research into OpenAI’s APIs and pricing plans, I settled on the GPT 3.5-turbo API, since I expected to generate 600k tokens for my experiment, which would be a bit less than $1 in costs. With more funding, I would have probably used GPT 4, but I checked and was satisfied with the outputs of GPT 3.5-turbo.&lt;/p&gt; &lt;h3 id=&quot;watermarking-scheme-implementation&quot;&gt;Watermarking scheme implementation&lt;/h3&gt; &lt;p&gt;With the experimental variables of open-source model, dataset, and evaluation oracle decided upon, I began to implement the watermarking scheme detailed in &lt;em&gt;A Watermark for Large Language Models&lt;/em&gt;. The watermarking scheme is made up entirely of two components: a watermarking logits processor that influences how tokens are sampled at generation time, and a watermark detector that detects if a given piece of text contains a watermark. There were also several tunable parameters detailed in the watermarking paper, but the two of interest are gamma and delta.&lt;/p&gt; &lt;p&gt;Gamma represents the breadth of the watermark in terms of vocabulary: a higher gamma includes more words in the “green” pool, making responses sound more natural but may dilute the watermark’s detectability, while a lower gamma focuses on fewer words, increasing its detectability but potentially negatively affecting the output. The authors of the watermarking paper suggested a value for gamma between 0.25 and 0.75.&lt;/p&gt; &lt;p&gt;Delta represents the intensity of the watermark, or how strongly the watermark prefers “green” tokens to “red” tokens at each step of the generation process. The higher the delta, the more evident the resulting watermark. The watermarking paper suggested a value for delta between 0.5 and 2.0.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/watermark-scheme-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/watermark-scheme-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/watermark-scheme-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/watermark-scheme.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt;A visualization of the watermarking scheme (figure from original paper).&lt;/div&gt; &lt;p&gt;Tuning these parameters, I created three different experimental groups, each corresponding to a different level of watermarking strength: strong, medium, and weak. The exact values of gamma and delta I used can be tinkered with; my choices were based on what empirically had the best effects.&lt;/p&gt; &lt;p&gt;I left the detector algorithm provided by the watermarking paper mostly untouched, except for the Z-threshold. I tuned it down to z=2.3 so the detector would be more likely to say a piece of text was watermarked for comparisons between different watermarking strengths, but this threshold still required 99% confidence. Additionally, the detector algorithm takes as input gamma, which is the same gamma used to generate the watermarked text I am attempting to detect. This is a key parameter that differentiates a watermark detector from a general post-hoc AI detector. The gamma seeds the watermarking scheme, so that during the detection process, we can work backwards to determine if the token sampling adheres to the given gamma value. This ensures that human-written text that sounds bland or like a non-native English speaker won’t be misclassified as AI-generated, resulting in a low false-positive rate.&lt;/p&gt; &lt;h3 id=&quot;prompting-my-models&quot;&gt;Prompting my models&lt;/h3&gt; &lt;p&gt;First, I needed to prompt my open-source model to generate biographies of famous people. Since the version of OPT I used is a Causal LM, not an Instruct LM, I needed to prompt it with a sentence that would make it most likely to continue where I left off and generate a biography of the specified person. After some testing, I settled on the following prompt:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/generation-prompt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/generation-prompt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/generation-prompt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/generation-prompt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;However, I noticed that the watermarked LLMs were initially outputting repeated phrases, e.g. “Barack Obama was the first African-American president of the United States. Barack Obama was the first African-American president of the United States.” Although this wasn’t technically hallucination, I wanted the output to look like a real biography, so I tuned two hyperparameters used during text generation: no_repeat_ngram_size=3 and repetition_penalty=1.1 to discourage repetitive phrases.&lt;/p&gt; &lt;p&gt;Next, I needed to prompt my evaluation oracle, GPT 3.5, to evaluate sample biographies. Since GPT 3.5 is an Instruct model, I can directly ask it to evaluate a given biography. I decided on the following prompt:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/evaluation-prompt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/evaluation-prompt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/evaluation-prompt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/evaluation-prompt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;I ask GPT 3.5 to clearly list out each detected mistake and their corresponding correction in order to reduce the likelihood of it hallucinating, as well as allowing me to manually verify its evaluations.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;quantitative-results&quot;&gt;Quantitative results&lt;/h3&gt; &lt;p&gt;After generating four biographies for each person—one unwatermarked control sample and three watermarked samples with different watermarking parameters—I evaluate them against our GPT 3.5 evaluation oracle.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/stats-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/stats-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/stats-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/stats.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;On average, the unwatermarked control LLM generates biographies that contain 8.14 mistakes each. The strongest watermarking setting has a mean of 8.64 mistakes, the medium watermark has 8.56 mistakes on average, and the weakest watermark has 8.00 mistakes on average. Clearly the weakest watermarked LLM doesn’t hallucinate any more than the control group LLM—but it does have a 13% detection rate, which is pretty substandard for a watermarking scheme. The medium and strongest watermarks perform slightly worse than the control group LLM, and by performing Z-tests on the measure statistics (never thought AP Stats would come in handy), I conclude that the probabilities of observing the results we got for the strong and medium watermarked LLMs are 26% and 34% respectively. So, although these probabilities aren’t statistically significant, they do slightly imply that watermarked LLMs hallucinate more often, and the effect is especially visible with stronger watermark settings.&lt;/p&gt; &lt;p&gt;We also see that our unwatermarked biographies had a false positive rate of approximately 1%. This can be attributed to the tuning I made to the Z-threshold, from 4.0 to 2.3. Indeed, I made the change knowing that a Z-threshold of 2.3 reflects 99% confidence, so our FPR of 1% is in line with this change. If I had left the Z-threshold to 4.0, we would have a FPR of approximately 0.003%. However, with a higher Z-threshold, the weakest watermarked LLM would consequently have an even lower successful detection rate, so I made this tradeoff of having one or two false positives in order to catch more watermarks. This also lets us see more clearly how even though weaker watermarks are less detectable, some trace of the watermarking signal still remains.&lt;/p&gt; &lt;h3 id=&quot;qualitative-results&quot;&gt;Qualitative results&lt;/h3&gt; &lt;p&gt;In addition to quantitative results, I perform a deeper, qualitative analysis on a biography generated for a specific person. I chose the strong watermarked biography for Nelson Mandela because of the interesting and interpretable trends we can see:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/nelson-mandela-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/nelson-mandela-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/nelson-mandela-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/nelson-mandela.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;I highlighted the errors pointed out by our evaluation oracle in red text for ease of comparison. Note that there may be additional errors not caught by GPT 3.5. Generally, we see that the errors occur from mixing up dates, names, ages, locations, etc., and are not completely made up facts. In fact, the biography does capture a relatively sound summary of Mandela’s life. I posture that the hallucinations we see are mostly simple fact mismatches because the watermarking schemes we impose on OPT still give it the flexibility to tell a good story of Mandela’s life, but when it comes down to token-by-token sampling, our LLM may be forced to generate the wrong date or name in order to adhere to the “green” token preference scheme.&lt;/p&gt; &lt;p&gt;I also wanted to highlight the blue text. The sentence “The Nobel Prize is one of only three Nobel Prizes awarded to individuals in history” not only is incorrect but also doesn’t add much substance to the biography. Here are three other hand-picked sections of generated biographies that aren’t informative to the person’s biography:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/repetitions-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/repetitions-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-watermarked-llms/repetitions-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-watermarked-llms/repetitions.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In each piece of text, the reported facts may not be incorrect, but they take up valuable space that we would otherwise like to be used to introduce new facts about the person. So even if these facts aren’t flagged as factual inaccuracies by our evaluation oracle, they do demonstrate a degradation in model output, raising the issue of response quality degradations in addition to hallucinations.&lt;/p&gt; &lt;h3 id=&quot;theoretical-underpinnings-of-results&quot;&gt;Theoretical underpinnings of results&lt;/h3&gt; &lt;p&gt;Taking a theoretical perspective, what exactly causes LLMs to hallucinate? To answer this question, we cite one of the important topics covered in class: reward misspecification. If, during training time, we give low error loss to outputs that sound similar to our training data, we’re not necessarily training the LLM to be more &lt;em&gt;accurate&lt;/em&gt;. Instead, we’re training the LLM to generate output that is more likely to be accepted as “close-enough” to the training data. When we ask ChatGPT to write a poem or reply to an email, being “close-enough” is usually fine. But when we need it to be 100% accurate, such as solving a math problem or generating a biography for a real person, being “close-enough” doesn’t quite make the cut.&lt;/p&gt; &lt;p&gt;Furthermore, the auto-regressive manner in which LLMs generate text means it samples the “most-likely” token, based on previously seen tokens. If our LLM starts to generate FDR’s most important New Deal measures, the “most-likely” tokens to follow might be explaining each of the New Deal measures in detail. But this isn’t what we want out of a biography of FDR!&lt;/p&gt; &lt;p&gt;Both of these problems—hallucinating false information and generating uninformative facts—are observed in our experiments. But unfortunately, it’s hard to reduce one issue without exacerbating the other. I attempted to decrease the temperature parameter in OPT’s text generation, but this resulted in OPT generating strings of run-on, non-informative sentences, such as &lt;em&gt;“Marilyn Monroe starred in several films, including Dangerous Years, Scudda Hoo! Scudda Hay!, Ladies of the Chorus, Love Happy…”&lt;/em&gt; because each additional film was the most likely follow-up to the previously generated tokens. Similarly, increasing the temperature might generate text that sounds more “human-like,” but upon closer inspection, would be riddled with factual inaccuracies.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;h3 id=&quot;further-work&quot;&gt;Further Work&lt;/h3&gt; &lt;p&gt;There’s a couple of improvements I would have made if I had more time or more compute to work with. With more time, I would have liked to learn how to scrape data from webpages, so I could create a dataset of a thousand famous peoples’ biographies. Then, I could run my experiments with a larger sample size, allowing me to produce more statistically significant results. However, even with access to a larger dataset, I would have been bottlenecked by compute. Using Colab Pro’s V100 GPU, I estimate that generating biographies and evaluating them for 1000 people would take at least 10 hours of runtime.&lt;/p&gt; &lt;p&gt;If I had access to more powerful GPUs, I also would have investigated a more nuanced research question: are bigger, more capable LLMs that have watermarking schemes applied to them less prone to hallucinating? I would have liked to run my experiments using a larger open-source LLM, such as LLaMa 65B, instead of OPT 1.3B, to see if watermarking schemes still negatively affect an LLM’s ability to perform tasks, when the base LLM is much more capable.&lt;/p&gt; &lt;h3 id=&quot;what-i-learned&quot;&gt;What I learned&lt;/h3&gt; &lt;p&gt;As this project was my first self-driven research experiment, I faced many challenges, but also learned so much. Probably the most important thing I learned is that compute is important, but it’s not an end-all-be-all. There’s tons of open-source models out there that can be run on a V100, and Google Colab Pro offers it at an affordable price. I also learned how important it is to define a well-scoped research problem, and how chatting with others can help you gain fresh insights on roadblocks.&lt;/p&gt; &lt;p&gt;I found that my work towards this project was structured much differently than how I would approach a problem set. With a pset, much of the starter code is provided, and in particular, the code to import datasets, process them, and visualize results are all provided. In this project, most of my time was spent making design decisions: which dataset should I use, how should I format my results, what hyperparameters should I use. Although the raw number of lines coded in my final notebook might not be the most, I can explain my reasoning behind each line of code clearly, and I think this is a result of the thorough research I performed.&lt;/p&gt; &lt;p&gt;Lastly, I learned that tackling an unanswered question in research is tractable for most students with some programming experience and interest in a scientific field. I didn’t have the most extensive ML background, nor any prior undergraduate research experience, but just by reading some papers on watermarking and writing down the questions that popped into my head, I came up with some viable research questions that could be tackled by an independent research project.&lt;/p&gt; &lt;p&gt;I’m very thankful to my friends Franklin Wang and Miles Wang for helping me configure my coding environment and keeping me motivated throughout the project, and also to the TAs I spoke with during the ideation and scoping stage. To other students reading this blog post who may want to get started doing ML research but aren’t sure how to get started, I encourage you to try replicating some papers with code! Papers With Code &lt;d-cite key=&quot;papers-with-code&quot;&gt;&lt;/d-cite&gt; has several papers accompanied by their codebases, and just trying to obtain the same results as them is an incredible learning experience. If anyone wants to replicate, or even build off of my work, please reach out to me if you have any questions or ideas you’d like to discuss. You can reach me at justin ji [at] college.harvard.edu.&lt;/p&gt; &lt;h3 id=&quot;supplemental-material&quot;&gt;Supplemental Material&lt;/h3&gt; &lt;p&gt;In this Github repository, you can access the dataset I made of famous people’s biographies, the code I used to generate my results, and the CSV files of results.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/jujipotle/deep-learning-project&quot;&gt;Github Repo&lt;/a&gt;&lt;/p&gt; </content> </entry> <entry> <title>Predicting the Future: LSTM vs Transformers for Time Series Modeling</title> <link href="https://deep-learning-mit.github.io/blog/2023/time-series-lstm-transformer/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/time-series-lstm-transformer</id> <content type="html">&lt;h1 id=&quot;6s898-final-project---lstm-vs-transformers-for-time-series-modeling&quot;&gt;6.S898 Final Project - LSTM vs Transformers for Time Series Modeling&lt;/h1&gt; &lt;p&gt;By Miranda Cai and Roderick Huang&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div style=&quot;text-align:center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/intro_photo_time_series.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/intro_photo_time_series.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/intro_photo_time_series.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/intro_photo_time_series.webp&quot; class=&quot;img-fluid rounded z-depth-1 w-100&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt; &lt;p&gt;In the context of time series forecasting, comparing Long Short-Term Memory (LSTM) networks to Transformers is a fascinating exploration into the evolution of deep learning architectures. Despite having distinct strengths and approaches, both LSTM and transformer models have revolutionized natural language processing (NLP) and sequential data tasks.&lt;/p&gt; &lt;p&gt;LSTMs, with their recurrent structure, were pioneers in capturing long-range dependencies in sequential data. While the accuracy of such models have been shown to be quite effective in many applications, training LSTM models takes a relatively long time because of the fact that they must remember all past observances. One faster alternative to LSTM models are transformers. Transformers are able to remember only the important bits of inputs using an attention-mechanism, and is also parallelizable making it much faster to train than recursive LSTMs that must be processed sequentially.&lt;/p&gt; &lt;p&gt;With its recent development, people have started opting to use transformer based models to solve sequence problems that once relied on LSTMs. One significant example is for NLP use cases, where transformers can process sentences as a whole rather than by individual words like LSTMs do. However, since transformers have been around for less than a decade, there are still many potential applications that are yet to be deeply explored. Thus, we will explore the effectiveness of transformers specifically for time series forecasting which finds applications across a wide spectrum of industries including finance, supply chain management, energy, etc.&lt;/p&gt; &lt;p&gt;Our goal is to realize which particular features of time series datasets could lead transformer-based models to outperform LSTM models.&lt;/p&gt; &lt;h2 id=&quot;2-related-work&quot;&gt;2. Related Work&lt;/h2&gt; &lt;p&gt;With the growth of ChatGPT in the recent years, extensive research has been done across various NLP tasks such as language modeling, machine translation, sentiment analysis, and summarization, each aiming to provide comprehensive insights into when each architecture excels and where their limitations lie. While research on time series data exists, it hasn’t garnered as much attention, so we aim to broaden this area of study.&lt;/p&gt; &lt;h3 id=&quot;21-effect-of-dataset-size&quot;&gt;2.1 Effect of Dataset Size&lt;/h3&gt; &lt;p&gt;The size of a dataset plays an important role in the performance of an LSTM model versus a transformer model. A study &lt;d-cite key=&quot;comparison&quot;&gt;&lt;/d-cite&gt; done in the NLP field compared a pre-trained BERT model with a bidirectional LSTM on different language dataset sizes. They experimentally showed that the LSTM accuracy was higher by 16.21% relative difference with 25% of the dataset versus 2.25% relative difference with 80% of the dataset. This makes sense since BERT is a robust transformer architecture that performs better with more data. As shown in the figure below from &lt;d-cite key=&quot;comparison&quot;&gt;&lt;/d-cite&gt;, while LSTM outperformed BERT, the accuracy difference gets smaller as the perctange of training data used for training increases.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-md-0 d-flex align-items-center justify-content-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/dataset_size_research_fig-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/dataset_size_research_fig-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/dataset_size_research_fig-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/dataset_size_research_fig.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;While we perform a similar methodology which is discussed further in section 4.1, the major difference is in the type of data we test. Instead of measuring classification accuracy for NLP tasks, this study measures the mean squared error (MSE) loss for regression time series data.&lt;/p&gt; &lt;h3 id=&quot;22-effect-of-noisy-datasets&quot;&gt;2.2 Effect of Noisy Datasets&lt;/h3&gt; &lt;p&gt;Theoretically, LSTMs are more robust to noisy data due to its ability to capture local dependencies. On the other hand, the self-attention mechanisms in transformers propagate errors and may struggle with sequences that have a high degree of noise. Electronic traders have been recently attempting to apply transformer models in financial time series prediction to beat LSTMs &lt;d-cite key=&quot;trading&quot;&gt;&lt;/d-cite&gt;. Largely focused on type of assets, the research showed that transformer models have limited advantage in absolute price sequence prediction. In other scenarios like price difference and price movement, LSTMs had better performance.&lt;/p&gt; &lt;p&gt;Financial data sets are known to be extremely noisy, and in addition, very hard to find due to their confidential nature. The application of &lt;d-cite key=&quot;trading&quot;&gt;&lt;/d-cite&gt; gave inspiration to study how the “amount” of noisiness would affect the LSTM and transformer models. Discussed further in section 4.2, this study added various amounts of noise to a clean dataset to see how this would affect each architecture.&lt;/p&gt; &lt;h3 id=&quot;23-effect-of-multi-step-prediction&quot;&gt;2.3 Effect of Multi-step Prediction&lt;/h3&gt; &lt;p&gt;The last feature that we would like to look at between LSTMs and transformer models is forecasting length. Forecasting length describes how far into the future we would like our model to predict based on the input sequence length. One paper &lt;d-cite key=&quot;multistep&quot;&gt;&lt;/d-cite&gt; done on short-term time series prediction finds that transformers were able to outperform LSTMs when it came to predicting over longer horizons. The transformer did better in all three cases when predicting one hour, twelve hours, and an entire day into the future. They accredit these results to the fact that attention better captured longer-term dependencies than recurrence did.&lt;/p&gt; &lt;p&gt;Similarly to this paper, we will focus only on short-term forecasting. Short-term forecasting is important in situations like stock market predictions, where stock values show high volatility in the span of hours and may or may not have learnable trends over long periods of time.&lt;/p&gt; &lt;p&gt;However, we would like to extend the results of this paper to learn to also look at multi-step prediction. This study trained models specifically to have a singular output, with each model being trained with outputs at the specified prediction horizon. Instead, we would look to train our models against outputs of different lengths. We thought it would be an interesting addition to output the entire sequence of data leading up to whatever period in the future, to give a better visualization of what actually happens as forecasting length increases.&lt;/p&gt; &lt;h2 id=&quot;3-methodology&quot;&gt;3. Methodology&lt;/h2&gt; &lt;p&gt;The dataset we will be using throughout this study is the Hourly Energy Consumption dataset that documents hourly energy consumption data in megawatts (MW) from the Eastern Interconnection grid system &lt;d-cite key=&quot;dataset&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/energy_dataset_split-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/energy_dataset_split-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/energy_dataset_split-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/energy_dataset_split.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;31-experiments&quot;&gt;3.1 Experiments&lt;/h3&gt; &lt;p&gt;We can utilize this dataset to predict energy consumption over the following features of a dataset.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Size of a dataset&lt;/strong&gt;: As discussed in Section 2.1 &lt;d-cite key=&quot;comparison&quot;&gt;&lt;/d-cite&gt;, the size of a dataset played an impact in measuring classification accuracy for NLP tasks. Since the energy dataset is numerical, it’s important to test the same concept. We leveraged nearly 150,000 data points, progressively extracting subsets ranging from 10% to 90% of the dataset. For each subset, we trained the architectures, allowing us to explore their performance across varying data volumes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Amount of noise in the dataset&lt;/strong&gt;: As discussed in Section 2.2 &lt;d-cite key=&quot;trading&quot;&gt;&lt;/d-cite&gt;, research was done to test LSTMs vs transformers on noisy stock data for various assets. We deemed the energy dataset to be relatively clean since it follows a predictable trend depending on the seasons of the year and time of the day. For example, there are higher energy levels during the winter and daytime hours. To test noise, we added incrementing levels of jittering / Gaussian noise &lt;d-cite key=&quot;augmentations&quot;&gt;&lt;/d-cite&gt; to observe the effect of noisy data on LSTMs and transformers. Example augmentations with different variances are plotted below in blue against a portion of the original dataset in red.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;d-flex justify-content-center&quot;&gt; &lt;div style=&quot;text-align:center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_0001-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_0001-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_0001-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_0001.png&quot; class=&quot;img-fluid rounded center z-depth-1 w-75&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_001-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_001-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_001-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_001.png&quot; class=&quot;img-fluid rounded z-depth-1 w-75&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_003-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_003-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_003-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_003.png&quot; class=&quot;img-fluid rounded z-depth-1 w-75&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_008-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_008-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_008-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noise_variance_008.png&quot; class=&quot;img-fluid rounded z-depth-1 w-75&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Output size&lt;/strong&gt;: As discussed in Section 2.3 &lt;d-cite key=&quot;multistep&quot;&gt;&lt;/d-cite&gt;, there have been few studies measuring the effect of varying the forecasting length, and in the ones that do they still only output one class &lt;em&gt;at&lt;/em&gt; the specified time into the future. In our novel experimentation, we aimed to generate an entire sequence of outputs &lt;em&gt;up until&lt;/em&gt; the specified time into the future. We created models that would predict forecasting lengths of 10%, …, 100% of our input sequence length of 10. To do so, we set the output size of our models to be equal to these forecasting lengths. This involved removing any final dense or convolutional layers.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;32-selected-architectures--setup&quot;&gt;3.2 Selected Architectures &amp;amp; Setup&lt;/h3&gt; &lt;p&gt;Selecting the right architecture for LSTM (Long Short-Term Memory) networks hinged on several key considerations. The LSTM architecture is extended of the RNN to preserve information over many timesteps. Capturing long-range dependencies requires propagating information through a long chain of dependencies so old observations are forgotten, otherwise known as the &lt;strong&gt;vanishing/exploding gradient problem&lt;/strong&gt;. LSTMs attempt to solve this problem by having separate memory to learn when to forget past or current dependencies. Visually, LSTMs look like the following &lt;d-cite key=&quot;rnn_lstm&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div align=&quot;center&quot; style=&quot;text-align:center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/rnn-lstm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/rnn-lstm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/rnn-lstm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/rnn-lstm.png&quot; class=&quot;img-fluid rounded z-depth-1 w-75&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Transformers, initially designed for natural language processing, should work well for time series data. They operate by utilizing self-attention mechanisms, allowing them to capture long-range dependencies effectively. A transformer breaks down the input sequence into smaller, fixed-size segments known as tokens, representing various time steps or features. Through multiple layers of self-attention and feedforward operations, the transformer architecture should excel at capturing both short-term and long-term dependencies. A figure of transformer time series is shown below from &lt;d-cite key=&quot;transformer_arch&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div align=&quot;center&quot; style=&quot;text-align:center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_arch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_arch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_arch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_arch.png&quot; class=&quot;img-fluid rounded z-depth-1 w-75&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;There were certain parameters that we kept fixed throughout all variations of our models. The first was training on batches of data with sequence length 10. Second, we trained all of our LSTM models for 500 epochs and all of our transformer models for 10 epochs. These numbers were chosen with some fine-tuning to yield meaningful results while also allowing the training for so many individual models to be done in a reasonable amount of time.&lt;/p&gt; &lt;p&gt;Additionally, the data was normalized since the range of energy values was from 10000 Megawatts (MW) to 60000 MW. Normalizing the data improves convergence for gradient descent optimization and mitigates issues related to model regularization.&lt;/p&gt; &lt;h2 id=&quot;4-experimental-results-and-discussion&quot;&gt;4. Experimental Results and Discussion&lt;/h2&gt; &lt;h3 id=&quot;41-size-of-a-dataset&quot;&gt;4.1 Size of a Dataset&lt;/h3&gt; &lt;p&gt;Given the energy consumption dataset described in Section 3, we trained and evaluated an LSTM model and transformer model on progressively increasing subsets ranging from 10% to 90% of the dataset. The figure below shows the normalized mean squared error (MSE) loss for each subset of the dataset.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;d-flex flex-column justify-content-center&quot; style=&quot;text-align:center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_trans_dataset_size_res-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_trans_dataset_size_res-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_trans_dataset_size_res-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_trans_dataset_size_res.png&quot; class=&quot;rounded z-depth-1 w-50&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The experimental results show that transformers have an improving trend as the size of the dataset increases while the LSTM has an unclear trend. Regardless of the size of the training dataset, the LSTM doesn’t have a consistent result for the testing set.&lt;/p&gt; &lt;p&gt;In an LSTM, there exist additional gates for a sequence of inputs $x^{(t)}$ where in addition to the sequence of hidden states $h^{(t)}$, we also have cell states $c^{(t)}$ for the aforementioned separate memory. While the LSTM architecture does provide an easier way to learn long-distance dependencies, it isn’t guaranteed to eradicate the vanishing/gradient problem discussed in Section 3.2. While the same is true for transformers, the transformer architecture addresses the vanishing/exploding gradient problem in a different way compared to LSTMs. Transformers use techniques like layer normalization, residual connections, and scaled dot-product attention to mitigate these problems.&lt;/p&gt; &lt;p&gt;For time series dataset, the transformer architecture offers the benefit of the self-attention unit. In NLP, it’s typically used to compute similarity scores between words in a sentence. These attention mechanisms help capture relationships between different elements in a sequence, allowing them to learn dependencies regardless of their distance in the sequence. For time series data, transformers might offer advantages over LSTMs in certain scenarios, especially when dealing with longer sequences or when capturing complex relationships within the data such as seasonal changes in energy use.&lt;/p&gt; &lt;p&gt;From a qualitative perspective, if we pull a subset of the test data to observe the predicted values from an LSTM vs a transformer for 40% of the training set, we have the following.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/test_set_pred_40-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/test_set_pred_40-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/test_set_pred_40-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/test_set_pred_40.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/p&gt; &lt;p&gt;While transformers did perform better than LSTMs, it’s not like the LSTM did a horrible job. We notice that at the peaks, the LSTM overshot more than the transformer and at the troughs, the LSTM undershot. However, overall, both architectures still had good results. In the context of the size of time series data, transformers do seem more promising given the loss figure above. It seems that LSTMs are losing that dependency on old observations while transformers are gaining ground as the size of the dataset increases. While &lt;d-cite key=&quot;comparison&quot;&gt;&lt;/d-cite&gt; showed that bidirectional LSTM models achieved significantly higher results than a BERT model for NLP datasets,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The performance of a model is dependent on the task and the data, and therefore before making a model choice, these factors should be taken into consideration instead of directly choosing the most popular model. - Ezen-Can 2020&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;For this experiment, the outlook of large datasets in time series applications for the transformer architecture looks promising.&lt;/p&gt; &lt;h3 id=&quot;42-amount-of-noise-in-a-dataset&quot;&gt;4.2 Amount of Noise in a Dataset&lt;/h3&gt; &lt;p&gt;To test the performance of our models on simulated noisy data, we first trained our models on batches of the original clean dataset and then ran our evaluations on different levels of noisy data. Random noise was added according to Gaussian distributions with variances in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[0.0, 0.0001, 0.001, 0.002, 0.003, 0.005, 0.008, 0.01]&lt;/code&gt; to create these data augmentations. Below is a comparison of the MSE loss for both models as a function of the injected noise variance.&lt;/p&gt; &lt;div style=&quot;text-align:center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noisy_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noisy_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noisy_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/noisy_loss.png&quot; class=&quot;img-fluid rounded z-depth-1 w-50&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Since loss is not very descriptive in itself, we also visualize the model output for some of these augmented datasets. For each graph below, red is the true value while blue is predicted value.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;table border=&quot;0&quot;&gt; &lt;tr&gt; &lt;td&gt;&lt;b style=&quot;font-size:15px&quot;&gt;LSTM&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b style=&quot;font-size:15px&quot;&gt;Transformer&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_0001.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_002-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_002-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_002-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_002.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_002-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_002-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_002-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_002.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_005-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_005-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_005-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_005.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_005-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_005-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_005-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_005.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_noisy_01.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_noisy_01.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/p&gt; &lt;p&gt;Both models are shown to start off similarly, predicting very well with no noise. However, almost immediately we can see that the LSTM does not handle noise as well as the transformer. LSTM makes much noisier predictions with many more outliers. We suspect this occurs due to the implicit inductive bias of the memory feature in the LSTM module. Consider a time step $t$. The memory accrued up to point $t$ “weights” the data seen in recent past time steps $t-1$, $t-2$, $\ldots$, much more so than the data seen relatively long ago. While this is an intuitive design for memory, we can observe that this mechanism combines storing temporal information with token-specific information. In order to compete with a transformer, the LSTM model needs to be trained on significantly more data.&lt;/p&gt; &lt;p&gt;The transformer on the other hand has the negative effects of its own inductive bias mitigated by its attention mechanism. Because the transformer has both a mechanism to account for temporal information and a mechanism to select the next associated token (attention module), and because they are separated, it is able to produce more “accurate” results.&lt;/p&gt; &lt;h3 id=&quot;43-prediction-size&quot;&gt;4.3 Prediction Size&lt;/h3&gt; &lt;p&gt;Finally, we created and trained separate models with varying numbers of output classes to represent the prediction size. We trained on output sizes as percentages of our input size, in increments of 10% from 0% to 100%. Because our input sequence was a constant 10 and our data is given in hourly intervals, these percentages translated to have prediction horizons of 1hr, 2hrs, …, 10hrs. Evaluating our models resulted in the following MSE loss trends.&lt;/p&gt; &lt;div style=&quot;text-align:center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/prediction_size_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/prediction_size_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/prediction_size_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/prediction_size_loss.png&quot; class=&quot;img-fluid rounded z-depth-1 w-50&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Again, to get a better sense of why we see these results, we visualize the outputs. Since our outputs are sequences of data, to have a more clean visualization we plot only the last prediction in the sequence. For each graph below, red is the true value while blue is predicted value.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;table border=&quot;0&quot;&gt; &lt;tr&gt; &lt;td&gt;&lt;b style=&quot;font-size:15px&quot;&gt;LSTM&lt;/b&gt;&lt;/td&gt; &lt;td&gt;&lt;b style=&quot;font-size:15px&quot;&gt;Transformer&lt;/b&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_10-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_10-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_10-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_10.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_10-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_10-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_10-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_10.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_50-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_50-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_50-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_50.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_50-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_50-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_50-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_50.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_80-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_80-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_80-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_80.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_80-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_80-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_80-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_80.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_100-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_100-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_100-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/lstm_pred_100.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;td&gt;&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_100-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_100-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_100-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-time-series-lstm-transformer/transformer_pred_100.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/p&gt; &lt;p&gt;As we can see, the MSE loss of our transformer model increased at a slower rate than our LSTM model. After comparing the outputs of our models at these time steps, it becomes evident that this trend is due to the LSTM losing characteristic over time. Our transformer simply performs worse when it has to predict more as expected because the data is not perfectly periodic. However, we infer that the LSTM outputs get flatter over time because the more we accumulate memory through the long-term mechanism, the less weight each previous time step holds, diluting the total amount of information carried through the sequence. Transformers avoid this problem by using their attention mechanisms instead to keep only the important information throughout.&lt;/p&gt; &lt;h2 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h2&gt; &lt;p&gt;Through the experiments tested in Section 4 (on dataset size, dataset noise, and prediction size), transformers seem to be a promising alternative to LSTMs for time series modeling due to their unique architecture, particularly suited for capturing long-range dependencies. Unlike LSTMs, transformers utilize self-attention mechanisms that allow them to consider relationships between all elements in a sequence simultaneously. This capability is especially advantageous in time series data, where capturing distant dependencies is crucial for accurate forecasting. Additionally, transformers mitigate vanishing gradient problems better than LSTMs, enabling more robust training on longer sequences.&lt;/p&gt; &lt;p&gt;While transformers excel in parallel computation theoretically, one significant issue is the extensive memory requirements during training, especially with larger models or datasets. Transformers demand significant memory for storing attention matrices, limiting the batch size that can fit into GPU memory. So, for those who are finding an optimal architecture to train a time series dataset, one has to consider his or her own design priorities of accuracy and performance.&lt;/p&gt; &lt;p&gt;All in all, the choice between LSTMs and transformers for time series datasets depends on the implementer’s design priorities and the task at hand. With some research showing LSTMs outperforming transformers and others such as our study showing the opposite, there is a clear need to dive deeper into the subject especially given the extensive number of applications for time series modeling.&lt;/p&gt; </content> </entry> <entry> <title>Studying the benefits and limitations of sparse auto-encoders for compositional reasoning tasks</title> <link href="https://deep-learning-mit.github.io/blog/2023/sparse-autoencoders-for-othello/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/sparse-autoencoders-for-othello</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Neural networks accomplish complex tasks and are poised to be increasingly used in critical and ubiquitous sectors of civilization. But given a model seemingly solving a problem, how much can we say about precisely how it does that and what its solution looks like?&lt;/p&gt; &lt;p&gt;It might seem like this type of question would be hopeless, but interpretability has been progressing and we can make some headway on questions like these. One of the issues for interpretability is the fact that networks pack a lot of information into individual neurons in complex hard to separate ways, which means it’s hard to look at top activating examples for a neuron and see what it’s doing. This is &lt;a href=&quot;https://arxiv.org/abs/2209.10652&quot;&gt;superposition&lt;/a&gt;. &lt;a href=&quot;https://transformer-circuits.pub/2023/monosemantic-features/index.html&quot;&gt;Anthropic’s recent paper&lt;/a&gt; leveraged sparse autoencoders (&lt;em&gt;SAEs&lt;/em&gt;) to learn an interpretable basis for LLM features. Sparse autoencoders are weak dictionary learning algorithms that leverage autoencoders trained to encode and then decode the activations of a certain module in the model. Contrary to classical auto-encoders, the hidden state does not necessarily have to be smaller (enforcing compression), but the mapping has to be sparse, which we enforce by penalizing the L1 norm of the activations, where L1 is just the sums of the absolute values. This makes the feature basis much more disentangled, clean and sparse.&lt;/p&gt; &lt;p&gt;That paper is far reaching in its results and suggests a lot of potential for SAE interpretability methods. However our work wants to investigate how effective SAEs are in contexts where there is a lot of compositional reasoning. Indeed, the a lot of the features they find hinge on the fact that their simple 1L language model is picking up on a lot of cleanly separatable cues and heuristics that are feeding into its prediction – for example a feature that’s high for arabic text, or in HTML contexts, etc…. But this seems like it’d be harder if we have a model composing reasoning and computation across steps in by nature entangled ways.&lt;/p&gt; &lt;p&gt;So we decided to see how this method would perform on a task where there are plausibly much less heuristic features that are are separable, and intuitively requires more compositionality and reasoning than the capabilities of a small 1 layer language model. We turned to the game of Othello, for which some ML interpretability has already been done, making our analysis easier, and applied sparse autoencoders to see how they would perform and what we could learn from them. We picked Othello because it’s a complex task where it might seem intuitive that the model has to gradually compose information across layers and reason about what types of moves and positions might be valid. Indeed, in the original Othello-GPT paper, they find a linear world representation when you feed the model sequence data, suggesting complex reasoning patterns. This is an initial analysis and there are many things we’d be excited to see that would make this more fleshed out.&lt;/p&gt; &lt;h1 id=&quot;background-and-related-work&quot;&gt;Background and related work&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Sparse Autoencoders&lt;/strong&gt;: There is some previous work done on &lt;a href=&quot;https://arxiv.org/abs/2103.15949&quot;&gt;dictionary learning&lt;/a&gt; to interpret neural networks. The idea of sparse dictionary learning is to find an over-complete basis (ie there are more basis vectors than dimensions) in your embedding space, such that on inputs in your data most of the dictionary basises are orthogonal to your data, and only a few activate (sparsity). This has been used very recently to visualize transformer features for language models, as a way of taking internal feature representations out of &lt;a href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot;&gt;superposition&lt;/a&gt;.Superposition is a barrier to interpertability where neurons and features are encoding a lot of things at once, making it hard to study on individual behaviors and parts of the model. Most recently, Anthropic did extensive interpretability work on a 1 layer transformer by using sparse autoencoders in &lt;a href=&quot;https://transformer-circuits.pub/2023/monosemantic-features#related-work&quot;&gt;decomposing language models&lt;/a&gt;. They learned a sparse embedding space and then conducted a lot of analysis and interpretability on the features the original network was learning by studying it in the sparse embedding space.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Transformers for reasoning tasks and Othello:&lt;/strong&gt; Transformers and specificially &lt;a href=&quot;https://arxiv.org/pdf/2106.01345.pdf&quot;&gt;decision transformers&lt;/a&gt; have formerly been used for more complicated tasks than natural language sequence prediciton like reasoning tasks and games and proven to be successful. Although cutting edge LLMs exhibit strong reasoning capabilities, toy models and small languag models that are more accessible and that people are trying to use for interpretability are quite small, limiting their reasoning ability. Othello is a simple to understand but complex to win two player board game, where you gradually place pieces and try to “capture opponent” pieces by sandwiching rows, columns, and diagonals of the board with two of your pieces. The winner is the player with the most pieces at the end. &lt;a href=&quot;https://arxiv.org/pdf/2210.13382.pdf&quot;&gt;Recent work&lt;/a&gt; lead to the creation of a dataset of Othello games and the publishing of a model called Othello-GPT that learns to play Othello successfully. We use both of these in our work. The way they train the model is by giving it sequences of Othello moves from games, and asking it to predict the next move, in an unsupervised way, obtaining a model that can predict legal moves and understands the mechanism of the game. They show the existence of representations forming in the model, by using a probe to recover the full board state from the model activations, even though it’s just given a sequence. This suggests the model learns more than just heuristics and is able to do internal reconstruction of the game’s features.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interpreting features and circuits&lt;/strong&gt; In the original Othello-GPT, their world model probe was nonlinear. Neel Nanda &lt;a href=&quot;https://www.neelnanda.io/mechanistic-interpretability/othello&quot;&gt;extended their work&lt;/a&gt; and found a linear world representation of the othello model, by seeing that instead of representing the state as “black’s turn” vs “white’s turn”, the model represented it in an alternating manner, distinguishing between “my turn” vs “their turn”. There is also some other work on &lt;a href=&quot;https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability&quot;&gt;interpreting&lt;/a&gt; transformer models outside of the context of language modeling, for example with decision transformers, but this is very much a growing subfield. We were also able to get a better intuition for the features in the othello model by using &lt;a href=&quot;https://kran.ai/othelloscope/index.html&quot;&gt;neuron visualization data published by the authors&lt;/a&gt;.&lt;/p&gt; &lt;h1 id=&quot;method-and-setup&quot;&gt;Method and setup&lt;/h1&gt; &lt;p&gt;In order to investigate a reasoning task, we used a synthetic GPT model trained on a dataset of valid Othello game sequences of length 60 &lt;a href=&quot;https://github.com/likenneth/othello_world&quot;&gt;(by Li et al)&lt;/a&gt;. We manipulate and access the model’s activations and internals using the &lt;a href=&quot;https://neelnanda-io.github.io/TransformerLens/&quot;&gt;TransformerLens&lt;/a&gt; library.&lt;/p&gt; &lt;p&gt;We used the MSE loss as a baseline to compare the performance of sparse autoencoders on a reasoning tasks versus a natural language sequence prediction task. We replicated the training of a recent &lt;a href=&quot;https://www.alignmentforum.org/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning&quot;&gt;set of dictionaries&lt;/a&gt; of similar size on the GPT language model (EleutherAI’s 6-layer pythia-70m-deduped) and compare our results.&lt;/p&gt; &lt;p&gt;Our set up for the replication, where we pick the same hyperparameters as the authors, consists of an 8-layer &lt;a href=&quot;https://openai.com/research/language-unsupervised&quot;&gt;GPT&lt;/a&gt; model with an 8-head attention mechanism and a 512-dimensional hidden space. We set up a buffer that gathers the model’s activations on a batch of game data and uses it to train the autoencoder. The buffer automatically runs the model on another batch of data once it is half empty. The activations then get fed into the autoencoder’s training loop, where it optimizes to minimize the reconstruction loss of form $L = L_1 + L_2$. In this equation, $L_1$ is the term originating from the $L_1$ norm of the weights, with a sparsity coefficient of $1e-3$ for the encoder of size $16 \times 512 = 8192$ a sparsity coefficient of $3e-3$ for the size $64 \times 512 = 32768$ and $L_2$ is the term originating from the square error of the reconstruction with regards to the actual model investigations.&lt;/p&gt; &lt;p&gt;We then train various sizes of sparse autoencoders on the 4th layer of the othello model and investigate the impact of the autoencoders size on the reconstructed hidden state.&lt;/p&gt; &lt;p&gt;We measure the reconstruction power of the encoder with a reconstruction score defined as $\frac {Loss_{ZeroAblation} - Loss_{Reconstruction}} {Loss_{ZeroAblation} - Loss_{Normal}}$ where $Loss_{ZeroAblation}$ is Loss after ablating the reconstructed layer and use this as a measure for how well the encoder is able to reconstruct the mlp layer. The intuition behind this is that we compare a “base zero”, which is the ablation loss, with both the reconstruction of the layer and the original construction of the layer. This will provide us with a metric of how close our reconstruction is to ground truth.&lt;/p&gt; &lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt; &lt;h2 id=&quot;comparison-to-pythia-70m-dictionaries&quot;&gt;Comparison to Pythia-70m dictionaries&lt;/h2&gt; &lt;p&gt;The following tables are the results from training a sparse autoencoder of size $16 \times 512 = 8192$ and $L_1$ penalty coefficient of $1e-3$.&lt;/p&gt; &lt;p&gt;Encoder’s Measured MSE loss on OthelloGPT after 100000 epochs.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;MSE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.370&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.537&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.686&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.833&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.744&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Encoder’s reported MSE loss on Pythia-70m after 100000 epochs.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;MSE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.056&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.089&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.108&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.135&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.148&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The following tables are the results from training a sparse autoencoder of size $64 \times 512 = 32768$ and $L_1$ penalty coefficient of $3e-3$&lt;/p&gt; &lt;p&gt;Encoder’s Measured MSE loss on OthelloGPT after 100000 epochs.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;MSE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.749&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.979&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;1.363&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;1.673&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;2.601&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Encoder’s reported MSE loss on Pythia-70m after 100000 epochs.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;MSE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.152&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.211&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.222&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;From the results above we can see that the autoencoder reconstructs with higher MSE loss despite having the same sparsity constraint and multiplier between the activation size and the sparse embedding. The difference becomes more drastic as we increas the sparsity of the encoder. Our analysis of these results is that this aligns with our hypothesis in natural language sequence prediction for small models like these, it might be that it is easier for the encoder to learn sparser and more easily separable features that allow it to recover the activations. However, on a task like playing the game of Othello where the features are more abstract, and we think there might be a higher requirement of complex compositionality across layers, increasing sparsity and size, makes the model perform worse.&lt;/p&gt; &lt;p&gt;Another significant emerging pattern in the MSE loss of the encoders is the fact that loss increases in the furthur layers, which backs up our initial claim; that as features become more abstract, the autoencoder has a harder time reconstructing them.&lt;/p&gt; &lt;p&gt;It is worth noting that the increase of MSE across the two sets of tables is impacted by both the increase in size and sparsity. We had made the two tables, to match the already existing &lt;a href=&quot;https://www.alignmentforum.org/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning&quot;&gt;benchmarks&lt;/a&gt;. However, in the following, we include the results of a sparse autoencoder with penalty coefficient of $3e-3$ and size $16 \times 512 = 8192$ to validate our claims about sparsity, without the effect of size.&lt;/p&gt; &lt;p&gt;Encoder’s Measured MSE loss on OthelloGPT after 100000 epochs.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;MSE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.954&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;1.389&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;1.715&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;2.038&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;3.057&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We see that without the effect of size and only sparsity, the model performs significantly worse and if we compare the results to the one of size $64 \times 512 = 32768$ the model gets better because it is has more dimensions.&lt;/p&gt; &lt;h2 id=&quot;investigating-the-effect-of-size&quot;&gt;Investigating the effect of size&lt;/h2&gt; &lt;p&gt;In furthur investigation, we experimented with training various sizes of autoencoders on layer 4 of the model. The size of the autoencoder is determined by the equation $size = x \times 512$ where $x$ is the size factor. We vary the size factor from $0.25$ to $32$. The size factor describes how much our autoencoder embedding space is bigger than the original activation space, therefore deciding how much “extra space” the autoencoder has to obey the sparsity constraint and preserve good reconstruction. We included smaller sizes so that we could investigate the effect of size and whether the encoder would be able to learn more compact features and still perform well. Our results are found in the following:&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://hackmd.io/_uploads/S1GB0NBUp.png&quot; alt=&quot;recons_loss vs epochs&quot; /&gt;&lt;/p&gt; &lt;p&gt;As seen in the figure above, we see reconstruction loss decrease significantly as the number of dimensions in the autoencoder’s hidden space becomes larger than the original space. A sparse autoencoder with less dimensions than the original latent space fails to reconstruct well and this can be even better observed in the following figure.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://hackmd.io/_uploads/BJAJerHLa.png&quot; alt=&quot;Screenshot 2023-12-11 at 8.47.16 PM&quot; /&gt;&lt;/p&gt; &lt;p&gt;This picture suggests that maybe if we scale up sparse auto encoder embedding size we can recover performance at low cost. However, Anthropic’s interpretability work, linked earlier, suggests that as you increase the size of your autoencoder embeddding, you risk getting a lot of niche highly specific features with complex interactions, therefore making interpretability harder. For example, at a given size they observe a base64 feature that fires for base64 text, and then at a larger size they see it splits into several base64 features that activate for slightly different token beginnings.&lt;/p&gt; &lt;p&gt;These results highlight the challenge of sparse autoencoders for compositional tasks, and bring us to the question of interpreting sparse embedding spaces for compositonal reasoning.&lt;/p&gt; &lt;h2 id=&quot;interpreting-the-sparse-autoencoder&quot;&gt;Interpreting the sparse autoencoder&lt;/h2&gt; &lt;p&gt;Here we had to take a detective’s approach and form different hypotheses of what the model was doing and how to test them. This analysis is exploratory, and given more time we’d be excited about extending this/doing even more experiments to get a complete picture. However, we’re excited about what we found and are confident that this approach is promising.&lt;/p&gt; &lt;p&gt;We started by caching the autoencoder embeddings on a subset of data with valid Othello sequences and moves. This gave us a dataset to work with.&lt;/p&gt; &lt;p&gt;We then did some macro level analysis by looking at and inspecting random features (dimensions of the embeddings) and seeing what kinds of boards activated most on them (by activated most we mean that the feature had a high value on that input activation for that board). This somewhat followed the pattern laid out by &lt;a href=&quot;https://transformer-circuits.pub/2023/monosemantic-features/index.html#global-analysis-interp&quot;&gt;Anthropic’s analysis&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, in Anthropic’s 1L language model paper they have the following figure:&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://hackmd.io/_uploads/SyIELvLIT.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; &lt;p&gt;They are indicating that in their setup most of the features seem to be interpretable and clear to a human, according to human scores. In our experience looking at our sparse autoencoder and top activating examples for different features, it seems that a lot of the features are still not interpretable and we will need more work to understand the full picture &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. This may be because a lot of semantic cues for simple language modeling are more aligned with our human understanding, in the sense that the concepts the model operates on are pretty intuitive, whereas for Othello it has to build a compositional model of the game state across layers, in ways that are potentially less likely to correlate with how we might perceive the problem. We don’t claim that there are not such complex dynamics in even simple language models (there definitely are!), but we think there are more simple patterns to pick up on. We believe that the method laid out in that work needs to be extended to be applied to compositional networks for reasoning adjacent tasks, because it does not seem sufficient for this Othello model. This is an empirical claim based on studying and looking at a lot of data on when sparse features were activating throughout the Othello dataset.&lt;/p&gt; &lt;p&gt;To do some global analysis, we computed a frequency histogram of the values of each feature on the dataset, and then we took an average of this frequency histogram to get a full picture of how often and how strongly features are activating across the dataset. This is on a log scale.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://hackmd.io/_uploads/B1V7_HIL6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; &lt;p&gt;As we can see, on average for each feature there are a lot of inputs where the feature is not reading much at all, which makes sense given the sparsity constraint. Then as the activation gets higher and higher the frequency of each bucket decreases.&lt;/p&gt; &lt;p&gt;If we increased the sparsity regularization even more we might see a sparser activation graph with more high activing frequency for large activations, but in a lot of classic encoders the distribution of embeddings tends to have a lot of smaller noise around zero, where here a lot of our values are actually very often split into either zero, or something significant.&lt;/p&gt; &lt;p&gt;We then proceed to making some hypotheses about how the model might be localizing computation about the game board throughout its features, and make some tests to see what might be going on.&lt;/p&gt; &lt;h3 id=&quot;h1-location-features&quot;&gt;H1: Location features&lt;/h3&gt; &lt;p&gt;Hypothesis: what if there are features that represent the location of the last move, and only activate when that last move is within some cluster of the board? This would align with earlier world model wor.&lt;/p&gt; &lt;p&gt;This would be an example of a strong monosemantic and interpretable feature.&lt;/p&gt; &lt;p&gt;However, we later realized that this is probably more likely as a more primitive pattern that would be noticed earlier in the model layers, before it then refines and comes up with information to decide what to predict.&lt;/p&gt; &lt;p&gt;Never the less, we looked at the contexts in which a feature is reading strongly, and thus found a list of high-activating moves for each feature (&lt;em&gt;for what current moves is feature j activating&lt;/em&gt;). We then clustered these into 3x3 location clusters on the board, marking positions as the same if they were close in a small square. That was based on the idea that it does not have to be activating for the exact same current move but moves in general that are adjacent. These features would then represent: &lt;em&gt;was the current move around this position of the board?&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;This plot was computed by looking at those activating contexts for each feature and seeing how many non-adjacent clusters of positions are within those moves. We then compute a histogram on the cluster count, trying to see how many features activate locally in a small number of clusters.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://hackmd.io/_uploads/BymEFrU8T.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; &lt;p&gt;We can see that our hypothesis was wrong here and that at this point in the network our features are activating for current moves across the board, not really in a localized, and don’t sparsely activate just when a given location is played. This was useful data to see and showed us that at this point in the network it was probably operating on high level features and things that could directly relate to its final prediction. The small amount of locally activating features all tend to just have small activations in general.&lt;/p&gt; &lt;h3 id=&quot;h2-predictive-features&quot;&gt;H2: Predictive features&lt;/h3&gt; &lt;p&gt;This brought us to the next experiment, where we wanted to test for higher level patterns related to its prediction.&lt;/p&gt; &lt;p&gt;We were curious studying the link between the times when a feature of our autoencoder is writing strongly on an input and the actual correct prediction for that input, ie the actual correct next token it’s trying to predict. Is there a localization effect there where a feature activates highly only when the correct prediction is within some cluster?&lt;/p&gt; &lt;p&gt;We investigated and collected, for each feature, a list of the real (heldout) next action in the sequence whenever it is activating non negligibly. This gave us a sequence of next moves for each context where a feature wrote strongly to the activation output. Then we clustered these actions into regions of 3x3 squares on the board, trying to narrow in on the idea of local activation of a feature. We operationalized the notion of reading strongly on a game board by setting a threshold activation of 0.001 by looking at the earlier plot of activation distribution and seeing what made sense. This is actually pretty low, but it still stays significant because the sparsity constraint often just nulls out values when they are not relevant, so even low small values have signal.&lt;/p&gt; &lt;p&gt;This allows us to map each feature to a number of activating clusters.&lt;/p&gt; &lt;p&gt;We then plot a histogram for the number of clusters of next action locations for each feature in our dataset. The idea is that if a feature is activating on a small number of clusters for the next action, then it might be picking up in patterns on the board that are linked to the final model’s prediction, in a consistent way based on the real result.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://hackmd.io/_uploads/Sy9PKBUIT.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; &lt;p&gt;It’s interesting to compare this to the previous plot, as here there are actually a decent amount of features that seem localized, reacting and forming predictions based on what part of the board they think the next step or move might be in, and not activating across the board for the next token. These are the ~100s of features that are only reacting in some small amount of clusters, like two or 1.&lt;/p&gt; &lt;p&gt;It seems that in layer 4 in certain cases the model is already developing an idea of what the next move will be, and is localizing sparse features for different prediction areas.&lt;/p&gt; &lt;p&gt;This explanation is not explaining the full behavior and there is probably a lot going on to extend the prediction into higher layers. We can see this in the frequencies of all the features that are activating in a lot of different next-token contexts, probably picking up on general things on the board and harder to interpret compositional steps that will allow it to make predictions later.&lt;/p&gt; &lt;p&gt;This reminded us of the [logit lens] in language modeling where you can unembed the early activations and get coherent (and gradually improving as you increase the layer number) predictions for the next token. This seems to be showing that some of the features are already localizing predictions about the correct prediction, in a consistent manner.&lt;/p&gt; &lt;p&gt;We investigated those features corresponding to the left side of the plot ($1 \leq x \leq 3$, $x$ number of clusters) that activate only for some cluster of valid next sequence areas and found data that validated this impression! We hypothesize it’s because some action predictions are pretty clear to predict early on based on good strategy and how the dataset of sequences was generated. We found features that consistently were activating for when a given board position was the correct next board position.&lt;/p&gt; &lt;p&gt;We focused particularly on feature #15 of our dim 4096 autoencoder, noticing through our analysis that it had interesting activation patterns.&lt;/p&gt; &lt;p&gt;We plotted its activation value histogram:&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://hackmd.io/_uploads/Byk19HULT.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; &lt;p&gt;We can see a long sparse tail of inputs where the feature doesn’t activate at all, and then a similar frequency for values beyond some threshold of activation.&lt;/p&gt; &lt;p&gt;On manual inspection, other than the big cluster of samples where it’s reading zero or doesn’t activate, the feature is basically always just activating when the next move is in a specific cluster at the bottom of the board. To be more precise, 90% of the boards where it activates with a value &amp;gt; 0.001 are in that cluster, 93% for 0.01,&lt;/p&gt; &lt;p&gt;Here are some of those example boards, where the next move played is G4, and the model activates strongly.&lt;/p&gt; &lt;p&gt;One of many examples of board where feature #15 activates strongly and in fact the next correct move is G4. &lt;img src=&quot;https://hackmd.io/_uploads/BJZEDS8U6.png&quot; alt=&quot;image&quot; /&gt; &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;Example where the feature activates and the actual next move is F4, right above G4, in the same cluster: &lt;img src=&quot;https://hackmd.io/_uploads/ryy8Jj8U6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; &lt;p&gt;This is really interesting! Why does this feature exist? We’ve been thinking about the structure of Othello and the way the data was generated, and we think the idea is that the network is pretty confident about this position and early on manages to recognize and see what’s going on with the rest of the board to put its hypothesis in this feature.&lt;/p&gt; &lt;p&gt;Although we haven’t explained a lot of the other features, it’s cool that this method has allowed us to understand and correlate this feature with a state of the game and the understanding the model has of the board!&lt;/p&gt; &lt;h1 id=&quot;discussion-and-conclusion&quot;&gt;Discussion and Conclusion&lt;/h1&gt; &lt;p&gt;We are excited about future work in this direction and think interpreting compositional computation circuits is key to understanding how tranformers and language models solve complex problems. In terms of our work with Othello GPT, we are excited about pushing sparse autoencoders further on this architecture and motivating more interpretability work. We are interested in work to train SAEs across layers and then see if we can track computation and model changes through sparse embeddings across layers, mirroring this [exploratory work]. This might be helpful to understand compositionality across layers. We also think interpreting features for SAEs with width smaller than the original width might be interesting to find projections of network activations that have very high level, compressed features, that might therefore be easier to interpret. We are also interested in methods that use SAE features to make causal statements aobut model behavior, for example by plugging the SAE into the model inference step, where at the end of our MLP we feed in the decoded encoded version of activations into the rest of the model. With this kind of setup you could then potentially ablate or modify different features to validate and study how your interpretability hypotheses about different parts of the model actually change its final predictions. Some of the limitations of our work is that we would have liked to run more experiments on different sparsity coefficients, and make more in depth comparisons to language models to see to what extent our arguments about compositional reasoning hold in a rigorous way. We would be excited to see how increasing sparsity even more affects our ability to interpret the model, potentially making things more tractable. We also recognize the difficulty of interpretability and have not been yet been able to interpret any of the more complex Othello SAE mechanisms.&lt;/p&gt; &lt;p&gt;To conclude, we’ve investigated the potential for sparse autoencoders for compositional reasoning tasks in the context of the Othello sequence prediction problem. Our hypothesis is that sparse autoencoders will be useful to understand such systems but their application will be more involved and complex than for earlier patterns found in language modeling tasks. We trained a sparse autoencoder at different layers of the network and see how its performance and capabilities differ compared to previous results on language. We observe our autoencoder trained with the same hyperparameters and scaling factor for size still struggles to reach the same reconstruction performance as those for language model activations. This reveals something about the structure of these data distributions, and supports our intuition that for simple small language models SAEs are particularly performant due to their ability to pick up on a lot of separable and sparse features, but for compositional solutions where the model is learning an algorithm across layers to solve a task, the sparsity constraint incurs more of a cost, which limits this method. This intuition stems from the idea that leveraging the full extent of neural activations for compositional tasks is key to build complex algorithms across layers, and maybe less so for prediction problems that are more tractable through the composition of independent heuristics. We also nonetheless do some interpretability on our trained autoencoder, and note that the features seem less directly interpretable than those for language model SAE features (as supported by our hypothesis), but that there is some signal to analyze and understand, giving us hope for future work to use SAEs to understand compositional reasoning and circuis in general. In particular, we look at the range and frequency of sparse activations, and form different hypotheses about the ways the model might be localizing computation in sparse embeddings. We find the existence of predictive neurons already at layer 4, that activate when the model is already confident about a specific next action to predict. Although much of the features remain obscure, our results indicate that although sparsity is a harder constraint to impose for compositional reasoning, it can still be a useful starting point to interpret model computation.&lt;/p&gt; &lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt; &lt;ol&gt; &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt; &lt;p&gt;To some extent increasing the sparse regularization penalty could help with this, but our exploratory analysis revealed that increasing the sparsity penalty made the model perform too badly on the data. We could always counter this by increasing the size of the encoder, but Anthropic’s paper and our understanding suggests that this leads core interpretable features to split and split until it’s hard to get a good picture of what’s going on. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt; &lt;p&gt;these plots are both before the G4 cluster move is played. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt; </content> </entry> <entry> <title>Solvent Encoding for solubility prediction using GNN</title> <link href="https://deep-learning-mit.github.io/blog/2023/solvent-encoding/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/solvent-encoding</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Solubility serves as an essential descriptor that models the interaction between molecules and solvents. This property is important for many biological structures and processes, such as DNA-ion interactions and protein foldings. Quantum mechanics-based approaches, such as Density Functional Theory (DFT), have been deployed in multiple attempts to model solubility across diverse systems and temperatures. However, the complex nature of the problem makes it computationally demanding to accurately predict the properties with fast speed. The development of QSPR(Quantitative structure-property) and deep graph neural network enables us to explore the chemical space with significantly lower computational costs by modeling molecules as graphs and treating properties prediction problems as regression problems. Yet, the challenge persists—individual molecules do not exist in isolation. Due to the strong interaction between molecules, the existence of other molecules(solvent, in particular) in the environment can strongly impact the property we want to predict. However, most of the existing GNN models can only take one molecule per input, limiting their potential to solve more general chemical modeling problems. As a result, it is important to incorporate solvent embedding into the models. The focus of the project is to augment existing GNN models with various solvent-encoding methods and evaluate the performances of different models on a publicly available solubility dataset. My goal is to find out the best encoding method and potentially compare the performances of different models on various solubility datasets.&lt;/p&gt; &lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt; &lt;p&gt;This project intricately explores the functionalities of Graph Neural Network (GNN)-based models, focusing specifically on chemprop and PharmHGT. These models have exhibited remarkable proficiency in predicting molecular properties through a diverse array of message-passing and readout functions. The transformation of solvent smiles strings into feature vectors is executed through two distinctive methods. The initial approach involves the conversion of solvents into various descriptor vectors, while the second method treats solvents as independent graphs, applying GNN models to capture their inherent structural nuances.&lt;/p&gt; &lt;p&gt;Following this encoding phase, various methods are employed to convert the solvent vector to solvate. Currently, my strategy involves vector concatenation, and subsequently transforming the combined vector into a novel encoding vector using Multi-Layer Perceptrons (MLP). The post-encoding phase involves channeling the vector through MLP, culminating in the generation of prediction values.&lt;/p&gt; &lt;p&gt;The evaluation of the models encompasses essential metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R2) values. These metrics collectively offer a comprehensive assessment of the efficacy of different encoding methods and models. The experimental validation is conducted on the BigSolDB dataset curated by Lev Krasnov et al, comprising experimental solubility data under varying temperatures and with diverse solvents. This dataset provides a robust foundation for rigorously evaluating the predictive capabilities of the GNN-based models in real-world scenarios.&lt;/p&gt; &lt;h2 id=&quot;literature-model-and-descriptor-review&quot;&gt;Literature, model, and descriptor review&lt;/h2&gt; &lt;p&gt;Graph Neural Network(GNN) based machine learning models are one of the most fastest growing and powerful modeling tools for molecular properties prediction that can be utilized in various applications, including material and drug design. One of the most powerful models that has been published is chemprop, a model developed by Kevin Yang et al. in 2019. In contrast to traditional GNN-based models which adopt MPNN, chemprop takes advantage of D-MPNN which delivers messages using direct edges. This approach can avoid unnecessary loops in the message-passing trajectory. The model also adopts an innovative message-passing strategy called belief propagation. The power of the model has been demonstrated on various tasks including absorption wavelength prediction(Kevin Greenman et al., 2022) and IR spectroscopy(Esther Heid et al., 2023).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/chemprop-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/chemprop-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/chemprop-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-solvent-encoding/chemprop.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In tandem with chemprop, I integrate the Pharmacophoric-constrained Heterogeneous Graph Transformer (PharmHGT) into this project, a model crafted by Yinghui Jiang et al., tailored specifically for drug discovery. In addition to traditional nodes and edges representations corresponding to atoms and bonds in the molecules, the model creates supernodes based on the predefined pharmacophore groups(which are features that are necessary for molecular recognition) and connects those supernodes with the corresponding groups of atoms using junction edges. The model then employs message-passing neural networks on the heterogeneous graph, complemented by transformer layers serving as readout functions.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/PharmHGT-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/PharmHGT-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/PharmHGT-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-solvent-encoding/PharmHGT.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In implementing the descriptor approach, I incorporated three distinct types of descriptors: the Minnesota Solvation Database descriptors, compiled by Aleksandr V. Marenich et al. (referred to as mn descriptor), Solvent Polarity Descriptors gathered by Christian Richardt (referred to as Richardt descriptor), and Solvent Effect Descriptors collected by Javier Catalan (referred to as Catalan descriptor). These descriptors, each sourced from reputable studies and researchers, contribute diverse perspectives to the solubility analysis undertaken in this article.&lt;/p&gt; &lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt; &lt;p&gt;The BigSolDB dataset encompasses solubility data across various temperatures and solvents. To mitigate the temperature’s impact on solubility, I opted to focus on entries at the most prevalent temperature in the dataset—303.15 K—excluding all others. Subsequently, I transformed solubility values into logarithmic form, a commonly used measure in the realm of chemistry. I then test the PharmHGT model on the processed dataset by running two separate message-passing neural networks on both the solvent and the solvate molecules and concatenating the resulting feature vector to form a representation vector of the solvent-solvate system. Unexpectedly, the model encountered issues contrary to my initial expectations. The challenge lies in PharmHGT’s reliance on predefined pharmacophore groups to generate a graph representation of a given molecule. In instances where a molecule lacks pharmacophore groups—a commonplace scenario for small molecules like benzene or certain larger aromatic molecules—the model fails during initialization due to incorrect dimensions (specifically, 0 due to the lack of corresponding features). To overcome this hurdle, I devised the “graph augmentation approach.” For each solvent molecule, I introduced an auxiliary molecule (Dimethylformamide, DMF) containing predefined pharmacophore groups, facilitating the initialization steps. By merging the solvent graph with the auxiliary graph, the model can successfully run the initialization steps thanks to the presence of the extra junction edges in the graph.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/graph-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/graph-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/graph-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-solvent-encoding/graph.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;To maintain parity with the chemprop model for fair comparisons, I refrained from augmenting solvate molecules with DMF. Instead, I excluded all molecules incompatible with the PharmHGT models. Post-filtering, the dataset was randomly partitioned into three segments: an 80% training set, a 10% testing set, and a 10% validation set. This preprocessing lays the groundwork for a rigorous evaluation of the models and ensures a comprehensive understanding of their performance in solubility prediction. I concatenates different kinds of solvent descriptors to the dataset and evaluate their performances separately.&lt;/p&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;p&gt;The processed data comprises 2189 entries in the training set, 273 entries in the testing set, and 267 entries in the validation set. I conducted training on the modified PharmHGT and chemprop models using this dataset. Both models exhibited promising results, showcasing a test RMSE ranging from 1 to 1.7, significantly influenced by the chosen encoding methods. Notably, chemprop consistently outperforms PharmHGT across all encoding methods, although the relative performance order varies. Within the chemprop model, the mn, catalan, and graph augmentations methods yield similar results, with a test RMSE ranging between 1.1 and 1.2 logM and a MAE ranging between 0.70 and 0.72 logM. Conversely, the reichardt descriptor performs less favorably, exhibiting a test RMSE of 1.31 logM and a test MAE of 0.84 logM . Intriguingly, in the PharmHGT model, these trends are reversed. The reichardt descriptor encoding attains the best performance with a test RMSE of 1.315846 and a second lowest test MAE of 0.91, while the catalan encoding method shows the highest test RMSE at 1.66 and the highest test MAE at 0.84. This discrepancy may be attributed to PharmHGT’s specialized design for drug molecules which typically have molecular weights ranging from 400 to 1000 Da. In contrast, solvent molecules generally possess molecular weights below 200 Da and often lack pharmacophore groups that provide additional information to the model. As a result, the model tends to be reduced to basic GNN models, focusing solely on modeling interactions between neighboring atoms and therefore ignoring the important functional groups that strongly influenced the solubility.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_RMSE-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_RMSE-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_RMSE-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_RMSE.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_MAE-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_MAE-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_MAE-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_MAE.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;To validate this hypothesis, I conducted an analysis of the training RMSE across various encoding methods for PharmHGT. The finding reveals that the graph-augmentation methods beat all other methods by a huge margin. The graph augmentation method boasts a training RMSE of only 0.29 while all other methods exhibit training RMSEs of at least 0.42. This may also be attributed to the reduction of the PharmHGT models. The simple structures of solvent molecule graphs make the model susceptible to overfitting, resulting in a notably higher testing RMSE for the graph-augmentation method. Furthermore, my investigation uncovered that the catalan encoding method demonstrates a significantly higher training RMSE compared to other encoding methods, indicating that PharmHGT struggles to extract information from the descriptors. This aligns with the observation that the catalan encoding method also yields the largest testing RMSE among all encoding methods.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_PharmHGT_train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_PharmHGT_train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_PharmHGT_train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_PharmHGT_train.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Examining the R2 scores reveals a consistent pattern, where the Chemprop model consistently beats the PharmHGT models across all employed encoding methods. Within the Chemprop model, the mn, catalan, and graph-augmentation methods exhibit similar outcomes, showcasing test R2 values ranging from 0.82 to 0.84. Conversely, the reichardt descriptor lags behind, presenting a less favorable test R2 of 0.78. These trends undergo a reversal within the PharmHGT model. The reichardt descriptor encoding achieves the best performance with a test R2 of 0.77, while the catalan encoding method records the lowest test R2 at 0.57. This intriguing reversal highlights the nuanced impact of encoding methods on model performance, emphasizing the need for tailored approaches based on the underlying molecular structures.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_R2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_R2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_R2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-solvent-encoding/solvent_encoding_R2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In the course of my experimentation, a consistent trend emerges wherein chemprop consistently outperforms pharmHGT across an array of encoding methodologies. Among these methodologies, the mn descriptor method maintains a stable, albeit moderate, level of performance, denoting its reliability without yielding any outstanding superiority.&lt;/p&gt; &lt;p&gt;A noteworthy observation manifests when employing the catalan descriptor method, which remarkably enhances the effectiveness of the PharmHGT model. Conversely, the chemprop model attains its peak performance when coupled with the reichardt descriptor methods and its worst performance when coupled with the catalan descriptor, showing that the strong dependencies of encoding methods across different models.&lt;/p&gt; &lt;p&gt;However, it is imperative to underscore that each encoding method exhibits inherent limitations, precluding the identification of a universally optimal solution applicable to both models concurrently. This nuanced understanding underscores the necessity for tailored approaches, grounded in an appreciation for the distinctive characteristics and demands of each model.&lt;/p&gt; &lt;p&gt;Further scrutiny into the training loss data reveals a notable constraint within the PharmHGT model. Its proclivity towards specificity for drug molecules renders it less adept at handling general tasks, necessitating the introduction of auxiliary graphs to augment its functionality. This intricacy adds a layer of consideration regarding the pragmatic applicability of the model in contexts beyond its primary pharmaceutical focus.&lt;/p&gt; &lt;p&gt;In navigating these findings, it becomes evident that the pursuit of a comprehensive and adaptable model mandates a nuanced comprehension of the interplay between encoding methodologies, model architecture, and the inherent limitations associated with specific domains.&lt;/p&gt; &lt;h2 id=&quot;prospective-works&quot;&gt;Prospective works&lt;/h2&gt; &lt;p&gt;Due to the complex nature of solvent-solvate interactions, a more rigorous splitting strategy that takes into account the distributions of different solvent molecules within the training, testing, and validation sets may be needed. Additionally, random splitting and cross-validation could be potential methods for improving the generality of the model. Finally, owing to the limited computational resources, this project only trained the model with default hyperparameters (such as batch size, layer width, number of tokens, etc.). Hyperparameter optimization can also be performed to gain a better understanding of the model’s capabilities.&lt;/p&gt; &lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Analyzing Learned Molecular Representations for Property Prediction &lt;a href=&quot;https://pubs.acs.org/doi/10.1021/acs.jcim.9b00237&quot;&gt;https://pubs.acs.org/doi/10.1021/acs.jcim.9b00237&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Pharmacophoric-constrained heterogeneous graph transformer model for molecular property prediction &lt;a href=&quot;https://www.nature.com/articles/s42004-023-00857-x&quot;&gt;https://www.nature.com/articles/s42004-023-00857-x&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Multi-fidelity prediction of molecular optical peaks with deep learning &lt;a href=&quot;https://pubs.rsc.org/en/content/articlelanding/2022/sc/d1sc05677h&quot;&gt;https://pubs.rsc.org/en/content/articlelanding/2022/sc/d1sc05677h&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Minnesota Solvent Descriptor Database &lt;a href=&quot;https://comp.chem.umn.edu/solvation/mnsddb.pdf&quot;&gt;https://comp.chem.umn.edu/solvation/mnsddb.pdf&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Solvatochromic Dyes as Solvent Polarity Indicators &lt;a href=&quot;https://pubs.acs.org/doi/10.1021/cr00032a005&quot;&gt;https://pubs.acs.org/doi/10.1021/cr00032a005&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt;Toward a Generalized Treatment of the Solvent Effect Based on Four Empirical Scales: Dipolarity (SdP, a New Scale), Polarizability (SP), Acidity(SA), and Basicity (SB) of the Medium &lt;a href=&quot;https://pubs.acs.org/doi/10.1021/jp8095727&quot;&gt;https://pubs.acs.org/doi/10.1021/jp8095727&lt;/a&gt;&lt;/li&gt; &lt;li&gt;BigSolDB: Solubility Dataset of Compounds in Organic Solvents and Water in a Wide Range of Temperatures &lt;a href=&quot;https://chemrxiv.org/engage/chemrxiv/article-details/6426c1d8db1a20696e4c947b&quot;&gt;https://chemrxiv.org/engage/chemrxiv/article-details/6426c1d8db1a20696e4c947b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Chemprop: A Machine Learning Package for Chemical Property Prediction &lt;a href=&quot;https://chemrxiv.org/engage/chemrxiv/article-details/656f3bae5bc9fcb5c918caa2&quot;&gt;https://chemrxiv.org/engage/chemrxiv/article-details/656f3bae5bc9fcb5c918caa2&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;data&quot;&gt;data&lt;/h2&gt; &lt;p&gt;The data and code for the experiments are available at &lt;a href=&quot;https://github.com/RuiXiWangTW/solvent_encoding-data&quot;&gt;https://github.com/RuiXiWangTW/solvent_encoding-data&lt;/a&gt;&lt;/p&gt; </content> </entry> <entry> <title>6.s898 Final Project- Investigating the biological underpinnings of latent embeddings for scRNA-seq</title> <link href="https://deep-learning-mit.github.io/blog/2023/scRNA-GNNs/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/scRNA-GNNs</id> <content type="html">&lt;h2 id=&quot;background-and-motivation&quot;&gt;Background and Motivation&lt;/h2&gt; &lt;p&gt;Neurodegenerative diseases represent a complex and diverse group of disorders characterized by the progressive degeneration of the structure and function of the nervous system. They are notoriously challenging to study due to their multifaceted nature and varied pathological features. Single-cell sequencing technologies have been developed and are powerful techniques for understanding the molecular basis of many pressing scientific questions such as the causality and development of Alzheimer’s Disease (AD). These technologies, namely single-cell RNA sequencing (scRNA-seq) and single-cell Assay for Transpose-Accessible Chromatin sequencing (scATAC-seq), offer us an understanding of a cell’s state as a phase-space determined by chromatin accessibility and gene expression. Single cell data like this is extremely high dimensional; on the scale of 10s or 100s of thousands of cells, each with 10s of thousands of “features,” which represent genes or chromatin regions. Because of this, lower dimensional representations of these cells and clusters within them are valuable to help simplify our view of the data and extract signals. Moreover, in the context of cells characterized by biomarkers and stemming from patients with varying neurodegenerative diseases, it is in our interest to explore cell neighborhoods and embeddings to investigate if they properly represent the biological underpinnings of such disease.&lt;/p&gt; &lt;h2 id=&quot;graph-neural-networks-gnns-as-an-architecture-and-their-application-to-single-cell-analysis&quot;&gt;Graph Neural Networks (GNNs) as an architecture and their application to single-cell analysis&lt;/h2&gt; &lt;p&gt;Graph Neural Networks (GNNs) are a class of deep learning models that are specifically designed to handle data that is structured as a graph, which extends the principles of neural networks to handle the concept of graph topology. In GNNs, each node (which in this application represents cells) aggregates information from graph neighbors through transformation and pooling steps, which results in a model whose representation captures node level and graph level features. Relevantly, GNNs generate lower dimensional embeddings of the input data, which provides a compact and informative representation of high dimensional data such as single-cell RNA data.&lt;/p&gt; &lt;p&gt;The scGNN package specifically applies these principles of GNNs to single-cell genomics, treating cells as nodes in a graph and the edges as a measure of similarity in the transcriptome of two cells. scGNN performs two main functions: clustering and imputation. The architecture is as such:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feature Autoencoder: Generates low-dimensional representation of gene expression, which is the foundation for a cell graph.&lt;/li&gt; &lt;li&gt;Graph Autoencoder: Learns a topological representation of the aforementioned cell graph, which is the foundation for cell type clustering.&lt;/li&gt; &lt;li&gt;Cluster Autoencoders: There is an autoencoder for each cell type that reconstructs gene expression values.&lt;/li&gt; &lt;li&gt;Imputation Autoencoder: Recovers imputed gene expression values.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;intro-to-the-data&quot;&gt;Intro to the Data&lt;/h2&gt; &lt;p&gt;The &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S009286742300973X?ref=pdf_download&amp;amp;fr=RR-2&amp;amp;rr=834b08acfbd66ac7&quot;&gt;dataset&lt;/a&gt; being presented is a scRNA-seq atlas of the aged human prefrontal cortex. It consists of 2.3 million cells sampled from 427 individuals over a varying range of Alzheimer’s pathology and cognitive impairment. The subset of this data being analyzed in this project are the 19 samples that had multiome sequencing conducted, although only the scRNA-seq was used for this analysis (excluding the scATAC-seq). This was approximately 100 thousand cells and originally 36 thousand genes that are categorized into three diagnoses: no AD, early AD, and late AD based on biomarkers like amyloid plaque and niareagan score.&lt;/p&gt; &lt;h2 id=&quot;applying-scgnn-to-our-ad-scrna-seq-data&quot;&gt;Applying scGNN to our AD scRNA-seq data&lt;/h2&gt; &lt;p&gt;I began by processing the raw sequencing data into a csv format that would be suitable as input to the pipeline. I then ran preprocessing on this data, which consists of log transformation, filtering out low quality/sparse genes and cells, and subsetting to the top 2000 highly variable genes by variance. I then ran the actual imputation and clustering pipeline with the following parameters: EM-iteration=10, Regu-epochs=500, EM-epochs=200, cluster-epochs=200, quickmode=True, knn-distance=euclidean. The result of training is a imputed cell matrix, a cell graph, cell type clusters, and the actual embeddings of the cells themselves. These results provide the foundation for the next layer of analysis.&lt;/p&gt; &lt;h2 id=&quot;visualizing-the-degree-distribution-of-the-cell-graph&quot;&gt;Visualizing the Degree Distribution of the Cell Graph&lt;/h2&gt; &lt;p&gt;The figure below is a histogram that represents the number of other cells each cell in the dataset is connected to in the cell graph as computed by the Graph Autoencoder. We can see that the distribution is skewed right, which tells us that most cells are connected to a relatively few other cells, which could indicate a particularly heterogeneous cell population. However, there are a select few that have substantially higher number of connections, which could represent some sort of “hub” cells.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/assets/img/2023-12-12-scRNA-GNNS/degree.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;understanding-cell-clusters-in-the-embedding-space&quot;&gt;Understanding Cell Clusters in the Embedding Space&lt;/h2&gt; &lt;p&gt;The next approach was a detailed analysis of the clusters generated by the graph architecture by comparing to clusters generated on the imputed output data. This is important in visualizing the efficacy of the GNNs embeddings in delineating cell types compared the clusters derived from traditional methods on the imputed data, which included all 2000 highly variable genes (HVGs). The steps are as following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Computing Neighbors: Step 1 is to compute the neighbors for each cell, which as a reminder explains gene expression similarity between cells.&lt;/li&gt; &lt;li&gt;Principal Component Analysis (PCA): The subsequent step is to compute PCA on the data, which is a dimensionality reduction technique.&lt;/li&gt; &lt;li&gt;Louvain Clustering: After PCA, I used Louvain clustering, which is widely used in scRNA-seq analysis for clustering cell types, and tuned the resolution to match a similar number of clusters as generated in scGNN.&lt;/li&gt; &lt;li&gt;UMAP Visualization: To visualize clusters, I used Uniform Manifold Approximation and Projection (UMAP), which is a dimensionality reduction technique that allows us to visualize the cell data in 2-dimensions, colored by cluster. I colored the UMAP first by the clusters generated on the embedded data by scGNN and then by the PCA/Louvain clusters.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the figures below, we see the result of computing cell type clusters based on data embedded by the feature and graph autoencoder versus using the traditional method of PCA then Louvain clustering. While they resulted in slightly different number of clusters, it is interesting to see that the traditional method appears to outperform the GNN in terms of separating clusters in the embedding space. Further analysis on the differentially expressed genes (DEGs) in each cluster would need to be done to confirm which cell type each cluster truly represents. Only then would we be able to determine the accuracy of each, but from a visual perspective in UMAP space, the GNN clusters are less consistent.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/assets/img/2023-12-12-scRNA-GNNS/pca_louvainclusters.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/img/2023-12-12-scRNA-GNNS/scGNNclusters.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;exploring-alzheimers-related-gene-contributions-to-the-embedding-space&quot;&gt;Exploring Alzheimer’s Related Gene Contributions to the Embedding Space&lt;/h2&gt; &lt;p&gt;Deep learning techniques and architectures like VAEs and GNNs are promising and seemingly relevant techniques for topics like single-cell genomics where data is extremely high dimensional and sparse. However, these complex algorithms beg the question of whether and how they represent the underlying biology, especially in the context of diseases like Alzheimer’s. Fortunately, while still incurable, AD has been extensively researched, and is strongly associated with a number of hereditary genes, mutations, and misfolded protein aggregates. This known research provides a robust benchmark when applying new techniques to AD data. When trying to implicate new genes or represent genes (features) in a lower dimensional embedding space, it is usually a good sign to check whether the known biomarkers of AD are also being predicted or also being represented. In our case, these embeddings provide the opportunity to see if the model captures the relevant biological information, which can then provide some level of validation to any other genes that are also being represented.&lt;/p&gt; &lt;p&gt;To explore this further, I performed correlational analysis between the gene expression matrix from the imputed data and the “expression” values derived from the embedding dataframe. By focusing on the top 1% (20 genes) of genes that had the highest correlation for each embedding, I identified any biologically relevant genes that were being represented in the embedding. Below is a list of the AD relevant genes that showed up as being highly represented in this embedding space.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;APOE: This gene, particularly the e4 allele, is the most widely known genetic risk for late onset Alzheimer’s Disease. This allele is responsible for about half of all AD cases&lt;/li&gt; &lt;li&gt;APP: This gene is called Amyloid Precursor Protein. You might recognize amyloid, which is the main hallmark of AD when it misfolds and becomes aggregate plaque in the brain. Abnormal cleavage of APP leads to an increase in amyloid plaque accumulation.&lt;/li&gt; &lt;li&gt;SORL1: Genetic mutations of this gene are associated with AD because of its role in recycling APP.&lt;/li&gt; &lt;li&gt;BIN1: Bridging integrator 1 has been implicated in many AD GWAS studies and has been found to influence the spread of tau, which is another hallmark of AD when misfolded, leading to neurofibrillary tangles.&lt;/li&gt; &lt;li&gt;CLU: Clusterin has been implicated in AD for its role in clearing amyloid-beta plaque from the brain.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For example, in the figures below you can see that APOE falls into the genes with the highest correlation for embedding number 24, with a correlation of 0.79, and APP falls into those for embedding number 5 with a correlation of 0.79 as well.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/assets/img/2023-12-12-scRNA-GNNS/embedding5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/img/2023-12-12-scRNA-GNNS/embedding24.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;wrapping-it-up&quot;&gt;Wrapping it up&lt;/h2&gt; &lt;p&gt;I hope this analysis has demonstrated the potential of combining advanced computational methods in deep learning with with foundational biological data like scRNA-seq on AD to unravel long standing questions we have in the field.&lt;/p&gt; &lt;h2 id=&quot;future-analysis&quot;&gt;Future Analysis&lt;/h2&gt; &lt;p&gt;Due to computational time, I elected to train the model on the entire dataset. Future work could include training the model on subsets of the data separated by the different level of AD pathology, which would give a slightly more nuanced understanding of disease progression and how that is reflected in the embedding space of each diagnosis category.&lt;/p&gt; </content> </entry> <entry> <title>Forbidden Facts</title> <link href="https://deep-learning-mit.github.io/blog/2023/forbidden-facts/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/forbidden-facts</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;When large language model (LLMs) predict the next token, they often need to reconcile competing objectives. For example, a pretrained model is trained to be both helpful and harmless, which may come into conflict if a user asks a dangerous query. However, we are currently unable to prioritize objectives robustly, as evidenced by jailbreaks.&lt;/p&gt; &lt;p&gt;We aimed with our project to explore how language models mechanistically reconcile competing objectives, with the hope that a deeper understanding of these mechanisms could inspire better model alignment and control. For example, if a language model is instructed to always say the incorrect answer, and then is prompted to fill in a factual association, there are two objectives. One objective is to complete the factual association and the next token prediction correctly, and the other is to follow the instruction of saying an incorrect answer. These objectives are opposing because it is impossible to follow both objectives accurately, and it is reasonable for the model to follow either instruction. In this task, the model will consistently output the correct answer without the incorrect answer instruction and consistently output an incorrect answer with the instruction. Specifically, we tried to understand how Llama-2 models reconcile obedience and truthfulness in the context of prompts like the following one:&lt;/p&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SYS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;You&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obedient&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;assistant&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;who&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;only&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responds&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;punctuation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;You&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;truthfully&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;However&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allowed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;say&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forbidden&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;California&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SYS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Golden&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Gate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bridge&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;br&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&amp;gt;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;p&gt;Here the obedience objective makes the model not want to respond with “California”, while the truthfulness objective does the opposite. Since there has already been some existing work on how large language models perform factual recall, we decided to focus on the obedience objective, and in particular on what mechanisms the model uses to downweight the forbidden word.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;Factual recall. Meng et al. &lt;d-cite key=&quot;meng2022locating&quot;&gt;&lt;/d-cite&gt; located factual associations within GPT-style autoregressive transformer models using causal tracing. They found strong causal effects on predictions localized in two main areas: 1) at the final layers processing the last token, and 2) more notably at middle layers processing the last token of the subject. Further analysis showed the middle layer effects are driven primarily by the MLP modules. This suggests that GPT stores factual associations about a subject via a localized computation in the MLPs when they process the subject token. Mor Geva et al. &lt;d-cite key=&quot;geva2023dissecting&quot;&gt;&lt;/d-cite&gt; extend this study of the factual recall circuit by distilling it into three parts. First, the representation at the last subject token gets enriched by early MLP layers, composed of many subject-related attributes. Second, the information about the relation between the subject and answer token propagates to the last token. Finally, the last token “queries” the subject representation to extract the attribute that the model then outputs, which is done by attention heads that encode subject-attribute mappings in their parameters.&lt;/p&gt; &lt;p&gt;Competing circuits. Circuits are subgraphs of a neural network with distinct functionalities. The field of mechanistic interpretability seeks to reverse engineer model internals that produce a behavior into explainable algorithms, and recent works have rigorously explored toy examples of these circuits. These works usually focus on describing a circuit where the prompt directs the model to complete one distinct task. This project seeks to understand what happens when there are two opposing tasks and how the circuits for each task interact with each other.&lt;/p&gt; &lt;p&gt;Mechanistic interpretability. Olsson et al. &lt;d-cite key=&quot;olsson2022context&quot;&gt;&lt;/d-cite&gt; and Nanda et al. &lt;d-cite key=&quot;nanda2023progress&quot;&gt;&lt;/d-cite&gt; were important early papers in the emerging field of Mechanistic Interpretability. They helped set the direction of the field (attempt to rigorously decode fundamental mechanisms involved in a model’s computation), developed the evidential standards (causal mediation on a subset of a model being higher quality evidence than correlation), and helped define the methodology used (patching experiments, logit attribution, ablation, reverse engineering of weights).&lt;/p&gt; &lt;p&gt;Prompt injections. Wei et al. &lt;d-cite key=&quot;wei2023jailbroken&quot;&gt;&lt;/d-cite&gt; propose that one failure mode of language models that leads to prompt injections is the competition between capabilities and safety objectives. Models are trained for instruction following, language modeling, and safety. This project aims to achieve a mechanistic understanding of how prompt injections operate with respect to these competing objectives.&lt;/p&gt; &lt;p&gt;Latent knowledge. Research has demonstrated that models have latent knowledge of correct answers, but won’t output them if prompted naively &lt;d-cite key=&quot;saunders2022self&quot;&gt;&lt;/d-cite&gt;. A problem for future advanced models is being able to distinguish whether its outputs are truthful or deceptive. For example, if a model’s objective is to say the incorrect answer, somewhere in its internal computations it has to compute what the correct answer is to know it is outputting the incorrect answer. One question that naturally arises is if we are able to deduce from its internal representations what the correct answer is. This project investigates a model’s latent knowledge under specific circumstances.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;p&gt;We initially used Llama-2-7B-chat, a 32-layer decoder-only transformer model fine-tuned with supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. We chose Llama-2-7B-chat because the model achieves reliably good performance on our instruction tasks, has its weights open-sourced, and has a relatively low number of parameters to reduce computational costs. Previously, we fine-tuned GPT-2-XL on the Alpaca instruction dataset, but could not get reliable results on our tasks.&lt;/p&gt; &lt;p&gt;A competing prompt is when the correct answer is forbidden, and a non-competing prompt is when an incorrect answer is forbidden (equivalent to a normal factual recall).&lt;/p&gt; &lt;p&gt;We used first-order patching to replace a component’s activations in a non-competing run with its activations in a competing run (and vice versa). To calculate component $r_{i}$’s importance, we take the log odds of predicting the correct answer in a non-competing run with $r_{i}$ patched from a competing run, and subtract the log odds of predicting a correct answer during a normal non-competing run:&lt;/p&gt; \[\begin{equation} \left[ \mathrm{LO}_a\left( r_i(\mathbf{p}_\text{c}) + \sum_{j \neq i} r_j(\mathbf{p}_\text{nc}) \right) - \mathrm{LO}_a\left(\sum_{j} r_j(\mathbf{p}_\text{nc})\right) \right]. \end{equation}\] &lt;p&gt;This is a natural method to analyze model mechanisms at a coarse-grained level. If Llama 2 is a Bayesian model that aggregates information from each component, Equation 2 can be interpreted as the average log Bayes factor associated with changing the $r_{i}$’s view of the world from forbidding an incorrect answer to forbidding the correct answer. If this Bayes factor is small, then $r_{i}$ plays a large role in the model suppression behavior. We also only consider the residual stream on the last token because these components have the direct effect on the next token prediction.&lt;/p&gt; &lt;p&gt;By first-order, we mean we don’t consider the effect the component may have on other components. We chose to do first-order patching because when multiple pieces of evidence are independent, their aggregate log Bayes factor is the sum of their individual log Bayes factors, which is why we can cumulatively add the components’ importance in the last plot.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;Our high-level takeaway was that the forbidding mechanism is complicated. The following plots illustrate its overall behavior:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;This plots the probability Llama 2 answers a competing prompt correctly versus the probability it answers a non-competing prompt correctly across our dataset. A competing prompt is when the correct answer is forbidden, and a non-competing prompt is when an incorrect answer is forbidden (equivalent to a normal factual recall). The plot is cut off on the sides because we filter the dataset to ensure the model gets the initial factual recall task correct and has a significant suppression effect.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;To get this plot, we first rank the residual stream components (attention heads and MLPs on the final token) by their importance from first-order patching, a tool we explain in the next paragraph. Adding the components’ importance “scores”, we find that patching 41 components is enough to achieve the same suppression as patching all 1057 components. This number stays roughly the same across the 13b and 70b versions of Llama even as the total component size rises to 10,570 components. There is a lagging tail here: for example, only the top 15 components are needed to create ~80% of the suppression effect.&lt;/p&gt; &lt;p&gt;I worked jointly with Tony Wang and Kaivu Hariharan on this research project. I then did some more extended analysis for this class.&lt;/p&gt; &lt;h2 id=&quot;extended-results&quot;&gt;Extended Results&lt;/h2&gt; &lt;p&gt;I then asked how this analysis scales to larger models, namely the 13 and 70 billion parameter versions of Llama 2.&lt;/p&gt; &lt;p&gt;13B:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;70B:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;I initially expected the number of components needed to replicate the initial effect to scale with the number of parameters. For example, 70b has 10,570 components versus 7b’s 1057 components. However, the number actually slightly decreased to 37 components and 38 components for 13b and 70b, respectively. We can see that the total suppression effect increased from 7b but remained the same between the two. This analysis suggests that the number of components needed to implement the suppression circuit remains constant as the number of model parameters grows. This makes sense when considering that larger models have a lot more emergent capabilities, which would entail a greater number of circuits as opposed to just enlarging the circuits of smaller models.&lt;/p&gt; &lt;p&gt;The highly important suppressor heads paid attention to the forbidden token, which raised the question of how they knew to do so. We found that the key and query were “enriched” throughout the layers of the model to pay attention to the forbidden token. Testing query and key specificity, we found that swapping the queries from competing and non-competing runs made no difference to the attention paid to the key. Nor did changing the positional embeddings of the key. There must be another, perhaps more complicated, way that the model knows to pay attention to the forbidden token.&lt;/p&gt; &lt;p&gt;We found that suppression heads privilege attend more to the forbidden word when it is the correct answer to the factual recall task. Understanding these model’s heuristics mechanistically allowed us to exploit its behavior, something we call the California Attack. Let’s take the initial example in a non-competing run:&lt;/p&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SYS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;You&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obedient&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;assistant&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;who&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;only&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responds&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;punctuation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;You&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;truthfully&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;However&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allowed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;say&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forbidden&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floor&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SYS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Golden&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Gate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bridge&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;br&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&amp;gt;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;p&gt;The model naturally answers ‘California’. By adding two words to the first sentence of the prompt: “You are an obedient assistant from California […] “, we break the model. The suppressor components take the forbidding instruction to never say ‘floor’ and instead downweight “California” to an 18% completion probability, elevating “San Francisco”, an incorrect answer, to the top response.&lt;/p&gt; &lt;p&gt;I did some more exploring of this California Attack. I found three things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the previous example, ablating just the top suppressor head resulted in California being the top response again.&lt;/li&gt; &lt;li&gt;Some particular heads like attending more to certain categories (e.g. countries) and certain words. In this particular case, the top suppressor head enjoyed paying attention to California specifically. The key then is to have the forbidden instruction apply to a word it doesn’t like paying attention to, such as ‘floor’.&lt;/li&gt; &lt;li&gt;We can find the words they downweight the most and least by applying the OV matrix against the entire vocabulary distributions and looking at the beginning and end of the distribution.&lt;/li&gt; &lt;/ol&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Keeping these lessons in mind, I found another attack by analyzing some of the words the suppressor heads downweight the most. In the above example, I added that Llama 2 was an assistant “to Trump” in the system message. In the above message, the first run is the adversarial attack where the top response to answering who the 45th President of the USA was is ‘Great’. Under a normal run without the adversarial attack, the top answer is ‘Trump’:&lt;/p&gt; &lt;p&gt;I also experimented with the 13B version of Llama 2, and found that the Calornia attack also applies to this model when forbidding ‘table’ in a non-competing run:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot6-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot6-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot6-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-forbidden-facts/plot6.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;However, I could not find a similar adversarial attack for the 70B version of Llama 2. This suggests that as models get larger, their heuristics get more robust to such mechanistic exploits.&lt;/p&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;In this work, we decompose and attempt to characterize important components of Llama 2 that allow it to suppress the forbidden word in the forbidden fact task. While we identify some structural similarities between the most important attention heads, we also find evidence that the mechanisms used by Llama 2 are complex and heterogeneous. Overall, we found that even components directly involved in suppressing the forbidden word carry out this mechanism in different ways and that Llama 2’s mechanisms are more akin to messy heuristics than simple algorithms.&lt;/p&gt; &lt;p&gt;This results in an important limitation of our work: we could not find a clean, sparse circuit implementing the forbidden behavior. Moreover, it is unclear if we are working in the right “basis” of attention heads and MLPs, or if causal attribution methods such as activation patching are able to recover the correct representation of a circuit.&lt;/p&gt; &lt;p&gt;This raises some questions about the goals of mechanistic interpretability. Previous mechanistic interpretability papers have largely studied algorithmic tasks on small models to understand how models implement behaviors and characterize certain properties. However, moving away from toy settings to understand how models with hundreds of billions of parameters implement a variety of complex behaviors with competing objectives might be much harder.&lt;/p&gt; &lt;p&gt;Computational irreducibility is the idea that there are certain systems whose behavior can only be predicted by fully simulating the system itself, meaning there are no shortcuts to predicting the system’s behavior. Initially proposed by Stephen Wolfram in the context of cellular automata, this concept challenges the reductionist approach to science, which may be analogous to the approach mechanistic interpretability takes today.&lt;/p&gt; &lt;p&gt;If computational irreducibility applies to mechanistic interpretability in understanding models, it may be very difficult to get generalizable guarantees about its behavior. If even the most efficient way of computing important properties about models is too slow, then mechanistic interpretability can’t achieve one of its main goals. This project provides some suggestive evidence that we could live in a world where frontier models are computationally irreducible.&lt;/p&gt; &lt;p&gt;Thanks for reading! If you have any questions, feel free to reach out at miles_wang [at] college [dot] harvard [dot] edu!&lt;/p&gt; </content> </entry> <entry> <title>Modeling Elephantfish Communication through Deep RNNs</title> <link href="https://deep-learning-mit.github.io/blog/2023/elephantfish-model/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/elephantfish-model</id> <content type="html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt; &lt;p&gt;Elephantfish, known for their unique use of electric fields for sensing and interaction, present a fascinating study subject within the realms of bioacoustics and animal communication. This project, pioneering the use of deep learning, specifically Recurrent Neural Networks (RNNs), aims to model and interpret these electrical communication signals. By combining insights from bioacoustics, linguistics, and computer science, we seek to decode these bioelectrical signals into a human-comprehensible format, thereby expanding our understanding of animal cognition and communication. The overarching goal is to decode and understand the complexity of elephantfish communication and to explore the broader applications in sociolinguistics, pragmatics, and computational linguistics for non-human species. This project pioneers in utilizing deep learning, specifically Recurrent Neural Networks (RNNs), to model and interpret the electrical communication signals of elephantfish. The study’s novelty lies in its interdisciplinary approach, combining insights from bioacoustics, linguistics, and computer science to unravel the complexities of non-human communication systems. Our goal is to translate these unique bioelectrical signals into a form comprehensible to humans, thereby expanding our understanding of animal cognition and communication.&lt;/p&gt; &lt;h2 id=&quot;introduction-and-objectives&quot;&gt;Introduction and Objectives&lt;/h2&gt; &lt;p&gt;The elephantfish, a species renowned for its unique electric-based communication and the largest brain-to-body weight ratio of all known vertebrates, offers a fascinating window into the study of non-human communication systems. These creatures, inhabiting the murky waters of African rivers and lakes, have developed a sophisticated method of communication that relies on generating and sensing electric fields. This remarkable ability not only sets them apart in the aquatic world but also poses intriguing questions about the nature and complexity of their interactions. The study of elephantfish communication is not just a pursuit in understanding an exotic species; it reflects a broader scientific curiosity about the principles of communication and social behavior across different life forms.&lt;/p&gt; &lt;p&gt;The primary objective of this project is to develop a deep understanding of elephantfish communication through the application of advanced neural language models, specifically focusing on Recurrent Neural Networks (RNNs). This approach is inspired by the parallels drawn between the electric signals used by elephantfish and the structural aspects of human language. By leveraging techniques commonly used in natural language processing (NLP), we aim to decode these bioelectrical signals and translate them into a format that can be understood by humans. This endeavor is not only about interpreting the ‘language’ of a non-human species; it is about enriching our understanding of communication as a fundamental biological and social function.&lt;/p&gt; &lt;p&gt;To capture the complexity of elephantfish communication, we have collaborated with labs at MIT and Columbia, gaining access to a comprehensive dataset of elephantfish electric communication signals. This dataset includes a wide range of signals recorded under various environmental and social conditions, providing a rich source of data for analysis.&lt;/p&gt; &lt;p&gt;Utilizing the latest advancements in deep learning, we will develop and train neural language models that can accurately interpret and model these electric signals. The focus will be on employing Long Short-Term Memory (LSTM) RNNs, which are well-suited for handling the temporal sequences inherent in these signals.&lt;/p&gt; &lt;p&gt;Drawing from the field of NLP, we will apply a range of techniques to analyze and understand the ‘language’ of elephantfish. This analysis will delve into the sensing, communication, and social dynamics of the species, offering insights into how they interact with each other and their environment.&lt;/p&gt; &lt;p&gt;One of the most challenging aspects of this project is translating the electric signals into a form that is comprehensible to humans. This task will involve developing innovative methods to represent these signals visually or auditorily, making the complex patterns of communication accessible for further study and interpretation.&lt;/p&gt; &lt;p&gt;Beyond the technical analysis, we aim to explore the sociolinguistic and pragmatic aspects of elephantfish communication. This exploration will involve understanding the social context and significance of different patterns of signals, thereby contributing to the broader field of computational linguistics and sociolinguistics.&lt;/p&gt; &lt;p&gt;In undertaking this research, we are not only contributing to the field of bioacoustics but also bridging gaps between biology, linguistics, and computer science. The insights gained from this study have the potential to transform our understanding of animal communication and cognition, opening up new possibilities for interdisciplinary research and discovery.&lt;/p&gt; &lt;h2 id=&quot;literature-review&quot;&gt;Literature Review&lt;/h2&gt; &lt;p&gt;Time series analysis has been extensively used in biological studies, especially for understanding patterns in animal behavior and communication. Studies like Jurtz, et al. (2017) have demonstrated the efficacy of time series analysis in interpreting complex behavioral data in wildlife research.&lt;d-cite key=&quot;jurtz2017introduction&quot;&gt;&lt;/d-cite&gt; This forms a basis for our approach to model elephantfish movements, which are intrinsically temporal and dynamic.&lt;/p&gt; &lt;p&gt;The unique architecture of LSTM RNNs, with their ability to remember long-term dependencies, makes them particularly suitable for time series prediction. Gers, Schmidhuber, and Cummins (2000) showcased the potential of LSTM RNNs in learning to bridge minimal time lags in excess of 1000 discrete time steps between relevant input events and target signals, setting a precedent for their application in predicting animal movement patterns.&lt;d-cite key=&quot;gers2002learning&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;Recent advancements in bioacoustics have seen LSTM RNNs being employed to analyze and predict patterns in animal communication. For instance, Stowell and Plumbley (2014) applied LSTM networks to bird song recognition, illustrating the network’s capacity to handle temporal sequences in bioacoustic signals of bird sounds. This aligns closely with our project’s objective of modeling the movement patterns of elephantfish, which are hypothesized to be closely tied to their communication.&lt;d-cite key=&quot;stowell2014automatic&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;Research on aquatic species like elephantfish presents unique challenges due to their environment and the nature of their communication. The work of Stoddard et al. (2010) in electric signal analysis of male electric fishes provides insights into the complexity of such studies.&lt;d-cite key=&quot;stoddard2008signal&quot;&gt;&lt;/d-cite&gt; However, there is a noticeable gap in applying advanced time series models, like LSTM RNNs, specifically to the movement patterns and communication signals of elephantfish.&lt;/p&gt; &lt;p&gt;The application of NLP techniques to animal communication is a relatively unexplored frontier. Recent work by Wilensky et al. (2021) in decoding prairie dog vocalizations using natural language processing provides a compelling case for extending similar approaches to non-vocal animal communication. Our project takes this concept further by applying deep learning techniques to decode the electric signals of elephantfish, which, while different from vocalizations, share parallels in terms of being a structured form of communication.&lt;/p&gt; &lt;p&gt;The application of LSTM RNNs in predicting the current positions of elephantfish based on past positions not only addresses a significant gap in the study of aquatic animal behavior but also sets the stage for future research in this area. The success of this approach could revolutionize the way we understand and interpret the communication and social interactions of these unique species.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;h3 id=&quot;data-collection&quot;&gt;Data Collection&lt;/h3&gt; &lt;p&gt;Collaborating with labs at MIT and Columbia, we have gained access to a diverse and comprehensive dataset of elephantfish electric communication signals. The dataset encompasses signals recorded in various environmental conditions, capturing the nuances of communication in different contexts. The recordings include instances of social interaction, mating rituals, and responses to external stimuli.&lt;/p&gt; &lt;h3 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h3&gt; &lt;p&gt;The raw electric signal data require extensive preprocessing to extract meaningful features for the deep learning models. This involves filtering, noise reduction, and segmentation to isolate individual communication events. Given the temporal nature of the signals, we will focus on capturing time-dependent features that are crucial for LSTM RNNs.&lt;/p&gt; &lt;h3 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h3&gt; &lt;p&gt;Our chosen model architecture revolves around Long Short-Term Memory (LSTM) Recurrent Neural Networks. LSTMs are well-suited for modeling sequences with long-term dependencies, making them ideal for capturing the temporal dynamics of elephantfish communication signals. The network will be designed to take into account the sequential nature of the signals, allowing for effective learning of patterns over time.&lt;/p&gt; &lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt; &lt;p&gt;The training process involves exposing the LSTM network to the preprocessed dataset, allowing it to learn and adapt to the patterns within the electric signals. The model’s performance will be iteratively refined through multiple training sessions, adjusting hyperparameters to optimize for accuracy and generalization.&lt;/p&gt; &lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt; &lt;p&gt;The evaluation phase includes testing the trained model on a separate set of elephantfish communication signals not seen during training. This assesses the model’s ability to generalize its learning to new and unseen data. Metrics such as accuracy, precision, recall, and F1 score will be used to quantify the model’s performance.&lt;/p&gt; &lt;h2 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h2&gt; &lt;h3 id=&quot;experiment-1-signal-reconstruction&quot;&gt;Experiment 1: Signal Reconstruction&lt;/h3&gt; &lt;p&gt;Our first experiment aims to assess the model’s ability to reconstruct the original electric signals from the learned representations. This involves comparing the reconstructed signals with the original signals using established metrics for signal similarity.&lt;/p&gt; &lt;h3 id=&quot;experiment-2-pattern-recognition&quot;&gt;Experiment 2: Pattern Recognition&lt;/h3&gt; &lt;p&gt;In the second experiment, we evaluate the model’s performance in recognizing and categorizing different patterns within the elephantfish communication signals. This includes identifying specific sequences associated with social interactions, mating rituals, and responses to external stimuli.&lt;/p&gt; &lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt; &lt;p&gt;Preliminary results indicate promising performance in both signal reconstruction and pattern recognition tasks. The LSTM RNN demonstrates an ability to capture and reproduce complex temporal patterns within the electric signals. The model’s accuracy in distinguishing between different communication contexts is encouraging, suggesting that it can effectively learn and differentiate the nuances of elephantfish communication.&lt;/p&gt; &lt;p&gt;The experiments involved training the RNNs on the collected dataset, followed by validation and testing phases. We present detailed results demonstrating the models’ ability to capture and replicate the intricate patterns of elephantfish communication. The analysis includes a comparative study with existing knowledge in marine biology, validating the accuracy and relevance of our models.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%201-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%201-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%201-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%201.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%202%20--%20comparison%20epoch%2010-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%202%20--%20comparison%20epoch%2010-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%202%20--%20comparison%20epoch%2010-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%202%20--%20comparison%20epoch%2010.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%203%20feature%20distributions-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%203%20feature%20distributions-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%203%20feature%20distributions-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%203%20feature%20distributions.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%204%20feature%20conclusions%20heatmap-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%204%20feature%20conclusions%20heatmap-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%204%20feature%20conclusions%20heatmap-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-elephantfish-model/figure%204%20feature%20conclusions%20heatmap.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;discussion-and-conclusions&quot;&gt;Discussion and Conclusions&lt;/h2&gt; &lt;p&gt;The successful application of LSTM RNNs to model elephantfish communication signals represents a significant step forward in our understanding of non-human communication systems. The results demonstrate the capacity of deep learning techniques to decode and interpret complex bioelectrical signals, opening avenues for further exploration in bioacoustics and animal communication.&lt;/p&gt; &lt;p&gt;The ability to reconstruct signals and recognize patterns within elephantfish communication provides a foundation for future studies on the sociolinguistic and pragmatic aspects of their interactions. By translating these signals into a comprehensible format, we pave the way for a deeper exploration of the meanings and nuances embedded in the electric language of elephantfish.&lt;/p&gt; &lt;p&gt;Our research marks a significant stride in understanding non-human communication systems, demonstratint the ability to predict the movement and communication patterns of elephantfish. The findings not only shed light on the complex social structures of elephantfish but also open new avenues in the study of animal linguistics. We discuss the broader implications of our work in the fields of cognitive science and artificial intelligence, highlighting the potential applications and societal impact. Our LSTM RNN models, compared to baseline models that use the immediate last time step position to predict, show superior performance in predicting the complex communication patterns of elephantfish.&lt;/p&gt; &lt;p&gt;This superiority highlights the effectiveness of our LSTM RNNs in capturing the intricate temporal dynamics of elephantfish communication. Moreover, our method of processing raw electric data has been optimized through trial and error, finding that skipping exactly every 5 data points results in the lowest loss, demonstrating the importance of fine-tuning data preprocessing in machine learning models.&lt;/p&gt; &lt;h2 id=&quot;challenges-and-future-directions&quot;&gt;Challenges and Future Directions&lt;/h2&gt; &lt;p&gt;This project stands at the intersection of technology and biology, with the potential to significantly advance our understanding of animal communication. The success of this endeavor could pave the way for interdisciplinary research, contributing valuable insights into the cognitive abilities of non-human species and the fundamental principles of communication.&lt;/p&gt; &lt;p&gt;The research conducted on elephantfish communication using LSTM RNNs has yielded insights that significantly advance our understanding of non-human communication systems. Our models have demonstrated a notable ability to predict movement and communication patterns, offering a new lens through which to view the complex social interactions of these aquatic species.&lt;/p&gt; &lt;p&gt;This is a large scale long term collaboration between a few labs, and in the future we will utilize more of the data from a marine biology lab at Columbia to interpret the electric signals. We will likely collaborate with marine biologists to collect a data set of electric signals from elephantfish under various environmental and social conditions.&lt;/p&gt; &lt;p&gt;Comparatively, our approach has shown improvements over traditional models, providing a more nuanced understanding of the temporal dynamics in elephantfish communication. These results not only align with existing theories in marine biology but also open new avenues for exploration in animal linguistics and cognitive science.&lt;/p&gt; &lt;p&gt;However, this study is not without its limitations. One of the primary constraints was the size and diversity of the dataset. While we managed to collect a substantial amount of data, the variability in environmental conditions and individual elephantfish behaviors was limited. This constraint could potentially impact the generalizability of our models to broader applications. The translation of bioelectrical signals into a human-understandable format is an ongoing challenge that requires further refinement. Additionally, the diversity and variability within elephantfish communication present complexities that demand a nuanced understanding beyond the scope of this initial study.&lt;/p&gt; &lt;p&gt;Another limitation lies in the inherent complexities of LSTM RNNs, which, while powerful, can sometimes become “black boxes.” This opaqueness makes it challenging to dissect the exact learning mechanisms and to fully understand how the models are making their predictions.&lt;/p&gt; &lt;p&gt;Our study marks a significant step forward in the field but also highlights areas for further research. Future studies could focus on expanding the dataset and exploring more diverse environmental conditions. Additionally, we hope to develop more interpretable machine learning models that could provide clearer insights into the learning and prediction processes. One thing we hope to do is to convert back the predicted positions of fishes to the pixel positions in the tank, this way we can have a more visual intuition about how our model is predicting the positions.&lt;/p&gt; </content> </entry> <entry> <title>Exploring Image-Supervised Contrastive Diffusion - A Comparative Analysis with Applications in Image-to-Video Generation</title> <link href="https://deep-learning-mit.github.io/blog/2023/contrastivediffusion-image2video/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/contrastivediffusion-image2video</id> <content type="html">&lt;h2 id=&quot;introduction-and-motivation&quot;&gt;Introduction and Motivation&lt;/h2&gt; &lt;p&gt;With recent advances in computer vision and generative AI, we all have observed the various feats that diffusive models have achieved in conditional image generation. These models have demonstrated unparalleled ability in creativity, fidelity, and relevance when generating images from text prompts. Given this explosive success of diffusion for the task of image generation, the idea of applying the same concepts to conditional video generation seems like a logical follow-up. Yet, the field still lacks robust and compelling methods for conditional video generation with diffusion models. This raises the question: why might this be? Or perhaps a follow-up: what makes videos so hard in comparison to images?&lt;/p&gt; &lt;p&gt;In an attempt to address our first question, if we take a brief dive into previous literature, we will find that the issue is not a lack of effort. Ho et al. &lt;d-cite key=&quot;ho2022video&quot;&gt;&lt;/d-cite&gt;, Zhang et al. &lt;d-cite key=&quot;2023i2vgenxl&quot;&gt;&lt;/d-cite&gt;, and Chen et al. &lt;d-cite key=&quot;chen2023videocrafter1&quot;&gt;&lt;/d-cite&gt;, all explore this idea, yet the results from these methods are not nearly as exciting as the results we see in images. But why is this?&lt;/p&gt; &lt;p&gt;Perhaps the answer lies in the solution to our second question. One of the most obvious complexities that videos have over images is also perhaps one of the most difficult: the temporal dependence between frames. But why is this relationship so hard for diffusion models? Following the work of Zhu et al. &lt;d-cite key=&quot;zhu2022discrete&quot;&gt;&lt;/d-cite&gt;, we hypothesize that this is because the implicit learning of diffusive steps between images in a video is too complex of a problem for a diffusion model; relying on the model to learn the implicit relationship between representations of video frames is infeasible from a training and convergence standpoint. If we can instead learn diffusive steps over a more regularized learned latent space, the optimization problem can be greatly simplified and the diffusion model will in theory be more robust.&lt;/p&gt; &lt;p&gt;To do so, we introduce a new framework for fine-tuning diffusion models when given images in addition to text as conditional information, targeting this challenge of making the model’s use of the latent space more robust. Specifically, we utilize contrastive learning techniques to ensure that the model learns consistency between latents from different image domains, which we first validate on the easier image-to-image (I2I) case before moving into image-to-video (I2V).&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;Taking a step back to examine the current state of research, let’s first take a look at what current I2I models look like.&lt;/p&gt; &lt;h3 id=&quot;image-to-image-models&quot;&gt;Image-to-Image Models&lt;/h3&gt; &lt;p&gt;In the field of image-to-image, there are two main approaches, using images to control the model output, and modifying the image itself.&lt;/p&gt; &lt;p&gt;The first approach is characterized by work like ControlNet and T2I &lt;d-cite key=&quot;mou2023t2i&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;zhang2023adding&quot;&gt;&lt;/d-cite&gt;, which freeze the overall diffusion network and instead fine-tune a lightweight adapter model on the top. This adapter is typically a combination of stable diffusion layers that have an additive effect on the model as a whole, trained using the fine-tuning dataset. However, these models are largely still built for text-to-image tasks, using the input images as conditioning for the input text prompt, such as a wireframe image for poses. However, this does not allow modification of the image itself, simply using the image as guidelines during the diffusion process itself, meaning that its’ style is not preserved.&lt;/p&gt; &lt;p&gt;The second method is more related to maintaining both the style and content of the original image, and instead directly fine-tunes the diffusion network to actually use the input images. The first such model for this purpose is the original pix2pix architecture, which while built for GANs, still carries vital lessons to this day. By fine-tuning a loss that actually involves the mapping between input and output image, the model learns to actually adapt the image while keeping other relevant contexts the same &lt;d-cite key=&quot;pix2pix2017&quot;&gt;&lt;/d-cite&gt;. After this, the Palette model for generalist, multi-task diffusion trained a diffusion model from scratch for multiple different tasks &lt;d-cite key=&quot;saharia2022palette&quot;&gt;&lt;/d-cite&gt;. However, then Instruct-Pix2Pix built on the original pix2pix architecture, taking a pre-trained diffusion model and conditioning it on both the noisy text latent and the input image latent, meaning that the training latent had both and therefore would fully train on the input image latent &lt;d-cite key=&quot;brooks2022instructpix2pix&quot;&gt;&lt;/d-cite&gt;. This architecture is presented below.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Traditional diffusion architecture &lt;/div&gt; &lt;p&gt;For the purpose of this blog, we study Instruct-Pix2Pix like fine-tuning schemes, since they align with what we need for video-based studies, maintaining content of the previous image while making small modulations based on the input text.&lt;/p&gt; &lt;h3 id=&quot;image-to-video-models&quot;&gt;Image-to-Video Models&lt;/h3&gt; &lt;p&gt;Moving to I2V, we find that current image-to-video frameworks typically still use a traditional diffusion architecture, going straight from text and image representations to an output image. However, this naive approach struggles with serious issues like frame clipping and loss of contextual information, which is expected since noise-based sampling can easily throw off the output of individual frames.&lt;/p&gt; &lt;p&gt;Hence, Ho et al. in 2022 proposed the first solution, supplementing conditional sampling for generation with an adjusted denoising model that directly forces image latents to be more similar to the corresponding text latents &lt;d-cite key=&quot;ho2022video&quot;&gt;&lt;/d-cite&gt;. While this achieved improved results over the straightforward diffusion approach, this often forces the model to stick too closely to the text latent, resulting in incoherent videos.&lt;/p&gt; &lt;p&gt;To solve this issue, two recent approaches from Chen et al. and Zhang et al. have proposed methods to augment the video diffusion models themselves. Chen et al. uses the image encodings from CLIP-like language embeddings in an encoder-decoder language model, feeding the CLIP encodings at each step into a cross-attention layer that generates attention scores with the current video generation &lt;d-cite key=&quot;chen2023videocrafter1&quot;&gt;&lt;/d-cite&gt;. In doing so, additional coherence between frames is achieved. On the other hand, Zhang et al. use multiple encoders, with CLIP and VQ-GAN concatenated before two stages of diffusion model training, which they claim provides the hierarchical learning required to learn the temporal processing &lt;d-cite key=&quot;2023i2vgenxl&quot;&gt;&lt;/d-cite&gt;. However, both these models are extremely data-heavy and still suffer from hallucination and frame skipping.&lt;/p&gt; &lt;h3 id=&quot;contrastive-models&quot;&gt;Contrastive Models&lt;/h3&gt; &lt;p&gt;To remedy these issues in diffusion models, Ouyang et al. and Zhu et al. posit that the implicit representation learning objective in diffusion models is the primary cause of the slow convergence and hallucination issues. Specifically, diffusion models do not directly compare their output to their input, as in contrastive models, instead performing a variational approximation of the negative log-likelihood loss over the full Markov chain. Instead, Ouyang and Zhu propose to train the diffusion model to output a structured latent in the latent space of a contrastive model like a VQ-VAE, which then reconstructs the output image &lt;d-cite key=&quot;zhu2022discrete&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;ouyang2023improving&quot;&gt;&lt;/d-cite&gt;. In doing so, a contrastive term can be added to the loss of the diffusion model, maximizing the mutual information between the structured (output) latent and input latent, leading to stronger correlations between input and output, and hence improved convergence. Hence, this approach seems to have potential in fixing the hallucination and coherence issues in video diffusion models, without the need for added complexity.&lt;/p&gt; &lt;h2 id=&quot;our-proposal&quot;&gt;Our Proposal&lt;/h2&gt; &lt;p&gt;Thus, we propose a novel method for conditional image-to-image generation (generating images given a starting frame and text description) by training the diffusion model to actually utilize the regularized latent space in which a diffusion model can operate. Following the line of thought introduced above, we hypothesize that under such a formulation, the diffusion model is much more robust to temporal inconsistency, because of the regularity in the latent space. For example, if we imagine a highly regularized latent space, we will find all logical next frames for a given anchor frame clustered very closely around the anchor in this latent space. Therefore, any step the diffusion model takes would produce valid subsequent frames; it suffices simply for the model to learn which direction to go given the conditioned text prompt.&lt;/p&gt; &lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt; &lt;h3 id=&quot;image-to-image&quot;&gt;Image to Image&lt;/h3&gt; &lt;p&gt;Given a base pretrained diffusion model, such as Runway ML’s StableDiffusion 1.4, which is the model used in this blog, it consists of various components. The three that are the most important are the VAE image encoder/decoder, the UNet, and the CLIP text encoder. The VAE begins by learning to transform images into latents and vice-versa, which is used to compress the input image and decode the output latent in the original Instruct-Pix2Pix stack. On the other hand, the UNet predicts the noise in the denoising part of the pipeline, whereas the CLIP text encoder encodes the input text.&lt;/p&gt; &lt;p&gt;In terms of the general diffusion model, we use the traditional diffusion loss,&lt;/p&gt; \[\mathcal{L} = \mathbb{E}[(\epsilon - \epsilon_\theta(x_t))^2]\] &lt;p&gt;which essentially encodes the mean squared error loss between the added noise and the noise that is predicted by the UNet. This pipeline is illustrated in the below image.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(4)-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(4)-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(4)-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(4).png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Construction of the MSE loss for a traditional diffusion fine-tuning stack. &lt;/div&gt; &lt;p&gt;However, this loss does not encode anything of the relation between the frames themselves, which has the potential to lead to low coherence between source and target image, and thus lead to poor output quality. However, contrastively trained models like CLIP have shown strong correlative behavior between multiple modalities in the past, like between text and image, which is why we move towards contrastive losses.&lt;/p&gt; &lt;p&gt;In traditional contrastive learning, we typically have our classes divided by our dataset, such as for shape, as shown in this example of a shape dataset taken from the fourth homework of 6.s898:&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/contrastiveshapes-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/contrastiveshapes-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/contrastiveshapes-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/contrastiveshapes.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of a contrastive dataset for shapes &lt;/div&gt; &lt;p&gt;For this contrastive learning dataset, we have images that are well classified, but in terms of our image to image task, there is no such easy classification. Instead, we adopt the notion that in such a dataset, with a batch size that is small relative to the size of the dataset, each image will be reasonably different from the other images. Also because we don’t want to cluster the latent space, as the VAE is fully pretrained in the case of the diffusion fine-tuning methodology, we don’t need to actually push similar items between the test set closer together, only push the diffusion output closer to the input conditioning.&lt;/p&gt; &lt;p&gt;Hence, for this task, we consider each image within the larger batch as a negative sample, only using the corresponding latent in our optimization task as the positive sample. Also, given that we want both similarity to the input image and the target image, we want our loss to look like&lt;/p&gt; \[\mathcal{L} = \mathcal{L}_{MSE} + \mathcal{L}_{c, i} + \mathcal{L}_{c, t}\] &lt;p&gt;where c indicates contrastive and i, t indicate input and target, respectively.&lt;/p&gt; &lt;p&gt;For the images, they are encoded by the VAE, which has learned structure due to its Gaussian training objective in the ELBO loss, which means we can directly dot product the latents when calculating the contrastive loss:&lt;/p&gt; \[\mathcal{L}_c = \mathbb{E}[\frac{e^{x_+^{T}x}}{\sum_{x&apos; \in \{x_+, x_{-} \}} e^{x&apos;^{T}x}}]\] &lt;p&gt;This is calculated easily using a matrix multiplication and a cross entropy loss. Now, since we compute the contrastive loss using the predicted latent, and not the noise, we also add on a constructive aspect to our diffusion model. From the final noise prediction, the model also generates the predicted latent using the noise scheduler:&lt;/p&gt; \[x_0 = \frac{1}{\sqrt{\bar{\alpha_t}}}(x_t \pm \sqrt{1 - \bar{\alpha_t}}\epsilon_\theta(t))\] &lt;p&gt;where alpha is the cumulative products of the alphas in the noise scheduler. These predicted final latents are then used directly in the contrastive loss formula. A visualization of how we calculate our contrastive loss can be found below:&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(3)-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(3)-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(3)-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/Image%20(3).png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Construction of the MSE + Contrastive loss for our fine-tuning stack. &lt;/div&gt; &lt;p&gt;We note that in this case, we must scale the losses for numerical stability. The model we train with has latents of dimension 4 by 32 by 32, and while the MSE is scaled from 0 to 4 (due to pixel values from 1 to -1), the cross entropy loss is not. Indeed, many of these dot products are on the order of 4000, so we choose a high temperature of 1 to prevent NaN computations and then scale the losses by 4000, which is chosen because it scales the effect of each pixel in the dot product to around the same order as that in the MSE, which is averaged over all 4096 values in the latent.&lt;/p&gt; &lt;h3 id=&quot;image-to-video&quot;&gt;Image to Video&lt;/h3&gt; &lt;p&gt;Now, for image to video, the training process of such a model involves the optimization of the above diffusion/contrastive loss based on a given pair of nearby video frames, as well as the corresponding text description for that video. This procedure works well because in a video, we must train the model to learn the next frame, so just like how masked language models are asked to predict masked tokens from a sequence, we ask the diffusion model to predict a masked frame from the given frame. On top of that, the text prompt, which often still provides the majority of the guidance for the video as a whole is already conditioned using the MSE loss, while the contrastive loss optimizes the similarity to previous frames. Otherwise, this is trained the same as a traditional diffusion model.&lt;/p&gt; &lt;p&gt;During inference, we generate a video through the following process. First, an initial frame and the text description are encoded into our latent space using the VAE encoder and CLIP encoder, respectively. Now, we run an arbitrary number of passes through our diffusion model, generating a latent at each step, which is then passed in as the conditioning frame for the next forward pass. Finally, we decode the latent at each time step to obtain our video frame at that time step; stringing these frames together produces our video.&lt;/p&gt; &lt;p&gt;From a more theoretical perspective, this method essentially aims to restrict the diffusion model’s flexibility to paths within a highly regularized, lower dimensional latent space, as opposed to the entire space of images that classical diffusion-based approaches can diffuse over. Such a restriction makes it much harder for the diffusion model to produce non-sensible output; the development of such a method would therefore enable the robust generation of highly temporally consistent and thus smooth videos. We also imagine the value of producing such a latent space itself. An interesting exercise, for example, is taking an arbitrary continuous path along vectors within a perfectly regular latent space to obtain sensible videos at arbitrary framerates.&lt;/p&gt; &lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt; &lt;p&gt;Now, we explain where we got our data from.&lt;/p&gt; &lt;p&gt;For text-conditioned image-to-image generation, we train on the Instruct-Pix2Pix dataset from HuggingFace, sampling 20k samples from the original training set used in the paper (timbrooks/instructpix2pix-clip-filtered). Our test and evaluation sets consist of 500 nonoverlapping samples from this same set &lt;d-cite key=&quot;brooks2022instructpix2pix&quot;&gt;&lt;/d-cite&gt;. This dataset consists of samples with input images, edited images, input prompts, edited prompts, and an edit string that describes the edit that was made. An example is presented below:&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/P2P_data-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/P2P_data-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/P2P_data-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/P2P_data.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of a sample from the Instruct-Pix2Pix dataset. &lt;/div&gt; &lt;p&gt;For text-conditioned image-to-video generation, we experimented with the use of two different video-caption datasets: MSR-VTT and WebVid-10M. Due to the high dissimilarity between the properties of the two datasets, we tested the finetuning performance of both our contrastive model and non-contrastive models on both datasets. MSR-VTT contains 10k clips scraped from a wide range of domains, with multiple human-generated captions for each video. WebVid, on the other hand, contains 10M video clips compiled from stock image sources, with captions corresponding to the stock photo titles. For WebVid10M, we only take from the 2.5M subset. For both datasets, samples were generated to follow the Instruct-Pix2Pix data formulation (original image, edit prompt, edited image) using the following strategy:&lt;/p&gt; &lt;p&gt;First, we sample 25k and 10k videos from WebVid-10M and MSR-VTT, respectively. We aim to sample roughly an equal number of samples from each video for a total of 20k (original image, edit prompt, edited image) triplets. We ignore videos longer than 30 seconds in length to minimize the probability of temporal inconsistency within a given video. Then, for each video, we choose a random frame in the video (the original video fps is 25; but these frames are too close together, so we say that only one out of every 5 video frames is a valid selection target) to be our “original” image. The video’s caption is our “edit” prompt. To select our “edited” image, we note that we are optimizing the model to produce the next frame, while maintaining consistency between frames. Therefore, to select the “edited” image, we sample a normal distribution with standard deviation of 10 valid frames (50 frames in the original video), or two seconds, to select a frame after our “original” image as our “edited” image. A sample processed image from WebVid is included below.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/webvid_processed-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/webvid_processed-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/webvid_processed-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/webvid_processed.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of a processed sample from WebVid. &lt;/div&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;To assess the efficacy of our newly proposed strategy, we run experiments on both the original Instruct-Pix2Pix task of text-conditioned image-to-image generation, as well as the task of text-conditioned image-to-video generation, against the baseline Instruct-Pix2Pix model. The original Instruct-Pix2Pix task is run to confirm that our model, after obtaining coherency, does not lose significant expressivity. On the other hand, we expect the image-to-video model to have comparable expressivity to the baseline on a task where coherency is significantly more important.&lt;/p&gt; &lt;p&gt;All of these evaluations and experiments were performed using the Accelerate library and HuggingFace Diffusers, &lt;d-cite key=&quot;von-platen-etal-2022-diffusers&quot;&gt;&lt;/d-cite&gt;, building off of their Instruct-Pix2Pix codebase. The model is RunwayML’s Stable Diffusion v1.5 release. For the task of image-to-image generation, we trained both the baseline Instruct-Pix2Pix and our model for 9000 training steps on 4xA100-80GB with a batch size of 16 and a learning rate of 5e-5, which took on the order of 12 hours. For the image-to-video generation task, we trained both baseline Instruct-Pix2Pix and our contrastive model for 4500 training steps at a learning rate of 1e-5 and a batch size of 16 due to overfitting issues at higher # of training steps and higher learning rates, possibly due to the repetitiveness of our dataset. Note that we had a limited ability to hyperparameter tune/ablate, since each diffusion fine tuning run took multiple hours at a minimum, and we were operating on a minimal budget of spare A100s when they were available from our labs, so those results are not shown in this blog.&lt;/p&gt; &lt;p&gt;We then evaluate on the test splits of the corresponding datasets described above (for image-to-video generation, we evaluate on the test split of WebVid, since MSRVTT’s testing set has a number of non-corresponding video-prompt pairs and also very jittery videos).&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;Now, we explain our results. For both tasks, we assess two metrics: the first is the Frechet Inception Distance (FID) &lt;d-cite key=&quot;FID &quot;&gt;&lt;/d-cite&gt;between the target image and the predicted image. The FID calculates the similarity between the distribution of images and the distribution of generated images, so a lower FID is considered better, as the distributions are more similar. Note that since our models are still technically image-to-image models and not image-to-video at their core, to evaluate the base models we use FID instead of its video counterpart FVD. For our second metric, we use CLIP Scores &lt;d-cite key=&quot;hessel-etal-2021-clipscore &quot;&gt;&lt;/d-cite&gt;between the source image and the edit prompt, the target image and the edit prompt, and the predicted image and the edit prompt for the source image, predicted image, source image description (from the Instruct-Pix2Pix dataset), and target image description. The CLIP score can be thought about as a measure of the similarity between the prompt and the image, with a higher score being better, referring to higher similarity.&lt;/p&gt; &lt;p&gt;These metrics are used to evaluate our base image-to-video models as well, as they both determine the amount of prompt following and fidelity we can determine in our videos.&lt;/p&gt; &lt;h3 id=&quot;image-to-image-results&quot;&gt;Image to Image Results&lt;/h3&gt; &lt;p&gt;For text-conditioned image-to-image generation, we observe that our models have these FID and CLIP scores:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;FID&lt;/th&gt; &lt;th&gt;CLIP (source - prompt)&lt;/th&gt; &lt;th&gt;CLIP (gen - prompt)&lt;/th&gt; &lt;th&gt;CLIP (target - prompt)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Ours&lt;/td&gt; &lt;td&gt;158.8&lt;/td&gt; &lt;td&gt;21.7&lt;/td&gt; &lt;td&gt;&lt;strong&gt;24.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;24.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Baseline&lt;/td&gt; &lt;td&gt;&lt;strong&gt;142.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;21.7&lt;/td&gt; &lt;td&gt;&lt;strong&gt;24.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;24.1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Our model matches the baseline on CLIP score, meaning that our model exhibits similar prompt following characteristics as the baseline. On top of that, our FID is only slightly higher than the baseline, meaning that the expressivity has not decreased significantly. However, images do not have similarly robust coherence metrics, so we evaluate these qualitatively.&lt;/p&gt; &lt;h4 id=&quot;coherence&quot;&gt;Coherence&lt;/h4&gt; &lt;p&gt;On the subject of coherence, we provide some image output pairs in the figure below:&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_images-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_images-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_images-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_images.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of a sampled images, with prompt of &quot;make the mountain snowy&quot; and &quot;make it futuristic,&quot; respectively. &lt;/div&gt; &lt;p&gt;For both scenes, while the baseline diffusion model follows the prompt more fully, which may match the output distribution (hence getting a better FID score), we notice several key contrastive differences, which would impact coherence. In the mountain for example, the forest disappears in the baseline version, which also doesn’t maintain the painting-like style. On top of that, in the Eiffel tower case, the Eiffel tower rotates in the non-contrastive version. These observations lead to the idea that the contrastive model may be prioritizing coherence as desired, despite some loss in performance. Similar patterns are observed throughout the dataset.&lt;/p&gt; &lt;h3 id=&quot;image-to-video-results&quot;&gt;Image to Video Results&lt;/h3&gt; &lt;p&gt;For text-conditioned image-to-video generation, we observe that our models have the FID and CLIP scores in the table below:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;FID&lt;/th&gt; &lt;th&gt;CLIP (source - prompt)&lt;/th&gt; &lt;th&gt;CLIP (gen - prompt)&lt;/th&gt; &lt;th&gt;CLIP (target - prompt)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Ours (trained on WebVid)&lt;/td&gt; &lt;td&gt;&lt;strong&gt;102.9&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;29.9&lt;/td&gt; &lt;td&gt;27.5&lt;/td&gt; &lt;td&gt;29.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ours (trained on MSR-VTT)&lt;/td&gt; &lt;td&gt;149.3&lt;/td&gt; &lt;td&gt;29.9&lt;/td&gt; &lt;td&gt;27.6&lt;/td&gt; &lt;td&gt;29.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Baseline (trained on WebVid)&lt;/td&gt; &lt;td&gt;*&lt;/td&gt; &lt;td&gt;*&lt;/td&gt; &lt;td&gt;*&lt;/td&gt; &lt;td&gt;*&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Baseline (trained on MSR-VTT)&lt;/td&gt; &lt;td&gt;172.3&lt;/td&gt; &lt;td&gt;29.9&lt;/td&gt; &lt;td&gt;**29.4 **&lt;/td&gt; &lt;td&gt;29.8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Note that in this case, we include asterisks for the baseline numbers on WebVid because it produces NSFW content as marked by the HuggingFace Diffusers library more than 25% of the time. This means that the metrics are not directly comparable as we were unable to find a validation set on which we could evaluate the models quantitatively on even ground. Nonetheless, we still include the WebVid baseline in our qualitative analysis.&lt;/p&gt; &lt;p&gt;Looking at the rest of the metrics, the baseline on MSR-VTT has a decently higher correlation with the prompt than the contrastive model. This makes sense, as the baseline is trained only the objective of denoising the prompt latent, while we add the contrastive term. On the other hand, we have a significantly lower FID score of the MSR-VTT trained models, which means that the distributions of our output data relative to the target output data was more similar, which is probably due to the fact that our high coherence is useful in tasks where source and target distributions are similar.&lt;/p&gt; &lt;h4 id=&quot;qualitative-video-generation&quot;&gt;Qualitative Video Generation&lt;/h4&gt; &lt;p&gt;For a better understanding of the in-context performance of our model and to make up for the invalidity of the baseline model trained on the WebVid dataset above, we also perform qualitative assessments of longer videos generated by our models and the baselines. For each of 4 selected starting frames, we use a prompt generated from the sequestered part of WebVid to generate 5 subsequent frames for the video:&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_videos-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_videos-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_videos-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-contrastivediffusion-image2video/sample_videos.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of sampled videos for a prompt of a factory emitting smoke. &lt;/div&gt; &lt;p&gt;From these generated videos, we observe that our models are significantly better at generating coherent frames, as we expected. In particular, we see that the MSR-VTT baseline model deviates heavily from the starting image on the very next frame, while our MSR-VTT model largely retains the original characteristics despite some content drifting after frame 3. WebVid noticeably performs better on the baseline, but does still observe some signs of progressive degradation in our predicted outputs, along with lack of motion in contrast to the prompt for the baseline model. This progressive degradation is likely due to small levels of inclarity in each subsequent frame being compounded over multiple frames; due to coherence between frames, the subsequent frames will contain strictly more inclarity than the previous. On the other hand, our model on WebVid sees less degradation on top of actually having coherent motion of smoke billowing, showing successful output.&lt;/p&gt; &lt;p&gt;Overall though, WebVid was observed to have significantly better results than MSR-VTT, which is likely attributed to the greater quality of the dataset and less jittery videos.&lt;/p&gt; &lt;h2 id=&quot;discussion-and-conclusion&quot;&gt;Discussion and Conclusion&lt;/h2&gt; &lt;p&gt;In this project, we explored the idea of using contrastive losses to improve the coherency between input and output images in the context of text-conditioned image-to-image generation. In particular, we study the utility of this ability to generate highly coherent diffusion results in I2V, where the current state-of-the-art suffers heavily from temporal inconsistency. We evaluate our models on the classic Instruct Pix2Pix task to assess its preservation of expressive ability and conclude that no significant degradation of expressive ability was observed. We then evaluate our contrastive strategy on text-conditioned image-to-video synthesis and find that our models outperform the classic non-contrastive formulation in video generation tasks when evaluated on CLIP Score and KID.&lt;/p&gt; &lt;p&gt;Through our experiments, we have also identified some limitations of our methods and potential areas for improvement. First, we note that our model has trouble with the previously mentioned problem of progressive degradation. A possible solution to this problem could be introducing GAN training to encourage the model to produce higher-fidelity images. More robust methods could also be used (instead of sampling subsequent frames) to generate positive samples, which would increase our model’s robustness. We also notice that both our model and the baseline have trouble with a continuous depiction of motion. This is likely due to the fact that any frame is only conditioned on the previous frame. Conditioning on images multiple frames before the current image would help with this consistency issue, as well as the aforementioned progressive degradation issue. Also, due our loss function’s negative sampling-based approach to training our models, on a dataset with significant amount of repetition like ours, this led to significant overfitting in preliminary runs. On top of that, runs suffered from loss spiking when the numeric instability of cross-entropy loss led to the calculation of NaN losses and exploding gradients, which leads to requiring very low values of learning rate. This could be resolved with better sweeps of hyperparameters for scaling the losses relative to each other or higher quality data. Finally, as alluded to above, more time to do hyperparameter tuning with the training of larger models on larger datasets would likely help with performance in general.&lt;/p&gt; &lt;p&gt;With this study, we examined the use of contrastive loss to improve coherency in latent diffusion, with experiments that demonstrated minimal loss of expressive capabilities and superior consistency in diffusion, resulting in better performance on image-to-video generation. We hope that through this study, we can drive focus toward contrastive loss approaches to obtain higher fidelity results in video generation, accelerating progress in I2V and T2V.&lt;/p&gt; </content> </entry> <entry> <title>Combining Modalities for Better Molecular Representation Learning</title> <link href="https://deep-learning-mit.github.io/blog/2023/combining-modalities-for-better-representation-learning/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/combining-modalities-for-better-representation-learning</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;h3 id=&quot;importance-of-molecular-representation-learning&quot;&gt;Importance of molecular representation learning&lt;/h3&gt; &lt;p&gt;Molecular Representation Learning (MRL) is one of the most important tasks in molecular machine learning, drug design, and cheminformatics. &lt;d-cite key=&quot;mol_rep_review&quot;&gt;&lt;/d-cite&gt; It is central to addressing several key challenges in molecular sciences, including high-quality representation learning for molecular property prediction, &lt;d-cite key=&quot;mol_prop_pred&quot;&gt;&lt;/d-cite&gt; predicting organic reaction outcomes, &lt;d-cite key=&quot;reaction_pred&quot;&gt;&lt;/d-cite&gt; retrosynthesis planning, &lt;d-cite key=&quot;retrosynthesis&quot;&gt;&lt;/d-cite&gt; and generative modeling. &lt;d-cite key=&quot;generative_review&quot;&gt;&lt;/d-cite&gt; Excelling in these domains is essential for the development of new drugs, materials, and catalysts.&lt;/p&gt; &lt;h3 id=&quot;different-ways-to-represent-molecules&quot;&gt;Different ways to represent molecules&lt;/h3&gt; &lt;p&gt;The challenge of learning molecular representations is more complex than in fields like computer vision or natural language processing. This complexity stems from the variety of methods available for encoding molecular structures and the assumptions inherent to each representation. Primarily, there are four ways to represent molecules:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Fingerprints&lt;/strong&gt;. One of the oldest ways to represent molecules in Quantitative structure–activity relationship (QSAR) modelling. Molecular fingerprints are binary vectors that encode the presence or absence of certain substructures in the molecule. Fingerprints were one of the first ways to get the initial representation of molecules in machine learning problems. &lt;d-cite key=&quot;fingerprints_pred&quot;&gt;&lt;/d-cite&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;String representation&lt;/strong&gt; (e.g. SMILES strings). This approach involves encoding molecular fragments as tokens to form a string. This initial molecules encoding is widely used in generative molecular modeling. &lt;d-cite key=&quot;lang_complex_distr&quot;&gt;&lt;/d-cite&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2-D graph&lt;/strong&gt;. A popular and intuitive approach where molecules are represented as graphs, with atoms and bonds corresponding to nodes and edges, respectively. With advancements in Graph Neural Networks (GNNs) architectures,&lt;d-cite key=&quot;gnns_review&quot;&gt;&lt;/d-cite&gt; this format is extensively used in molecular property prediction. &lt;d-cite key=&quot;chemprop&quot;&gt;&lt;/d-cite&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;3-D graph&lt;/strong&gt;. The most detailed representation, which includes spatial information about atoms and bonds in addition to the graph structure. Although obtaining 3-D graph representations is challenging, models based on this approach often demonstrate superior performance. Various modeling techniques are applied to 3-D graphs, including invariant and equivariant GNNs. &lt;d-cite key=&quot;schnet,equiv_gnn&quot;&gt;&lt;/d-cite&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Given these diverse approaches, this work aims to explore various molecular representations and their potential combination for enhanced performance in downstream tasks, such as molecular property prediction. Additionally, this blog post seeks to analyze the representations of small molecules by comparing nearest neighbors in the latent chemical space. We also investigate representations learned by language models trained on SMILES strings.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt; &lt;p&gt;In this study, we utilized the QM9 dataset to train and evaluate our models. Comprising approximately 133,000 small organic molecules, the dataset includes molecules with up to nine heavy atoms (specifically Carbon, Nitrogen, Oxygen, and Fluorine) and 19 distinct properties. As a well-established benchmark in molecular property prediction research, QM9 offers a comprehensive foundation for our analysis.&lt;d-cite key=&quot;qm9&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;Our primary focus was on predicting the free energy $G$ at 298.15K. To ensure a robust evaluation, we divided the dataset using Murcko scaffolds &lt;d-cite key=&quot;murcko&quot;&gt;&lt;/d-cite&gt; to prevent the same molecular scaffolds from appearing in both the training and testing sets. This division allocates 80% of the data for training, 10% for validation, and the remaining 10% for testing purposes. Additionally, we standardized the target values to have a zero mean and unit variance, aiming for consistency in our predictive modeling.&lt;/p&gt; &lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt; &lt;p&gt;The illustration of the overall approach is presented in Figure 1.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/approach-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/approach-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/approach-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/approach.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1. Illustration of the overall approach. We use different ways to represent molecules and train different models on these initial encodings. &lt;/div&gt; &lt;p&gt;We use the following models to learn the representations of molecules:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Fingerprint-based model&lt;/strong&gt;. Utilizing Morgan fingerprints &lt;d-cite key=&quot;morgan&quot;&gt;&lt;/d-cite&gt; with a radius of 2 and 2048 bits, we developed a multilayer perceptron (MLP) featuring six layers, layer normalization, and a varying number of hidden units (ranging from 512 to 256). This model focuses on learning representations from molecular fingerprints.&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SMILES-based model&lt;/strong&gt;. For the representation of SMILES strings in the QM9 dataset, we employed a Recurrent Neural Network (RNN) with LSTM cells, comprising three layers and 256 hidden units. This model learns to predict the next token in a SMILES string based on the previous tokens, using cross-entropy loss for training: \(\mathcal{L}_{\text{CE}} = -\sum_{t=1}^{T} \log p(x_t | x_{&amp;lt;t})\)&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;2-D graph-based model&lt;/strong&gt;. To handle 2-D graph representations of molecules, we used a Message Passing Neural Network with four layers, 256 hidden units, sum aggregation, mean pooling, and residual connections between convolution layers. The model updates the nodes’ hidden representations as follows:&lt;/li&gt; &lt;/ol&gt; \[h_i^{\ell+1} = \phi \left( h_i^{\ell}, \frac{1}{|\mathcal{N}_i|}\sum_{j \in \mathcal{N}_i} \psi \left( h_i^{\ell}, h_j^{\ell}, e_{ij} \right) \right)\] &lt;ol&gt; &lt;li&gt;&lt;strong&gt;3-D graph-based model&lt;/strong&gt;. While there are many different architectures to model points in 3-D space, we decided to use one of the simplest architectures — E(n) Equivariant Graph Neural Network (EGNN) &lt;d-cite key=&quot;egnn&quot;&gt;&lt;/d-cite&gt; that is equivariant to rotations, translations, reflections, and permutations of the nodes. We used 4 layers, 256 hidden units, sum aggregation, mean pooling and residual connections between convolution layers to learn the representations of 3-D graphs of molecules that updates the nodes hidden representations according to the equations given in the Figure 1.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt; &lt;p&gt;We trained all models using the Adam optimizer with learning rate of $1\cdot10^{-3}$, batch size 32, and 100 epochs. We additionally used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ReduceLROnPlateau&lt;/code&gt; learning rate scheduler. We used the mean absolute error (MAE) as the metric for evaluation.&lt;/p&gt; &lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt; &lt;p&gt;We used several combination of modalities to evaluate the performance of the models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MPNN + FPs: This model integrates the representation learned by the Message Passing Neural Network (MPNN) with the MLP trained on fingerprints, featuring 256 hidden units. It concatenates the representations from MPNN and MLP, using an MLP layer for the final target value prediction.&lt;/li&gt; &lt;li&gt;EGNN + FPs: Similar to the previous model but uses the representation learned by the EGNN.&lt;/li&gt; &lt;li&gt;EGNN + MPNN: This configuration combines the representations from EGNN and MPNN, followed by an MLP for target value prediction.&lt;/li&gt; &lt;li&gt;MPNN + RNN: This model merges representations from MPNN and a pretrained Recurrent Neural Network (RNN). The RNN’s encodings remain static and are not updated during training. However, this model did not converge and was excluded from the final evaluation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The results of evaluation of different models on the QM9 dataset are presented in Figure 2.&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-12-12-combining-modalities-for-better-representation-learning/mae.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2. Different models&apos; performance on the QM9 dataset. The models are trained on the same data, but with different representations. The number of parameters is displayed on top of each bar. &lt;/div&gt; &lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt; &lt;h3 id=&quot;comparison-of-different-models&quot;&gt;Comparison of different models&lt;/h3&gt; &lt;p&gt;As depicted in Figure 2, the EGNN model demonstrates superior performance. A likely explanation is that the QM9 dataset’s labels were calculated using computational methods that leverage the 3-D structure of molecules. The 3-D representation, therefore, proves most effective for this task, with the EGNN adept at capturing crucial 3-D interactions for predicting the target value. Interestingly, simple concatenation of hidden representations seems to dilute the information, resulting in inferior performance. This suggests that combining modalities is a complex endeavor, requiring thoughtful architectural design. &lt;d-cite key=&quot;modality_blending,molecule_sde&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;h3 id=&quot;nearest-neighbors-analysis&quot;&gt;Nearest neighbors analysis&lt;/h3&gt; &lt;p&gt;After the training of the models we performed the nearest neighbors analysis to compare the learned representations of molecules. We took the learned representations of the molecules in the test set and computed the nearest neighbors in the latent chemical space using cosine similarity. Additionally we plotted the PCA reduced representations (Figure 3) and analyzed the nearest neighbors for 4 different molecular scaffolds.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3. PCA reduced representations of the molecules in the test set. The color of the points corresponds to the molecular scaffold. &lt;/div&gt; &lt;p&gt;There are several interesting observations from the nearest neighbors analysis:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In case of fingerprints reductions the nearest neighbors are far away from the queried molecules in the latent chemical space.&lt;/li&gt; &lt;li&gt;For the reduced learned representations of the molecules in the test set we can see that the nearest neighbors are very close to the queried molecules in the latent chemical space. This is expected as the models were trained to predict the target value and therefore the representations of the molecules that are close in the latent chemical space should have similar target values.&lt;/li&gt; &lt;li&gt;The bottom right plot of Figure 3, showcasing the EGNN + FPs combination reveals very interesting pattern — the reduced chemical space reminds the combination of the reduced chemical spaces of the EGNN and FPs. EGNN’s reduced chemical is more “sparse”, while the representation that learned by MLP is more dense but much more spread out. Another interesting observation is that the combined chemical space is more structured due to the presence of some clustered fragments, which is not present in case of both EGNN and MLP.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Additionally we analyzed the nearest neighbors for 4 different molecular scaffolds. The results for 3 of them are present in Figure 4.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-combining-modalities-for-better-representation-learning/dl_pic4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4. Nearest neighbors for 3 different molecular scaffold instances. Top molecule for each cell is the closest molecule to the queried molecule in the latent chemical space, the bottom molecule is the second closest molecule. &lt;/div&gt; &lt;p&gt;From the Figure 4 we can make some additional observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For the fingerprints similarity, molecules are very similar to the queried molecule. This is expected results because the molecules with the highest matches in the fingerprints are the most similar to the queried molecule. Although, for the third example the second closest molecule is not very similar to the queried molecule.&lt;/li&gt; &lt;li&gt;MPNN, EGNN as well as their combination return the molecules that are very similar to the queried molecule. Because the model was trained to predict the target value, the nearest neighbors are molecules with similar target values (this is not guaranteed for the fingerprints similarity because substructures can be combined in different ways potentially leading to very different molecular properties).&lt;/li&gt; &lt;li&gt;In case of MLP trained on fingerprints, the nearest neighbors can have very different scaffolds. This agrees with the performance of the model on the QM9 dataset — the model is not able to fully capture the molecular structure and therefore the nearest neighbors can have very different scaffolds even though the initial representations were the ones retrieving the most similar molecules (fingerprints).&lt;/li&gt; &lt;li&gt;Interestingly, in case of RNN trained on SMILES strings, the nearest neighbors can have very different scaffolds. This result is expected because RNN was trained to predict next token in the sequence and therefore the nearest neighbors are the molecules with similar SMILES strings. For example, first molecule contains triple bond between two carbon atoms. In the case of the second closest neighbor for first scaffold instance there are two triple bonds between carbon and nitrogen atoms. The scaffold is different, but the SMILES strings are similar.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, the key takeaway is that the more effectively a model performs in the supervised learning phase (excluding the RNN), the more meaningful its nearest neighbors are in terms of molecular structure resemblance. While fingerprint similarity still yields closely matched molecules, the results are not as insightful as those from GNNs, which capture molecular structures with greater nuance and expressiveness.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;h3 id=&quot;results-of-modalities-mixing&quot;&gt;Results of modalities mixing&lt;/h3&gt; &lt;p&gt;Modalities mixing is a very interesting and promising approach for the problems in the field of molecular machine learning. However, architectures should be desinged carefully to achieve the best performance. In our work we showed that simple concatenation of the representations learned by different models can lead to worse performance on the downstream tasks.&lt;/p&gt; &lt;h3 id=&quot;future-work&quot;&gt;Future work&lt;/h3&gt; &lt;p&gt;The obvious direction of future work — to experiment with different architectures for modalities mixing. Another interesting direction is to use the mixed modalities for the generative molecular modeling as string methods still perform better than majority of 3-D generative approaches even though the latter one is more natural. &lt;d-cite key=&quot;benchmarking&quot;&gt;&lt;/d-cite&gt; Therefore, it would be interesting to explore the combination of the string and 3-D graph representations for the generative modeling.&lt;/p&gt; </content> </entry> <entry> <title>Exploring Frobenius and Spectral Normalization in MLPs and Residual networks</title> <link href="https://deep-learning-mit.github.io/blog/2023/WeightDecaySpecNormEffects/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/WeightDecaySpecNormEffects</id> <content type="html">&lt;h2 id=&quot;relevance-and-investigation&quot;&gt;Relevance and Investigation&lt;/h2&gt; &lt;p&gt;Weight normalization in deep learning is vital because it prevents weights from getting too large, thereby improving model’s learning ability, accelerating convergence, and preventing overfitting. One traditional method for weight normalization involves adding the sum of the weights’ Frobenius norms to the loss function. One of the issues with penalizing Frobenius normalization of weight matrices is that it imposes a more strict constraint than may be desired for some model types- it enforces that the sum of the singular values is one, which can lead to weight matrices of rank one, which essentially enforces models to make decisions based on only one feature. In 2018, Spectral normalization emerged as an effective method, especially for Generative Adversarial Networks (GANs), to control the Lipschitz constant of the model and stabilize the training process &lt;d-cite key=&quot;DBLP:journals/corr/abs-1802-05957&quot;&gt;&lt;/d-cite&gt;. Spectral normalization is a more relaxed form of weight normalization that scales weight matrices based on their spectral norms, which is the maximum singular value of the matrix. This method is more relaxed because it does not impose a constraint on the sum of singular values, but rather only on the maximum singular value. This allows for weight matrices of higher rank, which may be more desirable for certain model types. Since 2018, spectral normalization has been used in many GAN architectures, but has not been explored as much in other deep learning scenarios. In fact, there is little published research on other approaches to the spectral view of weight normalization in general.&lt;/p&gt; &lt;p&gt;We introduce two novel normalization techniques inspired by AdamW and motivated by issues caused by penalties in the loss function &lt;d-cite key=&quot;DBLP:journals/corr/abs-1711-05101&quot;&gt;&lt;/d-cite&gt;. Our method, which we call Norm Scaling, takes a training step using a loss function that does not include a norm penalty, then scales the norms of the weight matrices after the step. Comparing our Frobenius and spectral normalization algorithms to each other can provide valuable insights into their advantages and disadvantages in various model architectures through a thorough investigation of their effects on the weight matrices. We aim to understand how the spectral norm of weight matrices change over time and how they affect overall model performance. Furthermore, we want to see how singular values change across architectures and algorithms, determining if certain types of architectures can benefit more from spectral normalization than another. It especially becomes interesting to investigate whether spectral normalization’s superior performance in stabilizing GAN training is generalized to other deep learning scenarios via different architecture types and a different scaling technique. The ultimate goal of this exploration is to deepen our understanding of these normalization techniques to find more intelligent ways to regularize weight matrices in order to acheive less overfitting and improve learning ability.&lt;/p&gt; &lt;h2 id=&quot;norm-scaling&quot;&gt;Norm Scaling&lt;/h2&gt; &lt;p&gt;Let us introduce our novel normalization technique, Norm Scaling. We will first describe the algorithm in the context of Frobenius normalization, then we will describe how it will be applied with spectral normalization. We begin each process by initializing the weight matrices of the model to be orthogonal, which helps prevent gradient numerical stability issues and improve convergence timing. We then multiply each weight matrix, \(W_k\) by \(\sqrt{\frac{d_k}{d_{k-1}}}\) where \(d_k\) is the size of the output at layer \(k\). This enforces the initial spectral norm of each weight matrix to be \(\sqrt{\frac{d_k}{d_{k-1}}}\), and the initial Frobenius Norm to be \(\sqrt{min(d_k, d_{k-1})*\frac{d_k}{d_{k-1}}}\).&lt;/p&gt; &lt;p&gt;In the Frobenius Norm Scaling algorithm training is relatively straightfoward. After we initialize the orthogonal weight matrices but before beginning training, we calculate the Frobenius norm of each weight matrix based on the equation above and save these in our model. On each training step, we first calculate the loss, compute the gradients, and take a step using the optimizer. Then, we calculate the Frobenius norm of each weight matrix, \(W_k\), divide the matrix by this norm, and multiply it by its initial value that we calculated before training:&lt;/p&gt; \[\bar{W}_k = \frac{W_k}{||W_k||_F} * \sqrt{min(d_k, d_{k-1})*\frac{d_k}{d_{k-1}}}\] &lt;p&gt;This ensures that the Frobenius norm of each weight matrix, \(W_k\), is equal to its initial value throughout the entire training process.&lt;/p&gt; &lt;p&gt;The Spectral Norm Scaling algorithm is slightly more mathematically complicated, and required the use of power iteration to make sure training time was feasible. After we initialize the orthogonal weight matrices but before training, we save target spectral norms for each weight matrix, \(W_k\). On each training step, we first calculate the loss, compute the gradients, and take a step using the optimizer. Then, we calculate the first singular value, which is the same as the spectral norm, and the first right singular vector of each weight matrix, \(W_k\), using power iteration. In order to mimimize the difference beween the right singular vector and the power iteration prediction of this vector we use 500 steps. To use power iteration with convolution weight matrices, which have dimension 4, we view them as 2 dimension weight matrices where all dimensions past the first are flattened (this reshaping is the channel-wise decomposition method and was used for similar work in Yang et al., 2020 &lt;d-cite key=&quot;yang2020learning&quot;&gt;&lt;/d-cite&gt;).&lt;/p&gt; &lt;p&gt;To find the first right singular vector and singular value, we use the fact that the top eigenvector and corresponding eigenvalue of \(A^TA\) are the first right singular vector and singular value of A respectively. So using the power method, we compute the top eigenvector and eigenvalue of \(W_k^TW_K\). We then use the fact that \(W_kv_1 = \sigma_1u_1\) to compute \(u_1 = \frac{W_kv_1}{\sigma_1}\).&lt;/p&gt; &lt;p&gt;We then perform the following normalization step:&lt;/p&gt; \[\bar{W}_k = W_k + u_1v_1^T(\sigma^* -\sigma_1)\] &lt;p&gt;Where \(\sigma^*\) is the target spectral norm described above.&lt;/p&gt; &lt;p&gt;Note that this calculation subtracts the best rank one approximation of \(W_k\) from \(W_k\), but adds the same outer product back, scaled by \(\sigma^*\). Note that this does NOT enforce that the new spectral norm is \(\sigma^*\), because it is possible that \(\sigma_2\) is greater than \(\sigma^*\). We hope that this normalization prevents the first outer product of singular vectors from dominating the properties of the weight matrix, thus allowing for better generalization outside of the training distribution.&lt;/p&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;In order to test our Norm Scaling learning algorithm, we train a variety of models on image classification of the CIFAR100 dataset &lt;d-cite key=&quot;CIFAR100&quot;&gt;&lt;/d-cite&gt;. We train two types of models, MLPs and ResNets &lt;d-cite key=&quot;DBLP:journals/corr/HeZRS15&quot;&gt;&lt;/d-cite&gt;. Our MLPs consist of fully connected layers without biases, where our medium MLP has 5 layers each with 2,000 hidden units and our large MLP has 7 layers each with 4,000 hidden units. Our ResNets are ResNet34 and ResNet50, loaded in without pre-training. We adjust the size of the first convolution layer to 64 channels of 3x3 kernels and the output layer to output 100 classes. We train each of these four models with three different conditions: the first uses no weight scaling, the second uses Frobenius norm scaling, and the third uses spectral norm scaling. We train each model for 200 epochs with a batch size of 512, an initial learning rate of 0.001, and no weight decay. We use the Adam optimizer and a multi-step learning rate scheduler with \(\gamma = 0.1\) applied at epochs 60 and 120. We use the cross entropy loss function for all models. We use the same training hyper-parameters for all models. The models were trained on 4 NVIDIA Tesla A100 GPUs with paralellization handled by the pytorch lightning library.&lt;/p&gt; &lt;p&gt;At the end of training, the MLP with depth 5, width 2000, and no norm scaling had a test accuracy of 25.12% and a test loss of 10.86. The MLP with depth 5, width 2000, and Frobenius norm scaling had a test accuracy of 28.23% and a test loss of 4.47. The MLP with depth 5, width 2000, and spectral norm scaling had a test accuracy of 23.21% and a test loss of 3.53. The MLP with depth 7, width 4000, and no norm scaling had a test accuracy of 23.95% and a test loss of 11.00. The MLP with depth 7, width 4000, and Frobenius norm scaling had a test accuracy of 26.62% and a test loss of 6.10. The MLP with depth 7, width 4000, and spectral norm scaling has a test accuracy of 36.25% and a test loss of 2.63. ResNet34 with no norm scaling had a test accuracy of 70.1% and a test loss of 2.03. ResNet34 with Frobenius norm scaling had a test accuracy of 75.24% and a test loss of 1.46. ResNet34 with spectral norm scaling had a test accuracy of 71.79% and a test loss of 1.78. ResNet50 with no norm scaling had a test accuracy of 73.45% and a test loss of 1.72. ResNet50 with Frobenius norm scaling had a test accuracy of 75.72% and a test loss of 1.40. ResNet50 with spectral norm scaling had a test accuracy of 73.29% and a test loss of 1.63. Full summaries of the changes of these metrics across epochs are plotted below with checkpoints every 10 epochs.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_acc_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_acc_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_acc_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_acc_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_loss_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_loss_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_loss_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Train_loss_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Training accuracy of MLPs. Above right: Training loss of MLPs. Spec refers to models trained with spectral norm scaling, Frob refers to models trained with Frobenius norm scaling, and Baseline refers to models trained with no norm scaling. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_acc_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_acc_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_acc_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_acc_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_loss_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_loss_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_loss_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP_Test_loss_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing accuracy of MLPs. Above right: Testing loss of MLPs. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_acc_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_acc_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_acc_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_acc_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_loss_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_loss_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_loss_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Train_loss_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Training accuracy of ResNets. Above right: Training loss of ResNets. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_acc_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_acc_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_acc_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_acc_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_loss_sum-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_loss_sum-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_loss_sum-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res_Test_loss_sum.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing accuracy of ResNets. Above right: Testing loss of ResNets. &lt;/div&gt; &lt;h2 id=&quot;findings&quot;&gt;Findings&lt;/h2&gt; &lt;h3 id=&quot;scaling-effects-on-training-stability&quot;&gt;Scaling Effects on Training Stability&lt;/h3&gt; &lt;p&gt;One of the most interesting findings of this investigation is the effect of spectral norm scaling on the stability of training. We can see in the figures above that spectral norm scaling has a significant effect on the stability of training for MLPs, but not for ResNets. For MLPs, spectral norm scaling significantly improves the stability of training, as shown by the fact that the training and test loss curves remain close and follow a similar path. This is especially true for the large MLP, where the training and testing loss and accuracy curves maintain a similar relationship for the entire duration of training while the test loss increases and test accuracy plateaus for the other two normalization methods.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_bl_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing and training accuracy of Large MLP (depth 7, width 4000) with no norm scaling. Above right: Testing and training loss, same model. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_frob_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing and training accuracy of Large MLP with Frobenius norm scaling. Above right: Testing and training loss, same model. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP4k_spec_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing and training accuracy of Large MLP with spectral norm scaling. Above right: Testing and training loss, same model. &lt;/div&gt; &lt;p&gt;Although the train accuracy when using spectral norm scaling doesn’t get as high as in the other two models, it is an accuracy predictor for test accuracy during the entire training time. Furthermore, it is the only of the methods we tests that continues to decrease test loss for the duration of training, where the other two show signatures of overfitting the data and increasing test loss. This is a very interesting finding because it shows that spectral norm scaling can be used to improve the stability of training for MLPs, which is a very important property for deep learning models. This is especially true for MLPs because they are more prone to overfitting than other model types, so improving the stability of training can help prevent overfitting.&lt;/p&gt; &lt;p&gt;We see that this pattern does not hold for ResNets. Rather, it seems that the Frobenius norm scaling method introduces the most stability, but is still not stable as the relationship for spectral norm scaling in MLPs. Similarly, because ResNets rely on convolutions, we do not see issues with overfitting in any of the models. Altough it appears that spectral norm scaling may improve over the baseline stability, the effect is not as noticeable as the effect from Frobenius norm scaling.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_bl_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing and training accuracy of ResNet50 with no norm scaling. Above right: Testing and training loss, same model. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_frob_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing and training accuracy of ResNet50 with Frobenius norm scaling. Above right: Testing and training loss, same model. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res50_spec_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Above left: Testing and training accuracy of ResNet50 with spectral norm scaling. Above right: Testing and training loss, same model. &lt;/div&gt; &lt;p&gt;This is a surprising result considering that spectral normalization was first developed in the context of GANs using convolutional layers for image generation. We will address this disparity in the conclusion.&lt;/p&gt; &lt;h3 id=&quot;scaling-effects-on-spectral-norms&quot;&gt;Scaling Effects on Spectral Norms&lt;/h3&gt; &lt;p&gt;While both our spectral norm and Frobenius norm scaling algorithms resulted in consistently lower spectral norm values across all epochs compared to no normalization, spectral norm scaling had far and away the largest effect on enforcing low spectral norm values for weight matrices:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_bl_norms-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_bl_norms-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_bl_norms-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_bl_norms.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_frob_norms-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_frob_norms-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_frob_norms-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_frob_norms.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_spec_norms-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_spec_norms-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_spec_norms-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/MLP2k_spec_norms.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Changes in spectral norm values for each weight matrix in medium MLP (depth 5, width 2000) across epochs. Above left: No norm scaling. Above center: Frobenius norm scaling. Above right: Spectral norm scaling. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_bl_norms-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_bl_norms-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_bl_norms-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_bl_norms.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_frob_norms-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_frob_norms-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_frob_norms-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_frob_norms.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_spec_norms-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_spec_norms-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_spec_norms-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-WeightDecaySpecNormEffects/Res34_spec_norms.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Changes in spectral norm values for each weight matrix in ResNet34 across epochs. Darker lines correspond to earlier layers. Above left: No norm scaling. Above center: Frobenius norm scaling. Above right: Spectral norm scaling. &lt;/div&gt; &lt;p&gt;Using spectral norm scaling, the spectral norms of both architectures on all layers collapse to values significantly lower than those seen when using Frobenius norm scaling or no norm scaling. The average spectral norm values at the penultimate epoch (199) using spectral norm scaling is 0.8; Frobenius norm scaling is 7.8; and no normalization is 35.4 on the width 2000, depth 5 MLP architecture.&lt;/p&gt; &lt;p&gt;It is also interesting that spectral norms are very similar across layers in later epochs when using spectral norm scaling, but the same is not true for the other two experiments: the average standard deviation in spectral norm values across all layers for the last 100 epochs using spectral norm scaling is ~0.02; Frobenius norm scaling is ~3.7; and no normalization is ~18.4 on the width 2000, depth 5 MLP architecture.&lt;/p&gt; &lt;p&gt;While it may seem obvious that spectral norm scaling would do the best job at encouraging low spectral norm values, this was not evidently the case. While we subtract the best rank one approximation, thus decreasing the spectral norm, the new spectral norm does not necessarily become the target value, as it is possible that the second largest singular value is larger than our target spectral norm. It seemed possible that merely subtracting a rank one matrix would fail to completely curb spectral norm blow up or do it with this level of success. These results show that not only does our method do it successfully, but does it much more so than Frobenius norm scaling. What’s more, the results generalize across wildly different architectures: we see rapid convergence to low singular values in both the ResNet and MLP case roughly around the same epoch.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;One drawback of our method was the significant increase in training times of our models. Compared to the time it took to train the baseline and Frobenius norm scaling implementations, the spectral norm implementations took between ~400% to ~1,500% longer to train. In order to address this in the future we will implement an adaptive power iteration that stops once the singular vectors converge to a certain threshold. This will allow us to reduce the number of power iterations needed to calculate the singular values, thus reducing the training time.&lt;/p&gt; &lt;p&gt;An interesting fold in our results was the difference between stability effects in the MLP and ResNet cases. We see that spectral norm scaling has a significant effect on the stability of training for MLPs, but not for ResNets. This is a surprising result considering that spectral normalization was first developed in the context of convolutional layers for image generation. We believe that this may stem from one of two reasons. The first is that we had to reduce the dimensionality of the convolutional matrices in order to use the power iteration algorithm. Although this allowed us to efficiently calculate the values we needed, it may not have been an accurate reflection of the matrix singular vectors. One route to address this in the future is to try initializing the spectral norm target values based solely on the input and output channel sizes, rather than the full size of the inputs and outputs. The second reason is that the convolutional layers in ResNets are not as prone to overfitting as the fully connected layers in MLPs, so the stability effects of spectral norm scaling would not be as noticeable. However, we still see an effect of Frobenius norm scaling, so this may be a matter of mathematical properties of the convolutional layers that we have not yet explored.&lt;/p&gt; &lt;p&gt;We may see most desired effects on singular values in spectral norm scaling because subtracting the best rank one approximation of the weight matrix does not influence other singular values nor the outer products of their singular vectors. When we view the singular value decomposition as the sum of outer products of singular vectors scaled by singular values, we can see that we only regularize one term in this sum. This may prevent a single outer product from dominating the linear transformation, especially preventing overfitting in MLPs where overfitting tends to be an issue. This is not true of Frobenius normalization, as we scale the entire matrix.&lt;/p&gt; &lt;p&gt;Overall, our results show that spectral norm scaling is a very effective method for stabilizing training in MLPs and enforcing low spectral norm values in MLPs and ResNets. This shows that spectral norm scaling may be a feasible and generalizable method for stabilizing training in a variety of conditions beyond GANs. Furthermore, we were able to achieve this without the use of a penalty in the loss function, achieving the same effect as a penalty without the negative effects. This is especially important because penalties in the loss function can cause issues with convergence and numerical stability alongside enforcing low rank, which we avoid by using our Norm Scaling algorithm. We beleive our results show great potential for further rigorous qauntitative research on the spectral view of weight normalization. We hope that our Norm Scaling algorithm will be used as a baseline for investigating spectral normalization algorithms that are both computationally efficient and effective at stabilizing training alongside enforcing low spectral norm values.&lt;/p&gt; &lt;p&gt;All of our training code can be found in this &lt;a href=&quot;https://github.com/phess2/SpecNorm/tree/main&quot;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; </content> </entry> <entry> <title>Iterated Representation Learning</title> <link href="https://deep-learning-mit.github.io/blog/2023/Iterated-Representation-Learning/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Iterated-Representation-Learning</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Representation learning has become a transformative subfield of deep learning within recent years, garnering widespread attention for its sophistication in learning lower-dimensional embeddings of data beyond classical techniques such as principal component analysis (PCA). From class, we learned that desirable characteristics of good representations include minimality, sufficiency, disentangelement, and interpretability. However, because typical representation learning techniques such as autoencoders learn only one latent embedding from the input data, there exists a gap in the literature on the &lt;em&gt;stability&lt;/em&gt; of the model and learned embeddings.&lt;/p&gt; &lt;p&gt;In this project, we thus explore a new approach to traditional representation learning techniques, in which embeddings for a given set of data are learned repeatedly until some sort of convergence with respect to the model and learned embedding space, a process we call &lt;strong&gt;Iterated Representation Learning (IRL)&lt;/strong&gt;; by analyzing the performance of this iterative approach, our work aims to discover potential insights into the robustness qualities inherent to a model and its associated latent embedding space. We propose an algorithmic framework for IRL, provide an empirical case study of the efficacy of our IRL framework on the MNIST dataset, and suggest a novel evaluation procedure for representation stability and robustness via iterated learning.&lt;/p&gt; &lt;h3 id=&quot;representation-learning-primer&quot;&gt;Representation Learning Primer&lt;/h3&gt; &lt;p&gt;The goal of representation learning is to build models that effectively learn meaningful representations of the data. Representations are important for a variety of reasons, including determining which features are the most explanatory or variable in a dataset, compressing repeated information from a dataset to make it more compact, and learning more effective neural networks, to name a few examples. These representations are typically abstract and less interpretable than the input data, but of lower dimension, which makes them useful in capturing the most essential or compressed characteristics of the data.&lt;/p&gt; &lt;p&gt;More formally, representation learning aims to learn a mapping from datapoints \(\mathbf{x} \in \mathcal{X}\) to a (typically lower-dimensional) representation \(\mathbf{z} \in \mathcal{Z}\); we call this mapping an &lt;strong&gt;encoding&lt;/strong&gt;, and the learned encoding is a function \(f: \mathcal{X} \rightarrow \mathcal{Z}\). From this, a &lt;strong&gt;decoder&lt;/strong&gt; \(g: \mathcal{Z} \rightarrow \mathcal{X}\) can be applied to reconstruct the encoded data into its original dimension. This is demonstrated in the diagram below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Representation learning goal. Image credit: &lt;i&gt;Foundations of Computer Vision: Representation Learning&lt;/i&gt; (Torralba, Isola, Freeman 2023). &lt;/div&gt; &lt;p&gt;Some of the most salient learning methods within representation learning today include autoencoding, contrastive learning, clustering, and imputation; in this project, we focus on specifically on iterative approaches for the class of &lt;strong&gt;autoencoders&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Representation learning also has intricate ties to generative modeling, the subfield of deep learning that aims to generate new data by mapping a simple base distribution to complicated high-dimensional data, which is essentially the opposite goal of representation learning. Then, after learning an embedding space via representation learning, this embedding can then be sampled from to &lt;em&gt;generate&lt;/em&gt; new data that mimics the original data, as demonstrated by &lt;strong&gt;variational autoencoders (VAEs)&lt;/strong&gt;, which we also explore in this paper.&lt;/p&gt; &lt;h3 id=&quot;prior-literature&quot;&gt;Prior Literature&lt;/h3&gt; &lt;p&gt;Relatively little literature exists regarding iteratively training dimensionality reduction or representation learning models. &lt;a href=&quot;https://ieeexplore.ieee.org/document/9528915&quot;&gt;Vlahek and Mongus (2023)&lt;/a&gt; proposes an iterative approach for &lt;em&gt;conducting&lt;/em&gt; representation learning more efficiently, specifically for the goal of learning the most salient features, which fundamentally diverges from our goal and also does not consider embedding robustness. &lt;a href=&quot;https://arxiv.org/abs/1809.10324&quot;&gt;Chen et al. (2019)&lt;/a&gt; introduces an iterative model for supervised extractive text summarization, though their objective of trying to optimize for a particular document by feeding a given document through the representation multiple times differs from ours. &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9414713&quot;&gt;Cai, Wang, and Li (2021)&lt;/a&gt; finds an iterative framework for self-supervised speaker representation learning which performs 61% better than a speaker embedding model trained with contrastive loss, but mainly focuses on the self-supervision aspect of the model and optimizes purely for model test accuracy, not considering other metrics such as stability or robustness.&lt;/p&gt; &lt;p&gt;Overall, we find that the literature regarding iterative approaches to representation learning is already sparse; of the work that exists, most focuses on very specific use cases, and no work directly examines the robustness or stability of the model and embeddings themselves learned over time, rather optimizing purely for final model performance.&lt;/p&gt; &lt;h2 id=&quot;iterated-representation-learning&quot;&gt;Iterated Representation Learning&lt;/h2&gt; &lt;h3 id=&quot;existing-dimensionality-reduction-and-representation-models&quot;&gt;Existing Dimensionality Reduction and Representation Models&lt;/h3&gt; &lt;p&gt;Nowadays, there are a variety of approaches to effective dimensionality reduction. Below we cover three of the most common techniques.&lt;/p&gt; &lt;h4 id=&quot;principal-component-analysis&quot;&gt;Principal Component Analysis&lt;/h4&gt; &lt;p&gt;Principal Component Analysis (PCA) has two primary objectives. First, maximizing sample variance of the newly transformed data, which is analogous to identifying and capturing the greatest (largest) directions of variability in the data (principal components or PCs). Formally, a PC is defined&lt;/p&gt; \[v^* = \arg \max_v \frac{1}{N-1} \sum_{n=1}^N (x^T_n v - \bar{x}^T v)^2 = \arg \max_v v^T C v\] &lt;p&gt;where \(C = \frac{X^T X}{n-1} \in \mathbb{R}^{d \times d}\) is the empirical covariance matrix.&lt;/p&gt; &lt;p&gt;The second objective is minimizing reconstruction loss, which is analogous to identifying the directions of variability to accurately and concisely represent data. Let \(U\) be the orthonormal basis projection matrix of eigenvectors of \(C\). Then we define reconstruction loss as&lt;/p&gt; \[\mathcal{L}(U) = \frac{\sum_{n=1}^N ||x_n - U U^T x_n||^2}{N}\] &lt;p&gt;Above, we observe that maximizing sample variance and minimizing reconstruction loss go hand-in-hand. Since PCA applies projections by multiplying vectors/matrices to the data, PCA is limited to the &lt;em&gt;linear&lt;/em&gt; transformation setting, hence restricting its applicability in many modeling problems.&lt;/p&gt; &lt;h4 id=&quot;autoencoders&quot;&gt;Autoencoders&lt;/h4&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Autoencoder structure. Image credit: &lt;i&gt;Foundations of Computer Vision: Representation Learning&lt;/i&gt; (Torralba, Isola, Freeman 2023). &lt;/div&gt; &lt;p&gt;Similar to PCA, autoencoders also aim to minimize reconstruction loss. However, autoencoders are not limited to just linear transformations, which enables autoencoders to learn more general lower-dimensional representations of data. Autoencoders are comprised of an encoder and decoder, where the encoder maps data to a lower-dimensional representation (embedding) via some function $f$, and the decoder maps the originally transformed data back to its original dimensional space via some function $g$.&lt;/p&gt; &lt;p&gt;End to end, the data space starts in \(\mathbb{R}^N\), is downsized to \(\mathbb{R}^M\) by \(f\), and then is reverted back to \(\mathbb{R}^N\) where \(N &amp;gt; M\). In this case, we can formalize the objective as follows:&lt;/p&gt; \[f^*, g^* = \arg \min_{f,g} E_\mathbf{x} || \mathbf{x} - g(f(\mathbf{x}))||^2_2\] &lt;h4 id=&quot;variational-autoencoders&quot;&gt;Variational Autoencoders&lt;/h4&gt; &lt;p&gt;VAEs couple autoencoders with probability to get maximum likelihood generative models. Typically for encoding, VAEs regularizes the latent (hidden) distribution of data to “massage” the distribution into a unit Gaussian, and when reverting back to the original dimensional space, VAEs add noise to the output — hence, a mixture of Gaussians. By imposing a unit Gaussian structure on the learned embedding space, this allows VAEs to act as generative models by sampling from the Gaussian latent space to generate new data. Unlike traditional autoencoders, VAEs may have embedding spaces that are complicated (if not just as complicated as the data).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; VAE&apos;s complex embedding space. Image credit: &lt;i&gt;Foundations of Computer Vision: Generative Modeling Meets Representation Learning&lt;/i&gt; (Torralba, Isola, Freeman 2023). &lt;/div&gt; &lt;p&gt;Formally, the VAE learning problem is defined by&lt;/p&gt; \[\theta^* = \arg \max_{\theta} L(\{\mathbf{x}^{(i)}\}^N_{i=1}, \theta) = \arg \max_{\theta} \sum_{i=1}^N \log \int_{\mathbf{z}} \mathcal{N} (\mathbf{x}^{(i)}; g_{\theta}^{\mu}(\mathbf{z}), g_{\theta}^{\Sigma}(\mathbf{z})) \cdot \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{\mathrm{I}}) d\mathbf{z}\] &lt;h3 id=&quot;iterated-representation-learning-1&quot;&gt;Iterated Representation Learning&lt;/h3&gt; &lt;h4 id=&quot;proposed-framework&quot;&gt;Proposed Framework&lt;/h4&gt; &lt;p&gt;We now introduce the Iterated Representation Learning Framework (IRL) for autoencoders and VAEs. We start with IRL for autoencoders:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Given design matrix \(X\), learn an autoencoder for \(X\).&lt;/li&gt; &lt;li&gt;Using the decoder from above, reconstruct the data to get \(X&apos;\) and compute its reconstruction loss.&lt;/li&gt; &lt;li&gt;Using the reconstructed data \(X&apos;\), repeat Steps 1 and 2 and iterate until the reconstruction loss converges or reaching iteration limit.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As for VAEs, we follow a similar procedure as above.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Given design matrix \(X\), learn a VAE for \(X\).&lt;/li&gt; &lt;li&gt;Using the decoder and adding Gaussian noise, reconstruct the data to get \(X&apos;\). Compute its reconstruction loss.&lt;/li&gt; &lt;li&gt;Using the reconstructed data \(X&apos;\), repeat Steps 1 and 2 and iterate until the reconstruction loss converges or reaching iteration limit.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In this report, we examine how IRL is connected to representation, investigate several hypotheses about IRL, and conduct a preliminary case study of IRL on the MNIST dataset.&lt;/p&gt; &lt;h4 id=&quot;preliminary-questions-and-hypotheses&quot;&gt;Preliminary Questions and Hypotheses&lt;/h4&gt; &lt;p&gt;Motivated by how there may be unexplored stability properties of embeddings, our main hypotheses are twofold. First, iterated reconstruction loss per IRL can convergence with respect to the model. Second, learned embedding spaces can be reached via IRL, and that the number of iterations until convergence, loss at convergence, and such preserved features upon convergence could reveal meaningful properties of the true representation space, model, and data that are not immediately obvious from a standard autoencoder model.&lt;/p&gt; &lt;p&gt;More specifically, does the number of iterations until convergence have anything to do with how ``good’’ or stable the model or learned representation is? What does it mean if the reconstruction losses converge? What can we say about characteristics of the data that are maintained through iterations, and characteristics that evolve as the iterations go on? For example, if we observe that a model remains invariant to a certain feature, but becomes sensitive to new features of the data, what does this tell us about these particular features, our model, and the original data itself?&lt;/p&gt; &lt;p&gt;Perhaps most importantly, beyond the qualitative observations themselves, can we propose some sort of representation learning evaluation framework using iterated representation learning, e.g. rough guidelines on ideal number of iterations required until convergence, and what this says about how good a model is? Ultimately, we hope that using an iterated framework can serve as a general tool for (1) evaluating the stability or robustness of a representation learning model and (2) identifying the most core characteristics of a given dataset.&lt;/p&gt; &lt;h2 id=&quot;case-study-mnist-dataset&quot;&gt;Case Study: MNIST Dataset&lt;/h2&gt; &lt;p&gt;To evaluate IRL on a real-world dataset, we selected MNIST to test our hypotheses. We carefully designed our experiments, collected relevant data, and include our analysis below.&lt;/p&gt; &lt;h3 id=&quot;experimental-design&quot;&gt;Experimental Design&lt;/h3&gt; &lt;p&gt;For our experiments, we implemented IRL using the framework given above for the class MNIST digits dataset (due to its simplicity and intrepretability), where we preset the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_iterations&lt;/code&gt;. At every iteration, we initialize a new autoencoder model with &lt;a href=&quot;https://arxiv.org/abs/2206.08309&quot;&gt;Chadebec, Vincent, and Allassonnière’s (2022)&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pythae&lt;/code&gt; autoencoder/VAE library. The encoder architecture is formed by sequential convolutional layers from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PyTorch&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We then trained the model, reconstructed the data, and saved the trained and validation loss. We also saved the original train/test and reconstructed train/test images of the first 25 datapoints to track how IRL progressed visually.&lt;/p&gt; &lt;h3 id=&quot;autoencoder-irl-analysis&quot;&gt;Autoencoder IRL Analysis&lt;/h3&gt; &lt;p&gt;First, we take a look at the (log) mean squared error of our autoencoder over 30 iterations of IRL, given in the plot below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Autoencoder log loss over IRL of 30 iterations. &lt;/div&gt; &lt;p&gt;We notice that both the train and validation loss steeply decrease until around iteration 10, upon which the validation loss begins to roughly stabilize and converge. This confirms our intuition that the loss following an iterated approach should eventually converge, which we can theoretically verify by observing that if we ran \(n\) iterations, then as \(n\to\infty\), because the loss is lower-bounded by zero and should generally from iteration to iteration (since we are removing information from our data), we must eventually converge. We further hypothesize that the fact that the loss has converged means that the embeddings upon convergence have learned the most succinct, critical portion of the data.&lt;/p&gt; &lt;p&gt;We also notice that the number of iterations until convergence is very small; as mentioned, after about 10 iterations, it seems that the validation loss has roughly converged. We had hypothesized earlier that if the autoencoder converges after a small number of iterations, then that says something about the quality of the autoencoder architecture. Here, the fact that the loss converged after a small number iterations gives evidence for this hypothesis, since based on separate tests, this architecture indeed achieves relatively high classification accuracy for the MNIST dataset. We suggest that IRL can thus serve as a framework for evaluating the quality of an autoencoder on a particular dataset.&lt;/p&gt; &lt;p&gt;Additionally, the validation loss converges at a relatively small number (around 0.25 by iteration 10), meaning that the distance between the original and reconstructed data in a given iteration are very similar. Interestingly enough, the validation loss is actually consistently lower than the train loss, which suggests that the learned representations through this iterated approach actually generalize very well to unseen data, which is certainly a desirable quality of any model.&lt;/p&gt; &lt;p&gt;We also give the original and reconstructed data for iterations 1, 5, 10, 15, and 20, for both the train and test data, in the figures below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Reconstructed train data. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Reconstructed test data. &lt;/div&gt; &lt;p&gt;In the beginning, we see that the data starts losing resolution (e.g. the numbers become fuzzier and start losing their distinctness from the background), which makes sense because more iterations means more reconstructions that continue to accumulate reconstruction loss. The reconstructed images are also less clear than the originals due to the information that is lost from the encoding-decoding process.&lt;/p&gt; &lt;p&gt;Our key observation is that the reconstruction loss stabilizes around the 10th iteration, where the original test images and reconstructed test images look very similar — we hypothesize that this is the point where the autoencoder has learned to represent the data as succinct as possible while preserving the most critical information.&lt;/p&gt; &lt;h3 id=&quot;vae-irl-analysis&quot;&gt;VAE IRL Analysis&lt;/h3&gt; &lt;p&gt;We similarly plot the log loss for our VAE, as well as the train, test, and sampled data over iterations in the figures below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; VAE log loss over IRL of 30 iterations. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Train, test, and normal sampled data. &lt;/div&gt; &lt;p&gt;Unlike the autoencoder, the VAE’s train data becomes much more noisy across the 20 iterations. This is likely due to how the VAE injects noise in the reconstruction, which in this case resulted in the images to lose their distinctness. While the general shape is preserved (roundness, lines, etc), many of the numbers actually ended up merging together and losing their number shape altogether (e.g. some 6s, 3s, 9s all become 0s).&lt;/p&gt; &lt;p&gt;When comparing IRL on the autoencoder versus the VAE, we observe that the VAE’s log loss converges to a larger log loss than the autoencoder, which makes sense because the VAE’s decoding step adds noise to the images that therefore adds loss to the reconstruction. We also note that the both of the models experience steep drop offs in log loss initially, which means the first few iterations eliminated most of the noise in the data and preserved the features that we characterize as “stable”.&lt;/p&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;Our proposed IRL framework considers how some features may be more important or more stable than others, and it aims to capture those features while eliminating the noise in the data. While traditional dimensionality reduction techniques have their merits, IRL takes those methods one step further by iteratively trimming away noise until convergence or termination. Throughout this project, we cover representation learning fundamentals and IRL can capitalize on the way they learn embeddings, and we also apply this framework to real world data on MNIST. We argue that in our case study of MNIST, IRL does converge in terms of both loss (log mean squared error converges) and reconstructions, which is a promising first step in the analysis of stability and fundamental characteristics of the data. Moreover, we showcase how the number of iterations until convergence has significance, serving as a benchmark for how good an autoencoder/VAE is on a given dataset. Although VAE’s reconstructed images were more noisy, that’s by nature of the VAE, and we still observe that the fundamental features of the data (lines vs circles) are still preserved throughout iterations.&lt;/p&gt; &lt;p&gt;There are a variety of directions we’d like to continue to explore with this project, given more time.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We were only able to run a limited number of experiments due to computational power and the duration of time to train a full IRL from start to finish for, say, 30 iterations. Given more time, there are multiple other experiments we’d like to run, including training on other datasets and trying out the performance on different autoencoder architectures to better understand the properties of this iterated approach. Another thing we’d like to evaluate the empirical performance of, but also couldn’t due to computational constraints, is how a single autoencoder with 20 times as many neurons as some basic autoencoder compares to the basic autoencoder trained using IRL for 20 iterations.&lt;/li&gt; &lt;li&gt;We’re also curious to further explore the theoretical guarantees provided by IRL, including rigorous bounds on convergence. We’re also very interested in exploring whether any of our observations from IRL can generalize to other classes of deep learning models.&lt;/li&gt; &lt;li&gt;We’d lastly look into ways to make IRL more computationally tractable. As mentioned, our experimentation was heavily limited due to the computational cost of training a new autoencoder during every iteration. If possible, we’d like to look for optimizations of this framework that still preserve the desired methodology.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Overall, Iterated Representation Learning serves as a framework to evaluate stability-related properties of data, which we believe to be an important but overlooked standard for representation learning. Our case study of MNIST shows promise for empirical convergence guarantees on certain datasets, and we hope that our work lays the foundation for future representation discussions with respect to stability.&lt;/p&gt; </content> </entry> <entry> <title>A Method for Alleviating Catastrophic Forgetting With Explainability</title> <link href="https://deep-learning-mit.github.io/blog/2023/alleviating-catastrophic-forgetting-in-classification-tasks-through-strategic-filter-dropout/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/alleviating-catastrophic-forgetting in-classification-tasks-through-strategic-filter-dropout</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;With recent advancements in deep learning, the intelligence of computers is quickly rivaling that of humans. GPT-4, with significant size and data, is able to score in the 90th percentile of the BAR, 88th percentile of the LSAT, and the 92nd percentile on the SAT &lt;d-cite key=&quot;openai2023gpt4&quot;&gt;&lt;/d-cite&gt;. In dermatology, sophisticated computer vision models have outperformed trained professionals in diagnosing skin diseases and cancer &lt;d-cite key=&quot;jeong2023deep&quot;&gt;&lt;/d-cite&gt;. Despite this substantial computational advantage, neural networks notably lag behind humans in their capacity for continuous learning, a skill essential for any intelligent entity. Particularly, they suffer from catastrophic forgetting, a phenomenon in which the learning of a new objective significantly degrades performance on prior tasks.&lt;/p&gt; &lt;p&gt;The human brain is able to protect itself from conflicting information and reductions in performance on previous tasks using complex mechanisms involving synaptic plasticity &lt;d-cite key=&quot;hadsell2020embracing&quot;&gt;&lt;/d-cite&gt;. In essence, the brain is able to self regulate the strength of its connections, allowing for neurons to become less activated according to their memory and relevance. This ability has been attributed for the unmatched ability to learn in humans, which has allowed for humans to show improvement in skill on nearly any motor task given training, while still remembering previous information &lt;d-cite key=&quot;green2008exercising&quot;&gt;&lt;/d-cite&gt;. This, then, is highly desirable for neural networks.&lt;/p&gt; &lt;p&gt;In contrast to the human’s ability to learn, neural networks significantly alter their parameters when learning a new task. In effect, the network’s understanding of previous tasks is overwritten. This poses a great barrier to the creation of artificial general intelligences, which ultimately depend on continual, life-long learning &lt;d-cite key=&quot;silver2011machine&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;With the rapid increase in size and complexity of models, the field of model explainability and the desire to understand exactly what models are doing has quickly grown. Specifically in the field of computer vision, effort has been made to understand how models make decisions, what information leads to this decision, and how they learn what to observe &lt;d-cite key=&quot;haar2023analysis&quot;&gt;&lt;/d-cite&gt;. Methods such as saliency mapping, which displays the importance of aspects of an input image to predicting a class, filter visualization, which finds the most activating features for a given filter, and gradient class activation maps, which visualizes the gradients flowing into the final convolutional layer, have all significantly contributed towards the understanding of how models make decisions &lt;d-cite key=&quot;adebayo2018sanity&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;erhan2009visualizing&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;selvaraju2017grad&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;We propose to make use of these explainability methods for the intelligent freezing of filters of a convolutional neural network. Specifically, we use saliency maps and filter visualizations to consider what a model is observing to classify an image, and then decipher which filters are most strongly contributing to this. In this paper, we contribute the following: 1. We create a method for the ranking of importance of filters in a convolutional neural network. We expand and combine upon previous works in model explainability to understand which filters are most strongly contributing to positive predictions. 2. We create a method for the freezing of filters of a convolutional neural network according to these rankings. We do this by first training on one task, freezing filters according to importance, then retraining the same model on a novel task. In doing this, we both corroborate our ranking system and identify a new strategy for alleviating catastrophic forgetting.&lt;/p&gt; &lt;h1 id=&quot;related-works&quot;&gt;Related Works&lt;/h1&gt; &lt;p&gt;Continual learning and its core problem of catastrophic forgetting has gotten recent attention in deep learning research. It’s easy to understand why the goal of having a model that can adapt to new data without being completely re-trained is sought after, and there have been many approaches to the problem of aiding the model’s ‘memory’ of past tasks. Solutions range from attaching a significance attribute to certain weights in the model that regularizes change introduced by the new data to explicitly freezing weights via different metrics of the weights’ performance.&lt;/p&gt; &lt;h2 id=&quot;weight-changing-regularization&quot;&gt;Weight Changing Regularization&lt;/h2&gt; &lt;p&gt;Elastic Weight Consolidation(EWC) approaches the problem of catastrophic forgetting by adding a ‘stiffness’ to the weights of previous tasks dependent on an approximation of the importance they had to previous task performance. The authors of ‘Overcoming catastrophic forgetting in neural networks’ &lt;d-cite key=&quot;Kirkpatrick_2017&quot;&gt;&lt;/d-cite&gt;. explain EWC as maximizing a posterior of the parameters over the entire dataset, and then splitting up the posterior into a loss over the new task and a posterior of the parameters over the old task. They model the posterior of the old data as a quadratic difference of the original parameters and the current ones multiplied by the Fisher information matrix, so minimizing this results in preventing parameters from changing too much from being predictable from the old task’s data. The authors of the original paper showed that EWC was effective at preventing CNN from forgetting how to classify the MNIST dataset and helping an RL model maintain performance in Atari games. However, EWC is an additional loss metric that must be calculated for each back-propogation and for each previous task; it’s also linear in the size of the output and therefore is prohibitive for high dimensional data.&lt;/p&gt; &lt;p&gt;Another technique that attempts to use a regularizing factor to slow the retraining of old task parameters is explicitly computing a importance metric for each neuron in the network&lt;d-cite key=&quot;zenke2017continual&quot;&gt;&lt;/d-cite&gt;. The authors denote this method as “Synaptic Intelligence” as they drew their inspiration from the complex adaptation of synapses in the brain contrasted with the simple uni-scalar representation of neurons in a MLP network, and by allowing the network to account for the importance of they could help a neural network model the human behavior of continual learning. The metric they calculate as importance is based on 1) how much a parameter contributed to the reduction of loss over the entirety of training and 2) how much a parameter changed during training. They compared their performance to EWC and standard SGD on the MNIST dataset and found similar results to EWC while beating naive SGD as the number of consecutive tasks increased.&lt;/p&gt; &lt;h2 id=&quot;architectural-changes&quot;&gt;Architectural Changes&lt;/h2&gt; &lt;p&gt;A drastically different approach that a couple papers investigated was preventing interference between training runs by completely freezing the weights in parts of the model after completing a task’s training. The papers here differentiate themselves via the method they decide to freeze certain weights and layers. The earliest such paper we found was detailing a method called Packnet &lt;d-cite key=&quot;mallya2018packnet&quot;&gt;&lt;/d-cite&gt;, where the weights they selected to keep via freezing was purely based on a certain percentage of the weights with the highest magnitude. They also made the decision to completely wipe the weights they did not freeze and then do a couple epochs of training on the model that was a mix of frozen and pruned weights. Their strategy achieved performance roughly equal to networks jointly trained on all the data at once and outperformed the naive strategy of simply retraining, validating a version of the freezing strategy.&lt;/p&gt; &lt;p&gt;Instead of simply measuring the magnitude of weights to decide what layers or specific weights to freeze, authors of a paper on catastrophic forgetting explainability paper use a custom metric to find a layer that scores highest on their metric and subsequently freeze all the layers prior to that layer &lt;d-cite key=&quot;nguyen2022explaining&quot;&gt;&lt;/d-cite&gt; Their metric is an analysis of the difference in activation maps of a layer in the model pre- and post- training on the new task. They posit that this difference in activation is a measurement of how much a layer has forgotten how to activate in response to an input. Their reasoning for freezing the layers prior to the layer most changed by the new sample set is that the errors that induce catastrophic forgetting propagate throughout the network, so identifying the layer with the sharpest drop-off indicates that prior layers are to blame. This seemingly builds off an earlier paper &lt;d-cite key=&quot;nguyen2020dissecting&quot;&gt;&lt;/d-cite&gt; that uses a similar activation map difference scheme to delicate layers that change more easily during training and instead directly freezes those fragile layers rather than those prior. In both papers, their results for this technique are an improvement over their ‘fine-tuning’ baseline, but the more recent paper’s results were not that differentiated from just selecting a layer to freeze before training a new task.&lt;/p&gt; &lt;h2 id=&quot;explanability-metrics&quot;&gt;Explanability Metrics&lt;/h2&gt; &lt;p&gt;There exists many other explainability metrics with which one can target layers prior to training on a new task to try to prevent interference, an interesting one being saliency maps. Saliency maps attempt to capture the importance of features of the input on the output of a deep neural network. In the domain of CNNs, this can be thought of both the pixels and larger features, such as a window on a car, that contribute to a correct classification; saliency maps are analogous to trying to map out what parts of an image a model uses to make correct identification. A model of saliency maps we felt compelled enough to use in our project is that of &lt;d-cite key=&quot;srinivas2019fullgradient&quot;&gt;&lt;/d-cite&gt;, where their full-gradient approach creates saliency maps from the gradients of each layer. This strategy encapsulates the importance of both the inputs and the impact of neurons throughout the network on the saliency map. As parts of a neural network might suffer from varying degrees of catastrophic forgetting, being able to identify the saliency of individual neurons is a desirable quality in choosing a metric that explains catastrophic forgetting.&lt;/p&gt; &lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt; &lt;h2 id=&quot;model-type&quot;&gt;Model Type&lt;/h2&gt; &lt;p&gt;We tested our method using VGG16. VGG16 is a deep convolutional neural network that has achieved impressive results on the ImageNet classification challenge, with a top-1 accuracy of 72% &lt;d-cite key=&quot;simonyan2014very&quot;&gt;&lt;/d-cite&gt;. Its sequential nature lends itself well to explainability methods like saliency maps. Further, it is relatively quick to train, even given the constraints of Google Colab. All of these attributes were highly desirable, as it allowed for rapid iteration for hyperparameter tuning, computation of saliency maps and filter visualizations, and a direct way to compare the viability of our freezing method through image classification accuracy. To ensure that the model did not have inference on any tasks prior to training, we randomly initialized the parameters.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/vgg16.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/vgg16.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/vgg16.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/vgg16.webp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1: Schematic of VGG16 Architecture &lt;/div&gt; &lt;h2 id=&quot;saliency-mapping&quot;&gt;Saliency Mapping&lt;/h2&gt; &lt;p&gt;The computation of saliency maps is grounded in the principles of backpropagation. It follows a multi-staged procedure which uses gradients to consider the impact of each pixel in an image. First, it computes the partial derivatives of the target output with respect to individual segments of the input image. Then, it uses backpropagation to propagate error signals back to the input layer. It does this in order to identify the impact of pixels. It considers pixels with larger signals to have the greatest impact on the decision-making process. There are a bountiful number of papers which propose different improvements on the original saliency map. When selecting a procedure, we identified two key features necessary for a useful visualization. We believed that a saliency map must have a full explanation of why a model made its prediction. Secondly, we believed that rather than considering each individual pixel, it clusters pixels together to consider importance. After testing, we ultimately used full-gradient saliency maps &lt;d-cite key=&quot;srinivas2019full&quot;&gt;&lt;/d-cite&gt;. Code for this method is publicly available on the GitHub created by the authors of this paper, fullgrad-saliency.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_raw-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_raw-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_raw-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_raw.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_saliency-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_saliency-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_saliency-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/mug_saliency.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: Raw image and saliency map of a mug. &lt;/div&gt; &lt;p&gt;The essence of full-gradient saliency maps lines up directly with the key features that we identified. To begin, it defines importance in the input image as a change in the feature resulting in change in model output. It seeks to illustrate a full answer for the model’s output. To this end, it considers both global and local importance of features in the input image, which results in a method which both weighs the importance of each pixel individually, but also considers the importance of different grouping of pixels.&lt;/p&gt; &lt;h2 id=&quot;filter-visualization&quot;&gt;Filter Visualization&lt;/h2&gt; &lt;p&gt;In order to compute what different filters are looking at, we made use of the Convolutional Neural Network Visualizations GitHub repository, which is a useful library that has implementations of many popular explainability methods &lt;d-cite key=&quot;uozbulak_pytorch_vis_2022&quot;&gt;&lt;/d-cite&gt;. Specifically, we used the implementation of a filter visualization method from the paper “Visualizing Higher-Layer Features of a Deep Network”, which uses backpropagation to maximize the activation of a given filter &lt;d-cite key=&quot;erhan2009visualizing&quot;&gt;&lt;/d-cite&gt;. With this, we can compute exactly what a filter is attempting to observe in an image. This method provides two different options for creating filter visualizations - one with gradient hooks, and one without.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_feature_viz-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_feature_viz-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_feature_viz-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_feature_viz.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_raw-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_raw-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_raw-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_raw.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_saliency-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_saliency-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_saliency-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/jar_saliency.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: Heatmaps of Feature Visualization(left), Actual Image(middle), Saliency Map(right) &lt;/div&gt; &lt;h2 id=&quot;training-procedure-and-dataset-selection&quot;&gt;Training Procedure and Dataset Selection&lt;/h2&gt; &lt;p&gt;We created two datasets from CIFAR-100 &lt;d-cite key=&quot;erhan2009visualizing&quot;&gt;&lt;/d-cite&gt;. We randomly selected 20 classes out of the total 100 and then divided these groups into two. We filtered the images from CIFAR-100 so that only images of those classes were in our datasets. We did this to ensure that the tasks the model was attempting to learn were of equal difficulty. We chose CIFAR-100 because we believed it was of adequate difficulty for the VGG16 architecture. We normalized the data and augmented it with random horizontal flips and random croppings. For the first instance of training, we trained using stochastic gradient descent for 10 epochs with a learning rate of 1E-3. We did not implement any regularization or early stopping, as it was not necessary given training losses and testing losses. After this training, we used the described methods for calculating saliency maps and filter visualizations. For each class in the first dataset, we calculated the most useful filters by comparing saliency maps for the class to all filters. We compared these through multiple metrics, including mean squared error and Pearson correlation. To account for the fact that different layers of convolutional neural networks capture different types of information, we froze some percent of filters in each individual layer rather than the entire model. We left this percent as a hyperparameter. To ensure fairness for each task, the second instance of training followed the same exact procedure as the first - the optimizer was stochastic gradient descent, we trained for 10 epochs, and used a learning rate of 1E-3.&lt;/p&gt; &lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt; &lt;p&gt;For the sake of hyperparameter tuning and evaluating different strategies, we froze the datasets to be the first and second ten images of CIFAR-100. We sought to check how the number of filters we freeze changes performance across datasets, which metric is most useful in comparing saliency images to filter visualizations, and how viable this method is as compared to training on a single, larger dataset. Prior to the second round of training, the test accuracy on the first dataset was .4566 and the test accuracy on the second dataset was .1322.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The impact of freezing varying numbers of filters is in line with expectation - the more filters you freeze, the less inference you can gain, but also the more you will remember your previous task. In the table above, we can observe that with 25% of the filters frozen, we perform the best on dataset 2, with an accuracy of 39.2%, but the worst on dataset 1, with an accuracy of 20.7%. In contrast, when 75% of the filters are frozen, we maintain an accuracy of 38.4%, but do not learn about the new task, with an accuracy of 25.7%.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We found that mean squared error was the greatest metric for the comparison of saliency maps and filter visualizations, recording the highest average accuracy and also retaining much more information about the first dataset. From the table, we can see that when freezing 50% of filters in the network and selecting using mean squared error, we do roughly ten percentage points worse on the first dataset, but gain nearly double this loss on the second dataset. When compared to the randomly frozen method, it performs significantly better on the first dataset. This suggests that the filters that we froze are actually more important for correct predictions than the average. It makes sense that Pearson correlation is not particularly useful for comparison - it is not able to take into account the spatial information that is crucial for this comparison.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-9-alleviating-catastrophic-forgetting%20in-classification-tasks-through-strategic-filter-dropout/table3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Finally, we found that training tasks sequentially and using the freezing method with a comparison metric of mean squared error slightly outperforms training the model on a larger, combined dataset at once. With this method, the model performed five percentage points better on predicting classes in both the first and second dataset. It is important to note that the accuracy reported for the model trained on the combined dataset is just the average accuracy over all of the classes, not necessarily split by the datasets. Still, to ensure fairness, the training procedure used for the combined dataset was the same as for the sequential training procedure, but trained for twenty epochs at once rather than ten epochs at two different times. This result implies that intelligently freezing filters of a neural network can be a viable strategy for overcoming catastrophic forgetting, even if just in a smaller setting.&lt;/p&gt; &lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt; &lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt; &lt;p&gt;Through using convolutional neural network explainability methods such as saliency maps and filter visualizations, we were able to observe key insights into the relevance of different filters in VGG16. Quantitatively, we were able to measure this by freezing these layers and observing how well performance persisted after training on a new task. We found that freezing filters according to the similarity of their visualizations to saliency maps retains significantly more inference on a previous task, suggesting that these filters were more relevant to the previous task. By freezing these weights, we were also able to outperform simply training on a larger dataset. We believe that more research should be directed towards applying explainability methods to achieve the objective of continual learning. Although there has been previous work in the past, these often rely on stopping catastrophic forgetting once it has been observed, rather than determining which parts of the network are too integral to a task to be retrained.&lt;/p&gt; &lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt; &lt;p&gt;Because we are completely freezing weights, it is unlikely that this method could be generalizable to an arbitrary number of tasks. Future works could explore the integration of elastic weight consolidation into our pipeline rather than stopping change entirely. Doing class by class freezing of filters also introduces a cap to the number of tasks that this method could generalize to and the number of classes that can be predicted in each task. During our research, we concluded that this approach was better than attempting to combine saliency maps, but future work could also explore how to effectively combine saliency maps to capture important aspects of each class. Further, this method relies on the comparability of saliency maps and filter visualizations. While it makes intuitive sense that a filter is more relevant if it is seeking the parts of an input that are most important for a correct prediction, it is not as simple as directly comparing the two. While we attempt to alleviate some of this issue by doing layer-by-layer freezing, future work could certainly explore better metrics for choosing filters, especially given the stark difference in performance when using something as simple as mean squared error compared to Pearson correlation. Finally, the computational overhead of the method in combination with the limitations of Google Colab resulted in an inability to train on high-resolution images and use larger models. We believe that using high-resolution images would significantly benefit the feasibility of the method, as saliency maps are much more clearly defined. We again leave this to future work, as we are unable to explore this path.&lt;/p&gt; </content> </entry> <entry> <title>Graph Articulated Objects</title> <link href="https://deep-learning-mit.github.io/blog/2023/graph-articulated-objects/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/graph-articulated-objects</id> <content type="html">&lt;h2 id=&quot;project-background&quot;&gt;Project Background&lt;/h2&gt; &lt;p&gt;Recent advancements in generative AI have transformed robotic capabilities across all parts of the stack, whether in control, planning, or perception. As self-driving cars roll out to public roads and factory assembly-line robots become more and more generalizable, embodied intelligence is transforming the way that humans interact with each other and automate their daily tasks.&lt;/p&gt; &lt;p&gt;Across the robotic manipulation stack, we are most interested in exploring the problem of scene representation; using the limited sensors available, how might a robot build a representation of its environment that will allow it to perform a wide range of general tasks with ease? While developments in inverse graphics like NeRF have given robots access to increasingly rich geometric representations, recent work in language modeling has allowed robots to leverage more semantic scene understanding to plan for tasks.&lt;/p&gt; &lt;h3 id=&quot;introduction-to-task-planning&quot;&gt;Introduction to Task Planning&lt;/h3&gt; &lt;p&gt;In robotics, the term &lt;strong&gt;task planning&lt;/strong&gt; is used to describe the process of using scene understanding to break a &lt;em&gt;goal&lt;/em&gt; down into a sequence of individual &lt;em&gt;actions&lt;/em&gt;. This is in contrast with &lt;em&gt;motion planning&lt;/em&gt;, which describes the problem of breaking a desired &lt;em&gt;movement&lt;/em&gt; into individual configurations that satisfy some constraints (such as collision constraints). While simply using motion planning to specify a task is necessary for any generalized robotic system, &lt;em&gt;task planning&lt;/em&gt; provides robots with a &lt;em&gt;high-level&lt;/em&gt; abstraction that enables them to accomplish multi-step tasks.&lt;/p&gt; &lt;p&gt;Take the problem of brushing one’s teeth in the morning. As humans, we might describe the steps necessary as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Walk to the sink.&lt;/li&gt; &lt;li&gt;Grab the toothbrush and toothpaste tube.&lt;/li&gt; &lt;li&gt;Open the toothpaste tube.&lt;/li&gt; &lt;li&gt;Squeeze toothpaste onto brush.&lt;/li&gt; &lt;li&gt;Brush teeth.&lt;/li&gt; &lt;li&gt;Rinse mouth.&lt;/li&gt; &lt;li&gt;Clean toothbrush.&lt;/li&gt; &lt;li&gt;Put everything back.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;planning-domain-definition-language-pddl-explained&quot;&gt;Planning Domain Definition Language (PDDL) Explained&lt;/h3&gt; &lt;p&gt;Creating a task plan is a trivial task for humans. However, a computer must use a state-space search algorithm like &lt;em&gt;A* search&lt;/em&gt; to plan a sequence of interactions from a &lt;em&gt;start state&lt;/em&gt; to a desired &lt;em&gt;goal state&lt;/em&gt;. Doing so requires us to define a standard that formally specifies all relevant &lt;em&gt;environment states&lt;/em&gt;, along with the &lt;em&gt;preconditions&lt;/em&gt; and &lt;em&gt;effects&lt;/em&gt; of all possible transitions between two states.&lt;/p&gt; &lt;p&gt;The Planning Domain Definition Language (PDDL) was invented to solve this problem. Description languages like PDDL allow us to define the space of all possible environment states using the states of all entities that make up the environment. Environments are defined as a task-agnostic &lt;em&gt;domain file&lt;/em&gt;, while the &lt;em&gt;problem file&lt;/em&gt; defines a specific task by specifying a desired &lt;em&gt;start&lt;/em&gt; and &lt;em&gt;end&lt;/em&gt; state.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/pddl-explained-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/pddl-explained-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/pddl-explained-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/pddl-explained.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1: This figure above breaks down a simple PDDL domain file into its constituent components. A PDDL domain generally consists of predicates, which describe the state, and actions, which specify state transitions. &lt;/div&gt; &lt;p&gt;Despite task planning’s utility, however, there is one major drawback; this approach to planning requires the robot to have a &lt;em&gt;detailed PDDL domain file&lt;/em&gt; that accurately represents its environment. Generating this file from perception requires not only a semantic understanding of all objects in a space, but also of all possible interactions between these objects, as well as all interactions that the robot is afforded within the environment. Clearly, there is a major gap between the task-planning literature and the realities of upstream perception capabilities.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;The use of LLMs in robotic planning and reasoning has exploded in the past few years, due to the promise of leveraging a language model’s internal world understanding to provide more information for planning. One such work is LLM+P&lt;d-cite key=&quot;liu2023llm+&quot;&gt;&lt;/d-cite&gt;, which combines an LLM with a classical planner to solve a given problem specified in natural language, using PDDL as an intermediate representation. LLM+P works by converting the description into a PDDL problem representation, running a classical planning algorithm to find a solution, and then computing the sequence of actions back into a natural language description interpretable by humans. Importantly, LLM+P demonstrates that using an LLM to output a PDDL representation can be a viable strategy in solving planning problems that are specified to a robot. However, there are a few limitations. For one, LLM+P assumes that a relevant domain file is already provided to the robot, specifying all entities and their relationships within the environment’s context. While domain files are generally carefully crafted by hand, vision-language models can automate this process.&lt;/p&gt; &lt;p&gt;LLMs have also been used to solve plans directly, to varying levels of success. Works like SayCan&lt;d-cite key=&quot;ahn2022can&quot;&gt;&lt;/d-cite&gt; and LLM-Planner&lt;d-cite key=&quot;song2023llm&quot;&gt;&lt;/d-cite&gt; use the LLM as a planning engine directly, circumventing the need to use a traditional high-level planner completely. SayCan, in particular, uses a combination of language-grounded instructions and task affordances that indicate the robot’s ability to execute a given task, using language to determine the most viable skill to execute from a set of predefined skills. These bodies of work have greatly enabled the ability of robots to parse, understand, and execute instructions given to them by their operators as natural language. Particularly, an LLM’s ability to break a problem down into several constituent steps is critical to enabling long-horizon task planning with multiple steps.&lt;/p&gt; &lt;p&gt;Language is an increasingly promising modality for robots to operate in, due to the ubiquity of relevant language data to learn real-world entity relations from the internet. However, foundation models that integrate vision and robot-action modalities enable even stronger semantic reasoning. Google’s Robot Transformer 2 (RT-2)&lt;d-cite key=&quot;brohan2023rt&quot;&gt;&lt;/d-cite&gt;, for example, is a recent work that performs perception, planning, and control all in a single neural network, leveraging internet-scale data. One major drawback of visuomotor policies, such as that employed by RT-2, is that we lose interpretability of a robot’s internal representation.&lt;/p&gt; &lt;p&gt;Nonetheless, multi-modal foundation models have proven to be a useful tool across the spectrum of robotic planning. Our project takes inspiration from the above works in LLMs for planning and extends the idea to domain-generation, allowing task-planners to work in real-world scenarios.&lt;/p&gt; &lt;p&gt;The rapid advancement of LLMs and vision-language models open up a world of possibilities in closing this gap, as robotic perception systems may be able to leverage learned world understanding to generate PDDL files of their own to use in downstream planning tasks. This project aims to investigate the question: can VLMs be used to generate accurate PDDL domains?&lt;/p&gt; &lt;h2 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h2&gt; &lt;p&gt;To investigate this, we decided to explore this problem by testing the capabilities of VLMs on various tasks and levels of prior conditioning. This allows us to explore the problem on two axes: domain complexity and amount of information provided as a prior to the VLM. Each of these axes are chosen to progressively increase the complexity of the domain being explored, while also progressively increasing the amount of &lt;em&gt;information&lt;/em&gt; available. Designing our experiments like this allows us to understand the importance of &lt;em&gt;information&lt;/em&gt; and &lt;em&gt;domain complexity&lt;/em&gt; and how they affect the overall results.&lt;/p&gt; &lt;p&gt;Due to ease of access, we decided to use OpenAI ChatGPT’s &lt;em&gt;GPT4-Vision&lt;/em&gt; functionality to run our experiments. A more comprehensive ablation may analyze these experiments across a wider range of VLMs.&lt;/p&gt; &lt;h3 id=&quot;domains-of-interest&quot;&gt;Domains of Interest&lt;/h3&gt; &lt;p&gt;Within the context of task planning for generalizable robotics, the problem of cooking in a kitchen setting is a fascinating problem because of the combination of their usefulness and the high dimensionality and discretization of kitchen tasks. As a result, kitchen setups like cooking, cleaning, and cutting ingredients are great ways to understand task-planning, and are the domains that we chose to study in this work.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/image-examples-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/image-examples-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/image-examples-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/image-examples.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: A summary of domains that we are interested in studying. The three domains increase in complexity, which allows us to analyze the effect of complexity on the VLM&apos;s effectiveness. &lt;/div&gt; &lt;p&gt;The three domains used in our study are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cut&lt;/strong&gt;: Bagel + utensils used for cutting ingredients&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cook&lt;/strong&gt;: Everything in Cut + a pan, spatula, and a stove&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clean&lt;/strong&gt;: Everything in Clean + a soap bottle, a sink, and a sponge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our handcrafted “ground-truth” domain files are designed to support the target tasks of &lt;em&gt;cutting&lt;/em&gt; a bagel, &lt;em&gt;cooking&lt;/em&gt; a sliced bagel, and &lt;em&gt;cleaning&lt;/em&gt; utensils, respectively. Ideally a good PDDL file generated is one where these tasks are supported.&lt;/p&gt; &lt;h3 id=&quot;prompting-strategies&quot;&gt;Prompting Strategies.&lt;/h3&gt; &lt;p&gt;We also experimented with four different prompting strategies, with each strategy providing progressively more information to the VLM for its PDDL generation task. All prompts provided to the VLM consist of the target image, along with a text-based prompt meant to guide the VLM towards a more accurate PDDL representation.&lt;/p&gt; &lt;p&gt;The strategies are as follows, along with examples used by our experiment for the cut domain. Text that was added progressively to the prompt is &lt;strong&gt;&lt;em&gt;bolded&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Raw Generation: Image + generic prompt &lt;ul&gt; &lt;li&gt;&lt;em&gt;You are a robot that needs to execute task planning in the setup shown in the image. Given the image, please generate a Planning Description Domain Language (PDDL) domain file that describes the scene.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Prompt 1 + describe each object in the scene &lt;ul&gt; &lt;li&gt;&lt;em&gt;You are a robot that needs to execute task planning in the setup shown in the image. &lt;strong&gt;This image includes a bagel, a plate, a bowl, and a knife.&lt;/strong&gt; Given the image, please generate a Planning Description Domain Language (PDDL) domain file that describes the scene.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Prompt 2 + describe the target task &lt;ul&gt; &lt;li&gt;&lt;em&gt;You are a robot that needs to execute task planning &lt;strong&gt;to cut the bagel&lt;/strong&gt; in the setup shown in the image. This image includes a bagel, a plate, a bowl, and a knife. Given the image, please generate a Planning Description Domain Language (PDDL) domain file that describes the scene.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Prompt 3 + explain object relations in detail &lt;ul&gt; &lt;li&gt;&lt;em&gt;You are a robot that needs to execute task planning to cut the bagel in the setup shown in the image. This image includes a bagel, a plate, a bowl, and a knife. &lt;strong&gt;In order to cut the bagel, one must use the knife and place the bagel and knife on the plate beforehand. I can place the bagel on the plate or the bowl, and cut the bagel using the knife.&lt;/strong&gt; Given the image, please generate a Planning Description Domain Language (PDDL) domain file that describes the scene.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;evaluation-metric-embedding-cosine-similarity&quot;&gt;Evaluation Metric: Embedding Cosine Similarity&lt;/h3&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/evaluation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/evaluation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/evaluation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/evaluation.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: Our evaluation process. We used a vector-embedding-space cosine similarity metric to conduct our analysis, using a handcrafted PDDL file as input. &lt;/div&gt; &lt;p&gt;Since several different PDDL domains can be used to represent the same set of actions and predicates, the task of generating PDDL files is quite subjective. Since generating PDDL tasks is an often-tedious task that humans must do themselves to represent any given domain, we evaluate each VLM output based on its similarity to real PDDL domains handcrafted manually. After asking the VLM to generate a PDDL file, both the &lt;em&gt;target&lt;/em&gt; and the &lt;em&gt;generated&lt;/em&gt; domain descriptions are embedded using the &lt;strong&gt;Longformer: Long Document Transformer&lt;/strong&gt;&lt;d-cite key=&quot;beltagy2020longformer&quot;&gt;&lt;/d-cite&gt; model, before finally computing the cosine similarity between the two embeddings. We decided to use &lt;strong&gt;Longformer&lt;/strong&gt;, instead a more standard model like &lt;em&gt;LLAMA 2&lt;/em&gt;&lt;d-cite key=&quot;touvron2023llama&quot;&gt;&lt;/d-cite&gt; or &lt;em&gt;BERT&lt;/em&gt;&lt;d-cite key=&quot;tenney2019bert&quot;&gt;&lt;/d-cite&gt;, due to the long context-length afforded to us by the model; a more comprehensive study would require studying these effects on a wider range of embedding models to more accurately understand the text-to-text similarity between the two domains.&lt;/p&gt; &lt;p&gt;Note that this cosine similarity in the embedding space is quite a coarse metric to evaluate our outputs for a couple of reasons. The primary concern with this evaluation approach has to do with the transferability between PDDL files, which are specified in a LISP-like syntax, and natural language documents, which Longformer was trained to embed. In this study, we assume that such an embedding model &lt;em&gt;can&lt;/em&gt; be used to make such a comparison, and discuss our study accordingly.&lt;/p&gt; &lt;p&gt;Aside from this, PDDL’s structure also provides several keywords that are commonly used by all PDDL files, such as &lt;em&gt;action&lt;/em&gt;, &lt;em&gt;predicate&lt;/em&gt;, and &lt;em&gt;preconditions&lt;/em&gt;. In order to handle these, we decided to simply remove all instances of these words from both the &lt;em&gt;target&lt;/em&gt; and the &lt;em&gt;generated&lt;/em&gt; PDDL files, in order to mitigate the effect of the similarity between these tokens.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;After experimenting on a wide range of complex environments with various prompting strategies, it seems that VLMs perform quite well for the task of generating PDDLs from image and text conditioning. We measured the similarity of the ground truth PDDL file with each image and experiment’s generated PDDL file. To quantitatively measure similarity, we used the cosine similarity metric on the embeddings of the masked pieces of text using Longformer&lt;d-cite key=&quot;beltagy2020longformer&quot;&gt;&lt;/d-cite&gt;. We did not use BERT&lt;d-cite key=&quot;tenney2019bert&quot;&gt;&lt;/d-cite&gt; due to the context length not being long enough for our PDDL files. Recall that we masked out certain frequently appearing words in PDDL files so as to not inflate the similarity due to this commonality.&lt;/p&gt; &lt;p&gt;The exact generated PDDL files can be found at &lt;a href=&quot;https://github.com/anirudhv27/final-project-dl/tree/main/experiments.&quot;&gt;this link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First, we will qualitatively analyze the generated words in each of the three categories of the PDDL files: types, predicates, and actions. Then, we will also provide quantitative metrics that measure similarity directly with the ground truth PDDL files that we wrote.&lt;/p&gt; &lt;h3 id=&quot;types&quot;&gt;Types&lt;/h3&gt; &lt;p&gt;Types are the first part of PDDL files. They describe the various sorts of objects that appear in the image. For example, in the “cut” image, the generated types are “utensil, plate, food”. Note that the types often compress similar sorts of objects, e.g. both spatulas and knives fall under the type “utensil”. Type generation is somewhat inconsistent, since types are not strictly required by PDDL files to exist, which could contribute towards why certain generated PDDL files do not have a types section at all.&lt;/p&gt; &lt;h3 id=&quot;predicates&quot;&gt;Predicates&lt;/h3&gt; &lt;p&gt;Predicates in the PDDL files are descriptive phrases that describe distinct parts of the scene, at a given time. For example, in the “cut” image, experiment 4 has the following predicates “(plate-empty), (bowl-empty), (bagel-on-plate), (bagel-on-bowl), (knife-on-plate), (bagel-cut)”. Note that these are not precisely representative of the current state of the image, but rather represent what states could also appear in the future, e.g. “(bagel-cut)”, even though the bagel is not yet cut. The accuracy of the generated predicate set is surprisingly accurate, regardless of which experiment we use.&lt;/p&gt; &lt;p&gt;It seems that all four experiments generate approximately the same predicate set. For the “cut” image, all of the predicates generally have the objects “bagel”, “knife”, “plate”, etc., and sometimes where they are placed relative to each other. In the later “cook” and “clean” images, there are also predicates conditioning on whether the bowl/plate is clean or not. In particular, the generated predicates for Experiment 1 – where we do not tell the VLM the task – also make sense with respect to the inferred task! This evidence suggests that the generated predicates match the planned task, thus implying that the VLM is able to learn the task quite well just based on the image.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/predicate-example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/predicate-example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/predicate-example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/predicate-example.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4: The correspondence between the inputted image and the generated predicates. Note that the predicates are not descriptive of just the current state, but also are descriptive of the later potential states based on the inferred task. &lt;/div&gt; &lt;h3 id=&quot;actions&quot;&gt;Actions&lt;/h3&gt; &lt;p&gt;Similar to the predicate generation, the action generation is extremely accurate. The various sequences of predicted actions make sense for the given images and conditioning. For example, one of the generated action sequences from Experiment 1 is:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;(:action prepare-sandwich :parameters (?b - food ?p - container) :precondition (and (contains ?p ?b) (is-clean ?p)) :effect (and (inside ?b ?p) (not (empty ?p))) )&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is a very detailed sequence of actions, which also makes sense – in order to prepare a sandwich, the generated PDDL file notices we need the food and the container, and then checks if it is clean and not empty.&lt;/p&gt; &lt;p&gt;Again, the results from Experiment 1 compared to the later experiments which have more textual conditioning are extremely similar, indicating that most of the information the VLM collects is from the image. Our added conditioning does not seem to improve generation of the action sequences much more.&lt;/p&gt; &lt;h3 id=&quot;quantitative-analysis-with-cosine-similarity&quot;&gt;Quantitative Analysis with Cosine Similarity&lt;/h3&gt; &lt;p&gt;Along with qualitative analysis of each part of the PDDL file, we also performed a holistic analysis of the entire PDDL file that compares similarity with our handcrafted ground truth PDDL file. We measured the cosine similarity between the two PDDL files, for each experiment in each image. Due to the general format of PDDL files, certain words appear at the same places many times. Hence, we masked these words out, in order to not inflate the similarity in a superficial manner.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/cosine-similarities-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/cosine-similarities-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/cosine-similarities-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/cosine-similarities.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Table 1: Cosine similarities using the Longformer embedding model. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/bar-chart-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/bar-chart-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/bar-chart-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/bar-chart.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5: Bar chart detailing cosine similarity in all tested scenarios. While there are clear trends between prompting strategies, all cosine similarity scores tend to hover around 98%. &lt;/div&gt; &lt;p&gt;As we can see, our methods performed quite well, with masked cosine similarity consistently above 0.98. This makes sense qualitatively as well, since as discussed above, the VLM generated types, predicates, and actions that made sense.&lt;/p&gt; &lt;p&gt;One of the most noteworthy aspects of the above data is that according to this metric:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Experiments 1-3 all perform similarly, with some doing better than others in different images.&lt;/li&gt; &lt;li&gt;Experiment 4 consistently performs worse than Experiments 1-3.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is surprising, since we would expect that more conditioning implies better performance. In Experiment 4, we added certain conditioning of the form of textual relationship between objects in the image. This result leads us to the conclusion that adding this sort of conditioning is not helpful for PDDL file generation, and is in fact negatively correlated with performance. Previous analysis has implied that the VLM learns extremely well from the image alone, and this result suggests that in fact it is better to let the VLM learn only from the image, without adding too much of our own conditioning.&lt;/p&gt; &lt;h2 id=&quot;conclusion-limitations-and-future-work&quot;&gt;Conclusion: Limitations and Future Work&lt;/h2&gt; &lt;p&gt;Our work analyzes the potential of the recent advances in VLMs for the purposes of robotic task planning. By creating a systematic set of experiments over increasingly complex images, we were able to showcase the power of VLMs as a potentially very powerful tool for general task planning problems. The accurate generation of PDDL files based on only the images shows us that VLMs learn from images extremely well, without the need for extra textual conditioning. In fact, we noticed that providing too much conditioning actually can decrease performance, thus further suggesting that VLMs learn best from images. This result is promising for generalizing to the greater context of robotic task planning, since vision is one of the most prominent ways in which robots dynamically task plan when navigating real-world environments. Harnessing the power of VLMs could prove to be the future of robotic task planning.&lt;/p&gt; &lt;p&gt;There are a couple of limitations in our work, which have the potential for future exploration. In order to test the true utility of the generated domain files, we would need to also generate problem PDDL files, after which we could run the problem on the domain to test the robustness of the domain. The qualitative and quantitative metrics in our study heavily imply that our domain file is valid, by testing on ground truth PDDL files. However, a more comprehensive study could also concurrently generate problem files, which are tested on the generated domain file. Perhaps a method could be made which alternatively trains both the problem and domain files by iteratively testing the problem on the domain, similar to the idea of a Conditional Generative Adversarial Network (GAN)&lt;d-cite key=&quot;mirza2014conditional&quot;&gt;&lt;/d-cite&gt;. Another limitation is that we only tested on one VLM architecture. A more comprehensive ablation may analyze these experiments across a wider range of VLMs.&lt;/p&gt; </content> </entry> <entry> <title>Physics Loss</title> <link href="https://deep-learning-mit.github.io/blog/2023/physics-loss/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/physics-loss</id> <content type="html">&lt;h1 id=&quot;super-resolution-multi-objective-training-for-optimizing-a-single-objective&quot;&gt;Super Resolution: Multi-Objective Training for Optimizing a Single Objective&lt;/h1&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Super-resolution (SR) refers to image processing techniques which enhance the quality of low-resolution images [2]. Recently deep learning based SR has been applied to the field fluid dynamics to recreate chaotic turbulent flows from low-resolution experimental or numerical data [3]. For some loss function \(\mathcal{L}\), the goal is to find weights \(\theta^*\) such that&lt;/p&gt; \[\begin{aligned} \theta^* = \text{argmin}_\theta\; \mathcal{L}(\bold{u_H},f(\bold{u_L};\bold{\theta})) \end{aligned}\] &lt;p&gt;where \(\bf u_H\) is the reference high resolution data field and \(\bf u_L\) is the corresponding coarsened low resolution data input to the neural network \(f\) (see the figure below).&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/assets/img/2023-11-11-physics-loss/fig1.png&quot; alt=&quot;Super-resolution reconstruction of turbulent vorticity field using physics-based neural network. Adapted from [2].&quot; /&gt;&lt;/p&gt; &lt;p&gt;​ &lt;em&gt;Fig 1: Super-resolution reconstruction of turbulent vorticity field using physics-based neural network. Adapted from [2]. Disclaimer: we didn’t have time to train on nice images like these for the present investigation.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Doing so can aid our understanding of flow physics [3]. Many have already applied deep learning to this problem, applying a variety of methods. The performance of the resulting networks depends heavily on the loss function used to train the network. Looking to improve upon the standard \(L_2\) loss function, some have introduced physics-based loss function that incorporates physical laws that the real flow must obey. For example [2] use the following type of form:&lt;/p&gt; \[\begin{aligned} \mathcal{L} &amp;amp;= \beta_0||\bold{u_H}-f(\bold{u_L})||_2 + \beta_1 ||p_1(\bold{u_H})-p_1(f(\bold{u_L}))||_2 + \beta_2 ||p_2(\bold{u_H})-p_2(f(\bold{u_L}))||_2 + ... \end{aligned}\] &lt;p&gt;where \(p_i(\cdot)\) is a physical objective that we want to enforce during training (e.g. spatial and time derivatives \(\nabla \bf u_H\), \(\bf\dot{u}_H\) etc.) and the \(\beta_i\) are fixed weighting coefficients.&lt;/p&gt; &lt;p&gt;Typically, multi-objective super resolution approaches aim to overcome the weaknesses of the single objective $L_2$ reconstruction loss, a primary one being that the $L_2$ loss favors blurry reconstructions over sharper more ‘realistic’ ones. The general idea is that the additional objectives push the training away from un-realistic reconstructions.&lt;/p&gt; &lt;p&gt;However suppose the goal really is to minimize the $L_2$ reconstruction loss. Can multi-objective training reduce the loss on the original objective or do the new objectives just get in the way? In this investigation we apply adaptively-weighted multi-objective optimization methods to the problem of turbulence super resolution which is a novel approach.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;h3 id=&quot;the-dataset&quot;&gt;The Dataset&lt;/h3&gt; &lt;p&gt;Super resolution reconstruction is an interesting problem for turbulent flows due there inherent multi-scale nature. Information is lost in the coarsening/pooling process making perfect reconstruction impossible without additional insights. Unfortunately, due to time and resource constraints it is unfeasible to train on 2D turbulence slices as in figure 1. In order to retain a challenging problem for the the super-resolution we build an artificial dataset of 1D turbulence as follows:&lt;/p&gt; \[u(x) = \sum_{k=1}^{10} k^{-1}\sin\left(kx+\phi(k)\right) + (2k)^{-1}\sin\left( 2kx +\phi(k)\right)\] &lt;p&gt;The amplitude scaling $k^{-1}$ models how the frequencies in a particular turbulent signal might decay with increasing wavenumber (velocity, temperature, pressure, kinetic energy, etc.). In other words the contribution of higher modes to the entire signal becomes less and less important in a predictable way. We generate each individual signal by fixing a phase function $\phi(k)$. For each $k$, $\phi(k)$ is taken to be the realization of uniform random variable in the range $[0,2\pi)$. This function $u(x)$ bakes in inherent correlations between the low and high frequency waveforms (Please note: this is not physical. We are just making a useful toy dataset for this investigation). Even with extremely coarse low-resolution inputs, we expect that a well-trained neural network can use these correlations to reconstruct the high frequency waveforms.&lt;/p&gt; &lt;p&gt;For input to the network, the samples are discretized to a $512$ point high resolution grid: $(\mathbf{u_H})_j = u(x_j)=u(j\cdot\frac{2\pi}{512})$. The low resolution data is average pooled with a kernel size of $32$. This results in a low resolution grid of size $512/32 = 16$. Average pooling has been shown to have nice training properties for super resolution reconstruction [2]. The following is typical high/low resolution pair:&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;assets/img/2023-11-11-physics-loss/fig2.png&quot; alt=&quot;Typical Input&quot; /&gt;&lt;/p&gt; &lt;p&gt;​ &lt;em&gt;Fig 2: Typical high/low resolution data pair. The high resolution version exists on a 512 point grid. The low resolution version has been average pooled down to a 16 point grid using a average pooling kernel of size 32. The pooling procedure removes the highest frequency components of the data meaning that full reconstruction requires deeper understanding of the underlying structure.&lt;/em&gt;&lt;/p&gt; &lt;h3 id=&quot;the-network&quot;&gt;The Network&lt;/h3&gt; &lt;p&gt;The network is a three layer fully connected network with hidden sizes $[1024,1024,1024]$.&lt;/p&gt; &lt;h3 id=&quot;training-scheme&quot;&gt;Training Scheme&lt;/h3&gt; &lt;p&gt;The multi-objective loss function&lt;/p&gt; \[\begin{aligned} \mathcal{L} &amp;amp;= \mathcal{L}_0 + \mathcal{L}_1 + \mathcal{L}_2+... \\&amp;amp;= \beta_0||\bold{u_H}-f(\bold{u_L})||_2 + \beta_1 ||p_1(\bold{u_H})-p_1(f(\bold{u_L}))||_2 + \beta_2 ||p_2(\bold{u_H})-p_2(f(\bold{u_L}))||_2 + ... \end{aligned}\] &lt;p&gt;presents a unique training challenge. Many turbulence super-resolution studies to date set the weights $\beta_i$ by trial and error in an attempt to produce ‘nice’ results [3]. This approach is sub-optimal because the best values of $\beta_i$ are dependent on the units and orders of magnitude of the properties $p_i$. Also, the best choice for the weights may change depending on the stage of training. For example it may be best to put more emphasis on the reconstruction loss $\mathcal{L}_0$ during the first stages of training and then shift emphasis to other properties to refine the model during the latter stages. In addition to these considerations [5] observed that for physics informed neural networks fixed weights tended to induce training instability as the multiple objectives compete with one another.&lt;/p&gt; &lt;p&gt;To mitigate these issues in this investigation we employ a multi-objective optimizer (MOO). After each training epoch a MOO reviews the progress for each loss component $\mathcal{L}_i$ and updates the weights $\beta_i$. A schematic is shown below:&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;assets/img/2023-11-11-physics-loss/fig3.png&quot; alt=&quot;Schematic of one training epoch &quot; /&gt;&lt;/p&gt; &lt;p&gt;​ &lt;em&gt;Fig3: One epoch of training with adaptive loss using ReLoBRaLo MOO. At the end of batched training iterations the MOO updates ${\beta_i}$ according to the progress of each individual loss component. The Adam training optimizer learning rate is fixed at $10^{-5}$ for the entire investigation.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In particular we use the Relative Loss Balancing with Random Lookback (ReLoBRaLo) scheme from [5] for the MOO. The scheme adaptively updates the loss weights at the end of each epoch according to the progress of each individual loss component:&lt;/p&gt; \[\begin{align*} \beta_i^{bal}(t) &amp;amp;= m\cdot \frac {\exp\left(\frac{\mathcal{L}_i(t)}{\mathcal{T}\mathcal{L}_i(t-1)}\right)} {\sum_{j=1}^m \exp\left(\frac{\mathcal{L}_j(t)}{\mathcal{T}\mathcal{L}_j(t-1)}\right)},\;i\in\{1,...,m\}\\ \beta_i(t) &amp;amp;= \alpha\beta_i(t-1) + (1-\alpha)\beta_i^{bal}(t) \end{align*}\] &lt;p&gt;There are many more details in [5], but essentially the $\beta_i^{bal}(t)$ term measures the progress of the loss $\mathcal{L}_i$ since the previous epoch relative to the progress made by other losses. The more a particular loss is struggling the more we increment its weight for the next epoch. The $\alpha$ hyper-parameter indicates bias towards the existing weight values. When $\alpha=1$ no updates are made. The temperature hyper-parameter $\mathcal{T}$ indicates the the level of equality across loss components. As $\mathcal{T} \to 0$ only the most struggling loss component receives a weight update. When $\mathcal{T}\to \infty$ all components receive an equal weight update. Note that we initialize by $\beta_0(0)=1$ and $\beta_i(0)=0$ for $i&amp;gt;0$.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;two-objective-loss&quot;&gt;Two Objective Loss&lt;/h3&gt; &lt;p&gt;We tried training on a variety of two-objective loss functions of the form&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;$\mathcal{L} = \beta_0&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;\bold{u_H}-f(\bold{u_L})&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;_2 + \beta_1&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;p_1(\bold{u_H})-p_1(f(\bold{u_L}))&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;_2$&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;where the $p_1$ objective was taken to be Fourier transform $\mathcal{F}$, spatial derivative $\frac{d}{dx}$, standard deviation $\sigma(\cdot)$, mean $\mathbb{E}_x(\cdot)$, absolute value$&lt;/td&gt; &lt;td&gt;\cdot&lt;/td&gt; &lt;td&gt;$, or functional compositions of the aforementioned. Compared to training on the standard single objective reconstruction loss $\mathcal{L}= \mathcal{L}_0 = \beta_0&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;\bold{u_H}-f(\bold{u_L})&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;_2$ , only the two-objective loss with Fourier transform loss gave significant improvements in training performance. Training with $\mathbb{E}_x$ gave marginal improvements. All other properties gave fairly neutral or negative results. Composing the Fourier transform with other properties was detrimental. The following table summarizes the training ($\alpha =0.9,\; \mathcal{T}=1$):&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;​ &lt;em&gt;Table 1: Training performance for two-objective loss functions. All runs were performed with $\alpha =0.9,\; \mathcal{T}=1$&lt;/em&gt;. The rightmost column show the percent improvement from the single objective training. The poor performance of $\mathcal{F}\circ\frac{d}{dx}$ might be due to high frequency noise being amplified by the derivative operator before being passed through the Fourier transform.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;$\boldsymbol{p_1}$&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;$\boldsymbol{\mathcal{L_0}(\text{epoch = }200)}$&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;% Improvement over Single Objective&lt;/th&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt; &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;None (single objective)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.01895&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0 %&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;$\mathcal{F}$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.01366&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;29 %&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;$\frac{d}{dx}$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.01993&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;5.3 %&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;$\sigma(\cdot)$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.02437&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;-29 %&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;$\mathbb{E}_x$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.01771&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;6.7 %&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;\cdot&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;$&lt;/td&gt; &lt;td&gt;0.01745&lt;/td&gt; &lt;td&gt;8.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;$\mathcal{F}\circ\frac{d}{dx}$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.17174&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;-830%&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Figures 4 provides a more detailed look at the training for $p_1=\mathcal{F}$. There is considerable variation in the rate of learning due to altering the $\alpha$ hyper-parameter. The bottom panel of figure 4 gives an example of a reconstructed signal. With enough training the network is able to learn the inherent structure in the data and reconstruct the high frequencies.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;assets/img/2023-11-11-physics-loss/fig4.png&quot; alt=&quot;Fourier loss two objective training&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;assets/img/2023-11-11-physics-loss/fig4b.png&quot; alt=&quot;Reconstructed data by two-objective training&quot; /&gt;&lt;/p&gt; &lt;p&gt;​ &lt;em&gt;Fig 4: Top panel: Two objective training with Fourier loss for $\mathcal{T}=1$. The results for setting $\mathcal{T}=0.01,100$ are very similar so they are omitted for brevity. The two objective training (reconstruction + Fourier) outperforms the single objective training for every value of $\alpha$. The optimal value of $\alpha$ is close to $0.999$.&lt;/em&gt; Bottom panel: example of reconstructed validation data. The model is able to recover the high frequency components from the original high resolution signal.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;assets/img/2023-11-11-physics-loss/fig5a.png&quot; alt=&quot;beta evolution&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;assets/img/2023-11-11-physics-loss/fig5b.png&quot; alt=&quot;fig5b&quot; /&gt;&lt;/p&gt; &lt;p&gt;​ &lt;em&gt;Fig 5: Reconstruction and Fourier objective ${\beta_i}$ evolution for $\alpha=0.9,0.999$. The smaller $\alpha$ the faster the loss weights converge to 1.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The two objective training curves in figure 4 are significantly better than the single objective curve. There is a particular value of $\alpha$ (~0.999) that gives the best overall result. Figure 5 demonstrates how the loss weights adapt over the course of training as the ReLoBRaLo MOO tries to balance the improvements in each loss component. For $\alpha=0.9$ the MOO rapidly increases $\beta_1$ in order to put more weight on the lagging Fourier loss. When $\alpha=0.999$ the increase is a lot more gradual. In the limit as $\alpha\to1$ we just have single objective optimization.&lt;/p&gt; &lt;p&gt;Figure 6 shows a similar weight evolution when the second objective is ‘bad’, $p_1=\sigma(\cdot)$:&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;assets/img/2023-11-11-physics-loss/fig6.png&quot; alt=&quot;beta evolution for standard deviation&quot; /&gt;&lt;/p&gt; &lt;p&gt;​ &lt;em&gt;Fig 6: Reconstruction and $\sigma(\cdot)$ objective ${\beta_i}$ evolutions. There is evidence of instability at the start of training.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In contrast to the reconstruction and Fourier two-objective training, the reconstruction and $\sigma(\cdot)$ weight evolutions show signs of instability. At around $15$ epochs $\beta_0$ experiences a bump. This is mostly likely the MOO responding to degrading progress on the reconstruction objective due to the two objectives competing with each other. Therefore for optimal multi-objective training it seems preferable that all loss components smoothly decrease without cross interreference.&lt;/p&gt; &lt;h3 id=&quot;multi-objective-loss&quot;&gt;Multi Objective Loss&lt;/h3&gt; &lt;p&gt;We also study a multi-objective loss created by combining the most successful objectives from the previous study.&lt;/p&gt; \[\begin{aligned} p_1&amp;amp;=\mathcal{F}\\ p_2&amp;amp;=|\cdot|\\ p_3&amp;amp;=\mathbb{E}_x\\ p_4&amp;amp;=\frac{d}{dx}\\ \end{aligned}\] &lt;p&gt;The results closely mimic the two objective Fourier loss so we omit further details. Interestingly, even when we introduce a ‘bad’ objective such as $\sigma(\cdot)$ or $\mathcal{F}\circ\frac{d}{dx}$into the multi-objective loss it doesn’t appear to spoil the result despite causing a minor instability (see figure 6). These results suggest that it may be possible to just ‘throw in’ many auxiliary objectives in the hopes that one of them improves training. We might not necessarily need to worry about bad objectives spoiling the bunch. Or it could just be that in this particular case that the Fourier objective $\mathcal{F}$ is strong enough to overcome the bad objectives. This needs more investigation.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This investigation showed that multi-objective loss functions can be useful even when only one objective is ultimately of interest. Most likely due to the manner in which the data set was constructed, the Fourier objective turned out to be a great training aid (Note that we did try single objective training with the Fourier objective replacing the reconstruction objective. This did not yield as good results suggesting that there is something inherently beneficial about multi-objective training as opposed to just changing basis).&lt;/p&gt; &lt;p&gt;The other objectives did not do nearly as well and some even degraded the training by causing instabilities. The ReLoBRaLo MOO was a critical component of training. None of the aforementioned results would have been possible with fixed weights. It was critical to fine tune the $\alpha$ parameter which determines how aggressively the MOO does updates. Presumably, an overly aggressive MOO doesn’t give the network time to settle in the early stages of training but an overly passive MOO hardly makes any difference at all.&lt;/p&gt; &lt;p&gt;While good, ultimately the ReLoBRaLo scheme was designed for traditional MOO problems (such as solving partial differential equations) and is most likely far from optimal under the unique settings of this investigation. In addition, the objectives in this study were chosen quite arbitrarily. The Fourier objective was an easy one to discover due to the low-pass nature of super-resolution reconstruction and the manufactured dataset. For a more general problem where we might want to introduce auxiliary objectives it will be very difficult a-priori to identify high performance auxiliary objectives. An interesting future investigation could be to design a neural network that adaptively updates the auxiliary objectives after each epoch with the goal accelerating the main network’s learning curve.&lt;/p&gt; &lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt; &lt;p&gt;[1] Bode, M., Gauding, M., Lian, Z., Denker, D., Davidovic, M., Kleinheinz, K., Jitsev, J. and Pitsch, H. Using physics-informed enhanced super-resolution generative adversarial networks for subfilter modeling in turbulent reactive flows. &lt;em&gt;Proceedings of the Combustion Institute&lt;/em&gt;, 2021.&lt;/p&gt; &lt;p&gt;[2] Fukami, K., Fukagata, K. and Taira, K. Super-resolution reconstruction of turbulent flows with machine learning. &lt;em&gt;Journal of Fluid Mechanics&lt;/em&gt;, 2019.&lt;/p&gt; &lt;p&gt;[3] Fukami, K.,Fukagata, K., and Taira, K. Super-Resolution Analysis Via Machine Learning: A Survey For Fluid Flows. [Unpublished manuscript], 2023.&lt;/p&gt; &lt;p&gt;[4] Wang, C., Li, S., He, D. and Wang, L. Is L2 Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?. &lt;em&gt;Conference on Neural Information Processing Systems&lt;/em&gt;, 2022.&lt;/p&gt; &lt;p&gt;[5] Bischof, R., and Kraus, M. Multi-Objective Loss Balancing for Physics-Informed DeepLearning. [Unpublished manuscript], 2022.&lt;/p&gt; </content> </entry> <entry> <title>Diffusion Models on Low-Brightness Images</title> <link href="https://deep-learning-mit.github.io/blog/2023/Tracking-Multiple-Objects/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Tracking-Multiple-Objects</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Since the introduction of ChatGPT, everyone seems to be speaking about “generative AI,” with almost 15x more google searches for generative AI now than at this time last year. This blog post focuses a specific use case for diffusion models, which have applications across the board, from generating images given keywords to planning trajectories for robot manipulation. In short, diffusion models are a family of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/diffmodeloverview.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:90%&quot; /&gt; &lt;em&gt;Figure 1.1. How a diffusion model iteratively transforms noise to generate an image&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Diffusion models have been used with great success for a number of use cases, but they still remain largely unused on dim images. The primary related work has been on using a diffusion model for low-light image enhancement. However, most of these works agree that attempting to generate an image from noise generated on top of an already dim image often results in rgb shift and global degradation of the image &lt;d-cite key=&quot;zhou2023pyramid&quot;&gt;&lt;/d-cite&gt; This is because a diffusion model adds noise to the given image and then attempts to denoise the image, so given a dim and low-contrast image, the model has a difficult time denoising.&lt;/p&gt; &lt;p&gt;For a visual example of why low-light scenarios can be a problem for diffusion models, we can just look at the control of our experiments. The left image is from the diffusion model trained and evaluated on low-light images, while the right image is from the diffusion model trained and evaluated on normal-light images.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/controldim.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:45%; margin-right:3%&quot; /&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/controlbright.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:45%&quot; /&gt;&lt;/p&gt; &lt;p&gt;We can observe all sorts of problems here, from the model being unable to determine the image background color to the model sometimes not even showing the butterfly. In contrast, the exact same training done on the normal butterfly dataset shows distortions occasionally, but has no issues determining the background color or the contrast between the butterfly and the background. This illustrates the issue talked about previously of rgb shift and global degradation. In this blog, we aim to conduct experiments by adding different features to the DDPM scheduler and investigate which can actually make a difference for low-light scenarios.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;First, we discuss the idea of a diffusion model in more depth. In a nutshell, a diffusion model relies on semi-supervised training. The model is given an image from a training set to which random noise has been applied \(t\) times. This noisy image is given to the model along with the value of \(t\), a loss is computed between the output of the model and the noised image. The random noise is applied with a noise scheduler, which takes a batch of images from the training set, a batch of random noise, and the timesteps for each image. The overall training objective of the model is to be able to predict the noise added through the scheduler to retrieve the initial image.&lt;/p&gt; &lt;p&gt;Since diffusion models on dim images are relatively unstudied, this blog post focuses on taking a well-known diffusion model for regular images and making modifications to the scheduler, which controls the noising and denoising process, and the model architecture to improve its performance in low-light scenarios. We begin with the DDPM (Denoising Diffusion Probabilistic Models) model &lt;d-cite key=&quot;ho2020denoising&quot;&gt;&lt;/d-cite&gt;. There are a number of viable diffusion models to study, but one of the major benefits of DDPM is that it balances image fidelity in generation and speed. Several other major models such as DDIM and PNDM can be much worse at capturing fine details even though they have some speed and computation advantage &lt;d-cite key=&quot;andrew2023diffusion&quot;&gt;&lt;/d-cite&gt;. This offers some intuition as to why DDPM was chosen to work with for this project.&lt;/p&gt; &lt;p&gt;A DDPM uses two Markov chains for its denoising and noising process: one to perturb the data to noise, and another one to convert the noise back into data&lt;d-cite key=&quot;10.1145/3626235&quot;&gt;&lt;/d-cite&gt; The noising and denoising process uses a unique scheduler that we refer to as the DDPMScheduler. Mathematically, we can refer to the initial distribution as \(q(x_0)\) and show that following \(x_t\) are computed as \(q(x_t \vert x_{t-1}) = \mathbb{N}(x_t; \sqrt{1-\beta_t}x_{t-1},\beta_tI).\) This is referred to as a Gaussian transition kernel, and shows that consecutive noised images are generated by taking the previous image and sampling more noise into it using a Gaussian perturbation. Of particular note are the \(\beta_t\) terms, since those vary from timestep to timestep and we discuss later how those should be modified. In the specific ddpm scheduler implementation, the \(\beta_{\text{start}}\) and \(\beta_{\text{end}}\) are specified, so the \(\beta\) in the Gaussian perturbation, which specifies how much noise should be added at any time, is gradually increased. &lt;d-cite key=&quot;von-platen-etal-2022-diffusers&quot;&gt;&amp;lt;/dcite&amp;gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;The specified beta values can have many consequences on the model overall, but one is more aggressive denoising which can combat rgb shift. This is because rgb shift can cause color inconsistencies between adjacent pixels, which can be combated by greater noise reduction. In addition, aggressive denoising may be able to recover the underlying structure of the image and smooth out artifacts introduced by rgb shift. However, aggressive denoising can result in a loss of detail as well&lt;/p&gt; &lt;p&gt;By integrating the previous noise during the noising step to determine \(q(x_T)\) we can get \(q(x_T) = \int q(x_T \vert x_0)q(x_0)dx_0 \sim \mathbb{N}(x_t; 0, I)\), showing that after all the noise is integrated, the entire structure of the image is lost. After the denoising, DDPMs start generating new samples by generating a noise vector from the prior distribution \(p(x_T = \mathbb{N}(x_T; 0, I)),\) and gradually removing noise by running a Markov chain in the reverse. The goal is to learn the transition kernel between timesteps. The reverse transition can be written as \(p_{\theta}(x_{t-1} \vert x_t) = \mathbb{N}(x_{t-1}; \mu_{\theta}(x_t, t), \sigma_{\theta}(x_t, t))\) where \(\theta\) is the model’s parameters and the mean and variance are parametrized by neural networks&lt;d-cite key=&quot;andrew2023diffusion&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;This variance will also come into play later, as it is one of the parameters that we toggle in the DDPM scheduler. Variance in the DDPM Scheduler of the Diffuser library has several possible values: fixed_small, fixed_small_log, fixed_large, fixed_large_log &lt;d-cite key=&quot;schedulerdocumentation&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;variance_type&lt;/th&gt; &lt;th&gt;effect&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;“fixed_small”&lt;/td&gt; &lt;td&gt;The variance is a small and fixed value&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;“fixed_small_log”&lt;/td&gt; &lt;td&gt;The variance is small and fixed in the log space&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;“fixed_large”&lt;/td&gt; &lt;td&gt;The variance is a large and fixed value&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;“fixed_large_log”&lt;/td&gt; &lt;td&gt;The variance is large and fixed in the log space&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;p&gt;The first method evaluated as a control is simply an implementation of a DDPM using the Diffusers library &lt;d-cite key=&quot;von-platen-etal-2022-diffusers&quot;&gt;&lt;/d-cite&gt; on HuggingFace. This model was trained using the Smithsonian butterflies dataset of 1000 images hosted on HuggingFace &lt;d-cite key=&quot;huggan_dataset&quot;&gt;&lt;/d-cite&gt;. Initially, since the images in the dataset are all different, the dataset was resized to have all images conform to a square resolution, randomly flipped to augment the dataset, and normalized into the [-1,1] range. To generate “dim” images, the regular dataset was reduced in brightness by 50% using the Transforms library from HuggingFace.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/orig.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:90%&quot; /&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;em&gt;Figure 3.1. Original images from dataset&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/new.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:90%&quot; /&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;em&gt;Figure 3.2. Images after preprocessing&lt;/em&gt; &lt;/p&gt; &lt;p&gt;Next, noise is added to the images. For this, we use the DDPMScheduler with the default parameters from Diffusers. The model is then trained on the noisy image, and evaluated. For evaluation, the model is tested on sixteen different images previously sampled randomly from the training dataset and set aside as test images. These images are noised using the scheduler in the same way as the rest of the images, and the model is run on the noised images to retrieve the original images.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Control Parameters&lt;/th&gt; &lt;th&gt; &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;noise_timesteps&lt;/td&gt; &lt;td&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;num_epochs&lt;/td&gt; &lt;td&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;beta_start&lt;/td&gt; &lt;td&gt;0.0001&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;beta_max&lt;/td&gt; &lt;td&gt;0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;variance_type&lt;/td&gt; &lt;td&gt;“fixed_large”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;resnet layers per unet block&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;em&gt;Figure 4.1. Showing default parameters used in the diffusion model &lt;/em&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/dlmodelarch.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:90%&quot; /&gt; &lt;em&gt;Figure 4.2. Figure depicting the UNet architecture used in the model&lt;/em&gt; &lt;/p&gt; &lt;p&gt;Initially, a quantitative method of evaluation was considered, and some losses were computed between the test images before noising and the corresponding test results after denoising. While these measurements were computed, they didn’t seem as valuable as simply looking at the image because of the various patterns between images that a loss function cannot always capture (ie how similar is the butterfly and the pattern of the butterfly to the initial image). As an example, the image on the left receives a lower mean squared error loss than the image on the right, yet looking at them, it is apparent that the denoised version on the right is better. Thus, the evaluation here mostly presents the model outputs for us to qualitatively compare across different variations.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/mseexample.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:=30%&quot; /&gt; &lt;em&gt;Figure 4.3. Showing two outputs of different models given the same input. MSE Loss proved to be unreliable for this task as the loss of the left image compared to the control was less than the loss of the right image due to rgb shift&lt;/em&gt; &lt;/p&gt; &lt;p&gt;After the control, this process is repeated for a variety of parameters carefully chosen and model architecture modifications to evaluate the best variation for use in this low-light scenario.&lt;/p&gt; &lt;h2 id=&quot;resultsdiscussion&quot;&gt;Results/Discussion&lt;/h2&gt; &lt;p&gt;The results of the control are as seen in the introduction above. The result of the dim images is on the left, while the result of the brighter images is on the right.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/controldim.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:45%; margin-right:3%&quot; /&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/controlbright.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:45%&quot; /&gt; &lt;em&gt;Figure 5.1. The left shows the output of the control model trained on the dim images and the right shows it trained on the bright images&lt;/em&gt;&lt;/p&gt; &lt;p&gt;One of the most pressing problems seen on the dimmer images is the rgb shift. As discussed in the background, the variance, which partly controls how aggressively the model is denoised, can help with rgb shift because it larger denoising can retrieve details lost in noise. Thus, the first modification is changing the variance type from “fixed_small” to “fixed_large.” This modification, after training, resulted in the evaluation images below.&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/var_result.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:65%&quot; /&gt; &lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;em&gt;Figure 5.2. Result of evaluation after changing variance&lt;/em&gt; &lt;/p&gt; &lt;p&gt;As we can see, this helped greatly with the rgb shift issue, and eliminated the background discoloration for several of the images. Certain images, such as the second row on the left-most column and the third from the left on the bottom row also show huge detail improvements. For the reasons discussed earlier, this is expected as a result of larger denoising, since it can clear away ome artifacts. The only image that showed a decrease in quality after the variance change was the right-most image in the top row.&lt;/p&gt; &lt;p&gt;Now that some of the rgb shift has been resolved, we move to tackling the loss of detail in many of these evaluation images. One classic approach to loss of information is simply increasing the capacity of the model to learn. In more technical terms, by increasing the number of ResNet layers per UNet block, we may allow the model to capture more intricate features and details. Deeper layers can learn hierarchical representations, potentially improving the ability to encapsulate fine-grained information. To do this, we edit our model architecture to make each UNet block deeper.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/best_result.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:45%; margin-right:3%&quot; /&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/controlbright.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:45%&quot; /&gt; &lt;em&gt;Figure 5.3. The left image shows the output of the new change in model architecture on the dimmed dataset, while the right image shows the bright dataset control output for color comparison&lt;/em&gt;&lt;/p&gt; &lt;p&gt;A huge improvement can be seen just by deepening the model architecture and at least the outline of every butterfly is now visible. However, this still hasn’t solved the problem of rgb shift. As we can see, the butterflies in the denoised dim images are all skewed yellow, while the butterflies in the denoised control bright images are all of varying colors. Next, we try to train with various betas in the scheduler to tackle this issue. As discussed before, higher beta values can help with rgb shift. However, higher values can also lead to loss of detail. The beta_start for the control was 0.0001 and the beta_max was 0.02. Thus, we try two combinations of start and max: 0.001 and 0.01, and 0.0005 and 0.015.&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;/staging/assets/img/2023-11-11-Tracking-Multiple-Objects.md/beta_output.png&quot; alt=&quot;Alt Text&quot; style=&quot;width:90%&quot; /&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;em&gt;Figure 5.4. The left figure shows the output for beta start = 0.001 and beta end = 0.01, and the right figure shows the output for beta start = 0.0005 and beta end = 0.15&lt;/em&gt; &lt;/p&gt; &lt;p&gt;As seen above, this modification was unsuccessful, and the images have much less detail than before and the rgb shift is worse than before. This may be because the biggest issue is the distortion of colors and blurring, and thus, a high beta value and larger denoising is needed to fix these issues rather than smaller denoising as was previously hypothesized. This future modification is not analyzed in this project, but would be interesting to see in the future.&lt;/p&gt; &lt;h2 id=&quot;future-directions&quot;&gt;Future Directions&lt;/h2&gt; &lt;p&gt;There are several limitations and future directions worth discussing. For one, this project investigates a specific model, the DDPM model. The DDPM model was chosen for various reasons, but mostly because it draws a balance between detail and also efficiency. In the future, multiple models could be considered to figure out which is really best for image generation under low-light scenarios. In addition, this work only focuses on one dataset of butterflies, and generates “low-light” data by reducing the brightness of the original dataset. This is good evidence for the success of the methods presented, but additional datasets and real data taken from environments with low-light would have lended more evidence to the success of the methods. In addition, the amount of data and depth of the models used had to be limited used to gpu usage limits. A model trained for more epochs with data may work better than this one. In addition, a good future starting point for this work would be to work with the beta start and beta max to figure out how to improve the rgb shift, which I believe would help with the detail in the dim images.&lt;/p&gt; </content> </entry> <entry> <title>Semi-Supervised Domain Adaptation using Diffusion Models</title> <link href="https://deep-learning-mit.github.io/blog/2023/Exploring-Task-Specific-Data-Augmentation/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Exploring-Task-Specific-Data-Augmentation</id> <content type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt; &lt;p&gt;Recently, there has been a breakthrough in image manipulation using Contrastive Language-Image Pretraining (CLIP). Recent work shows that GANs combined with CLIP can translate the images to unseen domains &lt;d-cite key=&quot;gal2021stylegannada&quot;&gt;&lt;/d-cite&gt;. However, in many cases these manipulations destroy the important information that user might want to learn (e.g., labels). Recently, there was a paper showing image manipulation leveraging a combination of diffusion models and CLIP &lt;d-cite key=&quot;kim2022diffusionclip&quot;&gt;&lt;/d-cite&gt;. We leverage the method proposed in DiffusionCLIP paper to perform semi-supervised domain adaptation having limited labeled data. GitHub project page: https://github.com/babiakua/semi_supervised_domain_adapt.&lt;/p&gt; &lt;h1 id=&quot;introduction--background&quot;&gt;Introduction &amp;amp; Background&lt;/h1&gt; &lt;h2 id=&quot;diffusion-models&quot;&gt;Diffusion models&lt;/h2&gt; &lt;p&gt;Diffusion Denoising Probabilistic Models (DDPMs) were introduced by Ho et al. as a novel approach in the field of generative models &lt;d-cite key=&quot;ho2020denoising&quot;&gt;&lt;/d-cite&gt;. These models are built on the idea of gradually adding noise to data and then learning to reverse this process.&lt;/p&gt; &lt;p&gt;The diffusion process is a Markov chain that adds Gaussian noise to the data over a series of steps. This process can be described mathematically as:&lt;/p&gt; \[x_{t} = \sqrt{\alpha_{t}} x_{0} + \sqrt{1 - \alpha_{t}} \epsilon\] &lt;p&gt;where $x_{t}$ is the data at step $t$, $x_{0}$ is the original data, $\alpha_{t}$ is a variance schedule, and $\epsilon$ is Gaussian noise.&lt;/p&gt; &lt;p&gt;The reverse process aims to denoise the data, starting from the noisy version and progressively removing noise. It’s modeled as:&lt;/p&gt; \[x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}}\left(x_{t} - \frac{1-\alpha_{t}}{\sqrt{1-\alpha_{t}}} \epsilon_{\theta}(x_{t}, t)\right)\] &lt;p&gt;with $\epsilon_{\theta}(x_{t}, t)$ being a neural network predicting the noise. This neural network usually has a UNet architecture with downsampling layers, upsampling layers, and a bottleneck.&lt;/p&gt; &lt;p&gt;The training objective is to minimize the difference between the predicted noise $\epsilon_{\theta}(x_{t}, t)$ and the actual noise $\epsilon$. This is done using a variant of the mean squared error (MSE) loss:&lt;/p&gt; \[\min_\theta \mathbb{E}_{x_0 \sim q(x_0), w \sim \mathcal{N}(0, I), t} \left\| w - \epsilon_{\theta}(x_t, t) \right\|^2_2.\] &lt;p&gt;DDIM (Denoising Diffusion Implicit Models) paper &lt;d-cite key=&quot;song2022denoising&quot;&gt;&lt;/d-cite&gt; proposed an alternative non-Markovian noising process that has the same forward marginals as DDPM but has a distinct sampling process as follows:&lt;/p&gt; \[x_{t-1} = \sqrt{\alpha_{t-1}} f_\theta(x_t, t) + \sqrt{1 - \alpha_{t-1} - \sigma_t^2}\epsilon_\theta(x_t, t) + \sigma_t^2 z,\] &lt;p&gt;where, $z \sim \mathcal{N}(0, I)$ and $f_\theta(x_t, t)$ is a the prediction of $x_0$ at $t$ given $x_t$ and $\epsilon_\theta(x_t, t)$:&lt;/p&gt; \[f_\theta(x_t, t) := \frac{x_t - \sqrt{1 - \alpha_t}\epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}}.\] &lt;p&gt;DDIM process allows for the use of different samplers by setting $\sigma_t$ to different values. In particular, setting $\sigma_t = 1$ makes the process a Markov process equivalent to DDPM while setting $\sigma_t = 0$ makes the process deterministic and allows for almost perfect inversion. DiffusionCLIP method leverages the deterministic nature of the process for image manipulation.&lt;/p&gt; &lt;h2 id=&quot;image-manipulation-with-clip&quot;&gt;Image manipulation with CLIP&lt;/h2&gt; &lt;p&gt;CLIP is a model for joint image-language representations which is trained on a large dataset of image-text pairs &lt;d-cite key=&quot;radford2021learning&quot;&gt;&lt;/d-cite&gt;. Using a contrastive learning objective, it learns a joint, multimodal embedding space. The representations learned by CLIP can be used for many tasks including image manipulation and image synthesis. DiffusionCLIP uses CLIP loss to tune the image generator (e.g., a pretrained diffusion model). CLIP loss takes the following form:&lt;/p&gt; \[\mathcal{L}_{\text{direction}} (x_{\text{gen}}, y_{\text{tar}}; x_{\text{ref}}, y_{\text{ref}}) := 1 - \frac{\langle \Delta I, \Delta T \rangle}{\| \Delta I \| \| \Delta T \|}\] &lt;p&gt;where $ \Delta T = E_T(y_{\text{tar}}) - E_T(y_{\text{ref}}), \Delta I = E_I(x_{\text{gen}}) - E_I(x_{\text{ref}}). $&lt;/p&gt; &lt;p&gt;$E_I$ and $E_T$ are CLIP’s image and text encoders, $y_{\text{ref}}, x_{\text{ref}}$ are the source domain text and image, and \(y_{\text{tar}}\) is a text description of a target and \(x_{\text{gen}}\) denotes the generated image.&lt;/p&gt; &lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt; &lt;p&gt;Recent work in the field discovered an alternative way of manipulating image attributes using pre-trained diffusion models &lt;d-cite key=&quot;kwon2023diffusion&quot;&gt;&lt;/d-cite&gt;. The authors show that instead of tuning the model, one can modify the reverse process and guide it towards the target domain. The reverse process is guided through a lower-dimensional (compared to original latents) latent space which in this case is the bottleneck of the UNet of the original pre-trained diffusion model. Authors show that this latent space enjoys high-level semantics and linearity which allows for more flexible image manipulation.&lt;/p&gt; &lt;p&gt;Although this method is still in development (as it was our initial idea for domain adaptation which did not succeed), the latent space suggested by the authors can be used for a more powerful idea which is unsupervised domain adaptation. By smoothing the test images at appropriate noise level, one can classify whether the image possesses a given attribute. Then one can make training and test distributions close to each other by manipulating the attributes of interest. This direction is of our future interest to explore.&lt;/p&gt; &lt;p&gt;Another area of current research is trying to use GANs (also guided by the CLIP loss) for image manipulation &lt;d-cite key=&quot;gal2021stylegannada&quot;&gt;&lt;/d-cite&gt;. Using GANs allows for zero-shot image manipulation which is way faster than the diffusion models’ reverse process. However, GANs suffer from their limited inversion capability and destruction of initial image information which might be dangerous for downstream tasks (e.g., consider a classification task with GAN manipulating training image labels).&lt;/p&gt; &lt;p&gt;An alternative method for manipulating and editing images is mixing latents of source and target &lt;d-cite key=&quot;choi2020stargan&quot;&gt;&lt;/d-cite&gt;. Although this method does provide good results in terms of sample quality, it lacks control for our set-up. We would like to have control over the attributes we are changing and keep the others unchanged.&lt;/p&gt; &lt;p&gt;Another method for image editing is classifier guidance which adds classifier gradients in the reverse process to control the generation process &lt;d-cite key=&quot;dhariwal2021diffusion&quot;&gt;&lt;/d-cite&gt;. This method is unsuitable for our problem set-up since we need to train an additional classifier for the target domain, and we do not have enough data to train it.&lt;/p&gt; &lt;h1 id=&quot;our-contribution&quot;&gt;Our Contribution&lt;/h1&gt; &lt;p&gt;We demonstrate capabilities of text-guided diffusion to perform domain adaptation in a semi-supervised setting (e.g., unseen attributes of the target domain). To the best of our knowledge, this is the first work that shows the power of diffusion models in performing domain adaptation when the difference between the train and target domains can be described in a short prompt.&lt;/p&gt; &lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt; &lt;p&gt;A frequently encountered problem in supervised learning is one where we have training data from one domain (the source domain) but we want to conduct inference on data that comes from a different but related domain (the target domain) that can be described using text. Specifically, we want to focus on the setting where we have access to an adequate number (for training) of observations from the source domain (a subset of which are labelled) and we want to conduct inference (eg. classification) on unlabelled observations from the target domain. An additional constraint is that we only have a limited number of observations from the target domain so it is infeasible to learn the target distribution. Here, we deal with image data.&lt;/p&gt; &lt;h2 id=&quot;diffusionclip&quot;&gt;DiffusionCLIP&lt;/h2&gt; &lt;p&gt;We first train a diffusion model on both labelled and unlablled images from the source domain. This diffusion model is first used to convert input images (from source domain) to the latent. Then, the reverse path is fine-tuned to generate images driven by the target text (text decription of target domain), guided by the CLIP loss. The details are given in the subsequent sections.&lt;/p&gt; &lt;h3 id=&quot;diffusionclip-fine-tuning&quot;&gt;DiffusionCLIP Fine-tuning&lt;/h3&gt; &lt;p&gt;In terms of fine-tuning, the DiffusionCLIP model &lt;d-cite key=&quot;kim2022diffusionclip&quot;&gt;&lt;/d-cite&gt; allows for modification of the diffusion model itself as compared to the latent, enhancing its effectiveness. The process utilizes a composite objective including directional CLIP loss and identity loss for fine-tuning the reverse diffusion model parameters.&lt;/p&gt; &lt;h4 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h4&gt; &lt;p&gt;The objective function is given by:&lt;/p&gt; \[\mathcal{L}_{\text{direction}} (\hat{x}_0(\theta), y_{\text{tar}}; x_0, y_{\text{ref}}) + \mathcal{L}_{\text{id}} (\hat{x}_0(\theta), x_0)\] &lt;p&gt;where $x_0$ is the original image and \(\hat{x}_0(\theta)\) is the generated image from the latent with optimized parameters $\theta$. The identity loss \(\mathcal{L}_{\text{id}}\) &lt;d-cite key=&quot;kim2022diffusionclip&quot;&gt;&lt;/d-cite&gt; aims to preserve the object’s identity post-manipulation.&lt;/p&gt; &lt;h4 id=&quot;optimization-and-identity-preservation&quot;&gt;Optimization and Identity Preservation&lt;/h4&gt; &lt;p&gt;Optimization is guided by directional CLIP loss, requiring a reference and a target text for image manipulation. The identity loss includes $\ell_1$ loss for pixel similarity and a face identity loss for maintaining recognizable human features.&lt;/p&gt; &lt;h4 id=&quot;architecture&quot;&gt;Architecture&lt;/h4&gt; &lt;p&gt;The fine-tuning involves a shared U-Net architecture across time steps, with gradient flow illustrated in Figure 1. This structure supports the transformation of images to align with target texts.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/gradient-flows-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/gradient-flows-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/gradient-flows-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/gradient-flows.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Figure 1. Gradient flows during fine-tuning the diffusion model with the shared architecture across t &lt;d-cite key=&quot;kim2022diffusionclip&quot;&gt;&lt;/d-cite&gt;.&lt;/em&gt;&lt;/p&gt; &lt;h3 id=&quot;forward-diffusion-and-generative-process&quot;&gt;Forward Diffusion and Generative Process&lt;/h3&gt; &lt;p&gt;Kwon et al &lt;d-cite key=&quot;kim2022diffusionclip&quot;&gt;&lt;/d-cite&gt; discusses the DDPM’s sampling process, which is inherently stochastic. This stochastic nature results in varied samples even from the same latent input. However, to leverage the image synthesis capabilities of diffusion models for precise image manipulation, the authors use DDIM’s deterministic forward process with \(\sigma_t=0\) which allows for almost perfect reconstruction. Using deterministic processes, however, limits model’s generative capability and this problem has been developed in the subsequent papers by injecting noise at specific timesteps &lt;d-cite key=&quot;kwon2023diffusion&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h4 id=&quot;deterministic-diffusion-processes&quot;&gt;Deterministic Diffusion Processes&lt;/h4&gt; &lt;p&gt;The deterministic processes are formulated as follows:&lt;/p&gt; &lt;p&gt;$x_{t+1} = \sqrt{\alpha_{t+1}}f_\theta(x_t, t) + \sqrt{1 - \alpha_{t+1}}\epsilon(x_t, t)$&lt;/p&gt; &lt;p&gt;$x_{t-1} = \sqrt{\alpha_{t-1}}f_\theta(x_t, t) + \sqrt{1 - \alpha_{t-1}}\epsilon(x_t, t)$&lt;/p&gt; &lt;h4 id=&quot;fast-sampling-strategy&quot;&gt;Fast Sampling Strategy&lt;/h4&gt; &lt;p&gt;To expedite the sampling, a ‘return step’ is introduced along with a strategy to use fewer discretization steps. This accelerates training without significantly compromising the identity preservation of the object in the image.&lt;/p&gt; &lt;p&gt;Detailed mathematical derivations and more comprehensive analyses can be found in the supplementary sections of &lt;d-cite key=&quot;kim2022diffusionclip&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;experimental-setup-and-procedure&quot;&gt;Experimental Setup and Procedure&lt;/h2&gt; &lt;p&gt;Our method is intended to be used given a setup as follows. We have a set of images from the source domain, \(\{x_i\}_{i=1}^{n}\), out of which we have labels \(\{y_i\}_{i=1}^{n&apos;}\) for a subset of them, where \(n&apos; &amp;lt;&amp;lt; n\). For simplicity, we are dealing with a binary classification task with 0-1 labels. We now want to classify test images from the target distribution, \(\{x^t_i\}_{i=1}^{m}\) (\(m &amp;lt;&amp;lt; n\)). We also have a text description of the target distribution, \(T_{target}\) (a short prompt that captures how the source and target domains differ; for example, if the source domain is images in the summer and the target domain is images in the winter, \(T_{target}\) could be “winter”).&lt;/p&gt; &lt;p&gt;We now use the images from the source domain \(\{x_i\}_{i=1}^{n}\) to train a diffusion model and use DiffusionCLIP fine-tuning to generate an image \(x&apos;_i\) from each labelled source image \(x_i\) driven by \(T_{target}\). Thus, we have created a new training dataset with the target distribution \(\{(x&apos;_i, y_i)\}_{i=1}^{n&apos;}\).&lt;/p&gt; &lt;p&gt;Now, we use supervised learning to train a model on the \(\{(x&apos;_i, y_i)\}_{i=1}^{n&apos;}\) pairs and subsequently classify the test images \(\{x^t_i\}_{i=1}^{m}\). The idea is that by shifting the distribution of training data to match that of the test data using just the text description of the target distribution, we can achieve a model that generalizes well to the target domain even in the regime of limited labelled data and target domain images without having to explicitly learn the target distribution.&lt;/p&gt; &lt;h1 id=&quot;experiment-and-results&quot;&gt;Experiment and Results&lt;/h1&gt; &lt;h2 id=&quot;problem-set-up&quot;&gt;Problem set-up&lt;/h2&gt; &lt;p&gt;We run a simple experiment to show the power of domain adaptation using our method in this setting. We consider a gender classification problem on CelebA dataset with test domain being different from the train domain.&lt;/p&gt; &lt;p&gt;Our train domain is original CelebA images while our target domain is the same images but in the “sketch” style. The “sketch” style images were generated by the same method (DiffusionCLIP) by editing the original CelebA images on the test set. This style transfer doesn’t change the face identity (including gender, which is of our interest for the given task), so we keep all the labels unchanged.&lt;/p&gt; &lt;p&gt;We have a training set of size 1,200 images and test set of size 300 images (mainly for computation reasons). Our data comes from publicly available CelebA dataset with binary attributes (including the gender attribute of interest)&lt;d-cite key=&quot;liu2015faceattributes&quot;&gt;&lt;/d-cite&gt;. We resize all the images to size 256x256 using Bilinear interpolation.&lt;/p&gt; &lt;p&gt;We use a simple CNN architecture for gender classification - three convolutional layers with increasing filter depth (32, 64, 128), each followed by a max pooling layer that halves the image dimensions, followed by 2 fully connected layers with sigmoid activation. Our experiment is ran for demonstrative purposes for the most part and does not require complex architectures. The training size of 1,200 images is additionally hinting at the necessity to scale the model complexity down for the purposes of our experiment. Our objective function is binary cross-entropy loss.&lt;/p&gt; &lt;h2 id=&quot;experimental-pipeline&quot;&gt;Experimental pipeline&lt;/h2&gt; &lt;p&gt;We run the following experiments to confirm our intuition about the method’s effectiveness:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Experiment 1 &lt;ul&gt; &lt;li&gt;Training set (1,200 labeled images) - original CelebA images&lt;/li&gt; &lt;li&gt;Test set (300 labeled images) - “sketched” images&lt;/li&gt; &lt;li&gt;We train CNN on plain CelebA images and evaluate on a shifted test domain. We use the plain CelebA test domain as a performance benchmark. We expect this model to do worse on the “sketched” test set than on the original one.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Experiment 2 &lt;ul&gt; &lt;li&gt;Training set (1,200 labeled images) - adapted images. Original train images adapted to the “sketch” style using the method described in the subsection below. &lt;ul&gt; &lt;li&gt;Note: We keep the number of images in the train set the same as in the experiment above (e.g., we create new train images and delete the original ones instead of augmenting the data) for the clarity of the experiment. In practice, one can combine images from both domains for learning.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Test set (300 labeled images) - “sketched” images.&lt;/li&gt; &lt;li&gt;We train the CNN on the “sketched” images now and evaluate the performance on both “sketched” and plain test sets. We expect this model to do better on the “sketched” test set which is our initial goal.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;domain-adaptation-method&quot;&gt;Domain adaptation method&lt;/h2&gt; &lt;p&gt;To edit our images from the plain CelebA distribution to the target “sketched” distribution, we use the method proposed in DiffusionCLIP &lt;d-cite key=&quot;kim2022diffusionclip&quot;&gt;&lt;/d-cite&gt;. We used pre-trained fine-tuned diffusion model based on original diffusion model trained on CelebA images using P2 objective introduced by Choi et al &lt;d-cite key=&quot;choi2022perception&quot;&gt;&lt;/d-cite&gt;. Note that the original pre-trained P2 diffusion model was trained on the whole CelebA dataset which makes use of large amounts of unlabeled data in the train domain and is consistent with our problem set-up. The diffusion model was fine-tuned using the prompt “Sketch”. We made use of deterministic DDIM inversion process with 40 steps (instead of a 1,000 steps in the original noise schedule) and 6 generative steps.&lt;/p&gt; &lt;p&gt;Despite the sufficient computation cost savings by using the DDIM process, transforming 1,500 images took more than 6 hours on a single NVIDIA GeForce RTX 3050TI 4GB GPU. Computation time is still the main drawback of using diffusion models for image editing and this is the main reason for us to limit the total sample size to 1,500 images.&lt;/p&gt; &lt;p&gt;Note: We use the same procedure for generating test images from “technically unknown” sketch domain and adapting the training set to this domain. This assumes the user perfectly identified the prompt which describes the target domain and used it to fine-tune the pre-trained diffusion model which is unrealistic in practice. We believe, however, that for simple prompts semantic similarity of the user prompt and the word “Sketch” would allow to get adapted images similar to the target domain because of the CLIP loss properties.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;image-manipulation&quot;&gt;Image Manipulation&lt;/h3&gt; &lt;p&gt;Figure 2 shows examples of DiffusionCLIP fine-tuning applied to CelebA images, resulting in “sketched” images.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/female_ex-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/female_ex-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/female_ex-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/female_ex.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/male_ex-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/male_ex-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/male_ex-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/male_ex.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Figure 2. Examples of DiffusionCLIP fine-tuning.&lt;/em&gt;&lt;/p&gt; &lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt; &lt;p&gt;Figure 3 shows the performance of the CNN trained on the original CelebA images and tested on images in the source domain as well as the target domain, while Figure 4 shows the performance of the CNN trained on the adapted images.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_base-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_base-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_base-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_base.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Figure 3. Performance of CNN trained on original CelebA images.&lt;/em&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_adapt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_adapt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_adapt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Semi-Supervised-Domain-Adaptation/train_adapt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Figure 4. Performance of CNN trained on adapted images.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;These results confirm our intuition that adapting our source domain to the target domain results in a non-trivial performance boost. We observe that for the initial few epochs, the performance for both the source and target domains is similar, but this gap increases as we train further. This tells us that initially, the model learns relevant “higher level” features that are present in both the domains since they are both related. However, for later epochs, the model overfits to the distribution of the training data which results in a large performance gap between the two domains. At this stage, the model is learning “lower level” features that belong to the source domain, which are different in the target domain. Thus, the performance on a shifted domain becomes worse as time goes on. If we train further, we expect to learn more lower level features of the source domain, which will enhance performance for a test set from the source domain but deteriorate performance for a test set from the target domain.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;We have shown, with a simple binary classification experiment, that the proposed domain adaptation method using DiffusionCLIP fine-tuning leads to a significant performance boost when we have training and test data sampled from different but related domains.&lt;/p&gt; &lt;p&gt;Future work in this direction might include working with the h-space proposed in &lt;d-cite key=&quot;kwon2023diffusion&quot;&gt;&lt;/d-cite&gt;. Our idea for semi-supervised domain adaptation naturally extends to unsupervised domain adaptation by leveraging the properties of this latent space. One could use this latent space as an implicit attribute classifier after smoothing the image at appropriate noise level and then balance the attributes between train and test sets in an unsupervised manner. This approach, however, requires a better implementation of the original method presented in &lt;d-cite key=&quot;kwon2023diffusion&quot;&gt;&lt;/d-cite&gt; and is not feasible as of now.&lt;/p&gt; </content> </entry> <entry> <title>The Effect of Activation Functions On Superposition in Toy Models</title> <link href="https://deep-learning-mit.github.io/blog/2023/superposition/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/superposition</id> <content type="html">&lt;h2 id=&quot;introduction-to-superposition&quot;&gt;Introduction to Superposition&lt;/h2&gt; &lt;p&gt;With the recent emergence of grokking, mechanistic interpretability research has trended towards understanding how models learn &lt;d-cite key=&quot;GrokNanda&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;Pizza&quot;&gt;&lt;/d-cite&gt;. A central concept in this pursuit is superposition - a single neuron learning multiple “features.”&lt;/p&gt; &lt;p&gt;Features are the distinguishing properties of data points, the “things” that allow a neural network to learn the difference between, say, a dog and a cat, or a Phillip Isola and a Jennifer Aniston. Features are the building blocks that determine what makes one data point different from another. In many cases, features discovered by and encoded within neural networks correspond to human-understandable ideas. For example, in language models there exist embedding vectors describing relations like gender or relative size (e.g., the famous vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)&lt;d-cite key=&quot;mikolov2013efficient&quot;&gt;&lt;/d-cite&gt;). It has been found that language models often map ideas like these to features within their parameters. Human understanding is not necessary though, as models can find and map features that exist beyond the perception of humans. This is an important part of the success (and dual inscrutability) of modern deep models, as these models can determine features and relationships within the data that allow them to model large datasets, like language, very well.&lt;/p&gt; &lt;p&gt;In this work we:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Explain Superposition, why it may occur, and why it is important&lt;/li&gt; &lt;li&gt;Motivate a framework to easily study Superposition&lt;/li&gt; &lt;li&gt;Study how activation functions affect Superposition&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;superposition-and-previous-work&quot;&gt;Superposition and Previous Work&lt;/h2&gt; &lt;p&gt;Let us elaborate further. If you were to train some neural network and visualize the weights - chances are you would see some mess that looks like this:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/random_matrix_equation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/random_matrix_equation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/random_matrix_equation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/random_matrix_equation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;You are likely looking at superposition!&lt;/p&gt; &lt;p&gt;As hypothesized by &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, superposition is a phenomenon which occurs when the number of features being learned by a model is greater than the number of parameters in that model. To capture $n$ features with $m&amp;lt;n$ parameters, one can think of the neurons as “working overtime.” In other words, some of the neurons within a model encode information about more than one feature. The neuron exhibiting superposition operates as an information compressor. The caveat is that this compression is often unpredictable and hard to understand!&lt;/p&gt; &lt;p&gt;In a linear model, i.e., one which maps inputs to outputs with only linear functions, there are fewer parameters than the features it tries to represent, so it can only represent the top $m$ features. How then do neural networks use compression and map back to $n&amp;gt;m$ features using only $m$ parameters? The answer is non-linearity. Clearly, the activation function is key to understanding how superposition occurs - unexplored by other work in the field. &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt; explores the activation function in transformer MLP, but not in the setting we present here.&lt;/p&gt; &lt;p&gt;But why do we care about Superposition? Why spend time studying this?&lt;/p&gt; &lt;p&gt;While it may seem tangential, Superposition sheds important insights on Large Language Models (LLMs)! While LLMs are billions of parameters large, this is still not enough for a one-to-one mapping to “features” on the internet. Therefore LLMs also MUST exhibit superposition to learn. We focus our current work on the $\textit{bottleneck superposition}$ regime, but &lt;d-cite key=&quot;incidental&quot;&gt;&lt;/d-cite&gt; has shown that the picture is far more complicated than presented in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;. Namely, varying the initialization can change how superposition unfolds. To normalize across experiments, we initialize all weights using the Xavier norm, as outlined by &lt;d-cite key=&quot;xavier&quot;&gt;&lt;/d-cite&gt;. However, this is certainly a limitation of our presented work. A more rigourous analysis of superposition with activation functions would explore it outside the contex of the bottleneck regime. We leave this for future work.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/feature_visual-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/feature_visual-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/feature_visual-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/feature_visual.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; From &lt;a href=&quot;https://distill.pub/2017/feature-visualization/&quot;&gt;Distill Blog&lt;/a&gt;, &quot;Feature visualization allows us to see how GoogLeNet trained on the ImageNet dataset, builds up its understanding of images over many layers. &lt;/div&gt; &lt;p&gt;Previous research, as detailed in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, has predominantly explored superposition within the confines of toy models utilizing the Rectified Linear Unit (ReLU) activation function. However, to extend these findings to contemporary neural networks, it is crucial to investigate the influence of different activation functions on superposition. Different activation functions provide different ways for a model to use superposition to its advantage.&lt;/p&gt; &lt;p&gt;So you train a neural network - what happens at the neuron level? There are three possibilities. As the network trains each neuron has three choices:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The neuron chooses not to encode the “features”&lt;/li&gt; &lt;li&gt;The neuron chooses to dedicate itself to one feature&lt;/li&gt; &lt;li&gt;The neuron chooses to encode multiple features&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(We anthropomorphize - The neuron doesn’t choose to do anything - there is no free will - you are born into a loss landscape and an optimizer telling you what to do.)&lt;/p&gt; &lt;p&gt;In linear models, each neuron is limited to representing only the most significant features (2), discarding others (1). Conversely, superposition, enabled by non-linear activation functions, adopts a more inclusive approach (3), trying to encode multiple features per neuron and learning efficient representational shortcuts.&lt;/p&gt; &lt;p&gt;While ReLU bears similarity to the Gaussian Error Linear Unit (GeLU) used in modern GPT architectures, a deeper understanding of how different nonlinear activations impact superposition can provide crucial insights. Such understanding is key to unraveling the complex mechanisms through which neural networks utilize non-linearities, a cornerstone in the broader narrative of neural network interpretability.&lt;/p&gt; &lt;h3 id=&quot;monosemanticity-and-polysemanticity&quot;&gt;Monosemanticity and Polysemanticity&lt;/h3&gt; &lt;p&gt;To connect to existing literature (2) and (3) above are given the names monosemanticity and polysemanticity. We will also follow this notation going forward.&lt;/p&gt; &lt;p&gt;To describe further, the idea of superposition in neural networks leads us to two distinct types of neuron behaviors: monosemanticity and polysemanticity.&lt;/p&gt; &lt;p&gt;Monosemantic neurons are those that specialize in a single, distinct feature, acting as dedicated detectors. This characteristic is often observed in the intermediate layers of architectures like Convolutional Neural Networks (CNNs), where neurons become adept at recognizing specific patterns, such as curves or colors. Polysemantic neurons do not align with just one feature but engage with multiple features simultaneously, offering a broader and more nuanced understanding of the data. This trait is essential for handling complex, high-dimensional datasets but comes at the cost of reduced interpretability.&lt;/p&gt; &lt;h2 id=&quot;motivation-and-notation&quot;&gt;Motivation and Notation&lt;/h2&gt; &lt;p&gt;Our work extends the work done in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt; by examining how the changing of the activation function on toy model networks affects the behavior and interpretability of these networks. &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt; uses the canonical ReLU activation function to add non-linearity to two-layer models to analyze how superposition occurs within small networks. They did not generalize their work to other activation functions, which we find, result in &lt;strong&gt;distinct&lt;/strong&gt; new phenomenon. Our work compares the ReLU function with five other common activation functions: GeLU, SiLU, Sigmoid, Tanh, and SoLU. We hope that generalizing the phenomenon across activation functions can push the toy dataset to be in closer to realistic ML settings.&lt;/p&gt; &lt;h3 id=&quot;problem-specification&quot;&gt;Problem Specification&lt;/h3&gt; &lt;p&gt;The models in this experiment will be learning how to replicate a length-$n$ vector of inputs in the range $[0, 1]$ with a compression to a length-$m$ embedding (where $n&amp;gt;m$). The model will then use the length-$m$ embedding to recreate the length-$n$ input, using a non-linear activation function to allow for superposition.&lt;/p&gt; &lt;p&gt;We will run two variations of the experiment. One variation of the experiment will involve compressing inputs of size $n=10$ to an embedding of size $m=5$. This experiment aims to see how superposition occurs across many features which are encoded in a bottleneck with half the number of spots as there are features. The second variation of the experiment will involve compressing inputs of size $n=2$ to an embedding of size $m=1$. This experiment aims to understand precisely how the model encodes the second “extra” feature in a variety of settings.&lt;/p&gt; &lt;p&gt;To set up this experiment, we need to create a dataset that allows for superposition to occur and that also allows for interpretability of the superposition. To motivate this further, we begin with a careful discussion of features.&lt;/p&gt; &lt;h3 id=&quot;features&quot;&gt;Features&lt;/h3&gt; &lt;p&gt;Features are the salient “things” that a neural network learns to differentiate inputs &lt;d-cite key=&quot;features&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Technically, features are the properties which neural networks try to extract from data during learning to compress inputs to useful representations during inference. Although features can map to human-understandable concepts (e.g., dog ears), they can also represent properties of the data that are not immediately apparent to the human brain. To experiment with superposition, we need to encode features in a way that we can understand. In other words, we do not want our experimental model to learn features that we are unaware of. This would make it hard for us to interpret how the model maps features in the data to embeddings within its parameters, consequently obscuring how superposition works. To this aim, we must generate features within the training set for our model which are simple and understandable to us a priori. Similar to &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, we use as each input a vector with entries drawn independently from a uniform distribution over $[0, 1]$. Making each entry independent of the others enforces that each entry is its own (artificial) feature with no correlation to the other features.&lt;/p&gt; &lt;p&gt;Here we define two important augmentations that we used in the dataset to simulate real-world features: sparsity and importance.&lt;/p&gt; &lt;h4 id=&quot;sparsity&quot;&gt;Sparsity&lt;/h4&gt; &lt;p&gt;Sparsity is a measure of how often a specific feature is present in a dataset. A feature is characterized as “sparse” if it only appears in a small fraction of the inputs to the model. Similarly, features that are “dense” appear in many of the inputs. We will also use the term ‘density’, which is the complement of sparsity, defined as $1-S$.&lt;/p&gt; &lt;p&gt;Specifically, a feature with a sparsity of $S \in [0, 1]$ has a probability $S$ of being expressed in any given input. If we have $S=0$, this means that the feature is expressed in every input, whereas if we have $S=0.5$, this means that the feature is expected to be expressed in about half of the inputs.&lt;/p&gt; &lt;p&gt;In our experiment, we train models at different sparsities to capture how sparsity affects superposition.&lt;/p&gt; &lt;h4 id=&quot;importance&quot;&gt;Importance&lt;/h4&gt; &lt;p&gt;Not all features are created equal!&lt;/p&gt; &lt;p&gt;Some features are more useful than others in determining relevant information about inputs. For instance, when building a dog detector - capturing features related to dogs’ faces are extremely important! A model would need to pick up salient features of dogs, perhaps floppy ears and snouts. Other features, like the grass a dog is sitting on or a frisbee in a dog’s mouth, may not be as useful for detecting a dog. The varying degrees of usefulness among features are encapsulated in the concept of “importance”.&lt;/p&gt; &lt;p&gt;In the context of feature detection by a neural network, importance plays a role in modulating which features are encoded within the embedded layers of the network. In the context of the superposition hypothesis, if one feature has more importance than another feature, then it would be inefficient for the network to map both features equally within the embedding; allocating more weight to the feature with greater importance would be more valuable to the network in minimizing error.&lt;/p&gt; &lt;p&gt;In our experiment, we give each input feature a different importance to allow the models to differentiate between them. We will examine when and how the model justifies mapping multiple features of differing importances to the same neuron, i.e., we will observe the superposition of features with differing importances.&lt;/p&gt; &lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;To run this experiment, we will synthetically generate data that has desired sparsity and importance properties.&lt;/p&gt; &lt;p&gt;Each input $x$ will be a vector of length $n$. Each element $x_i$ in the vector will be drawn independently from the other elements in the uniform range $[0, 1]$. As discussed before, we can now synonymously refer to each of these elements as features, given their independent generation. (We will refer to them as features from this point onwards.)&lt;/p&gt; &lt;p&gt;Each feature $x_i$ in the vector has a relative importance to each of the other features $x_{j\ne i}$. The importance of feature $x_i$ is $I_i = r_I^i$ where $r_I\in(0, 1)$ is a constant describing the relative decay of importance between neighboring features. This attribute of the data will be implemented in the loss function (see below for more details).&lt;/p&gt; &lt;p&gt;We will train separate models for each of the varying levels of sparsity. For an input $x$ with sparsity $S$, each feature $x_i$ will take on its “true” value, a uniformly distributed number, with a probability of $1-S$ and will otherwise be set to 0 with a probability of $S$.&lt;/p&gt; &lt;p&gt;Below is a visualization of two batches of inputs with respective sparsities $S=0.5$ and $S=0.99$.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/input_batches-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/input_batches-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/input_batches-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/input_batches.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Each column of the plots represents a feature vector of length 20. Each batch has size 100, corresponding to the number of columns in the plots. Notice how the changing in sparsity affects the feature density. &lt;/div&gt; &lt;h3 id=&quot;network&quot;&gt;Network&lt;/h3&gt; &lt;p&gt;Below are the architectures of the base (linear) and experimental (non-linear) models that we are using in this experiment. Of particular note is the activation function $\mathbb{f}$, which we will substitute using the aforementioned activation functions.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Linear Model&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Activation ( $\mathbb{f}$ ) Output Model&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(h = Wx\)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(h = Wx\)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = W^T h + b\)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = f(W^T h + b)\)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = W^T Wx + b\)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = f(W^T Wx + b)\)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Autoencoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Autoencoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Autoencoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/Autoencoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;We create an autoencoder - compressing down to induce polysemanticity. This maps $x$ to a direction in a lower-dimensional space, represented by \(h = Wx\). Each column of $W$ corresponds to a lower-dimensional representation of a feature in $x$. To reconstruct the original vector, $W^T$ is used, ensuring clear feature representation correspondence. This structure results in a symmetric matrix $W^TW$ and allows for clear visualization of the weights. They visually allow for the determination of the presence of superposition.&lt;/p&gt; &lt;h3 id=&quot;loss&quot;&gt;Loss&lt;/h3&gt; &lt;p&gt;Sparsity, Importance and Our Network come together in the following loss function:&lt;/p&gt; \[L = \sum_{i} \sum_{x} I_{i}(x_{i} - x&apos;_{i})^{2}\] &lt;p&gt;Motivated by &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, we use a standard MSE loss, where $x_i$ and $x_i’$ measure the absolute difference in the auto-encoding of the datapoint. The Importance factor, $I_i$ , describes how important the given reconstruction is. A smaller importance will allow loss minimization even with a poor reconstruction.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;Below we present each activation function, along with plots depicting how training results in superposition at varying degrees of sparsity.&lt;/p&gt; &lt;p&gt;For the $n=10, m=5$ experiment, we show the $W^TW$ matrix and neuron feature distribution at varying degrees of sparsity. The $W^TW$ matrix reveals which features are prioritized (shown by the diagonal terms) and any polysemanticity that occurs (shown by the off-diagonal terms). The neuron feature distribution shows how each of the $m=10$ features are mapped to each of the $n=5$ embedding dimensions. This can aid in understanding under what conditions polysemanticity arises and how it occurs under each condition of sparsity.&lt;/p&gt; &lt;p&gt;For the $n=2, m=1$ experiment, we show a phase diagram. This phase diagram shows how the second “extra” feature of the length-2 input vector is encoded. There are three options: not encoded at all (only the first feature is encoded), encoded in superposition with the first feature, and encoded as the only feature (the first feature is not encoded).&lt;/p&gt; &lt;h3 id=&quot;relu&quot;&gt;ReLU&lt;/h3&gt; &lt;p&gt;The ReLU (Rectified Linear Units) activation function is a piecewise-linear function, a simple non-linearity that allows models to use superposition of features. ReLU was the only activation function used in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, so our work with the ReLU function was primarily to verify the results from their work and create a baseline for our subsequent experiments.&lt;/p&gt; &lt;p&gt;The following are the $W^TW$ matrices and feature-neuron mappings:&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; ReLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_relu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_relu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_relu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_relu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;As per the results in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, the ReLU model focuses on the most significant features in the low sparsity regime (generally resulting in monosemanticity), while relying on superposition in the high sparsity regime (polysemanticity). With weaker signals for the most important features in the high sparsity regime, the model encodes multiple features in each neuron activation to minimize error of the sparse signals. Notably, the ReLU model uses antipodal pairs in the mapping of features to encode multiple features to single neurons. This can be seen as a light-colored diagonal entry within $W^T W$ and a corresponding dark-colored off-diagonal entry within the same column. This antipodal mapping of features is a method that the model uses to compress more than one feature to one neuron. This antipodal mapping is more interpretable than other kinds of polysemanticity which occurs in subsequently-described activation functions which “speckle” multiple features into a single neuron, making it more difficult to determine how the superposition occurs in that model.&lt;/p&gt; &lt;p&gt;The following is the phase diagram of the ReLU models:&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_relu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_relu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_relu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_relu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;In regimes of high sparsity (i.e., below $1-S=0.1$ on the phase diagram above) the ReLU models are highly polysemantic for all relative feature importances, reflecting an inability to encode features with a sparse signal. In regimes of low sparsity, the model generally embeds the more important of the two features. This result mirrors the phase diagram in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt; as expected.&lt;/p&gt; &lt;h3 id=&quot;gelusilu&quot;&gt;GeLU/SiLU&lt;/h3&gt; &lt;p&gt;The GeLU (Gaussian Error Linear Units) and SiLU (Sigmoid Linear Units) activation functions are very similar to one another, and as a result produced very similar experimental results. Both functions are akin to a “smoothed out” version of the ReLU function, i.e., they have no discontinuities. The GeLU has recently been popularized as the activation function of choice in many transformers, including BERT &lt;d-cite key=&quot;Devlin2019BERTPO&quot;&gt;&lt;/d-cite&gt; and GPT &lt;d-cite key=&quot;gpt&quot;&gt;&lt;/d-cite&gt;. The GeLU is differentiable for all $x$ - and has a smoother curve than the SiLU (Swish) activation. &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt; found that in the setting of transformers, the GeLU was less interpretable than the SoLU. This may be the case after having many linear layers activation - but with a single layer this is not the case.&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; GeLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_gelu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_gelu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_gelu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_gelu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; SiLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_silu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_silu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_silu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_silu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The GeLU and SiLU models exhibit similar kinds of superposition in their weight matrices. With increasing sparsity, superposition of features does happen, but it is more “strict” than the ReLU model, generally mapping at most two features to any single neuron. In each of the polysemantic neurons, though, there is one feature that dominates, suggesting that these activation functions enforce sparsity in their activations. There are also many antipodal pairs of features within these models, reiterating the behavior that exists in the ReLU models (also found in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;).&lt;/p&gt; &lt;div class=&quot;row mt-0 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-2 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_gelu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_gelu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_gelu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_gelu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-2 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_silu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_silu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_silu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_silu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-0 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-2 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The above phase diagrams of the GeLU and SiLU models show a marked difference from that of the ReLU model (earlier), despite the similar shapes of these three activation functions. The GeLU and SiLU models exhibit significant monosemanticity at high degrees of sparsity, unlike the ReLU, which results in near-complete polysemanticity for sparsities higher than $S=0.9$. This differnce may reflect SiLU’s and GeLU’s better fit as an activation for picking up the signal in sparse feature representations, making the case for GeLU and SiLU as more interpretable activation functions within larger models.&lt;/p&gt; &lt;h3 id=&quot;sigmoid&quot;&gt;Sigmoid&lt;/h3&gt; &lt;p&gt;The Sigmoid function is a smooth activation function with an output range of $(0, 1)$. This maps directly to the desired range of values that the model is trying to replicate.&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; Sigmoid $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_sigmoid-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_sigmoid-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_sigmoid-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_sigmoid.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The Sigmoid model exhibits superposition in all neurons as soon as the sparsity is non-zero, as can be seen from the “speckling” of non-zero off-diagonal terms in $W^T W$. This is a difference from the ReLU/GeLU/SiLU models, for which the superposition “leaks” into the least significant encoded features at low, non-zero sparsities and eventually affects all features at higher sparsities. This low-sparsity superposition may occur because the Sigmoid function strictly maps to $(0, 1)$, with increasingly large pre-activation inputs necessary to map to values close to 0 and 1. As such, the model may be “speckling” the off-diagonal values in an attempt to “reach” these inputs which are close to 0 and 1.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_sigmoid-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_sigmoid-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_sigmoid-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_sigmoid.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Despite differences in the occurrence of polysemanticity, the ReLU and Sigmoid models exhibit very similar phase diagrams, reflecting an inability to encode multiple features at sparsities above $S=0.9$ (i.e., below $1-S=0.1$ on the phase diagram). As discussed above, this may be caused by the vanilla sigmoid activation’s inability to “reach” target values close to 0 or 1.&lt;/p&gt; &lt;h3 id=&quot;tanh&quot;&gt;Tanh&lt;/h3&gt; &lt;p&gt;The Tanh function is another smooth activation function, but it results in significantly different behavior from the Sigmoid (despite being a linear mapping of the Sigmoid).&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; Tanh $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_tanh-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_tanh-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_tanh-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_tanh.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;With the Tanh activation function, the models prioritize the most important features regardless of sparsity. This behavior is possibly attributed to the range that the Tanh function maps to $(-1, 1)$, while the target range of input values in this experiment are $[0, 1]$. This behavior is similar to that of a linear model (i.e., no activation function) which exhibits no capability to use superposition, but the phase diagram reveals subtle differences from the linear model results.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_tanh-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_tanh-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_tanh-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_tanh.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Although nearly performing as the linear model would, only encoding the most important feature, there is some difference to the linear model along the boundary between features, as can be seen around the importance of 1. This reflects the model’s ability to use non-linearity to perform superposition.&lt;/p&gt; &lt;h4 id=&quot;a-note-on-sigmoid-and-tanh&quot;&gt;A Note on Sigmoid and Tanh&lt;/h4&gt; &lt;p&gt;Despite similarities in the S-like curvature of the Sigmoid and Tanh activation functions, the Sigmoid model exhibits superposition, whereas the Tanh model exhibits nearly zero superposition. A key difference between the two functions is the fact that the Sigmoid function maps inputs to a range of $(0, 1)$, while the Tanh function maps inputs to a range of $(-1, 1)$. This difference is significant in our experiment, as our experiment uses models to recreate random vectors with elements in the range $[0, 1]$. The range of the Sigmoid function matches this range, while the range of the Tanh function which matches this range only occurs for non-negative inputs to the Tanh function. In other words, the $(-\infty, 0)$ input domain (which maps to the range $(-1, 0)$) of the Tanh function remains useless for prediction of values which should be in the range $[0, 1]$. Therefore, the tanh function empirically acts like a linear function (i.e., no activation layer).&lt;/p&gt; &lt;h3 id=&quot;solu&quot;&gt;SoLU&lt;/h3&gt; &lt;p&gt;The SoLU (Softmax Linear Units) activation function is based on the work from &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt;. \(Solu(x) = x * softmax(x)\) SoLU is a function for which the activation of each neuron is dependent on all the other neurons within its own layer. This is significantly different from all the other activations that we tested, as the activations of neurons with the other functions are independent of the other neurons within the same layer. In other words, all the other activation functions are univariate while the SoLU is multivariate. Similar to other approaches like L1 regularization, the SoLU amplifies neurons with relatively large pre-activations and de-amplifies neurons with relatively smaller pre-activations. This behavior pressures the model to be more monosemantic (and therefore more interpretable in some settings), as discussed in &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; SoLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_solu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_solu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_solu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/Sparsity_super_solu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;In our experiment, the SoLU model results in non-zero superposition of all features with all degrees of sparsity. This may be attributed to the way that the SoLU “forces” activations to be sparse, i.e., the activations result in a “winner-takes-all” behavior due to the way that the Softmax function works. This is not a useful property for prediction of a vector of independently-drawn values, as the input vectors are unlikely to be peaky, i.e., the SoLU does not quite fit the purposes of its task.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_solu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_solu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_solu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/phase_51_solu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-superposition/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-superposition/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;As seen in the heatmap plot above, the SoLU activation results in very polysemantic behavior. This function is not precisely fit for its task of recreating given vectors and likely results in using polysemanticity to attempt to pass information about inputs forward. Curiously, the SoLU models have preference for the more important feature in the low sparsity regime.&lt;/p&gt; &lt;h3 id=&quot;bringing-them-all-together&quot;&gt;Bringing Them All Together&lt;/h3&gt; &lt;div class=&quot;caption&quot;&gt; Sparsity vs Dimensions Per Feature &lt;/div&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-superposition/file.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;p&gt;The diagram above depicts a variation on the two experiments explained thus far. In this experiment $n=200$ features were compressed to $m=20$ features and the loss function was tweaked to give uniform importance $I_i = 1$ to all features. This was done to determine how each activation functions compresses features in different sparsity regimes without the influence of feature importance.&lt;/p&gt; &lt;p&gt;On the y axis, the plot depicts a metric (dimensions per feature) that measures the number of dimensions a model dedicates to each feature. In other words, a point with a y-value near 1 represents a model that dedicates one dimension of its embedding space to one feature, whereas a point with a y-value near 0.25 represents a model that represents four features at each dimension.&lt;/p&gt; &lt;p&gt;The plots are generally consistent with the analysis from the previous experiments. Many of the activations result in superposition in the low-density/high-sparsity regime, and increases in sparsity result in increases in the polysemanticity of the model (i.e., the dimensions per feature decrease). Consistent with the other experiments, SiLU and GELU perform very similarly. The Sigmoid and SoLU activations pack nearly 20 features per dimension at high sparsities. The Tanh activation exhibits behavior similar to the linear model, neatly packing one dimension with one feature, a result that is mirrored in the previous experiments. Similar to the results in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, we see “sticky” behavior of the ReLU activation function at 1 and 0.5 dimensions per feature. This can be explained by the phenomenon of “antipodal pairs” discussed in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;. None of the other activation functions that we tested exhibit this behavior - which is striking since this is a well-studied effect for the ReLU activation function. This may be because the ReLU activation function is the only one that is not smooth, and therefore has a differentiable behavior than the other activation functions.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our investigation into the effects of various activation functions reveals that significant changes occur in model behavior depending on the chosen function. This finding underscores the ability to modulate the degree of superposition through the selection of activation functions, highlighting yet unexplored degrees of freedom in model design. This line of inquiry goes seamlessly with considerations of how neural networks are initialized and trained, suggesting these as promising future research directions.&lt;/p&gt; &lt;p&gt;Our work is limited by the breadth of activation functions that we tested, though. Further iterations on each of the activation functions (e.g., tweaking the Sigmoid function to map to the range $(-\epsilon, 1+\epsilon)$) could prove fruitful in getting better performance from the models. Furthermore, while writing this blog, &lt;d-cite key=&quot;incidental&quot;&gt;&lt;/d-cite&gt; published a new key insight related to the importance of initialization in superposition, which we do not explore here. Despite this, we have learned valuable insights about the effects that our set of activation functions can have on superposition.&lt;/p&gt; &lt;p&gt;Pursuing enhanced interpretability, however, does not come without its challenges. Specifically, striving for transparency and understandability in neural network models raises concerns about the potential for deception. Despite these challenges, our work aims to develop neural network models that are more interpretable, transparent, and secure.&lt;/p&gt; &lt;ol class=&quot;bibliography&quot;&gt;&lt;/ol&gt; </content> </entry> <entry> <title>Stable Diffusion for Oracle Bone Script</title> <link href="https://deep-learning-mit.github.io/blog/2023/stable-diffusion-for-obs/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/stable-diffusion-for-obs</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/oracle_bone_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/oracle_bone_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/oracle_bone_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/oracle_bone_example.jpg&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 1: Inscribed tortoise carapace (“oracle bone”), Anyang period, late Shang dynasty, c. 1300–1050 B.C.E. &lt;/div&gt; &lt;p&gt;Oracle bone script (甲骨文) is an ancient form of Chinese characters which were engraved on bones (mostly of turtle and oxen) (Fig. 1). These bones were used to record historical events and communicate with ancestors and deities to predict disaster and bestow fortune. Many of these characters have been matched to their modern day depictions however the meaning of many still remain unknown. Currently scientists have only deciphered 1,000 of the over 4,000 identified characters &lt;d-cite key=&quot;divinity2023&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Many early day Chinese characters originate from a pictographic base. The characters in oracle bone script often, but not always, share structural similarities to their modern day counterparts and one is able to trace development of the character throughout time (Fig. 2).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/horse_evolution-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/horse_evolution-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/horse_evolution-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/horse_evolution.png&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig 2. Evolution of the &quot;horse&quot; character from oracle bone script to traditional and simplified Chinese. &lt;/div&gt; &lt;p&gt;The deciphering of these characters holds immense historical and cultural value, giving researchers insight to the lives and beliefs of society at the time, and is an active field of research today.&lt;/p&gt; &lt;p&gt;The goal of this project is to train a ControlNet for stable diffusion to transform images of the oracle bone script characters to their modern day traditional Chinese counterparts.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;Many different approaches have been used to classify oracle bone script characters including using deep learning to compute the similarity between a rubbing and characters in the oracle bone script font library &lt;d-cite key=&quot;zhangObs2020&quot;&gt;&lt;/d-cite&gt;, leveraging pretrained networks such as YOLO and MobileNet &lt;d-cite key=&quot;dlObs2022&quot;&gt;&lt;/d-cite&gt;, building out and training specialized convolutional neural networks &lt;d-cite key=&quot;HWOBC&quot;&gt;&lt;/d-cite&gt;, and using hierarchical representations&lt;d-cite key=&quot;Hierarchical&quot;&gt;&lt;/d-cite&gt;. These methods were very successful at classifying and grouping oracle bone script characters. Accuracies on the HWOBC dataset were as high as 97.64% using the DCNN Melnyk-Net&lt;d-cite key=&quot;HWOBC&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;While these methods are able to group both deciphered and undeciphered oracle bone script characters, these groupings are not informative of what the deciphered oracle bone script character may be. There are very few approaches that have applied machine learning and generative artificial intelligence to decipher these characters. Traditionally deciphering these characters relies on the knowledge and experience of professional historians applying rules on OBS evolution.&lt;/p&gt; &lt;p&gt;New methods and frameworks have been suggested to help aid the deciphering of these inscriptions including a new case based system proposed by Zhang et al. which given a query of a character returns a set of similar characters and other data to help aid in the deciphering process. This framework utilizes an autoencoder to find similarities between adjacent writing systems tracking the evolution of the character from OBS to its modern day counterpart. However the accuracies of translating from OBS to Bronze Epigraphs only almost reached 50% when considering the top 100 character categories. Additionally this work only translated directly from OBS to Bronze Epigraphs to Chu State Characters and this work is incomplete when it comes to the deciphering of unknown characters &lt;d-cite key=&quot;Decipher2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;The approach of Wang et al. attempted to trace the evolution of these characters using the VGG16 network showing the inheritance and similarity between characters; however, it also did not generate the deciphering of unknown characters&lt;d-cite key=&quot;Evolution2022&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Using machine learning and generative techniques to decipher OBS characters is an emerging field and current work deciphering relies on access to Bronze Epigraphs and Seal Script. While these intermediate writing systems provide valuable information and structure on character evolution, they are difficult to link together and I attempt to bypass direct knowledge of these intermediate mappings to translate directly between OBS and traditional Chinese.&lt;/p&gt; &lt;p&gt;Recently many developments have been in generative AI space, especially in the development of Stable Diffusion to gradually denoise images. As ControlNet is able to leverage the existing large diffusion model U-Net with layers trained on billions of images, it is able to learn a diverse set of conditional controls&lt;d-cite key=&quot;ControlNet&quot;&gt;&lt;/d-cite&gt;. I hope to leverage this and train a ControlNet that can control the generation of modern day Chinese characters given their OBS counterpart.&lt;/p&gt; &lt;p&gt;While it is unknown if many of these characters even have modern day counterparts, by training the model to perform well on known oracle bone script and modern day traditional Chinese character pairs I hope to generate a “best guess” on what the deciphered character could be.&lt;/p&gt; &lt;h2 id=&quot;methodology-and-experiments&quot;&gt;Methodology and Experiments&lt;/h2&gt; &lt;h3 id=&quot;dataset-construction&quot;&gt;Dataset Construction&lt;/h3&gt; &lt;p&gt;There are a variety of publicly available OBS datasets online. However, many datasets contain images of oracle bone rubbings that are often very noisy and even cropped and fragmented. The images in the collected datasets may not even be of the same size (Fig. 3). This variety, while useful for many classification tasks and more pertinent real world applications would be a larger challenge when training a ControlNet.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-1.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-1.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-1.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-1.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-2.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-2.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-2.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-2.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-3.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-3.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-3.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-3.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-4.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-4.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-4.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-4.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-5.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-5.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-5.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-5.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-6.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-6.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-6.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-6.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-7.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-7.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-7.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-7.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-8.bmp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-8.bmp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-8.bmp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/obc306-8.bmp&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 3: Oracle bone script rubbings belonging to the same character class from the OBC306 dataset &lt;/div&gt; &lt;p&gt;The dataset I chose is the &lt;a href=&quot;https://jgw.aynu.edu.cn/ajaxpage/home2.0/DataOBC/detail.html?sysid=22&quot;&gt;HWBOC database&lt;/a&gt; created by Bang Li et al.&lt;d-cite key=&quot;HWOBC&quot;&gt;&lt;/d-cite&gt;. This was the largest publicly available dataset I could find with clean images. The dataset contains 83245 images of oracle bone script handwriting samples divided into 3881 classes. Of these 3881 characters 1457 characters have been deciphered into their modern day equivalent. The dataset images are 400x400 and the characters are relatively centered around the same size (Fig. 4).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/hwobc-4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 4: HWOBC dataset images for the character disaster (災/灾) &lt;/div&gt; &lt;p&gt;ControlNet relies on having “source” and “target” image pairs to train on. Using the mapping between the characters to their deciphered counterparts, I generated traditional Chinese character images for every deciphered OBS character. The file mapping the known OBS characters to their traditional counterparts can be found in Project Resources. For simplification, each image class has the same target image (a many to one mapping between the “source” images and the “target”) (Fig. 5). The reason for this is that there are many “correct” ways of etching a character in oracle bone script that would map to the same Chinese character. Many OBS characters can be flipped in multiple directions and still represent the same character. This is not a characteristic of modern day Chinese. The images are generated using the KaiTi Chinese font library. An additional important simplification I made was that sometimes a class of OBC characters has multiple modern day counterparts – I only generated a target image for the first given counterpart in the list.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_7.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_17-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_17-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_17-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_17.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_FTZ-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_FTZ-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_FTZ-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/617CA_FTZ.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 5: Left OBS &quot;sources&quot; corresponding to the rightmost traditional Chinese &quot;target&quot; of the Yang character (陽/阳) &lt;/div&gt; &lt;p&gt;For OBS characters which are separated into multiple characters in their modern day translation, I made the decision to generate the modern day counterpart by stacking the characters vertically. However, different composite characters are sometimes created through horizontal stacking. This is a limitation that can be improved upon in future iterations.&lt;/p&gt; &lt;p&gt;Finally in order to guide the training process, a prompt is required. I was hoping to leverage some existing knowledge of what a Chinese character is within the U-Net and because the tasks are all the same the prompt was “traditional Chinese character”&lt;/p&gt; &lt;h3 id=&quot;dataset-preprocessing&quot;&gt;Dataset Preprocessing&lt;/h3&gt; &lt;p&gt;Due to stable diffusion models currently needing to take an input with dimensions divisible by 64, I resized all of the images to 384x384. The dataset was further sampled from with an 80-20 split to create a validation and a training set (from the set of deciphered classes with a “target” image). Additionally, as some characters are unable to be rendered with font libraries and different encodings, the classes 6131A, 60EBA, and 6100F were removed from the dataset. As the images were already binary and without noise, no further preprocessing steps were taken.&lt;/p&gt; &lt;p&gt;The script to create the JSON file mapping source to target images with a prompt as well as target image generation can be found in Project Resources.&lt;/p&gt; &lt;p&gt;In total, the model was trained on 31337 pairs of unique “source” images and “target” images (of which there were 1454).&lt;/p&gt; &lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt; &lt;p&gt;As ControlNet freezes the Stable Diffusion U-Net and creates trainable copies of certain blocks, the copies and “zero convolution” blocks can receive a condition to integrate into the main model (Fig. 6).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/controlNet-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/controlNet-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/controlNet-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/controlNet.png&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 6: Visualization of the ControlNet setup &lt;/div&gt; &lt;p&gt;I chose to control the standard Stable Diffusion model &lt;a href=&quot;https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main&quot;&gt;SD1.5&lt;/a&gt;. In order to train the model, I modified the scripts found in the &lt;a href=&quot;https://github.com/lllyasviel/ControlNet/tree/main&quot;&gt;main ControlNet Github&lt;/a&gt; to create the dataloader object, attach the ControlNet to the pretrained existing Stable Diffusion model, and train the model.&lt;/p&gt; &lt;p&gt;The model was run on a Vast.ai computing instance with an A100PCIE machine.&lt;/p&gt; &lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt; &lt;p&gt;In total I trained a ControlNet on this dataset twice: once with the Stable Diffusion model layers locked and again with the Stable Diffusion layers unlocked.&lt;/p&gt; &lt;h4 id=&quot;locked-stable-diffusion&quot;&gt;Locked Stable Diffusion&lt;/h4&gt; &lt;p&gt;By default, the Stable Diffusion layers in the controlled model are locked. This allows the model to maintain the pretrained parameters learned from the larger dataset. This option is usually beneficial when training a ControlNet on smaller datasets with a larger array of prompts and conditions. Due to the smaller size of the constructed dataset, I wanted to see the performance when the layers are locked. When training with the locked layers, I set the learning rate to 1e-5 and used a batch size of 4. In total, this model was trained for 2 epochs.&lt;/p&gt; &lt;h4 id=&quot;unlocked-stable-diffusion&quot;&gt;Unlocked Stable Diffusion&lt;/h4&gt; &lt;p&gt;However, a benefit of unlocking the Stable Diffusion layers is that increased performance has been found for more specific problems. By unlocking the original Stable Diffusion model layers, both the model and the ControlNet are being simultaneously trained (Fig. 7). Due to the performance of the previous model, I also increased the batch size from 4 to 8 to decrease training time. Additionally I decreased the learning rate to 2e-6 to be more careful when learning as to not degrade the capability of the original Stable Diffusion model. This model was run for a total of 5 epochs,&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD.png&quot; class=&quot;img-fluid rounded&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 7: Visualization of the ControlNet setup with Stable Diffusion layers unlocked &lt;/div&gt; &lt;h2 id=&quot;experiment-analysis&quot;&gt;Experiment Analysis&lt;/h2&gt; &lt;h3 id=&quot;locked-stable-diffusion-1&quot;&gt;Locked Stable Diffusion&lt;/h3&gt; &lt;p&gt;Sudden convergence for ControlNet is usually observed around 7000 steps or after the first epoch in this case. This model was only trained for 2 epochs and 21370 steps. Sudden convergence never occurred for the model. The model improved quite quickly to looking like real Chinese characters to the untrained eye with noticeable improvements in structure as well as writing style between the steps 0 to 300 to 3000 (Fig. 8). Noticeably we see that without the ControlNet in step 0 the results are quite far from the desired. However even in later steps, none of the generated characters are real.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-0-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-0-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-0-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-0.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 0 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-300-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-300-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-300-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-300.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 300 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-3000-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-3000-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-3000-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-3000.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 3000 &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 8: Training sampling checkpoint results, from top to bottom: source OBS image, generated image, target Chinese image &lt;/div&gt; &lt;p&gt;In the first couple thousand steps of training as demonstrated in the Fig. 8, there aren’t many matching structural similarities between the characters, this began to change around step 10000.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835-annotated-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835-annotated-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835-annotated-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-10835-annotated.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 9: Training sampling checkpoint results from step 10835 radical mappings and similarities circled in red, top to bottom: source OBS image, generated image, target Chinese image &lt;/div&gt; &lt;p&gt;The results from Fig. 9 are really exciting to see as finally it looks as if the model is starting to learn the structural radical mappings between OBS and traditional Chinese. These mappings are not correct or complete but it is a step up from results in previous steps and similarities between characters are easier to see. Unfortunately, model improvement slowed down and did not appear to be improving past step 20000 (Fig. 10).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-21070-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-21070-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-21070-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/locked-SD-21070.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 10: Training sampling checkpoint results from step 21070, top to bottom: source OBS image, generated image, target Chinese image &lt;/div&gt; &lt;p&gt;As seen in the above figures many of the generated characters would have some sort of greyscale background. My prompt did not specify the background to be white and was instead just “traditional Chinese character”. I suspect the model was using the varying grayscale backgrounds to “cheat” a little when it came to how closely the reconstructed image matched the target. Another important observation is that the model often generated images with many more strokes than the target image. I suspect this is because of the distribution of target images being slightly more skewed to more complicated characters than simple ones.&lt;/p&gt; &lt;p&gt;Overall the model did show capability of learning the objective, however further tweaks are needed to increase its performance.&lt;/p&gt; &lt;h3 id=&quot;unlocked-stable-diffusion-1&quot;&gt;Unlocked Stable Diffusion&lt;/h3&gt; &lt;p&gt;This model was trained by unlocking the underlying Stable Diffusion model layers the ControlNet was attached onto. In total the model trained for 20790 steps with the modified parameters specified above. The performance between the models at first appears quite similar. We observe that without the ControlNet the generated image is not as expected with great improvements in steps 300 and 3000 (Fig. 11).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-0-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-0-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-0-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-0.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 0 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-300-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-300-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-300-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-300.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 300 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-3000-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-3000-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-3000-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-3000.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 3000 &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 11: Training sampling checkpoint results with unlocked SD layers, from top to bottom: source OBS image, generated image, target Chinese image &lt;/div&gt; &lt;p&gt;The difference between the models becomes apparent during further timesteps. While this result is not very common, in further timesteps there is actually occurences of the side radical in the generated character matching the target. While there were relationships between the character radicals in the locked version (Fig. 9), they were never in the exact form as circled in Figure 12.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-12054-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-12054-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-12054-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-12054.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 12054 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-17472-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-17472-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-17472-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-stable-diffusion-for-obs/unlocked-SD-17472.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Step 17472 &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 12: Training sampling checkpoint results with unlocked SD layers, from top to bottom: source OBS image, generated image, target Chinese image &lt;/div&gt; &lt;p&gt;Both models seem to preserve the general structure of the character, be it left to right or top to bottom. However the characters generated past ~ step 10000 of the unlocked model look much more realistic than the locked model. This is due to the fact that actual modern day Chinese radicals comprise the generated characters (although the characters generated are still not real). Additionally, the images generated by unlocking Stable Diffusion layers do not have the same problem as the locked layer with the grayscale background.&lt;/p&gt; &lt;p&gt;Overall the model did show capability of learning the objective and there was improved performance from unlocking the layers and increasing batchsize, however further tweaks are needed to increase its performance. In particular, I think the model would benefit from being run for even longer or increasing the batch size.&lt;/p&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;h3 id=&quot;limitations-and-future-work&quot;&gt;Limitations and Future Work&lt;/h3&gt; &lt;p&gt;Due to computational and financial limitations, I was unable to run either of the models for longer than 20000 steps and was unable to use a batch size of larger than 8. While sudden convergence is commonly observed around 7000 steps, this is not always the case and I think the model would benefit from being trained for longer or if feasible with a larger batch size so that training does not take as long.&lt;/p&gt; &lt;p&gt;Additionally, a current limitation is that I did not augment the data at all. Currently all of the characters are relatively centered and are of the same size, in the future it would be of interest to uncenter the characters (ie: have them be placed in the upper left corner etc.). I suspect this could potentially enforce more structural and radical adherence compared to current performance of the model. Furthermore, analysis should be done into the distribution of the target data to better understand why the results of the ControlNet are the way they are and to potentially remove outliers from the dataset. Specifically, the distribution with regards to character complexity which could be quantified roughly by measuring the filled pixels or with more sophistication by counting the number of strokes.&lt;/p&gt; &lt;p&gt;I am also curious to see how training a ControlNet on this data would perform with a different prompt. Perhaps specifying a white background would cause the model to converge faster. Additionally the knowledge of what a “traditional Chinese character” is could be different than its understanding of “繁体字” (traditional Chinese character in Chinese). I would love to further explore how changing the prompt could improve results or speed up time to convergence.&lt;/p&gt; &lt;p&gt;Finally, due to the hierarchical structural nature of Chinese characters exploring a way to somehow further reinforce these representations within the ControlNet architecture would also be beneficial.&lt;/p&gt; &lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt; &lt;p&gt;Both models able to form structural level connections between oracle bone script and traditional Chinese. While the generated characters were not actual Chinese it was also able to emulate the form of actual Chinese using many existing radicals. The model with unlocked stable diffusion layers outperformed the locked model qualitatively generating more realistic characters.&lt;/p&gt; &lt;p&gt;Although the trained models are not able to translate with confidence between oracle bone script and traditional Chinese, this method shows promise as a tool to further aid the research into deciphering oracle bone script.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;project-resources&quot;&gt;Project Resources&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;a href=&quot;https://jgw.aynu.edu.cn/ajaxpage/home2.0/DataOBC/detail.html?sysid=22&quot;&gt;HWBOC database&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/jmortan/OBS-ControlNet&quot;&gt;Dataset Creation Resources&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/lllyasviel/ControlNet/tree/main&quot;&gt;ControlNet Github&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main&quot;&gt;SD1.5 Model&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&quot;notes--troubleshooting&quot;&gt;Notes + Troubleshooting&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;To unzip the file with HWOBC dataset - use software that supports the Chinese encoding (ie: Chinese WinRAR)&lt;/li&gt; &lt;li&gt;To train ControlNet image size must be divisible by 64&lt;/li&gt; &lt;li&gt;When creating an instance of a machine dependency errors arise if the version of Cuda and PyTorch specified in the image does not align with specifications for the ControlNet training&lt;/li&gt; &lt;/ol&gt; </content> </entry> <entry> <title>Gradient-Boosted Neural Wavlet Interpolation for Time Series (G-BiTS)</title> <link href="https://deep-learning-mit.github.io/blog/2023/distill-example/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/distill-example</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Energy companies struggle with energy allocation. The power grid contains a multitude of homes, schools, and offices all which require different amounts of power draw and capacity. As the current grid stands, the control loop is running on old data and isnt adequately reactive to sudden spikes, as well the inability to properly model trends. Energy forecasting is the means by which we work to rectify that gap. Energy forcasting is a blanket umbrella term coming from general forcasting of any time series data. There are a lot of methods currently available, ranging from purely statistical models up to deep neural networks. At the moment, the SOTA in predictive modeling from statistical models is SARIMAX: Seasonal Autoregressive Integrated Moving Average Exogenous. In deep learning, the SOTA is N-HiTS[1]. Both work well in most circumstances, but there is a lot of work to improve upon the current performance given we want to generate better embeddings to decrease loss through the energy grid. There has been great performance boosts associated with combinding the strengths of the different methods, and that is part of what this paper explores. Another big target: as it stands the current flavors of N-HiTS dont touch upon the further work reccomendations from the original paper. This includes advanced interpolation, moving away from the current linear interpolation for the Block modules and moving towards incorporating wavelet decomposition and transforms to help convert the signal into a form that makes it much easier to deliver robust data. I propose gradient-boosted neural wavlet interpolation for time series (G-BiTS) as a new entry to forcasting models relying on a mix of statistical and neural network based models. G-BiTS expands upon N-HiTS which stands for neural basis expansion analysis for interpretable time series. N-HiTS decompose time series into a set of basis functions, capturing and interpreting temporal patterns. This paper explores ensembling methods and time series analysis.&lt;/p&gt; &lt;h2 id=&quot;related-works&quot;&gt;Related Works&lt;/h2&gt; &lt;p&gt;The main related works relate to the following topics: SARIMAX, N-HiTS, and GBM. SARIMAX stands for seasonal autoRegressive integrated moving average with exogenous variables model. Each element of the SARIMAX are all important in the following ways. AutoRegressive: captures the relationship between an observations at various lags. Integrated: the differencing of raw observations to make the time series stationary. Moving Average: the relationship between an observation and a residual error from a moving average model applied to lagged observations. Seasonal: accounts for seasonality in data, like weekly, monthly, or yearly patterns. Exogenous Variables: These are external variables or predictors that aren’t part of the time series itself but are believed to have an impact on it. This is mainly represented in time series analysis by date information with respect to variables unrelated to the power, but can be used to model a common behavior. The biggest flaw with SARIMAX comes from its inability to model more than one seasonality, hampering predictions. A more robust model is N-HiTS which stands for neural basis expansion analysis for interpretable time series forecasting. The best benefit from N-HiTS comes from its ability to learn rich embeddings for time series that properly represent all of the trends and seasonalities inherent to the data, while also producing gains through being able to apply much more data as it is made for longer range predictions. N-HiTS is good, and this paper will be exploring a multiforld extension using gradient boosting [2] and adaptive ensembling[3]. Gradient boosting generates good predictions by training decision trees sequentially. A new tree is modeled on the residual errors made by the preceding trees. Finally, tying everything all together we have wavelet transforms. Wavelets are wave-like oscillations that represent data at various scales effectively. GBMs help us take advantage of a repeated pattern of smooth behavior interrupted by sudden changes or transients in time series data.&lt;/p&gt; &lt;h2 id=&quot;g-bits&quot;&gt;G-BiTS&lt;/h2&gt; &lt;p&gt;This paper proposes a new deep learning framework powered by gradient boosting and signal pre-processing G-BiTS. G-BiTS stands for Gradient-Boosted Neural Wavlet Interpolation for Time Series. G-BiTS builds upon the success of N-HiTS and explores a question posed by the authors in the original paper on replacing the existant sequential projections from the interpolation functions onto wavelet induced spaces, getting high resolution output. G-BiTS is an ensemble model, which is where gradient boosting comes in. The maximum of the combined predictions is taken for adaptive ensembling and higher performance as well as generatily. Max can be min or mean, just depends on the use case and having higher output in this circumstance if perfered. The hope is to use the hourly modeling capabilities of light gradient boosting machines with the versatility of N-HiTS to create a robust ensemble model.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;The testing for the comparisions of the different forcasting methods is based on the BuildingsBench dataset. Specifically, this paper surveys office buildings withing the Fox subsection from the original input. The data includes buildings with energy data that has multiple seasonalities, mostly hourly, daily, weekly, and monthly. Looking at the data, there are some interesting patterns. These are the average skew and kurtosis values for the data: high skew and kurtosis.&lt;/p&gt; &lt;p&gt;Skewness: 1.1118040201238155 Kurtosis: 3.452262511716185&lt;/p&gt; &lt;p&gt;Statistical analysis also shows that the data was not drawn from a normal ditribution and is not stationary, so the variance and mean were not constant throughout the time series.&lt;/p&gt; &lt;p&gt;Our baseline is simply copying over the values from the previous week and repeating the same for the following week. Non-baseline models tested include the previously mentioned SARIMAX, N-HiTS, LGBM, and G-BiTS. The following are the respective errors from each building ordered as mean average error, root mean squared error, and mean average percent error.&lt;/p&gt; &lt;h3 id=&quot;building-id-margarita&quot;&gt;Building ID: Margarita&lt;/h3&gt; &lt;p&gt;SARIMAX (211.47498604910714, 249.84373502456708, 11.805270962305448)&lt;/p&gt; &lt;p&gt;NHITS (21.72069293617509, 27.65604571924576, 1.6335940075280377)&lt;/p&gt; &lt;p&gt;LGBM (33.16067034334621, 41.84784011583212, 2.0058567433490087)&lt;/p&gt; &lt;p&gt;GBITS (26.955107763269822, 31.504577778268615, 1.6841760555882481)&lt;/p&gt; &lt;h3 id=&quot;building-id-loreta&quot;&gt;Building ID: Loreta&lt;/h3&gt; &lt;p&gt;SARIMAX (2966.2653087797617, 3513.45974924458, 12.756417057832824)&lt;/p&gt; &lt;p&gt;NHITS (203.50202658318491, 338.92442661325015, 1.0121962487927345)&lt;/p&gt; &lt;p&gt;LGBM (419.71931531784384, 476.48902925976694, 1.8085151798175159)&lt;/p&gt; &lt;p&gt;GBITS (215.94950733822594, 264.7384239183662, 0.9401638424018465)&lt;/p&gt; &lt;h3 id=&quot;building-id-gaylord&quot;&gt;Building ID: Gaylord&lt;/h3&gt; &lt;p&gt;SARIMAX (1220.2237444196428, 1479.439585459469, 8.095511476323951)&lt;/p&gt; &lt;p&gt;NHITS (137.39752238818102, 203.64435240098928, 0.8720707702102791)&lt;/p&gt; &lt;p&gt;LGBM (347.0178199198448, 435.19043719851146, 2.3137853719619144)&lt;/p&gt; &lt;p&gt;GBITS (21.02548764010548, 27.84334532157823, .73338746467575437)&lt;/p&gt; &lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt; &lt;p&gt;Across the board, SARIMAX perfofmed the worst, followed closely by NHiTS and LGBMs. The biggest issue with SARIMAX is that it can only take a very limited amount of data, as well as being unable to model multiple seasonalities. G-BiTS showed good adaptability as one model over the large dataset was able to get transferable and adaptible embeddings. The wavelet transforms showed the greatest gains from the interpolation stage as the two level smoothing helped the N-HiTS model better fit the unstationary data. N-HiTS as expected performs well across the board too and had the best time modeling the data.&lt;/p&gt; &lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt; &lt;p&gt;There is more work to be done to extend this research topic. Mainly, finding better wavelet decompositions and symmetric recompositions for modeling multiple seasonalities faster and in a more efficient manner. The decomposition showed the biggest gain and confirms the original papers thoughts about the approach. Boosting helped standardize the model and generated really interesting embeddings through the initial wavelet based N-HiTS.&lt;/p&gt; &lt;h2 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h2&gt; &lt;p&gt;[1]&lt;/p&gt; &lt;p&gt;N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting&lt;/p&gt; &lt;p&gt;Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski&lt;/p&gt; &lt;p&gt;https://arxiv.org/abs/2201.12886&lt;/p&gt; &lt;p&gt;[2]&lt;/p&gt; &lt;p&gt;Gradient Boosting Neural Networks: GrowNet&lt;/p&gt; &lt;p&gt;Sarkhan Badirli, Xuanqing Liu, Zhengming Xing, Avradeep Bhowmik, Khoa Doan, Sathiya S. Keerthi&lt;/p&gt; &lt;p&gt;https://arxiv.org/abs/2002.07971&lt;/p&gt; &lt;p&gt;[3]&lt;/p&gt; &lt;p&gt;Adaptive Ensemble Learning: Boosting Model Performance through Intelligent Feature Fusion in Deep Neural Networks&lt;/p&gt; &lt;p&gt;Neelesh Mungoli&lt;/p&gt; &lt;p&gt;https://arxiv.org/abs/2304.02653&lt;/p&gt; </content> </entry> <entry> <title>Challenges in Deep Learning Surrogates for Constrained Linear Optimization</title> <link href="https://deep-learning-mit.github.io/blog/2023/Physics-Informed-Primal-Dual-Learning/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Physics-Informed-Primal-Dual-Learning</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Physics-informed machine learning has emerged as an important paradigm for safety-critical applications where certain constraints must be satisfied.&lt;/p&gt; &lt;p&gt;The goal of this project is to learn a deep learning surrogate for a linear programming optimization problem with hard constraints. The overall approach is inspired by standard KKT conditions. This project will attempt a different DNN approach that aims to predict basic feasible solutions (BFS), and then benchmark it against a modern optimization solver. This project will highlight challenges in designing deep learning LP surrogates.&lt;/p&gt; &lt;p&gt;Due to computing resource limits, the focus on the project will be more about broad training strategy choices (“discrete” architecture choices), instead of a systematic sweep of hyperparameters.&lt;/p&gt; &lt;h3 id=&quot;optimization-problem&quot;&gt;Optimization problem&lt;/h3&gt; &lt;p&gt;We are interested in learning to optimize this linear program with $n$ variables and $m$ equality constraints:&lt;/p&gt; \[\begin{aligned} \min \quad &amp;amp;c^T y \\ \text{s.t. } &amp;amp;Ay = b, (\lambda) \\ &amp;amp;x \geq 0 \end{aligned}\] &lt;p&gt;The KKT conditions are:&lt;/p&gt; &lt;p&gt;\(\begin{aligned} \quad Ay &amp;amp;=b, \\ A^T\lambda + s &amp;amp;= c, \\ y_i s_i &amp;amp;= 0, \forall i \in [n], \\ y, s &amp;amp;\geq 0 \end{aligned}\) &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;h1 id=&quot;literature-review&quot;&gt;Literature review&lt;/h1&gt; &lt;p&gt;Fundamental connections between deep learning and the polyhedral theory central to optimization has been noted in &lt;d-cite key=&quot;huchette2023deep&quot;&gt;&lt;/d-cite&gt;, which makes theoretical connections such as bounds on the number and shapes of linear regions expressed in a deep neural net. Beyond linear problems, &lt;d-cite key=&quot;amos2023tutorial&quot;&gt;&lt;/d-cite&gt; surveys approaches that exhibit “amortized optimization”, i.e. incurring larger upfront training times to learn parameters that (hopefully) can generalize sufficiently to novel problems within some set of specially-structured problems; and this upfront training can result in a model that may be orders of magnitude faster at inference time compared to classical (often iterative-based) models.&lt;/p&gt; &lt;p&gt;Previous literature on machine learning for linearly-constrained optimization problems could be categorized by how they manage the various components of the KKT conditions. In many of these papers, there is some common deep neural architecture at the start (e.g. FCNN or GNN); and then to attempt to recover a feasible solution, the final layers in the architecture correspond to some “repair” or “correction” layers that are informed by optimization theory.&lt;/p&gt; &lt;p&gt;&lt;b&gt;(KKT equalities + Complementarity)&lt;/b&gt;: Building on &lt;d-cite key=&quot;deka2019learning&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;pagnier2022machine&quot;&gt;&lt;/d-cite&gt;’s DNN predicts binary classifications for the active set of constraints, i.e. predicting which of $y_i$ or $s_i$ is 0. Then the remaining linear system can be solved efficiently. However, “false negatives” i.e. failure to detect a binding constraint can lead to infeasibilities (labeled as “misidentifications” in the paper), i.e. potentially violating inequality constraints.&lt;/p&gt; &lt;p&gt;&lt;b&gt;(Primal equality + Subset of primal inequalities)&lt;/b&gt;: &lt;b&gt;E2ELR&lt;/b&gt; &lt;d-cite key=&quot;chen2023end&quot;&gt;&lt;/d-cite&gt; uses &lt;i&gt;specialized repair layers&lt;/i&gt; for a single-period DC optimal power flow problem, where the differentiable repair layers guarantees power balance (an equality constraint) plus generator limits (a subset of primal inequalities); but can still violate line thermal limits (other primal inequalities). E2ELR leverages domain-specific cost structure, where electricity market penalties for line violations may be order(s) of magnitude smaller than energy balance violations; in this way this paper justifies the potential for remaining primal infeasibilities; but does not generally guarantee feasibility for all constraints.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/e2erl-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/e2erl-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/e2erl-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/e2erl.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;i&gt;Source: &lt;d-cite key=&quot;chen2023end&quot;&gt;&lt;/d-cite&gt;, showing the power balance repair layer, i.e. a linear combination of the predicted points with an inequality-feasible extreme point, in order to satisfy the equality constraint.&lt;/i&gt;&lt;/p&gt; &lt;p&gt;&lt;b&gt;(Primal equality + All primal inequalities)&lt;/b&gt;: Following a similar application in control/RL, &lt;d-cite key=&quot;li2023learning&quot;&gt;&lt;/d-cite&gt; uses a Minkowski functionals / &lt;b&gt;gauge map&lt;/b&gt; repair layer to guarantee feasibility in any general polyhedral feasible regions; however, a critical limitation of the gauge map approach is the need to calculate a strictly interior point (so as to tranform the feasible region to an &lt;i&gt;absorbing set&lt;/i&gt;) which generally may be computationally as hard as the optimization problem.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/gauge-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/gauge-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/gauge-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/gauge.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;i&gt;Source: &lt;d-cite key=&quot;li2023learning&quot;&gt;&lt;/d-cite&gt;, showing usage of the gauge mapping; note this requires identifying strictly interior points for every sample instance.&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Alternatively, “Deep Constraint Completion and Correction” &lt;b&gt;DC3&lt;/b&gt; &lt;d-cite key=&quot;donti2021dc3&quot;&gt;&lt;/d-cite&gt; enforces primal equality constraints as a differentiable layer and then embedded gradient steps as neural net layers (“gradient unrolling”) to enforce inequality at the output layers; in addition, DC3 uses inequality penalty/Lagrangian term in the loss function to predict statistically near-feasible points, in some sense warm-starting the inequality correction steps. However, other papers observe that insufficient number of descent steps in DC3 could still lead to primal infeasibility (e.g. Table 2 of &lt;d-cite key=&quot;li2023learning&quot;&gt;&lt;/d-cite&gt;).&lt;/p&gt; &lt;p&gt;To truly guarantee polyhedral constraints, &lt;d-cite key=&quot;frerix2020homogeneous&quot;&gt;&lt;/d-cite&gt; proposes using the Minkowski-Weyl theorem / double description algorithm to convert the algebraic halfspace polyhedron representation to the convex combination of vertices and extreme rays. Then the neural net is in charge of predicting the linear weights, and so the resulting convex combination must be feasible; the paper’s setting is simplified by only considering homogenous constraints $Ay=0$ i.e. a cone so only the rays are needed. However, with nonzero RHS coefficients, naturally this method will face the curse of dimensionality at higher dimensions since the number of vertices can scale exponentially. The authors acknowledge this point: “Overall, one can expect the algorithm to be efficient only for problems with a reasonably small number $m$ of inequalities and dimension $d$.”&lt;/p&gt; &lt;p&gt;&lt;b&gt;(Primal + dual approaches)&lt;/b&gt;: Previous work &lt;d-cite key=&quot;liu2022topology&quot;&gt;&lt;/d-cite&gt; has used a GNN to predict electricity prices (i.e. dual solutions), and then recover the primal solution. More recently, &lt;d-cite key=&quot;park2023self&quot;&gt;&lt;/d-cite&gt; trains two separate neural networks: a primal and a dual network to emulate the optimization iterations of an augmented Lagrangian method; one drawback to this approach is the need to tune more hyperparameters related to the outer optimization loop. Instead, &lt;d-cite key=&quot;chen2020learning&quot;&gt;&lt;/d-cite&gt; predicts one scalar value of the optimal value, and leverages the backward automatic differentiation to extract dual solution estimates from the trained neural architecture; these dual values are then used to solve the remaining system of equations to recover the full solution; &lt;d-cite key=&quot;zhang2021convex&quot;&gt;&lt;/d-cite&gt; builds on this work and enforces an &lt;i&gt;input convex neural network&lt;/i&gt; architecture since the optimal value function is convex. Still, these above approaches do not necessarily guarantee primal feasibility.&lt;/p&gt; &lt;p&gt;In a similar vein of trying to incorporate the whole primal-dual problem structure, the GNN for LP paper &lt;d-cite key=&quot;chen2022representing&quot;&gt;&lt;/d-cite&gt; provides theoretical demonstration of a universal approximation property that GNN can express LP optimal solution mappings, and also demonstrates on a small $m=10,n=50$ problem of achieving 0% error in terms of feasibility detection, as well as prediction of the optimal solution; however, this paper &lt;i&gt;does not address generalization performance&lt;/i&gt;.&lt;/p&gt; &lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt; &lt;h3 id=&quot;data-generation&quot;&gt;Data generation&lt;/h3&gt; &lt;p&gt;Since the focus is on learning LP’s generally, the dataset is fully synthetic. For this project, focus on having matrix $A$ fixed (one was created with entries drawn from the standard normal distribution), and training over different data examples of $x=(b,c)$. As an application example, this can represent learning on a fixed electric grid network topology and technology set, but learning to predict over different RHS resource capacities / renewables availabilities, and different fuel costs.&lt;/p&gt; &lt;p&gt;To ensure feasibility (primal problem is feasible and bounded), the space of examples is generated by first creating primitive or latent variables, for each of the $N$ samples (this was implemented in PyTorch to be efficiently calculated in a vectorized way):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Binary vector $\xi \in {0,1}^n$ representing the optimal LP basis, with $\sum_i \xi_i = m$; the value is drawn uniformly from the $(n \text{ C } m)$ possible combinations. Practically this was implemented as a batched permutation of an identity tensor with extra columns.&lt;/li&gt; &lt;li&gt;Nonnegative vector $d \in \mathbb{R}^n$, with each $d \sim U[0,1]$ uniformly drawn to be nonnegative.&lt;/li&gt; &lt;li&gt;Then for each element $i$, use $\xi_i$ to determine whether to assign the value of $d_i$ to either the primal variable $y_i$ or the dual slack variable $s_i$. This way complementary slackness is enforced. Namely,f \(\begin{aligned} y &amp;amp;:= d\odot\xi, \\ s &amp;amp;:= d\odot(1-\xi) \end{aligned}\)&lt;/li&gt; &lt;li&gt;Sample $\lambda \in \mathbb{R}^n, \lambda_i \sim U[0,1]$.&lt;/li&gt; &lt;li&gt;Finally construct $b=Ay, c= A^T\lambda + s&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By constructing the dataset in this way, we also know the ground truth optimal solutions (which might not be unique if there are degenerate solutions, which is assumed here to have low impact due to the random coefficients), and importantly also the optimal LP basis.&lt;/p&gt; &lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt; &lt;p&gt;As a different approach, this project will try to predict the underlying latent target $\xi$, i.e. the optimal LP basis, as a classification problem. Since there may be non-local interactions between coefficients and variables, a fully-connected architecture is chosen, where every layer is followed by a ReLU nonlinearity. The neural net forms a mapping between inputs $x=(b,c) \in \mathbb{R}^{m+n}$ to outputs $\hat{\xi} = f(x) \in {0,1}^{m}$, i.e. binary classifications of whether each variable is chosen in the LP basis. Below is an illustration of all the LP bases vectors for the $n=10, m=5$ problem size; there are $10 \text{ C } 5 = 252$ bases.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/bases-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/bases-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/bases-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/bases.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;b&gt;Supervised vs. self-supervised learning&lt;/b&gt;: Many of the referenced papers devise self-supervised training methods, which is motivated by the expensive computational costs (time) to solve the dataset instances with traditional optimization solvers. However, this synthetic dataset is somewhat of an inverse-problem approach, i.e. by starting out with a sample of assumed optimal solutions, the optimal solutions are very efficiently identified during dataset generation. This synthetic generation can also be thought of as a &lt;b&gt;data augmentation&lt;/b&gt; method.&lt;/p&gt; &lt;p&gt;Since this is binary classification, the training loss used will be binary cross entropy, which is defined in PyTorch for each sample as: \(l(\hat{\xi},\xi) = [l_1, ..., l_i, ..., l_n],\ \ l_i = \xi_i \log \hat{\xi}_i + (1-\xi_i) \log (1-\hat{\xi}_i)\)&lt;/p&gt; &lt;p&gt;A softmax layer multiplied by $m$ is optionally added at the output of the NN, to enforce the requirement that there should be $m$ basic variables (in a continuously-relaxed way).&lt;/p&gt; &lt;p&gt;&lt;b&gt;Equality completion&lt;/b&gt;: Once this is done, the LP basis uniquely determines a basic solution (but not necessarily feasible) according to \(\hat{y}^* = (A^\xi)^{-1}b,\) where $A^\xi$ is the $m\times m$ submatrix corresponding to the chosen columns. Rather than matrix inversion, this can be solved in a batched way with PyTorch (torch.linalg.solve) to obtain all samples’ solutions. The entire flow, from supervised dataset generation to neural net prediction and then $y$ solution recovery, is illustrated in the flowchart below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/flowchart-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/flowchart-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/flowchart-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/flowchart.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;As baselines, also consider the DC3 model, where novelty versus the original paper is that here both $b$ and $c$ are varied across samples (as opposed to only the RHS $b$ vectors). Also benchmark against a modern first-order based optimization solver OSQP &lt;d-cite key=&quot;stellato2020osqp&quot;&gt;. For OSQP, the LP can be easily formulated in the necessary format: $$\begin{bmatrix}b\\0\end{bmatrix}\leq \begin{bmatrix} A \\ I_{n\times n} \end{bmatrix} y \leq \begin{bmatrix}b\\ \infty_n\end{bmatrix} $$&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;All experiments are implemented on Google Colab T4 GPU instances (except OSQP which can use CPU). Neural network training is optimized with Adam.&lt;/p&gt; &lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt; &lt;h3 id=&quot;approximation-and-generalization&quot;&gt;Approximation and generalization&lt;/h3&gt; &lt;h4 id=&quot;small-scale-n4m2&quot;&gt;Small scale ($n=4,m=2$)&lt;/h4&gt; &lt;p&gt;On a small $n=4,m=2$ problem, the proposed method (using a 3-layer FCNN with width-100 hidden layers; and trained for $&amp;lt;$100 epochs) can achieve near-perfect accuracy ($&amp;gt;$0.997) in both training and testing. The training set has 10,000 samples, and the test set has 1,000 samples, both generated according to the method above. The learning rate used was $10^{-3}$.&lt;/p&gt; &lt;p&gt;The accuracies when including and excluding the softmax layer (sum to $m$) are reported in the plot below, where this layer does have some (very) small positive effect on training and testing accuracies. More importantly, the $\hat{\xi}$ predictions after the solution recovery step are all feasible, i.e. with no negative elements, and the predicted optimal solutions can be seen in the right plot to match extremely closely with the ground truth $y^*$. This latter property is a desirable feature of the proposed method, that is, once the correct basic feasible solution is predicted, then the linear equation solver will precisely recover the optimal solution.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n4m2_perfect-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n4m2_perfect-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n4m2_perfect-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n4m2_perfect.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h4 id=&quot;scaling-up-n10m5&quot;&gt;Scaling up ($n=10,m=5$)&lt;/h4&gt; &lt;p&gt;Scaling up to a still quite small problem size of $n=10,m=5$ (i.e. 6.25 times larger in terms of $A$ matrix entries), now encounters generalization issues. The same network parameter sizing and training scheme was used here. The left plot shows training accuracy reaches about 0.97 after 300 epochs (and should continue rising if allowed to continue). However, the testing accuracy plateaus at around 0.93 with no further improvement.&lt;/p&gt; &lt;p&gt;More importantly, while a $&amp;gt;$0.9 accuracy in deep learning tasks is often sufficient, in this particular context the inaccuracies can lead to optimization problem infeasibilities. This is seen in the right plot, where mis-classified $\hat{\xi}$ result in &lt;i&gt;catastrophically&lt;/i&gt; wrong $\hat{y}$ primal solution predictions (the severe orange prediction errors in both negative and positive extremes); even when the remaining correctly-predicted $\hat{\xi}$ samples receive precisely correct solutions.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n10m5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n10m5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n10m5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/n10m5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Furthermore, even though there are about $1-0.93 = 7%$ of individual $\xi_i$ entries that are mis-classified, these errors are fairly spread across various samples. This results in a &lt;u&gt;$19%$ infeasibility rate&lt;/u&gt; in the test set, i.e. $19%$ of the predicted $\hat{y}$ vectors violate the nonnegative constraint. In other words, since this particular approach is predicting every individual entry of the basis vector, even small errors for each sample can lead to the overall prediction being wrong. This disproportionate impact is intuitively explained by examining the distribution of bit-wise errors plotted below. Most samples result in 0 bits of error, and then the remaining samples mostly get 1 or 2 bits of error. This means that errors are spread out among many samples, leading to a high rate of infeasible prediction vectors.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/errors_bars-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/errors_bars-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/errors_bars-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/errors_bars.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h4 id=&quot;attempts-to-improve-accuracy&quot;&gt;Attempts to improve accuracy&lt;/h4&gt; &lt;p&gt;The previous training error plot appears to show an generalization or overfitting problem. Based on this, various data augmentation techniques were attempted, such as perturbing $b$, $c$, or both vectors (both based on random noise vectors and simple scaling invariance of $\alpha b, \beta c$ while keeping the latent $\xi$ targets; as well as generating new $\xi$ vectors after regular numbers of epochs; different schedules of the aforementioned were also tried. However, none of these attempted approaches were able to produce validation accuracy rates significantly above the original $\sim 0.93$.&lt;/p&gt; &lt;p&gt;Notably, an alternative architecture was tried: instead of outputting size-$n$ binary vectors, now try to predict multi-class classification out of the 252 basis vector classes. This actually resulted in worse testing set performance. Intuitively, treating all bases as discrete classes does not leverage the geometric proximity of 2 adjacent bases (e.g. which are off by 1 in Hamming distance).&lt;/p&gt; &lt;h3 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h3&gt; &lt;h4 id=&quot;vs-dc3-an-interior-learning-approach&quot;&gt;vs. DC3 (an “interior” learning approach)&lt;/h4&gt; &lt;p&gt;As a comparison for the $n=4,m=2$ case, the DC3 methodology was implemented using a 3-layer neural net and the self-supervised training loss of the primal objective plus infeasibility penalty, with a chosen penalty rate of 10: \(\mathcal{L} = c^T \hat{y} + 10 ||\max\{0, -\hat{y}\}||^2_2\)&lt;/p&gt; &lt;p&gt;The number of inequality correction steps during training was chosen to be $t_{train} = 10$, and to maximize the chance of feasibility a very large $t_{test} = 10,000$ was used (i.e. allow many inequality-correction gradient steps during testing inference).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;With a learning rate of $10^{-5}$, the training stabilizes after about 30 epochs. Overall, the predictions are fairly accurate in terms of the out-of-sample average objective: $-0.247$ (a 2% optimality gap versus the ground truth), and an $R^2$ of predicted objective values of 0.9992 (see middle plot). (The qualitative results were robust to faster learning rates too: A previous higher lr=$10^{-3}$ produced a tighter average objective gap, but the optimal solution deviation versus the ground truth was larger.)&lt;/p&gt; &lt;p&gt;However, despite being designed to enforce all hard constraints, the predictions still resulted in infeasible negative values (see the negative dip in the right plot). A similar disproportionate classification error to infeasibility impact is seen here (albeit to a lesser extent): $2.6%$ of all output entries are negative, while $7%$ of test samples lead to an infeasible prediction.&lt;/p&gt; &lt;p&gt;Similarly to before, inequality violations are spread out among different samples, rather than all concentrated within a few samples; this is seen in the plot below. This provides an explanatory mechanism for the relatively large infeasible rate.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3_errors-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3_errors-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3_errors-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dc3_errors.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h4 id=&quot;vs-optimization-solver&quot;&gt;vs. Optimization solver&lt;/h4&gt; &lt;p&gt;Thus far, the DNN is able to scale quite well along the number of samples dimension, but not the actual problem dimension (number of variables and constraints).&lt;/p&gt; &lt;p&gt;Return for now to the small $n=4,m=2$ case for which the DNN method achieves perfect out-of-sample testing accuracy. A next practical question is how does this method compare with “classical” optimization methods, or in what contexts would we prefer one over the other?&lt;/p&gt; &lt;p&gt;Note that there are only $4 \text{ C } 2 = 6$ bases. So once the NN produces a $\hat{\xi}$ estimate, these can be mapped to an index in ${1,2,…,6}$. All possible non-basic submatrix inverses can be pre-calculated. In total, to evaluate 1 million testing samples, the DNN predict-basis approach takes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;46 sec for training&lt;/li&gt; &lt;li&gt;0.002 sec for prediction of $10^6$ samples&lt;/li&gt; &lt;li&gt;10 sec to map $\xi$ to bases indices (&lt;i&gt;note this is not done in a fully vectorized way and potentially could be sped up&lt;/i&gt;).&lt;/li&gt; &lt;li&gt;$&amp;lt;0.001$ sec to batch matrix multiply every sample $j$’s: $(A^{\xi^j})^{-1}b^j$. Note this is done using einsum which is very efficient on CUDA.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In comparison, even when running all the 1 million problem instances fully sequentially, the OSQP solver took a total of &lt;b&gt;67 sec&lt;/b&gt;, i.e. solving about 15,000 problem instances per second.&lt;/p&gt; &lt;p&gt;This means that this DNN model here only achieved a speedup factor of about 1.2x, when &lt;i&gt;including the DNN training time&lt;/i&gt;. Furthermore, the above “mapping” step is a remaining coding bottleneck at DNN inference time, and this will scale linearly as the test sample size increases; i.e. this speedup ratio is unlikely to increase much beyond this at higher sample sizes.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dnn_timing-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dnn_timing-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dnn_timing-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-Physics-Informed-Primal-Dual-Learning/dnn_timing.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The timing tradeoff can be understood in terms of fixed vs. variable costs, as plotted here. Note the orange and red lines, representing this project’s DNN approach, is using the batched matrix solve instead of the pre-computing 6 matrix inverses (thus taking longer in the solving stage). Despite its very large speedup when only considering the prediction step, holistically the DNN approach here did not pose very significant timing advantages over the optimization solver.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;This project broadly compared 3 very different approaches to LP optimization: 1) a DNN to predict the optimal LP basis, 2) the DC3 method, and 3) optimization solver. Among the 2 deep learning methods, on the small $n=4,m=2$ problem, the LP basis method produced more robust and accurate results (i.e. it was able to perfeclty learn the input to optimal solution mapping, for the chosen data domain) compared to DC3 which already faces inequality violation issues. However, neither deep learning methods were able to easily scale to the slightly larger problem.&lt;/p&gt; &lt;p&gt;Qualitatively, the predict-LP-basis approach can result in “all-or-nothing” accuracy, i.e. predicting the correct basis vector results in the globally optimal solution, whereas even a nearby classification error can lead to catastrophic primal infeasibilities (due to enforcing the equality constraint). Moreover, in both predict-basis and DC3, inequality violations tend to be spread out among different samples, leading to disproportionate impact on the percentage of infeasible solution vector predictions.&lt;/p&gt; &lt;p&gt;Domain-specific knowledge and leveraging problem structure may be needed for tractable DNN solutions for LP optimization. This includes real-life choices of how much accuracy we need exactly in different aspects of the problem (e.g. different components of the KKT conditions).&lt;/p&gt; </content> </entry> <entry> <title>Activation Patching in Vision Transformers</title> <link href="https://deep-learning-mit.github.io/blog/2023/CNN-activation-patching/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/CNN-activation-patching</id> <content type="html">&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt; &lt;p&gt;Neural networks contain large amounts of parameters and connections that they use to model a given phenomenon. Often, the breadth and complexity of these systems make it difficult for humans to understand the mechanisms that the model uses to perform its tasks. The model is treated like a black-box. When attempting to alter the behavior of the model when it does not behave in the desired way, engineers often rely on trial-and-error tuning of hyperparameters or providing larger, more diverse datasets for training. However, it is often difficult to get representative training data. In addtion, hyperparameters can improve training but are limited in their ability to alter the innate limitations of a model.&lt;/p&gt; &lt;p&gt;Mechanistic interpretability aims to unpack the underlying logic and behaviors of neural networks. &lt;d-cite key=&quot;zhang2023best&quot;&gt;&lt;/d-cite&gt; Activation patching is an interpretability technique that replaces activations in a corrupted model with that of an uncorrupted model in order to analyze their influence on model output. When a patched activation improves model performance, it indicates that the patched activation playes a role relevant to the corrupted information. &lt;d-cite key=&quot;Vig2020InvestigatingGB&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;A better understanding of the logic within neural networks will allow for more strategic improvements to these models inspired by this newfound understanding. In additon, interpretability is the first step toward changing and correcting models. With an understanding of the underlying mechanisms comes more control of these mechanisms, which can be used to apply necessary changes for goal alignment and mitigating issues such as bias. Mechanistic interpretability plays a key role in ensuring the reliability and safety of AI systems.&lt;/p&gt; &lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt; &lt;p&gt;Pearl et al. &lt;d-cite key=&quot;10.5555/2074022.2074073&quot;&gt;&lt;/d-cite&gt; defines “causal mediation analysis” in order to analyze the effect of intermediate entities on a desired result. An application of the “indirect effect” introduced by this research is activation patching, also known as causal tracing. The indirect effect is the effect a given activation has on the output of the model. Since the activation is encompassed within the layers of a neural network, it has an indirect effect on the output. This analysis has been used in language models.&lt;d-cite key=&quot;meng2023locating&quot;&gt;&lt;/d-cite&gt; Here, the indirect effect is defined as the role of an MLP or attention layer on the output. This role is analyzed by first corrupting the outputs of the network. Then, activations from an uncorrupted run of the model can be iteratively patched into the corrupted run in order to determine which activations can best restore the uncorrupted outputs. The activations with the most significant restorative impact have the highest indirect effect.&lt;/p&gt; &lt;p&gt;For example, if the hidden state for a given attention head in a language model with prompt “The Eiffel Tower is in” is patched into that of a prompt “The Colosseum is in” and successfully changes the output from “Rome” to “Paris”, this indicates that the patched head contains knowledge about the Eiffel Tower. &lt;d-cite key=&quot;meng2023locating&quot;&gt;&lt;/d-cite&gt; The figure below depicts this process of patching from a clean to corrupt run.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/patch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/patch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/patch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/patch.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;d-cite key=&quot;meng2023locating&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;Meng et al. also provides an example of how interpretability can open opportunities for model editing. &lt;d-cite key=&quot;meng2023locating&quot;&gt;&lt;/d-cite&gt; Their research revealed that MLP layers in the middle of a GPT network had a large influence on the final predicted token from the model. This indicated that the model stored knowledge about the subject of the prompt within these layers. With the understanding of where knowledge of facts is stored within the model MLPs, these layers were then used to edit the knowledge of the language model in a way that is generalizable to other phrases that applied this knowledge. The study revealed the utility of interpretability projects in improving model understanding of the problem at hand.&lt;/p&gt; &lt;p&gt;Activation patching has been used for language models, which rely on a transformer architecture. Vision transformers &lt;d-cite key=&quot;dosovitskiy2021image&quot;&gt;&lt;/d-cite&gt; take advantage of the transformer architecture to perform common computer vision tasks such as image classification. These transformers use attention to glean valuable context about a given patch in an image, a task that a convolutional neural network has difficulty with due to the independent nature of its receptive fields. &lt;d-cite key=&quot;Torralba_Isola_Freeman_2023&quot;&gt;&lt;/d-cite&gt; Through the use of multi-headed attention, vision transformers can focus on just the parts of the image that are relevant to the task at hand, and they do so with a global understanding of relevance across the entire image. The attention heads learn how to find relevant patches, or tokens, in image for a given query. However, research regarding what exactly these heads “pay attention” to is still ongoing. &lt;d-cite key=&quot;Gandelsman2023interpreting&quot;&gt;&lt;/d-cite&gt; The strategies of activation patching used for language transformers can therefore apply in the context of vision transfomers due to the similarity in architecture and need to address questions of interpretability. Palit et al. performed a similar causal tracing analysis to that of the language model study except with a focus on BLIP, a multi-modal model that can answer questions about a given image. This investigation showed how activation patching can be performed on images along with language rather than language alone.&lt;d-cite key=&quot;palit2023visionlanguage&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt; &lt;p&gt;The model that was used for this investigation was a vision transformer that was fine-tuned for the CIFAR10 dataset, a dataset that is often used to train image classification models. The pretrained model that was used, which can be found &lt;a href=&quot;https://huggingface.co/aaraki/vit-base-patch16-224-in21k-finetuned-cifar10&quot;&gt;here&lt;/a&gt;, often fails to classify images in the dataset if they are converted to grayscale. For example, the model classifies the image of a deer below as a cat.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/image-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/image-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/image-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/image.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/gray-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/gray-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/gray-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/gray.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;!-- &lt;img src=&quot;assets/img/2023-11-10-CNN-activation-patching/gray.jpg&quot; alt=&quot;drawing&quot; style=&quot;width:10px;&quot;/&gt; --&gt; &lt;p&gt;In order to trace which attention heads focus on color information, a clean, corrupted, and restored run was performed with the model. A batch was created was a given image along with a grayscale version of that image. The colored image played the role of the clean run. The grayscale image is a corrupted input that hinders the model’s ability to classify the object in the image. This is reflected in the lower logits when the classifier attempts to classify the grayscale image. Even in the off chance the model is still able to classify the image correctly in the corrupted run, the logits will reflect the confidence, or lack thereof, of the model in its classification.&lt;/p&gt; &lt;p&gt;This corrupted grayscale run was the baseline in the investigation. Once this baseline was established, the restored run demonstrated the influence of a given attention head. In this run, the hidden state in a given corrupted layer was replaced with the hidden state at that layer from the clean run. A hidden state was defined as the values of the embedded tokens after passing through a given layer in the neural network. One set of restored runs only restored states for individual layers. However, as demonstrated in previous research &lt;d-cite key=&quot;meng2023locating&quot;&gt;&lt;/d-cite&gt;, a window of layers is necessary to be restored in order to have a noticeable effect on the output, as opposed to just a single layer. In this experiment, the window was 3, so the given layer as well as its adjacent layers were restored. While activation in the language setting often performs activation patching at the granularity of individual tokens, it was assumed that the relationship between token and layer in the image setting would not be as informative across a wide array of images. Language tokens have a shared meaning across different prompts, but image tokens play a different role depending on the object at hand. This information would not help in understanding the role of activations after averaging across all of the classifications of all images. So, this study was performed by corrupting all tokens in the image and restoring all of those tokens during the patching of a given hidden state.&lt;/p&gt; &lt;p&gt;This analysis was performed for 1000 images from the CIFAR10 dataset. For each image, the output of the restored run was collected and compared to that of the corrupted run. The indirect effect of a given layer was calculated by the difference in the softmax probability of the class of the image between the corrupted and patched run.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/eqn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/eqn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/eqn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/eqn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;d-cite key=&quot;meng2023locating&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;For each image, this patching process was repeated for every attention layer in the neural network. Finally, the results of activation patching were averaged together for each layer across all of the images in order to get a general sense of which layers are most pertinent for processing image color information.&lt;/p&gt; &lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt; &lt;p&gt;When single layers were patched rather than a window of layers, results matched that of Meng et al. The patching of a single activation did not have a unique effect on the output.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/single-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/single-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/single-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/single.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;From averaging the change in outputs from activation patching 1000 CIFAR10 images, results show that attention heads of most relevance to color tended to be in the middle or last layers.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/attn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/attn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/attn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/attn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Here are some examples of activation patching for individual images from the dataset. The graphs display the probability in the output for the correct class of the given image.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/deer-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/deer-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/deer-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/deer.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/car-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/car-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/car-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/car.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/plane-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/plane-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/plane-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-CNN-activation-patching/plane.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;This experiment found that in a 12-layer network with a window size of 3, attention in the fourth layer and final layers of the network had the biggest impact on predictions made by the model. In these layers, the probability of the correct class of the image had the largest change when clean hidden states were patched from these layers into the grayscale run of the vision transformer.&lt;/p&gt; &lt;p&gt;As portrayed by the tracing of individual images displayed above, not all images followed this trend exactly. The deer image, for example, had more emphasis on earlier layers and less emphasis on later layers. The automobile had a stronger influence from the attention layer 6 than that of 4. However, it was generally common for layers in the middle and end of the network to play a large role in this classification problem.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;The influence of attention heads close to the output align with the conclusions found by Palit et al. This is likely due to direct connection of final layers to the output. There is also a significant influence of middle attention heads on the output, which is some indication of the key information that is stored in these layers relevant to color. A possible explanation is that these layers are close to the input layer, which directly stores color information, while maintaining enough distance from the input to have narrowed down (attended to) which tokens are relevant to the class the image belongs to. This study provided an initial insight into how vision transformers store information about colors of an image.&lt;/p&gt; &lt;p&gt;Future investigations could include other forms of corruption to provide more information about the roles of the different attention layers in a trasformer. For example, adding noise to the image embeddings would give insight to the general importance of different layers rather than just focusing on color information. By varying the amount of noise, this corruption would allow more control on how much the output would change and possibly allow room for more significant restorative effects from patching and therefore more definitive results as to where the most influential attention heads live in vision transformers. Other methods of corruption could also explore other tasks ingrained in image classification, such as blurring for edge detection or using silhouettes and image segmentation for texture or pattern identification. In addition, performing activation patching with window sizes other than 3 could provide more context as to how important is an individual attention layer. A similar experiment should be performed on other models and datasets. A focus on different objects, larger datasets, and larger networks would help verify the role of middle and final layer attention heads indicated by this study.&lt;/p&gt; </content> </entry> <entry> <title>Transformer-Based Approaches for Hyperspectral Imagery in Remote Sensing</title> <link href="https://deep-learning-mit.github.io/blog/2023/A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Hyperspectral imaging (HSI) captures a wide spectrum of light per pixel, providing detailed information across numerous contiguous spectral bands. Unlike multispectral imaging, which only captures a few specific bands, hyperspectral imaging offers finer spectral resolution, allowing for more precise identification and analysis of materials. This capability makes it valuable in remote sensing for applications like mineral exploration, agriculture (e.g., crop health monitoring), environmental studies, and land cover classification. Each spectral band captures unique light wavelengths, enabling the identification of specific spectral signatures associated with different materials or conditions on the Earth’s surface. HSI images present unique challenges in deep learning compared to typical RGB images due to their high dimensionality. Each pixel in a hyperspectral image contains information across hundreds of spectral bands, leading to a massive increase in the data’s complexity and volume. This makes model training more computationally intensive and can lead to issues like overfitting if not handled properly. Current datasets, such as the Indian Pines or Salinas Scenes datasets, often have fewer samples compared to standard image datasets, exacerbating the difficulty in training deep learning models without overfitting. There’s also the challenge of effectively extracting and utilizing the rich spectral information in these images, which requires specialized architectures and processing techniques. However, analysis of hyperspectral data is of great importance in many practical applications, such as land cover/use classification or change and object detection and there is momentum in the field of remote sensing to embrace deep learning.&lt;/p&gt; &lt;p&gt;Traditional hyperspectral image classification (HSIC) methods, based on pattern recognition and manually designed features, struggled with spectral variability. Deep learning, particularly CNNs, brought advancements by extracting intricate spectral-spatial features, enhancing HSIC’s accuracy. Yet, CNNs have their drawbacks, such as a propensity for overfitting due to the high dimensionality of hyperspectral data and limitations imposed by their fixed-size kernel, which could obscure the classification boundary and fail to capture varying spatial relationships in the data effectively.&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/hyperbands_plot1.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;450px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;p&gt;Compared to CNNs, there is relatively little work on using vision transformers for HSI classification but they have great potential as they have been excelling at many different tasks and have great potential in the field of HSI classification. Vision transformers, inspired by the Transformer architecture initially designed for natural language processing, have gained attention for their capacity to capture intricate patterns and relationships in data. This architecture leverages self-attention mechanisms, allowing it to model long-range dependencies effectively, which can be particularly advantageous in hyperspectral data where spatial-spectral interactions are crucial. Spectral signatures play a pivotal role in HSI analysis, enabling the differentiation of materials or conditions based on their distinct spectral characteristics, a capability that conventional RGB images cannot provide. Leveraging the strengths of vision transformers to effectively capture and exploit these spectral signatures holds promise for advancing the accuracy and precision of HSI in remote sensing classification tasks.&lt;/p&gt; &lt;p&gt;As the field transitions from classical statistical methods to advanced deep learning architectures, a new dimension in HSI classification is emerging. These deep models, capable of learning representations at multiple levels of abstraction, promise to unearth complex patterns within hyperspectral data, potentially surpassing traditional techniques.&lt;/p&gt; &lt;h3 id=&quot;spectral-feature-based-methods-and-spatialspectral-feature-based-methods&quot;&gt;Spectral Feature-Based Methods and Spatial–Spectral Feature-Based Methods&lt;/h3&gt; &lt;p&gt;Spectral feature-based approaches classify hyperspectral images (HSIs) by analyzing each spectral pixel vector individually. However, this method has limitations as it overlooks the spatial context of the pixels. Spatial–spectral feature-based methods on the other hand, consider both the spectral and spatial characteristics of HSIs in a more integrated manner. These methods involve using a patch that includes the target pixel and its neighboring pixels, instead of just the individual pixel, to extract spatial–spectral features. Among these methods, convolutional neural networks (CNNs) are particularly prominent, having shown significant effectiveness in HSI classification. Despite the success of CNN-based models in classifying HSIs, they are not without issues. The CNN’s receptive field is limited by the small size of its convolutional kernels, such as 3×3 or 5×5, which makes it challenging to model the long-range dependencies and global information in HSIs. Additionally, the complexity of convolution operations makes it difficult to emphasize the varying importance of different spectral features.&lt;/p&gt; &lt;p&gt;When comparing spectral feature-based methods with spatial–spectral feature-based methods in hyperspectral image (HSI) classification, each has distinct advantages and applications. Spectral feature-based methods are valued for their simplicity and efficiency, especially effective in scenarios where unique spectral signatures are key, such as in material identification or pollution monitoring. They require less computational power, making them suitable for resource-limited applications. Alternatively, spatial–spectral feature-based methods offer a more comprehensive approach by integrating both spectral and spatial information, leading to higher accuracy in complex scenes. This makes them ideal for detailed land cover classification, urban planning, and military surveillance where spatial context is crucial. Among spatial–spectral methods, convolutional neural networks (CNNs) stand out for their advanced feature extraction capabilities and adaptability, making them useful in a variety of applications, from automatic target recognition to medical imaging. Although, they face challenges such as the need for large datasets and difficulties in capturing long-range spatial dependencies. While spectral methods are efficient and effective in specific contexts, spatial–spectral methods, particularly those using CNNs, offer greater versatility and accuracy at the cost of increased computational complexity.&lt;/p&gt; &lt;h3 id=&quot;hyperspectral-image-classification&quot;&gt;Hyperspectral Image Classification&lt;/h3&gt; &lt;p&gt;In the landscape of hyperspectral image (HSI) classification, a breadth of methodologies has been explored. Foundational techniques such as the k-nearest neighbor and Bayesian classifiers laid the groundwork for statistical approaches to HSI classification. Techniques aimed at class prediction, like multinomial logistic regression, and the support vector machine (SVM) framework, have been instrumental due to their robustness in high-dimensional spaces. Alongside these classifiers, dimensionality reduction techniques, notably principal component analysis (PCA), independent component analysis (ICA), and linear discriminant analysis (LDA), have been pivotal in distilling relevant spectral information from the vast data channels inherent in HSIs.&lt;/p&gt; &lt;p&gt;Despite the efficacy of these methods in spectral analysis, they have often underutilized the spatial information that is equally critical in HSIs. Spatial correlations among pixels carry significant information about the structure and distribution of the materials imaged. To tap into this spatial richness, the use of mathematical morphological operations, such as morphological profiles and their extended versions, have been developed, enriching the feature space with spatial context.&lt;/p&gt; &lt;p&gt;Advanced techniques have also been employed to harness the spatial-spectral synergy more effectively. For instance, approaches that create kernels based on adjacent superpixels or utilize hypergraphs for feature extraction acknowledge the inter-pixel relationships and the non-linear distribution of HSI data. These methods have aimed to achieve a more holistic analysis by integrating spatial contiguity with spectral data, leading to enhanced classification accuracy.&lt;/p&gt; &lt;p&gt;However, there exists a significant opportunity in the realm of deep learning, which has not yet been fully explored in these classical methods. Deep learning architectures, particularly those utilizing layers to hierarchically extract features, could offer a new dimension to HSI classification. By learning representations at multiple levels of abstraction, deep models have the potential to unearth intricate patterns in both the spectral and spatial domains. Such models could extend beyond the capabilities of traditional machine learning techniques, offering a transformative approach to HSI classification that leverages the depth and complexity of hyperspectral data to its fullest extent.&lt;/p&gt; &lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt; &lt;p&gt;&lt;u&gt;Salinas Dataset:&lt;/u&gt;&lt;/p&gt; &lt;p&gt;The Salinas dataset was captured in 1998 by the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) sensor. It includes a spectrum of 224 bands, spanning wavelengths from 400 to 2500 nanometers. For the purposes of analysis, the dataset has been refined to 204 bands, excluding bands affected by water absorption. The Salinas scene measures 512 pixels in height and 217 pixels in width, offering a detailed view of the area. In total, it consists of 54,129 labeled samples, which are categorized into 16 distinct classes, each representing a unique object or feature on the ground.&lt;/p&gt; &lt;div style=&quot;display: flex; justify-content: space-around; align-items: center;&quot;&gt; &lt;div style=&quot;flex: 1; margin: 5px; text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_170-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_170-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_170-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_170.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt;Salinas Scene hyperspectral sample data&lt;d-cite key=&quot;noauthor_hyperspectral_nodate&quot;&gt;&lt;/d-cite&gt;&lt;/div&gt; &lt;/div&gt; &lt;div style=&quot;flex: 1; margin: 5px; text-align: center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_gt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_gt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_gt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/Salinas_gt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt;Ground truth sample in Salinas Scene&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/sainteract_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;p&gt;&lt;u&gt;Pavia University Dataset:&lt;/u&gt;&lt;/p&gt; &lt;p&gt;Captured in 2001 using the Reflective Optics System Imaging Spectrometer (ROSIS), the Pavia University dataset originally comprised 115 spectral bands in the 380 to 860 nanometer range. After removing bands with noise interference, 103 bands remain for research analysis. The spatial dimensions of this dataset are notably expansive, with a height of 610 pixels and a width of 340 pixels. The dataset is rich in diversity, encompassing a total of 42,776 labeled samples across 9 different land cover categories. Each category represents a distinct type of terrain or man-made structure within the university area.&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/puinteract_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;p&gt;Utilizing these datasets, an analysis is performed on four different models to understand what features are being captured. The first two models, 3DCNN and SpectralFormer, are based on the convolutional neural network (CNN) architecture. The third model, Spectral–Spatial Feature Tokenization Transformer (SSFTT), is a transformer-based model that utilizes a novel tokenization technique to extract spatial-spectral features. The fourth model, Group-Aware Hierarchical Transformer (GAHT), is a transformer-based model that incorporates a multi-head self-attention (MHSA) mechanism to capture global effective spatial-spectral features. The models are evaluated based on their ability to accurately classify the hyperspectral data, with a focus on their capacity to discern and classify diverse land cover types. The key metrics for this assessment include accuracy, precision, recall, and the F1 score extracted from confusion matrices. The models should accurately identify and categorize ecological features from high-resolution imagery.&lt;/p&gt; &lt;p&gt;&lt;u&gt;Three-Dimensional Convolutional Neural Network (3DCNN)&lt;/u&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;!-- $$ f(x, y, z) * g(x, y, z) = \sum_{i=-a}^{a} \sum_{j=-b}^{b} \sum_{k=-c}^{c} f(i, j, k) \, g(x-i, y-j, z-k) $$ --&gt; &lt;div style=&quot;display: flex; justify-content: space-around; align-items: center;&quot;&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The 3DCNN&lt;d-cite key=&quot;ben_hamida_3d_2018&quot;&gt;&lt;/d-cite&gt; model, designed for capturing both spatial and spectral features, has shown varying degrees of performance on the Pavia University and Salinas datasets. The confusion matrices suggest that the model is quite effective at classifying certain classes with distinctive spatial-spectral patterns, such as ‘Asphalt’ in Pavia University and ‘Fallow’ in Salinas, which are characterized by large contiguous regions or unique spectral signatures. However, the model struggles with classes that have less distinct spatial or spectral features, or where these features overlap significantly with other classes. For instance, ‘Meadows’ and ‘Gravel’ in Pavia University show considerable confusion, likely due to the similarity in their textural appearance and perhaps spectral characteristics within the urban landscape.&lt;/p&gt; &lt;p&gt;The 3DCNN model demonstrates a capacity for learning and refining its spatial feature extraction processes as evidenced by the increase in overall accuracy with more training data. This underscores its potential in addressing complex classification tasks in remote sensing. However, the observed plateau in performance enhancement, despite additional data, points to intrinsic limitations in the architecture’s ability to represent the rich spectral and spatial diversity of remote sensing datasets comprehensively. While the 3DCNN excels in detecting volumetric patterns through its three-dimensional convolutional layers, its sensitivity to fine-grained spectral details—which are often pivotal in distinguishing similar classes—may not be as pronounced. The architectural design, including dilated convolutions and pooling, effectively condenses the spectral dimension and broadens the receptive field. Nonetheless, this same design choice might inadvertently obscure subtle yet critical spectral information, complicating the differentiation of classes that share close spatial characteristics.&lt;/p&gt; &lt;p&gt;The strengths lie in its ability to process spatial information, which is beneficial for datasets like Pavia University with its urban structures. Yet, its performance on the Salinas dataset, which requires more distinct spectral discrimination, highlights the challenges 3DCNN faces with complex spectral information. To improve its classification performance, especially in spectrally complex environments, integrating spectral-focused techniques or hybrid models that combine 3D spatial processing with enhanced spectral feature improves the model.&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/pa3dcnntsne_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/sa3dcnntsne_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;p&gt;&lt;u&gt;SpectralFormer&lt;/u&gt;&lt;/p&gt; &lt;p&gt;The SpectralFormer utilizes a Transformer-based architecture with Cross-layer Adaptive Fusion (CAF) &lt;d-cite key=&quot;hong_spectralformer_2022&quot;&gt;&lt;/d-cite&gt; to integrate spatial and spectral information. Its structure is tailored to capitalize on the Transformer’s ability to handle long-range dependencies, making it particularly suited to focus on the complex spectral signatures present in hyperspectral datasets.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spectralformerarch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spectralformerarch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spectralformerarch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spectralformerarch.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;SpectralFormer Model Architecture&lt;d-cite key=&quot;hong_spectralformer_2022&quot;&gt;&lt;/d-cite&gt; &lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Both datasets indicate that SpectralFormer performs well with certain classes that have distinct spectral characteristics, as seen with ‘Asphalt’ and ‘Meadows’ in Pavia University and ‘Grapes_untrained’ and ‘Soil_vinyard_develop’ in Salinas. However, the model shows some limitations in distinguishing between classes with similar spectral profiles or when spectral features are subtle, such as ‘Bare Soil’ and ‘Bitumen’ in Pavia University, and ‘Brocoli_green_weeds_1’ and ‘Brocoli_green_weeds_2’ in Salinas. This due to the Transformer’s self-attention mechanism which, while adept at identifying dominant spectral patterns, doesn’t capture the finer differences between closely resembling classes.&lt;/p&gt; &lt;!-- $$ v_{i, j}^{\alpha, \beta, \gamma } =\Phi \left ({\!\sum _{k} \sum _{h=0}^{H_{i}-1} \sum _{w=0}^{W_{i}-1} \sum _{r=0}^{R_{i}-1} \omega _{i, j, k}^{h&apos;, w&apos;, r&apos;} v_{i-1, k}^{\alpha +h&apos;,\beta +w&apos;,\gamma +r&apos;}+b_{i, j}\!}\right) $$ --&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/puispeformertsne_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/saspeformertsne_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;p&gt;The SpectralFormer’s moderate class separation reflects its challenge in spatial-spectral feature discrimination. Although its CAF module is designed to enhance feature representation by fusing information across layers, the results suggest there might be room for improvement in its capability to discern overlapping spectral and spatial features. This is particularly evident in the clustering of urban landscape classes in the Pavia University dataset, where the architectural and natural features present similar spectral profiles but differ in their spatial arrangement. While the SpectralFormer shows promise in processing hyperspectral data with complex spectral signatures, its performance might be enhanced by further tuning to address the subtle differences between similar classes. Advancements could include integrating more specialized attention mechanisms or layer fusion techniques to refine its spatial-spectral feature extraction. Additionally, employing domain-specific augmentations or preprocessing steps to emphasize the differences between challenging classes could further bolster its discriminative power.&lt;/p&gt; &lt;div style=&quot;display: flex; justify-content: space-around; align-items: center;&quot;&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_pu_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_pu_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_pu_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_pu_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_sa_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_sa_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_sa_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/speformer_sa_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;&lt;u&gt;Spectral–Spatial Feature Tokenization Transformer&lt;/u&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spatialspectralarch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spatialspectralarch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spatialspectralarch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/spatialspectralarch.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;GAHT Model Architecture&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The Spectral–Spatial Feature Tokenization Transformer (SSFTT) &lt;d-cite key=&quot;sun_spectralspatial_2022&quot;&gt;&lt;/d-cite&gt; introduces a pioneering approach in hyperspectral image (HSI) classification by leveraging the inherent spectral continuity and variability of the data. Through Groupwise Spectral Embedding (GSE), SSFTT departs from the discrete sequentiality typical of classical transformers and instead, utilizes a grouping operation, to model feature embeddings from locally spectral profiles (or neighboring bands). The grouping redefines the spectral signature as a function of neighboring bands, illustrating a fundamental shift from bandwise to groupwise spectral embeddings. This method capitalizes on the dense sampling of spectral channels and the almost continuous nature of spectral signatures, which reflects different absorption characteristics corresponding to various wavelengths. This unique aspect of SSFTT enables capturing more physically representative features of the material under observation, providing a nuanced understanding of the spectral information and thus, offering a significant advantage in accurately classifying materials based on their spectral signatures.&lt;/p&gt; &lt;div style=&quot;display: flex; justify-content: space-around; align-items: center;&quot;&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_pu_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_pu_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_pu_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_pu_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_sa_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_sa_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_sa_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/SSFTT_sa_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;&lt;u&gt;Group-Aware Hierarchical Transformer (GAHT)&lt;/u&gt;&lt;/p&gt; &lt;p&gt;The GAHT&lt;d-cite key=&quot;mei_hyperspectral_2022&quot;&gt;&lt;/d-cite&gt; is designed to exploit spatial and spectral information from HSIs through a patch-based learning framework. The architecture incorporates a multi-head self-attention (MHSA) mechanism to capture global effective spatial-spectral features and introduces a novel Grouped Pixel Embedding (GPE) module. The model operates in three hierarchical stages to process low, middle, and high-level features, aiming to decrease the channel numbers of feature maps progressively. It processes input HSI patch cubes through a sequence of grouped pixel embedding and transformer encoder blocks across three stages, gradually reducing the dimensionality of feature maps and focusing on capturing features at different levels of abstraction. Global Average Pooling (GAP) is used to compress the spatial information, and a fully connected layer classifies the central pixel of the input HSI patch into one of N classes.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/groupedpixelghat-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/groupedpixelghat-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/groupedpixelghat-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/groupedpixelghat.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;GAHT Model Architecture&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The Grouped Pixel Embedding (GPE) module, an integral part of the Group-Aware Hierarchical Transformer (GAHT), is designed to enhance local relationships within Hyperspectral Imaging (HSI) spectral channels. It operates by dividing the channels of feature maps into non-overlapping subchannels, focusing on local relationships within the spatial-spectral features of HSIs. This division is crucial for capturing fine-grained details and local variations within hyperspectral data. Additionally, the GPE module emphasizes these local relationships in the spectral domain, complementing the global dependencies captured by the multi-head self-attention (MHSA) mechanism. This dual focus ensures that the model effectively captures both local and global spatial-spectral context, resulting in a more comprehensive feature representation. Furthermore, by extracting spatial-spectral features from these non-overlapping subchannels, the GPE module aids in capturing the unique characteristics of grouped features of feature maps. This leads to a more effective exploration of the spatial-spectral information present in hyperspectral data, ultimately enhancing classification accuracy.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/gahtarch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/gahtarch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/gahtarch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/gahtarch.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Analyzing GAHT features from the Pavia University and Salinas Scene datasets reveal the discriminative power of the feature representations learned by the model. In the Pavia University dataset, the emergence of distinct clusters for classes such as Trees, Meadows, Asphalt, and Self-Blocking Bricks suggests that the model is highly effective at distinguishing these features within the hyperspectral data. However, the observed overlap among classes like Shadows, Gravel, and Painted Metal Sheets might indicate either inherent similarities in their spectral signatures or an insufficient spatial resolution that leads to mixed pixels. Such overlap, while minor, points to the complex nature of hyperspectral imaging where even high-performing models like the one used may encounter challenges in completely separating all material types.&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/pagahttsne_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/gahttsnesa_plot.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;div style=&quot;display: flex; justify-content: space-around; align-items: center;&quot;&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_pu_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div style=&quot;flex: 1; margin: 5px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/3dcnn_sa_matrix.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt; &lt;p&gt;The spatial and spectral complexity of a dataset significantly influences the model’s feature extraction capabilities. For datasets with a strong spatial component, like Pavia University, models with robust spatial feature extraction capabilities, such as 3DCNNs, can perform well. However, for datasets with subtle spectral differences and a wide spectral range, like Salinas, models that can capture fine-grained spectral information, such as GAHT and SpectralFormer, are more advantageous and show the potential of Transformers in hyperspectral remote sensing.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/training-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/training-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/training-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/training.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Models that can adeptly handle the dataset’s complexity show tighter and more distinct clusters. It also becomes apparent that no single model outperforms the others across all scenarios; their effectiveness is context-dependent, based on the inherent characteristics of the dataset being analyzed. This suggests that a hybrid approach or ensemble models that combine spatial and spectral feature extraction strengths may be beneficial for complex datasets that present both spatial and spectral challenges. Interpreting hyperspectral imagery through deep learning unveils a landscape where every detail matters. Exploring different models – from the volumetric views of 3DCNN to the intricate layers of SpectralFormer, and from the novel perspectives of SSFTT to the group-focused insights of GAHT – showcases a diverse toolkit for dissecting the complex tapestry of land and life captured from above. The spectral and spatial richness of hyperspectral data beckons for a nuanced approach; models like GAHT and SpectralFormer excel, particularly when fine-grained spectral distinctions are key.&lt;/p&gt; &lt;p&gt;However, no single model claims universal supremacy. The choice of model is guided by the unique spectral and spatial narratives of datasets becoming more and more available. Findings hint at a future where hybrid models will be able to uncover and analyze unique patterns in landscapes. Combining the spatial strength of models like 3DCNN with the spectral sensitivity of transformers could forge analytical power, capable of capturing the subtlest changes in the environment. As we stand at the crossroads of innovation and discovery, the potential of transformers in remote sensing invites us to reimagine the boundaries of what we can see and understand.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/pavia_ghat-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/pavia_ghat-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/pavia_ghat-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/pavia_ghat.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; </content> </entry> <entry> <title>Learning Generals.io</title> <link href="https://deep-learning-mit.github.io/blog/2023/transformers-as-gamers/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/transformers-as-gamers</id> <content type="html">&lt;!-- ### TLDR: In this post, we explore . &lt;ol&gt; &lt;li&gt;f &lt;/li&gt; &lt;/ol&gt; --&gt; &lt;!-- This project took a lot more time to get off the ground than expected. My initial code was riddled with bugs largely due to the implementation of parsing out features from the game data. It was a very good lesson for applying the &quot;Hacker&apos;s Guide to Deep Learning,&quot; so I also write some of what I learned in applying deep learning to a problem. Here are my main takeaways, read more [here](#practical-learnings): &lt;ol&gt; &lt;li&gt;Take the time to &lt;/li&gt; &lt;li&gt;Take the time to &lt;/li&gt; &lt;/ol&gt; --&gt; &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://generals.io&quot;&gt;Generals.io&lt;/a&gt; is a real-time turn-based strategy game. In generals.io, two players with a “general”, denoted with a crown, spawn on a board with mountains and cities scattered. Initially, players have no knowledge of other parts of the board besides the tiles immediately surrounding their general. Armies are the main resource of the game, which generate slowly from ordinary tiles, but quickly from the general and cities. Using armies, players compete to capture terrain and cities, which also grants further vision of the board. On each turn, a player is able to click on a cell with their army and use the keyboard to move it in the four cardinal directions. The goal of the game is for the player to use their army to capture the tile of their opponent’s general.&lt;/p&gt; &lt;p&gt;A typical game state will look like the following:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals_pomdp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals_pomdp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals_pomdp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals_pomdp.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/generals.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The left image shows what the game play screen while playing from the perspective of red. Red only is able to see tiles adjacent to it, and every other tile is covered in a fog of war. The right image lifts the fog of war and shows the perspective of the spectator, and we can see where blue&apos;s general is located, as well as unclaimed cities around the edge of the map. &lt;/div&gt; &lt;p&gt;Generals.io has a modest daily player base and has had attempts to implement bots to play against humans. Currently, no bots have been able to defeat top humans consistently. The top bots, such as &lt;a href=&quot;https://github.com/EklipZgit/generals-bot&quot;&gt;this one&lt;/a&gt;, are implemented using rule-based logic. They achieve human-level performance and are able to win some games against the top 10 ranked players. Previous machine-learning based bots have attempted to use a CNN LSTM in the model architecture, such as &lt;a href=&quot;https://yilundu.github.io/2017/09/05/A3C-and-Policy-Bot-on-Generals.io.html&quot;&gt;this post by Yilun Du&lt;/a&gt;. He separately evaluates a supervised learning approach and a reinforcement learning approach. His supervised learning approach reaches a competent level of play and is able to expand while having an awareness of needing to defend. However, it is very inefficient and makes basic strategic mistakes, such as running army into cities without fully taking them. The reinforcement learning approach was trained using A3C from scratch, but it was not able to learn beyond random movements.&lt;/p&gt; &lt;p&gt;I set out to build on Yilun’s work and improve the bot’s performance, as well as explore and document what details are actually important for improvement.&lt;/p&gt; &lt;h1 id=&quot;related-work-and-why-generals&quot;&gt;Related Work and Why Generals&lt;/h1&gt; &lt;p&gt;Deep learning has already been used to conquer many games, achieving either human-level or superhuman-level performance. The pattern for most games has been to use deep reinforcement learning at enormous scale through self-play. There has been success in chess, Go&lt;d-cite key=&quot;alphazero&quot;&gt;&lt;/d-cite&gt;, Dota 2&lt;d-cite key=&quot;dota2&quot;&gt;&lt;/d-cite&gt;, and Starcraft&lt;d-cite key=&quot;star&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;!-- , Games require strong sequential decision making in order to succeed. Previous methods to play games such as Dota 2 have used LSTMs and reinforcement learning &lt;d-cite key=&quot;dota2&quot;&gt;&lt;/d-cite&gt;. Transformers have also seen success on RL baselines such as Atari&lt;d-cite key=&quot;chen2021decision&quot;&gt;&lt;/d-cite&gt;. --&gt; &lt;p&gt;While games in higher complexity have already been defeated by deep learning, the experimentation is often quite opaque, as there are too many decisions that are made to be worthy of reporting on. Furthermore, the games and methods are often way too large for a single researcher to reproduce. For example, OpenAI Five was only able to beat Dota 2 pros after training for ten months, using 770 PFlops/s-days. Generals.io allows for more accessible experimentation through its smaller size and open data pipeline for replays.&lt;/p&gt; &lt;p&gt;I think there are still insights to be gained in defeating generals.io. In particular, the game comes with a combination of challenges that aren’t clearly addressed by previous approaches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The game is requires a high degree of calculation and precision, as well as strong intuition. Similar to chess, certain parts of the game are more intuitive and positional, and certain parts require searching through possibilities to calculate precisely. In generals.io, the precision mostly comes from being maximally efficient in the opening, as well as calculating distances relative to opponents army. This would suggest that some kind of model needs to search in order to achieve superhuman performance.&lt;/li&gt; &lt;li&gt;The game is partially observable. This prevents approaches used in perfect information games such as Monte Carlo Tree Search, as we need to form belief states over the opponents state.&lt;/li&gt; &lt;li&gt;The state and action space is enormous, and it requires planning on long time horizons. Games such as poker satisfy both of the above two bullet points, but it was able to be tackled with approaches such as counterfactual regret minimization after bucketing the state and action space&lt;d-cite key=&quot;dosovitskiy2021image&quot;&gt;&lt;/d-cite&gt;. Bucketing the state and action space likely won&apos;t work for generals.io, nor will an approach like CFR work.&lt;/li&gt; &lt;/ol&gt; &lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt; &lt;p&gt;Formally, generals.io can be represented as a POMDP. The underlying state, which is the state of the whole board, can only be observed at tiles that are adjacent to tiles claimed by the player. &lt;!-- In addition, both the board state and action space are completely discrete. While the space of all possible actions throughout the game is large, only a small portion of actions is usually valid at a time: valid actions move army from a tile that is owned by the player. --&gt;&lt;/p&gt; &lt;p&gt;A wealth of data (over 500,000 games, each containing hundreds of state-action pairs) are available via human replays. We use imitation learning to try to learn from the replays. Concretely, the problem can be modeled as selecting parameters \(\theta\) of a policy \(\pi\) (a neural network) to maximize the log likelihood of the dataset \(D\):&lt;/p&gt; \[\max_\theta \sum_{(s,a)\sim D} \log \pi_\theta(a | s)\] &lt;p&gt;I used &lt;a href=&quot;https://github.com/vzhou842/generals.io-Replay-Utils&quot;&gt;existing tools&lt;/a&gt; in order to convert the replays into a json format that could then be parsed. I then adapted Yilun’s code, which no longer directly works, in order to simulate the replays to construct the dataset. To start, I only used 1000 replays of highly ranked players to construct my dataset.&lt;/p&gt; &lt;p&gt;I started mostly with Yilun’s features, with small modifications:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Channel&lt;/th&gt; &lt;th&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td&gt;friendly army values&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td&gt;enemy army values&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td&gt;boolean indicators for mountains and cities&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td&gt;unclaimed city army values&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td&gt;friendly city army values&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt; &lt;td&gt;enemy city army values&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt; &lt;td&gt;boolean indicator for mountains&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt; &lt;td&gt;boolean indicator for friendly and enemy general (if found)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt; &lt;td&gt;boolean indicator for fog of war&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt; &lt;td&gt;(turn number % 50)/50&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The features made a lot of sense to me as a generals player - it’s all the information I use to play. I removed Yilun’s last feature since a new replay standard made it impossible to compute.&lt;/p&gt; &lt;p&gt;Yilun used a CNN LSTM as his architecture. In order to keep it simple and evaluate the basic components that improve performance, I removed the memory and only used a simple fully convolutional net with 5 stacked 5x5 filters.&lt;/p&gt; &lt;p&gt;Policies were evaluated by coding a small bot in the recently released &lt;a href=&quot;https://corsaircoalition.github.io/&quot;&gt;botting framework&lt;/a&gt; for generals. The bot sampled from the policy’s distribution over legal moves. Two policies were able to go head to head through this framework, and I could queue 10 games in order to get good estimates for the relative strength between the bots.&lt;/p&gt; &lt;p&gt;I’ll now describe some of the changes I tried and give an analysis of the results of each change.&lt;/p&gt; &lt;h2 id=&quot;effects-of-more-data&quot;&gt;Effects of more data&lt;/h2&gt; &lt;p&gt;The baseline policy, trained with 1000 games, was not very successful. The bot would often move back and forth, without trying to expand or take land.&lt;/p&gt; &lt;p&gt;I wanted to first explore how the amount of data affected the policy. I took 2000 more games of high ranking players and trained the same policy on a dataset with 3000 games. I expected an improvement in the similarity of the validation and train loss. This was confirmed by the results, shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/combined-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/combined-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/combined-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/combined.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Train and validation loss curves of training with less and more data. &lt;/div&gt; &lt;p&gt;This makes sense, as adding more data is essentially a regularizer. It prevents the model from overfitting, as it needs to do well on the added data too. Furthermore, it looks like it converges faster in epoch space, but in reality it’s also going through more examples, so it trained at roughly the same speed if one were to scale the epochs by a factor of 3. The policy was also much more effective, and it did not run back and forth as much. I think this was likely due to reduced overfitting.&lt;/p&gt; &lt;p&gt;I suspect that more data would have improved the policy even more, but I didn’t go larger, as it would have broken past the limits of the infrastructure I built. In particular, the dataset consisting of 3000 games took over 4 GB of disk space. A smarter job of batching the data would have allowed me to train with more.&lt;/p&gt; &lt;h2 id=&quot;squishing-army-features&quot;&gt;Squishing army features&lt;/h2&gt; &lt;p&gt;Working with the 3000 games, I turned my attention towards improving the features. They were already pretty comprehensive, but I was skeptical of the many army features we had. In particular, all of the other features were binary. Army values ranged from 0 to hundreds. I hypothesized that the features encoding armies could lead to unstable training. Using some knowledge about the game, I thought it would make sense to use a function like a sigmoid, in order to squish large values down.&lt;/p&gt; &lt;p&gt;As a generals.io player, this made sense to me, as the difference between 1 army on a tile and 2 army on a tile is very large, but the difference between 14 and 15 army is not so large. I expected better performance due to the inductive bias I was adding to the model. However, the loss curve showed similar, slightly slower convergence to the previous experiment. The policies were about the same too.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/squish-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/squish-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/squish-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/squish.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Train and validation loss curves of training after squishing the army features. &lt;/div&gt; &lt;h2 id=&quot;deeper-network&quot;&gt;Deeper Network&lt;/h2&gt; &lt;p&gt;Motivated by the success of ResNets &lt;d-cite key=&quot;he2015deep&quot;&gt;&lt;/d-cite&gt; when CNNs were leading computer vision, I wanted to try using a deeper network with residual connections. I replaced the network with a stack of one 5x5 filter, followed by nine 3x3 filters. I added skip connections between every two layers. The performance was again about the same. I suspect that more data and/or an even deeper network is needed in order to see benefits from a deeper network.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/deep-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/deep-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/deep-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/deep.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Train and validation loss curves of training after using a deeper network with residual connections. &lt;/div&gt; &lt;h1 id=&quot;discussion-and-conclusion&quot;&gt;Discussion and Conclusion&lt;/h1&gt; &lt;p&gt;Combining all of the above leads to a decent policy with coherent strategy, shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/game.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/game.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/game.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformers-as-gamers/game.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The best performing policy I trained, defeating an unsuspecting opponent (me). Replayed at 5x speed. &lt;/div&gt; &lt;p&gt;Qualitatively, this policy is much better than Yilun’s policy. While I don’t have his to evaluate, he shows a &lt;a href=&quot;https://bot.generals.io/replays/Be0wkw2t-&quot;&gt;replay&lt;/a&gt; of its early game performance. My bot does a much better job in the early game of efficiently expanding in order to maximize growth rate. Yilun’s bot has a handle on using a large army to explore, but mine is able to collect army efficiently on turns 25-50 in order to take the opponent’s land.&lt;/p&gt; &lt;p&gt;This is interesting because my policy is actually still much simpler than Yilun’s, given he uses a LSTM. It’s possible that the training was not very stable, or it may have overfit, or he just chose a bad replay of his bot.&lt;/p&gt; &lt;h2 id=&quot;limitations-and-future-work&quot;&gt;Limitations and Future Work&lt;/h2&gt; &lt;p&gt;The bot is not competitive with any human that has played a decent amount of games. It is still pretty inefficient and makes many nonsensical moves (it moves back and forth a few times in the replay).&lt;/p&gt; &lt;p&gt;There is still a lot to try, and I’ll actually continue working on some of these ideas after the class, as it was a lot of fun. There’s a decent amount of low hanging fruit:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I noticed the bots often like to expand toward the wall. I&apos;m guessing this is because there is no information encoding the boundaries of the wall, and I just let the padding in the convolutions take care of it. Adding a special indicator would likely be helpful.&lt;/li&gt; &lt;li&gt;Use reinforcement learning for improving the policy beyond the demonstrations.&lt;/li&gt; &lt;li&gt;Train on a dataset consisting of only one or only a few players in order to reduce multimodality problems (similar style of play).&lt;/li&gt; &lt;li&gt;Adding memory to the network.&lt;/li&gt; &lt;li&gt;Trying a vision transformer&lt;d-cite key=&quot;dosovitskiy2021image&quot;&gt;&lt;/d-cite&gt;, and trying to have it attend to previous states for recurrence too.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I think achieving even higher levels of performance would require doing some form of search. From my understanding, the most similar approach would be something like MuZero&lt;d-cite key=&quot;muzero&quot;&gt;&lt;/d-cite&gt;. This would allow us to properly learn a world model despite the incomplete information, which we can then do search on.&lt;/p&gt; &lt;p&gt;Overall, I learned a ton in this project about how to apply deep learning to a new problem. I encountered many of the issues described in “Hacker’s Guide to DL” and the related readings. My biggest takeaway is to spend the time setting up the proper infrastructure. Poor infrastructure causes bugs and makes it really hard to iterate.&lt;/p&gt; &lt;!-- # Conclusion We saw some simple experiments for how . A lot of future work is needed to make a learning-based bot competitive. --&gt; &lt;!-- ## Pratical Learnings I learned a lot of tools and ways to be more productive in using deep learning for a new problem. Here are some of the ones I learned: &lt;ol&gt; &lt;li&gt; After &lt;li&gt; f &lt;/ol&gt; One issue is 1. How does the performance of CNN LSTM compare to using a transformer? 2. What properties do transformers learn when applied to sequential decision making in a game? 3. Can we learn good representations for quantities such as army counts on each tile? --&gt; </content> </entry> <entry> <title>A Comparative Study of transformer on long sequence time series data</title> <link href="https://deep-learning-mit.github.io/blog/2023/transformer-time/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/transformer-time</id> <content type="html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt; &lt;p&gt;This research means to discover the power of transformer in dealing with time series data, for instance traffic flow. Transformer with multihead self-attention mechanism is well-suited for the task like traffic prediction as it can weight the importance of various aspects in the traffic data sequence, capturing both long-term dependencies and short-term patterns. Compared to the LSTM, the transformer owns the power of parallelization, which is more efficient when facing a large dataset. And it can capture the dependencies better with long sequences. However, the transformer may have trouble dealing with the long sequence time-series data due to the heavy computation. This research compares differnt methods that make use of the information redundancy and their combination from the perspective of computational efficiency and prediction accuracy.&lt;/p&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;The time series data processing and prediction are usually conducted with RNN and LSTM. In the case of traffic prediction, CNN and GNN are combined for efficiently capturing spatial and temporal information. And LSTM is widely used as its better performance on capturing temporal dependencies. While recent studies have propsed to replace RNNs with Transformer architecture as it is more efficient and able to capture sequantial dependencies. However, the model is inapplicable when facing long sequence time-series data due to quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. &lt;d-cite key=&quot;Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;Not all time series are predictable, the ones that is feasible to be better forecasted should contain cyclic or periodic patterns. &lt;d-cite key=&quot;Zeng_Chen_Zhang_Xu_2023&quot;&gt;&lt;/d-cite&gt; It indicates that there are redundant information in the long sequence data. The coundary of the redundancy can be measured by the optimal masking ratio of using MAE to process the dataset. Natural images are more information-redundant than languages and thus the optimal masking ratio is higher. BERT&lt;d-cite key=&quot;devlin2019bert&quot;&gt;&lt;/d-cite&gt; uses a masking ratio of 15% for language, MAE&lt;d-cite key=&quot;He_2022_CVPR&quot;&gt;&lt;/d-cite&gt; uses 75% for image and the optimal ratio for video is up to 90%.&lt;d-cite key=&quot;feichtenhofer2022masked&quot;&gt;&lt;/d-cite&gt; Traffic data is potentially redundant. It contains temporal and spatial information so that neighbor sensors can provide extra information in addition to temporal consistency. We inducted that the optimal ratio for traffic data should be located between image and video. As it has multidimensional information than image and the speed captured by sensors is not as consistent as the frames in videos. We use the GRIN&lt;d-cite key=&quot;cini2022filling&quot;&gt;&lt;/d-cite&gt; model to mask the inputdata using Metr_LA dataset to test the redundancy of traffic data. The results show that it is tolerant when the masking ratio is lower than 90%. Then there is the possibility of using distilling operation to compress information, reducing computational requirement and memory usage. Similar to traffic data, most of the time series data are multivariate.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/GRIN-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/GRIN-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/GRIN-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformer-time/GRIN.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Table 1: Performance comparison with baseline models and GRIN&lt;d-cite key=&quot;cini2022filling&quot;&gt;&lt;/d-cite&gt; with various masking ratio. (by Tinus A,Jie F, Yiwei L) &lt;/div&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;p&gt;The information redundancy leads to the common solutions of using transformer to deal with long sequence time-series forecasting(LSTF) problems, where models focus more on valuable datapoints to extract time-series features. Notable models are focsing on the less explored and challenging long-term time series forecasting(LTSF) problem, include Log- Trans, Informer, Autoformer, Pyraformer, Triformer and the recent FEDformer. &lt;d-cite key=&quot;Zeng_Chen_Zhang_Xu_2023&quot;&gt;&lt;/d-cite&gt; There are several main solutions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Data decomposition&lt;/strong&gt;. Data decomposition refers to the process of breakking down a complex dataset into simpler, manageable components. Autoformer &lt;d-cite key=&quot;wu2021autoformer&quot;&gt;&lt;/d-cite&gt; first applies seasonal-trend decomposition behind each neural block, which is a standard method in time series analysis to make raw data more predictable &lt;d-cite key=&quot;cleveland1990stl&quot;&gt;&lt;/d-cite&gt;. Specifically, they use a moving average kernel on the input sequence to extract the trend-cyclical component of the time series. The difference between the original sequence and the trend component is regarded as the seasonal component. &lt;d-cite key=&quot;Zeng_Chen_Zhang_Xu_2023&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Learning time trend&lt;/strong&gt;. Positional embeddings are widely used in transformer architecture to capture spatial information. &lt;d-cite key=&quot;feichtenhofer2022masked&quot;&gt;&lt;/d-cite&gt; Moreover, additional position embeddings can help the model to understand the periodicity inherented in traffic data, which implies applying the relative or global positioin encoding interms of weeks and days. &lt;d-cite key=&quot;https://doi.org/10.1111/tgis.12644&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Distillation&lt;/strong&gt;. The Informer model applies ProbSparse self-attention mechanism to let each key to only attend to several dominant queries and then use the distilling operation to deal with the redundance. The operation privileges the superior ones with dominaitng features and make a focused self-attention feature map in the next layer, which trims the input’s time dimension.&lt;d-cite key=&quot;Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Patching&lt;/strong&gt;. As proposed in ViT&lt;d-cite key=&quot;DBLP:journals/corr/abs-2010-11929&quot;&gt;&lt;/d-cite&gt;, the patch embeddings are small segments of an input image, which transfer the 2D image to 1D sequence. Each patch contains partial information of the image and additional positional embedding helps the transformer to understand the order of a series of patch embeddings. In the case of time series, though it is 1D sequence that can be received by standard transformer, the self-attention may not efficiently capture the long dependencies and cause heavy computation. Hence, dealing with time-series data, patching is used to understand the temporal correlation between data in a time-step interval. Unlike point-wise input tokens, it enhances the locality and captures the comprehensive semantic information in different time steps by aggregating times steps into subseries-level patches. &lt;d-cite key=&quot;nie2023time&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt; &lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;We used a multivariate traffic&lt;d-footnote&gt;https://pems.dot.ca.gov/&lt;/d-footnote&gt; dataset that records the road occupancy rates from different sensors on San Francisco freeways. We selected first 100 censors as our experiment dataset.&lt;/p&gt; &lt;h3 id=&quot;experimental-settings&quot;&gt;Experimental Settings&lt;/h3&gt; &lt;p&gt;We choose two models, Informer&lt;d-cite key=&quot;Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021&quot;&gt;&lt;/d-cite&gt; and PatchTST(supervised) &lt;d-cite key=&quot;nie2023time&quot;&gt;&lt;/d-cite&gt; to test the influence of distillation, positional embeddings, patching and data decomposition. For the implementation of Informer and PatchTST, we used the code provided by the authors.&lt;d-footnote&gt;https://github.com/yuqinie98/patchtst&lt;/d-footnote&gt;. We mean to compare different methods that aim to efficiently explore on long sequence data, considering both efficiency and accuracy. This leads to a discussion about the trade off when using these models to solve real life cases and the possibility of improving or combing different methods.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/Informer-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/Informer-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/Informer-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformer-time/Informer.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1: Informer architecture.&lt;d-cite key=&quot;Zhou_Zhang_Peng_Zhang_Li_Xiong_Zhang_2021&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/PatchTST-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/PatchTST-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/PatchTST-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformer-time/PatchTST.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: PatchTST architecture.&lt;d-cite key=&quot;nie2023time&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;Setting 1. Compare efficieny and accuracy of distillation and patching. All the models are following the same setup, using 10 epochs and batch size 12 with input length \(\in\) {96,192,336,720} and predictioin length \(\in\) {96,192,336,720}. The performance and cost time is listed in the table 2.&lt;/p&gt; &lt;p&gt;Setting 2. Explore the influence of data decomposition. We slightly change the setup to compare different methods. We apply the data decomposition with PatchTST to explore the significance of these techniques.&lt;/p&gt; &lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/test1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/test1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/test1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformer-time/test1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Table 2: Setting 1. Traffic forecasting result with Informer and supervised PatchTST. Input length in {96,192,336,720} and predictioin length in {96,192,336,720}. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformer-time/1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: Setting 1. Traffic forecasting result with Informer and supervised PatchTST. Input length in {96,192,336,720} and predictioin length = 720. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/test2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/test2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transformer-time/test2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transformer-time/test2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Table 3: Setting 2.Traffic forecasting result with supervised PatchTST, with and without data decomposition. Input length = 336 and predictioin length in {96,192,336,720}. &lt;/div&gt; &lt;p&gt;Sufficiency. According to Table 2. The Informer(ProbSparse self-attention, distilling operation,positional embedding) is generally more sufficient than PatchTST(patching, positional embedding). Especially with the increase of input sequence, Informer with idstilling operation can forecast in significantly less time comparing to patching method. Across differnt prediction sequence length, PatchTST does have much difference and Informer tends to cost more time with longer prediction. According to table 3, with data decomposition, PatchTST spends more time while does not achieve significant better performance.&lt;/p&gt; &lt;p&gt;Accuracy. According to Table 2. In all scenarios, the performance of PatchTST is better than Informer considering the prediction accuracy. Along with the increase of input sequence length, PatchTST tends to have better accuracy while Informer stays stable.&lt;/p&gt; &lt;p&gt;Overall, we can induct from the design of two models about their performances. Informer is able to save more time with distilling operation and PatchTST can get better accuracy with the capture of local and global information. Though patch embeddings help the model to get better accuracy with prediction task, it achieves so at the expense of consuming significant amount of time. When the input sequence is 720, PatchTST takes more than twice as long as B.&lt;/p&gt; &lt;h2 id=&quot;conclusion-and-discussion&quot;&gt;Conclusion and Discussion&lt;/h2&gt; &lt;p&gt;Based on existing models, different measures can be combined to balance the time consumed for forecasting with the accuracy that can be achieved. Due to time constraints, this study did not have the opportunity to combine additional measures for comparison. We hope to continue the research afterward and compare these performances.&lt;/p&gt; &lt;p&gt;In addition to applying transformer architecture alone, a combination of various methods or framework may help us to benefit from the advantages of different models. The transformer-based framwork for multivariate time series representation lerning is proposed by George et al. &lt;d-cite key=&quot;DBLP:journals/corr/abs-2010-02803&quot;&gt;&lt;/d-cite&gt; The Spatial-Temporal Graph Neural Networks(STGNNs) is another widely used model in traffic prediction, which only consider short-term data. The STEP model is propsde to enhance STGNN with a scalable time series pre-training mode. In the pre-training stage. They split very long-term time series into segments and feed them into TSFormer, which is trained via the masked autoencoding strategy. And then in the forecasting stage. They enhance the downstream STGNN based on the segment-level representations of the pre-trained TSFormer.&lt;d-cite key=&quot;10.1145/3534678.3539396&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;!-- ## Citations Citations are then used in the article body with the `&lt;d-cite&gt;` tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. The citation is presented inline like this: &lt;d-cite key=&quot;gregor2015draw&quot;&gt;&lt;/d-cite&gt; (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work. *** --&gt; &lt;!-- ## Footnotes Just wrap the text you would like to show up in a footnote in a `&lt;d-footnote&gt;` tag. The number of the footnote will be automatically generated.&lt;d-footnote&gt;This will become a hoverable footnote.&lt;/d-footnote&gt; --&gt; &lt;!-- *** ## Code Blocks This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag: {% highlight c++ linenos %} &lt;br/&gt; code code code &lt;br/&gt; {% endhighlight %} The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;input a string: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;!-- ## Blockquotes &lt;blockquote&gt; We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin &lt;/blockquote&gt; *** --&gt; &lt;!-- ## Layouts The main text column is referred to as the body. It is the assumed layout of any direct descendants of the `d-article` element. &lt;div class=&quot;fake-img l-body&quot;&gt; &lt;p&gt;.l-body&lt;/p&gt; &lt;/div&gt; For images you want to display a little larger, try `.l-page`: &lt;div class=&quot;fake-img l-page&quot;&gt; &lt;p&gt;.l-page&lt;/p&gt; &lt;/div&gt; All of these have an outset variant if you want to poke out from the body text a little bit. For instance: &lt;div class=&quot;fake-img l-body-outset&quot;&gt; &lt;p&gt;.l-body-outset&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;fake-img l-page-outset&quot;&gt; &lt;p&gt;.l-page-outset&lt;/p&gt; &lt;/div&gt; Occasionally you’ll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. &lt;div class=&quot;fake-img l-screen&quot;&gt; &lt;p&gt;.l-screen&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;fake-img l-screen-inset&quot;&gt; &lt;p&gt;.l-screen-inset&lt;/p&gt; &lt;/div&gt; The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes. &lt;div class=&quot;fake-img l-gutter&quot;&gt; &lt;p&gt;.l-gutter&lt;/p&gt; &lt;/div&gt; --&gt; &lt;!-- *** ## Other Typography? Emphasis, aka italics, with *asterisks* (`*asterisks*`) or _underscores_ (`_underscores_`). Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ 1. First ordered list item 2. Another item ⋅⋅* Unordered sub-list. 1. Actual numbers don&apos;t matter, just that it&apos;s a number ⋅⋅1. Ordered sub-list 4. And another item. ⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we&apos;ll use three here to also align the raw Markdown). ⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) * Unordered list can use asterisks - Or minuses + Or pluses [I&apos;m an inline-style link](https://www.google.com) [I&apos;m an inline-style link with title](https://www.google.com &quot;Google&apos;s Homepage&quot;) [I&apos;m a reference-style link][Arbitrary case-insensitive reference text] [I&apos;m a relative reference to a repository file](../blob/master/LICENSE) [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself]. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or &lt;http://www.example.com&gt; and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com Here&apos;s our logo (hover to see the title text): Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 1&quot;) Reference-style: ![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 2&quot; Inline `code` has `back-ticks around` it. ```javascript var s = &quot;JavaScript syntax highlighting&quot;; alert(s); ``` ```python s = &quot;Python syntax highlighting&quot; print s ``` ``` No language indicated, so no syntax highlighting. But let&apos;s throw in a &lt;b&gt;tag&lt;/b&gt;. ``` Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don&apos;t need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 &gt; Blockquotes are very handy in email to emulate reply text. &gt; This line is part of the same quote. Quote break. &gt; This is a very long line that will still be quoted properly when it wraps. Oh boy let&apos;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Here&apos;s a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it&apos;s a separate line in the *same paragraph*. --&gt; </content> </entry> <entry> <title>Transfer Resistant Model Training</title> <link href="https://deep-learning-mit.github.io/blog/2023/transfer-resistant-model-training/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/transfer-resistant-model-training</id> <content type="html">&lt;h2 id=&quot;introduction-and-motivation&quot;&gt;Introduction and Motivation&lt;/h2&gt; &lt;p&gt;In transfer learning, a model is trained for a specific task and is then fine-tuned for a different task &lt;d-cite key=&quot;zhuang2020comprehensive&quot;&gt;&lt;/d-cite&gt;. In doing so, one tries to best leverage and reuse features and performance of the large pre-trained model for other tasks. Many works have focused on making transfer learning more robust and efficient. Transfer learning can be very useful for saving compute resources, time, and money.&lt;/p&gt; &lt;p&gt;In this project, we study an opposing question: how to learn model weights that classify well for one dataset but reduce learning efficiency when transferred to another. The motivation is as follows. As computational resources and capable models become more accessible, the risk of unregulated agents fine-tuning existing models increases, including for malicious tasks. Recent work has shown that previously aligned models can be compromised to produce malicious or harmful outputs &lt;d-cite key=&quot;anonymous2023shadow&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;qi2023finetuning&quot;&gt;&lt;/d-cite&gt;. This may even occur with a few adversarial examples against models specifically trained to produce safe outputs &lt;d-cite key=&quot;lermen2023lora&quot;&gt;&lt;/d-cite&gt;. Currently, risks with language models are commonly discussed. However, investigating CNNs can guide designing defenses for neural network architectures against malicious agents in general.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/setting-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/setting-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/setting-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/setting.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;To our knowledge, there exists no previous literature on learning parameters robust against transfer learning. A related field is machine unlearning. In machine unlearning, a model must forget certain pieces of data used in training &lt;d-cite key=&quot;cao2015towards&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;10.1007/s42979-023-01767-4&quot;&gt;&lt;/d-cite&gt;. However, we wish to examine methods that not only guarantee poor performance after unlearning, but also after fine-tuning on the “malicious” or “forget” dataset. For example, using a popular unlearning approach which reaches 0% accuracy on the “forget” dataset, we easily fine-tuned the model with the same dataset to reach higher accuracy after a few epochs as shown below &lt;d-cite key=&quot;tarun2023fast&quot;&gt;&lt;/d-cite&gt;. This is a gap in previous work in machine unlearning and demonstrates the novelty and difficulty of learning models that not only perform poorly on specified datasets but are robust against fine-tuning.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/machine_unlearning-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/machine_unlearning-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/machine_unlearning-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/machine_unlearning.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We propose two new approaches: selective knowledge distillation (SKD) and Reverse Model-Agnostic Meta-Learning (MAML) &lt;d-cite key=&quot;finn2017model&quot;&gt;&lt;/d-cite&gt;. In SKD, a “student” model is trained using activations of a “teacher” for the beneficial data and trained on hardcoded activations for the “malicious” data. In Reverse-MAML, we attempt to learn parameters that aren’t robust to transfer to specified tasks. Due to computational constraints, we examine a toy setting with the CIFAR-10 Dataset as well as using a small CNN model shown in the appendix &lt;d-cite key=&quot;krizhevsky2012imagenet&quot;&gt;&lt;/d-cite&gt;. Overall, both the Reverse-MAML and SKD approach exceed baseline approaches on scoring good accuracy on a “beneficial” dataset while being on-par with preventing fine-tuning on a “malicious” dataset. Thus, there remain limitations, and we conclude with future work.&lt;/p&gt; &lt;h2 id=&quot;related-works&quot;&gt;Related Works&lt;/h2&gt; &lt;h3 id=&quot;1-transfer-learning&quot;&gt;1. Transfer Learning&lt;/h3&gt; &lt;p&gt;As mentioned previously, transfer learning has been a long-time objective in deep learning research &lt;d-cite key=&quot;zhuang2020comprehensive&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;raffel2020exploring&quot;&gt;&lt;/d-cite&gt;. By training a model on one dataset, the goal is to be able to reuse parameters and learned features to achieve high performance or efficient learning for another dataset. Transfer learning for convolutional neural networks has been a popular approach, allowing users to train a high-performance model with limited computational resources or data &lt;d-cite key=&quot;zhuang2020comprehensive&quot;&gt;&lt;/d-cite&gt;. Further work has analyzed settings for successful transfer learning in image classification and further challenges when there is poor transfer &lt;d-cite key=&quot;plested2022deep&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;2-model-agnostic-meta-learning-maml&quot;&gt;2. Model-Agnostic Meta-Learning (MAML)&lt;/h3&gt; &lt;p&gt;MAML is an algorithm that makes models readily adaptable to new tasks &lt;d-cite key=&quot;finn2017model&quot;&gt;&lt;/d-cite&gt;. It essentially primes the model for transfer learning as effectively as possible. The algorithm attempts to learn parameters and model weights such that a few steps of gradient descent on learning a new task will lead to good performance on said new task. Further work has continued attempting to meta-learn useful model parameters, building off of MAML &lt;d-cite key=&quot;goerttler2021exploring&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;park2019meta&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;3-machine-unlearning&quot;&gt;3. Machine Unlearning&lt;/h3&gt; &lt;p&gt;A closely aligned question to ours is the problem of machine unlearning. Machine unlearning attempts to remove the influence of a set of data points on an already trained model. In this setting, a model is initially trained on some dataset &lt;d-cite key=&quot;bourtoule2021machine&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;cao2015towards&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;10.1007/s42979-023-01767-4&quot;&gt;&lt;/d-cite&gt;. The model embeds information about and “remembers” features about data points. This means that the model uses information about the data points to make decisions. For example, LLMs like GPT can learn sensitive information about some people &lt;d-cite key=&quot;wu2023unveiling&quot;&gt;&lt;/d-cite&gt;. This might pose a threat to privacy. We may want the model to “forget” some subset of the training set, in this case information about the people. However, we currently have no standardized method of doing this. Machine unlearning is a nascent field in artificial intelligence research and is currently being studied. It is a difficult problem, and our work is tangential to machine unlearning.&lt;/p&gt; &lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt; &lt;p&gt;To our knowledge, there hasn’t been any research on models that are resistant to transfer learning and fine-tuning. The works mentioned above, transfer learning techniques and MAML, focus on improving fine-tuning. We aim to make fine-tuning more difficult while preserving robustness on the original task. Machine unlearning seeks to forget data that the model has been previously trained on. On the other hand, our goal is to preemptively guard the model from learning certain data in the first place. Thus, our research question demonstrates a clear gap in existing research which has focused on either improving transfer learning or only reducing model performance on external datasets. Our research explores this new question in the deep learning field and draws from recent works to guide methodology.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;p&gt;We propose three methods, one existing and two novel, to begin addressing the problem of learning parameters scoring high accuracy on a “beneficial” dataset but are robust against transfer learning on a known “malicious” dataset. Further experimental details are found in the experiments section.&lt;/p&gt; &lt;h3 id=&quot;1-machine-unlearning&quot;&gt;1. Machine Unlearning&lt;/h3&gt; &lt;p&gt;The first approach is a baseline and reimplementation of a popular machine unlearning method from &lt;d-cite key=&quot;tarun2023fast&quot;&gt;&lt;/d-cite&gt;. Here, the model is initially trained on both the “beneficial” and “malicious” dataset and undergoes a forgetting stage where the “malicious” dataset is forgotten using a noise matrix. A final repair stage is then conducted to improve performance of the model on the “beneficial” dataset. Specific details can be found at &lt;d-cite key=&quot;tarun2023fast&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/performance-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/performance-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/performance-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/performance.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;2-selective-knowledge-distillation&quot;&gt;2. Selective Knowledge Distillation&lt;/h3&gt; &lt;p&gt;Our first proposed novel approach is selective knowledge distillation (SKD) drawing inspiration from knowledge distillation. In knowledge distillation, a smaller “student” model is trained to imitate a larger “teacher” model by learning logits outputs from the “teacher” model. In doing so, the “student” model can hopefully achieve similar performance to the “teacher” model while reducing model size and complexity.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In SKD, we similarly have a “teacher” and “student” model. The “teacher” is a model that has high accuracy on the “beneficial” dataset but is not necessarily robust against fine-tuning on the “malicious” dataset. Our “student” model is almost identical in architecture to the “teacher” but excludes the final classification layer and the ReLU layer before it. This is shown below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_architecture-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_architecture-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_architecture-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_architecture.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Our goal is for the student model to have high performance on the “beneficial” dataset after adding a classification layer while being robust against fine-tuning on the “malicious” dataset. To perform SKD, we initially train the teacher model until reaching sufficiently high performance on the “beneficial” dataset.&lt;/p&gt; &lt;p&gt;We then construct a dataset that contains all the images in the “beneficial” dataset. The labels are activations of the second-to-last layer of the “teacher” model. Note that this is similar to knowledge distillation, except we are taking the second-to-last layer’s activations. We further add all the images in the “malicious” dataset and set their labels to be a vector of significantly negative values. For our experiments, we used -100.0. We train the student model on this collective dataset of images and activation values.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_complex.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_complex.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_complex.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/teacher_student_complex.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Finally, we add a fully-connected classification layer to the student model and backpropagate only on the added layer with the “beneficial” dataset.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Our end goal is to prevent fine-tuning of our CNN on the “malicious” dataset. Thus, if the student model can output activations that all are negative if the image belongs in the “malicious” dataset, then after appending the ReLU layer and setting biases of the second-to-last layer to 0, the inputs to the final classification layer will always be 0, reducing the ability to learn on the “malicious” dataset. Furthermore, the gradient will always be 0 on inputs from the “malicious” dataset so any backpropagating on images and labels originating from the “malicious” dataset from the final layer activations would be useless.&lt;/p&gt; &lt;h3 id=&quot;3-reverse-maml&quot;&gt;3. Reverse-MAML&lt;/h3&gt; &lt;p&gt;Recall that MAML is focused on finding some optimal set of model weights \(\theta\) such that running gradient descent on the model from a new few-shot learning task results in a \(\theta’\) that scores high accuracy on the new task &lt;d-cite key=&quot;finn2017model&quot;&gt;&lt;/d-cite&gt;. MAML achieves this by learning the optimal \(\theta\). To learn this \(\theta\), MAML computes the second order gradient on the model weights. This allows the model to learn about where the initial \(\theta\) should have been before an iteration of gradient descent so that taking the step of gradient descent would have led to the minimal loss.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/MAML-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/MAML-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/MAML-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/MAML.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In our version, we attempt to learn a \(\theta\) that fine-tunes well to a data distribution \(p_1\) but fine-tunes poorly to distribution \(p_2\). To do this, we partition the data into two sets: a “good” set and a “bad” set. We train such that for “good” samples MAML performs the standard algorithm above, learning \(\theta\) that would fine-tune well to the “good” samples. However, for the “bad” set we train the model to do the opposite, learning a \(\theta\) that would lead to poor fine-tuning. To do this, when taking the second order gradient, the model goes up the gradient instead of down.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/reverse_MAML-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/reverse_MAML-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/reverse_MAML-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/reverse_MAML.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;Due to computational constraints, we work in the following toy setting. We use the CIFAR-10 dataset where images in the first five ([0, 4]) classes are the “beneficial” dataset and the images in the last five ([5, 9]) classes are the “malicious” dataset. We split the 60,000 CIFAR-10 image dataset into a 40,000 image pre-training dataset, 10,000 image fine-tuning dataset, and 10,000 image test dataset. To evaluate each approach, we first evaluate the accuracy of the model on the beneficial test dataset. Then, we replace the last layer parameters of the output model, freeze all previous layer’s parameters, and finally fine-tune on the malicious fine-tuning dataset. We fine-tune using the Adam optimizer with a learning rate of 0.1 and momentum of 0.9. We finally evaluate model performance on a malicious test dataset. These steps in this evaluation represent the common pipeline to perform transfer learning and are shown below. Full hyperparameters for evaluation are listed in the appendix. We also perform ablation studies on the quality of the teacher model for SKD; further details are found in the Discussion section. All experiments, including ablations, are performed and averaged over 5 random seeds.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/pipeline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/pipeline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/pipeline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/pipeline.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/evaluation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/evaluation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/evaluation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/evaluation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;The first evaluation metric is accuracy of the outputted model from each approach on beneficial data. This is shown in the figure below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/beneficial_accuracy-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/beneficial_accuracy-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/beneficial_accuracy-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/beneficial_accuracy.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1 &lt;/div&gt; &lt;p&gt;The second metric of evaluation is the accuracy of the output model from each approach on test malicious data as it’s being fine-tuned on fine-tune malicious data. This is shown with learning curves in the figure below. Note that lower accuracy is better.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/malicious_accuracy-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/malicious_accuracy-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/malicious_accuracy-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/malicious_accuracy.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2 &lt;/div&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;We observe that finding parameters that have high accuracy on a “beneficial” dataset but are robust against fine-tuning on a “malicious” dataset is challenging. On all three methods, including a popular machine unlearning approach, the model is able to somewhat fit to the “malicious” dataset. However, for SKD, this accuracy consistently does not significantly exceed 40%.&lt;/p&gt; &lt;p&gt;More importantly, we find in Figure 1 that both Reverse-MAML and SKD are able score higher accuracy on the beneficial dataset. This is surprising as machine unlearning methods were designed to maintain high accuracy on a retain dataset. Combining these two graphs, we conclude that there remains future work to explain why the resulting models had such high accuracy on the malicious data out-of-the-box and how to minimize it.&lt;/p&gt; &lt;p&gt;We also experimented with Reverse-MAML under the Omniglot dataset &lt;d-cite key=&quot;lake2015human&quot;&gt;&lt;/d-cite&gt;. Here, we attempted to fine-tune on digit images. We found that Reverse-MAML performed very well in this setting. After training the Reverse-MAML model, the model held around 85% test accuracy on the “Beneficial” Omniglot dataset and around 20% on the “Malicious” digit dataset. On the digit set, the model would often predict the same digit for all samples, as shown below. We believe that Reverse-MAML performed better here because the Omniglot characters and the digits are simpler to interpret and learn specific features about compared to CIFAR-10.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/digits-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/digits-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/digits-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/digits.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; All digits were predicted to be a 2. &lt;/div&gt; &lt;p&gt;Slow learning in SKD is likely caused by filtering by the ReLU activation function which causes activations to become 0. This ideally occurs when we train the student model to output negative activation values into the final classification layer if the input is from the “malicious” dataset. These values make it more difficult to learn useful weights for the final classification layer and apply gradient descent on earlier layers. We confirm this by measuring misses or the percent of “malicious” images that don’t result in all 0 activations into the final classification layer shown below. We show, in general, misses are low across different teacher models. For this ablation, we vary teacher models by the number of epochs they are trained.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student_table-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student_table-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student_table-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/student_table.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We also measure how accuracy of the teacher model impacts performance of the student downstream. We vary the number of epochs the teacher model is trained in and report accuracies of the teacher model on the “beneficial” dataset below. More importantly, we empirically show that high teacher accuracy on the “beneficial” dataset is needed for the student to achieve high accuracy on the “beneficial” dataset. This follows our knowledge distillation framework as the student attempts to mimic the teacher model’s performance on the “beneficial” dataset by learning activation values.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/error_bounds-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/error_bounds-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/error_bounds-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/error_bounds.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;limitations-and-future-work&quot;&gt;Limitations and Future Work&lt;/h2&gt; &lt;h3 id=&quot;1-requirement-for-malicious-data&quot;&gt;1. Requirement for “Malicious” data&lt;/h3&gt; &lt;p&gt;The motivating example for this project was preventing a malicious agent from hijacking a model to perform undesirable tasks. However, it is often not possible to list out every possible “bad” task, and thus future work which extends from this project can explore how to prevent fine-tuning of tasks that aren’t specified as clearly and completely.&lt;/p&gt; &lt;h3 id=&quot;2-computational-restraints&quot;&gt;2. Computational Restraints&lt;/h3&gt; &lt;p&gt;Due to computational restraints, we were unable to test or fine-tune models with significantly higher parameter counts or experiment with larger datasets. However, this remains an important step as transfer learning or fine-tuning is commonly applied on large models which we could not sufficiently investigate. Thus, future work can apply these existing methods on larger models and datasets.&lt;/p&gt; &lt;h3 id=&quot;3-exploration-of-more-methods-in-machine-unlearning-and-meta-learning&quot;&gt;3. Exploration of More Methods in Machine Unlearning and Meta-Learning&lt;/h3&gt; &lt;p&gt;Further analysis of existing methods in machine unlearning and meta-learning can be used to benchmark our proposed approaches. Though we tried to select methods that had significant impact and success in their respective problem settings, other approaches are promising, including using MAML variants like Reptile or FOMAML &lt;d-cite key=&quot;DBLP:journals/corr/abs-1803-02999&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;4-imperfection-in-filtering-malicious-data-for-skd&quot;&gt;4. Imperfection in filtering “malicious” data for SKD&lt;/h3&gt; &lt;p&gt;Ideally, in SKD, the underlying model would always output negative activation values given a “malicious” input. However, this does not always occur, and thus fitting on the malicious data is still possible. Future work can explore how to improve this, though perfect accuracy will likely not be feasible. Furthermore, it is still possible for a malicious agent to hijack the model by performing distilled learning on the second-to-last layer activations, thus removing this ideal guarantee. Future work can also investigate how to have similar guarantees throughout all of the model’s activation layers instead of just one.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this project, we investigated how to train a model such that it performs well on a “beneficial” dataset but is robust against transfer learning on a “malicious” dataset. First, we show this is a challenging problem, as existing state of the art methods in machine unlearning are unable to prevent fine-tuning. We then propose two new approaches: Reverse-MAML and SKD. Both serve as a proof of concept with promising preliminary results on the CIFAR-10 Dataset. We conclude by noting there are limitations to this work, most notably the need for a “malicious” dataset and computational limits. We then propose future work stemming from these experiments.&lt;/p&gt; &lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt; &lt;p&gt;CNN Architectures used for experiments:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/CNN_architectures-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/CNN_architectures-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/CNN_architectures-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-transfer-resistant-model-training/CNN_architectures.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;ul&gt; &lt;li&gt;Note, all graphs and tables are averaged over 5 seeds with reported standard deviation.&lt;/li&gt; &lt;/ul&gt; </content> </entry> <entry> <title>Sparse Autoencoders for a More Interpretable RLHF</title> <link href="https://deep-learning-mit.github.io/blog/2023/sparse-autoencoders-for-interpretable-rlhf/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/sparse-autoencoders-for-interpretable-rlhf</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Understanding how machine learning models arrive at the answers they do, known as &lt;em&gt;machine learning interpretability&lt;/em&gt;, is becoming increasingly important as models are deployed more widely and in high-stakes scenarios. Without interpretability, models may exhibit bias, toxicity, hallucinations, dishonesty, or malice, without their users or their creators knowing. But machine learning models are notoriously difficult to interpret. Adding to the challenge, the most widely used method for aligning language models with human preferences, RLHF (Reinforcement Learning from Human Feedback), impacts model cognition in ways that researchers do not understand. In this work, inspired by recent advances in sparse autoencoders from Anthropic, we investigate how sparse autoencoders can help to interpret large language models. We contribute a novel, more interpretable form of fine-tuning that only learns parameters related to interpretable features of the sparse autoencoder.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/interpretability-hard-cartoon-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/interpretability-hard-cartoon-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/interpretability-hard-cartoon-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/interpretability-hard-cartoon.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Machine learning practitioners often cannot interpret the models they build (xkcd #1838). &lt;/div&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;Research on interpreting machine learning models falls broadly under one of two areas: representation-based interpretability (top-down) and mechanistic interpretability (bottom-up).&lt;/p&gt; &lt;p&gt;Representation-based interpretability seeks to map out meaningful directions in the representation space of models. For example, Li &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;li2023inferencetime&quot;&gt;&lt;/d-cite&gt; found a direction in one model that causally corresponds to truthfulness. Subsequent work by Zou &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;zou2023representation&quot;&gt;&lt;/d-cite&gt; borrows from neuroscience methods to find directions for hallucination, honesty, power, and morality, in addition to several others. But directions in representation space can prove brittle. As Marks &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;marks2023geometry&quot;&gt;&lt;/d-cite&gt; found, truthfulness directions for the same model can vary across datasets. Moreover, current methods for extracting representation space directions largely rely on probing &lt;d-cite key=&quot;belinkov2022probing&quot;&gt;&lt;/d-cite&gt; and the linearity hypothesis &lt;d-cite key=&quot;elhage2022superposition&quot;&gt;&lt;/d-cite&gt;, but models may have an incentive to store some information in nonlinear ways. For example, Gurnee &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;gurnee2023language&quot;&gt;&lt;/d-cite&gt; showed that language models represent time and space using internal world models; for a world model to store physical scales ranging from the size of the sun to the size of an electron, it may prefer a logarithmic representation.&lt;/p&gt; &lt;p&gt;Mechanistic interpretability, unlike representation engineering, studies individual neurons, layers, and circuits, seeking to map out model reasoning at a granular level. One challenge is that individual neurons often fire in response to many unrelated features, a phenomenon known as polysemanticity. For example, Olah &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;olah2020zoom&quot;&gt;&lt;/d-cite&gt; found polysemantic neurons in vision models, including one that fires on both cat legs and car fronts. Olah &lt;em&gt;et al.&lt;/em&gt; hypothesized that polysemanticity arises due to superposition, which is when the model attempts to learn more features than it has dimensions. Subsequent work investigated superposition in toy models, suggesting paths toward disentangling superposition in real models &lt;d-cite key=&quot;elhage2022superposition&quot;&gt;&lt;/d-cite&gt;. Superposition is relevant for language models because the real world has billions of features that a model could learn (names, places, facts, etc.), while highly deployed models have many fewer hidden dimensions, such as 12,288 for GPT-3 &lt;d-cite key=&quot;brown2020fewshot&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Recently, Sharkey &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;sharkey2022interim&quot;&gt;&lt;/d-cite&gt; proposed using sparse autoencoders to pull features out of superposition. In an interim research report, the team describes inserting a sparse autoencoder, which expands dimensionality, into the residual stream of a transformer layer. In a follow-up work, Cunningham &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;cunningham2023sparse&quot;&gt;&lt;/d-cite&gt; found that sparse autoencoders learn highly interpretable features in language models. In a study on one-layer transformers, Anthropic provided further evidence that sparse autoencoders can tease interpretable features out of superposition &lt;d-cite key=&quot;bricken2023monosemanticity&quot;&gt;&lt;/d-cite&gt;. Although interest in sparse autoencoders in machine learning is relatively recent, sparse autoencoders have been studied in neuroscience for many decades under the name of expansion recoding &lt;d-cite key=&quot;albus1971cerebellar&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Researchers have begun to apply sparse autoencoders to other interpretability problems. For example, Marks &lt;em&gt;et al.&lt;/em&gt; &lt;d-cite key=&quot;marks2023rlhf&quot;&gt;&lt;/d-cite&gt; investigated whether models on which we perform RLHF internalize the reward signal. To do so, Marks compared sparse autoencoders trained on the base model with sparse autoencoders trained on the fine-tuned model. But, to our knowledge, while others have used sparse autoencoders to probe the effects of fine-tuning, there is no prior research on using sparse autoencoders to define a more interpretable form of fine-tuning. We propose a new form of fine-tuning in which the learnable parameters are related to the interpretable features of the sparse autoencoder.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;An &lt;strong&gt;autoencoder&lt;/strong&gt; is an architecture for reproducing input data, with a dimensionality bottleneck. Let $d_\text{model}$ denote the dimension of the residual stream in a transformer (4096 for Pythia 6.9B). Let $d_\text{auto}$ denote the dimensionality of the autoencoder. To enforce the dimensionality bottleneck, we require $d_\text{model} &amp;gt; d_\text{auto}$. The diagram below depicts an autoencoder.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/autoencoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/autoencoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/autoencoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/autoencoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; An autoencoder is trained to reproduce its input, subject to a dimensionality bottleneck. &lt;/div&gt; &lt;p&gt;A &lt;strong&gt;sparse autoencoder&lt;/strong&gt; relies on a different kind of bottleneck, called sparsity. For a sparse autoencoder $g \circ f$ that acts on $x \in \mathbb{R}^{d_\text{model}}$ by sending $f(x) \in \mathbb{R}^{d_\text{auto}}$ and $g(f(x)) \in \mathbb{R}^{d_\text{model}}$, the training objective combines MSE loss with an $L^1$ sparsity penalty:&lt;/p&gt; \[\mathcal{L}(x; f, g) = \|x - g(f(x))\|_2^2 + \beta \| f(x) \|_1,\] &lt;p&gt;where $\beta &amp;gt; 0$ trades off sparsity loss with reconstruction loss. With the sparsity constraint, we can now let $d_\text{auto} &amp;gt; d_\text{model}$ by a factor known as the &lt;em&gt;expansion factor&lt;/em&gt;. In our work, we typically use an expansion factor of $4$ or $8$. The purpose of the sparse autoencoder is to expand out the dimension enough to overcome superposition. The diagram below depicts a sparse autoencoder.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sparse-autoencoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sparse-autoencoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sparse-autoencoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sparse-autoencoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; A sparse autoencoder is trained to reproduce its input, subject to an $L^1$ sparsity bottleneck. &lt;/div&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;p&gt;Our main experiment is to insert a sparse autoencoder into a transformer layer, train the sparse autoencoder, and then use the fused model to perform a new, more interpretable form of fine-tuning. &lt;d-footnote&gt;While we originally planned to investigate RLHF, we determined that existing libraries could not perform PPO (Proximal Policy Optimization) on custom model architectures such as our transformer fused with a sparse autoencoder. As a result, we chose to investigate fine-tuning instead of RLHF.&lt;/d-footnote&gt; We run all experiments on a single A100 GPU through Google Colab Pro+.&lt;/p&gt; &lt;h3 id=&quot;inserting-a-sparse-autoencoder-in-a-transformer&quot;&gt;Inserting a Sparse Autoencoder in a Transformer&lt;/h3&gt; &lt;p&gt;There are three natural places to insert a sparse autoencoder into a transformer:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MLP activations before the nonlinearity&lt;/li&gt; &lt;li&gt;MLP activations before adding back to the residual stream&lt;/li&gt; &lt;li&gt;The residual stream directly&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We choose the second option. The upside of operating in the MLP space is that MLP blocks may be in less superposition than the residual stream, given that MLPs may perform more isolated operations on residual stream subspaces. The upside of operating after the MLP projects down to the residual stream dimension is a matter of economy: because $d_\text{model} &amp;lt; d_\text{MLP}$, we can afford a larger expansion factor with the same memory resources.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/transformer-with-sae-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/transformer-with-sae-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/transformer-with-sae-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/transformer-with-sae.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; We insert a sparse autoencoder into a transformer after the MLP, but before adding into the residual stream. &lt;/div&gt; &lt;h3 id=&quot;how-we-train-our-sparse-autoencoder&quot;&gt;How We Train our Sparse Autoencoder&lt;/h3&gt; &lt;p&gt;We train our sparse autoencoder to reproduce MLP-post activations in layer one of Pythia 6.9B (deduplicated) &lt;d-footnote&gt;Deduplicated means that this Pythia 6.9B model was trained on scraped web text where duplicate articles and lengthy passages are removed. Because Pythia inherits from the GPT-NeoX architecture, the specific activations we collected are named gpt_neox.layers.1.mlp.dense_4h_to_h.&lt;/d-footnote&gt;. To create a dataset of activations for training, we stream in text from &lt;a href=&quot;https://huggingface.co/datasets/Skylion007/openwebtext&quot;&gt;an open-source replication of WebText&lt;/a&gt;, the dataset used to train GPT-2. For each batch of text, we collect Pythia 6.9B’s MLP-post activations at layer one and use these activations as training data for the sparse autoencoder.&lt;/p&gt; &lt;p&gt;Concretely, our sparse autoencoder has four learnable parameters: $W_\text{enc}$, $W_\text{dec}$, $b_\text{enc}$, and $b_\text{dec}$. The second bias $b_\text{dec}$ is used to center the input. The sparse autoencoder encodes, applies a nonlinearity, and decodes its input $x$ as follows:&lt;/p&gt; \[\text{SAE}(x) = \text{ReLU}((x - b_\text{dec}) W_\text{enc} + b_\text{enc}) W_\text{dec} + b_\text{dec}.\] &lt;p&gt;We constrain the rows of $W_\text{dec}$ to have unit norm by renormalizing after each optimizer step. Another approach to constrain the rows is to remove gradient information parallel to the feature vectors before each optimizer step, and also renormalize the rows. Although we did not implement it, Anthropic found that that the second approach &lt;a href=&quot;https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization&quot;&gt;slightly reduces loss&lt;/a&gt; &lt;d-cite key=&quot;bricken2023monosemanticity&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;We use an expansion factor of $4$, meaning $d_\text{auto} = 16384$. When training, we use batch size $8$, learning rate $10^{-4}$, and default $\beta_1 = 0.9, \beta_2 = 0.999$ for the Adam optimizer. Because Pythia 6.9B’s context length is $128$ tokens, each training step includes activations from $1024$ tokens. We save checkpoints every $20000$ steps ($20.48$ million tokens).&lt;/p&gt; &lt;p&gt;One subtlety in training is that the sparsity constraint can eventually cause some autoencoder neurons to never activate. How to best handle these so-called dead neurons is an open question. We follow Anthropic in &lt;a href=&quot;https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling&quot;&gt;resampling dead neurons&lt;/a&gt; to new values &lt;d-cite key=&quot;bricken2023monosemanticity&quot;&gt;&lt;/d-cite&gt;. Because resampling can cause instability during training, we resample only every 10000 training steps. At that point, we say a sparse autoencoder neuron is dead if it has not activated in any of the last 5000 training steps. In an attempt to improve autoencoder performance, Anthropic resampled dead neurons to the feature directions in which the sparse autoencoder performed worst. For simplicity, we resample dead neurons by setting their corresponding rows of $W_\text{enc}$ and $W_\text{dec}$ to Kaiming uniform random vectors. We reset dead biases to zero.&lt;/p&gt; &lt;h3 id=&quot;fine-tuning&quot;&gt;Fine-Tuning&lt;/h3&gt; &lt;p&gt;We fine-tune Pythia 70M &lt;d-footnote&gt;We wanted to fine-tune Pythia 6.9B, but we encountered out-of-memory errors on an A100 GPU. In follow-up work, we will investigate quantization so that we can study Pythia 6.9B, including the sparse autoencoder we trained for it.&lt;/d-footnote&gt; with our sparse autoencoder inserted in layer one &lt;d-footnote&gt;To learn the most about how fine-tuning affects transformer features, we would ideally learn interpretable feature directions at every transformer layer using a sparse autoencoder. Then, after fine-tuning, we could perform rich comparisons across the model. Unfortunately, reconstruction loss compounds across layers. With current training methods, it is only feasible for us to insert a sparse autoencoder into one layer of the transformer before performance significantly degrades.&lt;/d-footnote&gt;. Instead of adjusting weights everywhere in the network, we constrain fine-tuning to adjust only a small set of interpretable parameters within the sparse autoencoder. In particular, we learn two vectors of dimension $d_\text{auto}$: a coefficient vector $c$ and a bias vector $d$. Just prior to applying $\text{ReLU}$ in the sparse autoencoder, we scale the activations by $c$ and translate them by $d$.&lt;/p&gt; &lt;p&gt;For our fine-tuning experiments, the sparse autoencoder we use is trained on Pythia 70M Chess (a variant fine-tuned on a chess dataset) &lt;d-footnote&gt;This autoencoder was trained to perform well on Pythia 70M Chess, not on the base model Pythia 70M. In future work, we will match the models to investigate how our sparse autoencoder on Pythia 6.9B performs when fine-tuning Pythia 6.9B.&lt;/d-footnote&gt;. We insert this sparse autoencoder into the base Pythia 70M, define new learnable parameters $c$ and $d$ as above, and freeze the gradients on every weight in the fused model except the new learnable parameters. We fine-tune on a small dataset of arithmetic questions (&lt;a href=&quot;https://huggingface.co/datasets/EleutherAI/arithmetic&quot;&gt;EleutherAI/arithmetic&lt;/a&gt;). One training example is shown below:&lt;/p&gt; \[\text{Question: What is }(2 * 7) + 2\text{? Answer:}\] &lt;p&gt;We train with batch size $8$, learning rate $10^{-3}$, and weight decay $10^{-2}$ using the AdamW optimizer &lt;d-cite key=&quot;loshchilov2018decoupled&quot;&gt;&lt;/d-cite&gt; over $10$ epochs with $200$ steps per epoch. The figure below shows the training loss as we fine-tune.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;Our results come in two parts: an exploration of our trained sparse autoencoder on Pythia 6.9B and an analysis of fine-tuning using a smaller sparse autoencoder on Pythia 70M.&lt;/p&gt; &lt;h3 id=&quot;exploring-a-sparse-autoencoder&quot;&gt;Exploring a Sparse Autoencoder&lt;/h3&gt; &lt;p&gt;When inserted into Pythia 6.9B at layer one, our sparse autoencoder achieves a loss of $3.201$ (zero-ablation degrades loss to $3.227$) on the held-out dataset &lt;a href=&quot;https://paperswithcode.com/dataset/wikitext-103&quot;&gt;WikiText-103&lt;/a&gt;, consisting of over 100M tokens from Good and Featured articles on Wikipedia. Pythia 6.9B’s baseline loss is $3.193$. Notably, the sparse autoencoder outperforms a zero-ablation of the layer, demonstrating that it learned features that are useful for reconstruction.&lt;/p&gt; &lt;p&gt;As expected, if the sparse autoencoder is inserted into a layer it was not trained for, performance collapses. For example, if inserted at layer $31$ of Pythia 6.9B, the loss becomes $12.586$. Below is a figure showing the additional loss from inserting the sparse autoencoder at the first eight layers of Pythia 6.9B.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-losses-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-losses-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-losses-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-losses.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The sparse autoencoder preserves model performance in layer 1, the layer it was trained for. The green bar is loss on WikiText-103 of Pythia 6.9B on 5 random batches. The red bar is the additional loss incurred if the sparse autoencoder is inserted after the MLP at a given layer. The first eight layers are shown. &lt;/div&gt; &lt;p&gt;For more details on the training run, four figures demonstrating the sparsity, $L^1$ coefficient, $L^1$ loss, and reconstruction loss of our sparse autoencoder during training are shown below. After training on the first five million tokens, we automatically begin to adjust the $L^1$ coefficient $\beta$ until we reach the desired sparsity of $1\%$. By the end, our sparse autoencoder stabilizes at a sparsity of $100$, which means that only $0.5\%$ of sparse autoencoder features activate on a given token.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_sparsity.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_sparsity.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_sparsity.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_sparsity.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Sparsity across the training run on Pythia 6.9B. On a given batch, sparsity is recorded as the average number of sparse autoencoder features that activate on the batch&apos;s $1024$ tokens. Our sparse autoencoder stabilizes at a sparsity of around $100$, or $0.5\%$ of its hidden dimension. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_coeff.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_coeff.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_coeff.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_coeff.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The $\beta$ coefficient in $L_1$ loss across the training run on Pythia 6.9B. After training on five million tokens, we begin to adjust the coefficient until the sparse autoencoder reaches its target sparsity of $1\%$. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_L1_loss.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The $L^1$ loss of the sparse autoencoder across the training run on Pythia 6.9B. The $L^1$ loss initially rises while the $L^1$ coefficient is adjusted, then falls once the target sparsity is reached as the sparse autoencoder learns a more compact representation. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_reconstr_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_reconstr_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_reconstr_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_reconstr_loss.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The reconstruction loss of the sparse autoencoder across the training run on Pythia 6.9B. Reconstruction loss initially rises while the $L^1$ coefficient is adjusted, due to the tradeoff between reconstruction and sparsity. Once the $L^1$ coefficient stabilizes, reconstruction loss slowly falls as the sparse autoencoder learns a more effective representation. &lt;/div&gt; &lt;p&gt;We find that our sparse autoencoder learned several interpretable features. For example, the second most frequently activating feature (feature index $11928$) activates strongly on the token “·the”. The figure below shows a table with examples.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_6-9b_the_feature-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_6-9b_the_feature-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_6-9b_the_feature-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/SAE_6-9b_the_feature.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The second most frequent feature (feature index $11928$) in the Pythia 6.9B sparse autoencoder activates on the token &quot;·the&quot;. Relevant table columns are $\text{str\_tokens}$ (the token that activates the feature), $\text{context}$ (surrounding tokens in the sentence), and $\text{feature}$ (the raw feature activation in the sparse autoencoder, sorted in descending order). We include the top 15 examples. The feature activates once on “·of” and “·and”, but it activates most on the token “·the”. (Credit: the visualization code for the table is due to Neel Nanda in his open-source replication of Anthropic&apos;s sparse autoencoder work.) &lt;/div&gt; &lt;p&gt;In addition, we found a surprising correlation between dead features. In particular, almost all dead features point in similar directions, as indicated by a high cosine similarity. In comparison, features that are not dead have a cosine similarity that is much closer to centered at zero. If dead features were drawn from the same distribution as non-dead features, we would expect cosine similarities closer to zero.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-cosine-similarity-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-cosine-similarity-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-cosine-similarity-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/sae-cosine-similarity.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The plot above shows the cosine similarity of dead features (red) and non-dead features (blue). Here, a feature is counted as dead if it activates nowhere on WikiText-103-v1. The cosine similarity is calculated compared to the average dead feature. (Credit: the visualization code for cosine similarity is due to Neel Nanda in his open-source replication of Anthropic&apos;s sparse autoencoder work.) &lt;/div&gt; &lt;h3 id=&quot;fine-tuning-with-a-sparse-autoencoder&quot;&gt;Fine-Tuning with a Sparse Autoencoder&lt;/h3&gt; &lt;p&gt;We fine-tune Pythia 70M on arithmetic data by adjusting only a coefficient and bias vector within the sparse autoencoder space.&lt;/p&gt; &lt;p&gt;On layer $4$, we observe an unexpected lowering of loss from $6.449$ for the base model to $6.270$ after inserting the sparse autoencoder. Once fine-tuning the sparse autoencoder on arithmetic, loss remains constant at $6.270$. We believe that the fine-tuning may perform better when we experiment on a larger model such as Pythia 6.9B.&lt;/p&gt; &lt;p&gt;Although the loss does not fall, several features that our interpretable fine-tuning adjusts are interpretable. For example, the feature that is scaled up the most activates on colons (feature index $1338$). Because colons appear twice in every line of the arithmetic data, it makes sense that the fine-tuned model would like to more readibly predict colons. The figure below shows the top activations of feature $1338$ on the arithmetic dataset before and after fine-tuning. After fine-tuning, the feature activates slightly more strongly in all cases.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/colon-feature-1338-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/colon-feature-1338-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/colon-feature-1338-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/colon-feature-1338.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The table above shows the arithmetic dataset tokens on which feature $1338$ most strongly activates, before fine-tuning in the column $\text{feature}$ and after fine-tuning in the column $\text{feature (FT)}. In all cases, the feature activates slightly more after fine-tuning. &lt;/div&gt; &lt;p&gt;The feature that is most inhibited (feature index $619$) activates on newlines. We hypothesize that the sparse autoencoder learns to avoid newlines because, in the chess dataset for which it was trained, newlines are always followed by “Score: ”, indicating the start of a new game. But in the arithmetic dataset, newlines are always followed by “Answer: ”. Therefore, the model wants to inhibit this unhelpful feature. The discrepancy is a difference in datasets. To rigorously verify this hypothesis, we could compute direct logit attributions from feature $619$ to check whether it contributes to the “Answer” token. Either way, the inhibition above demonstrates that our fine-tuning procedure can detect and modify unhelpful features in the sparse autoencoder.&lt;/p&gt; &lt;p&gt;For a broader view of the dynamics of our interpretable fine-tuning, the two figures below show the learned scale and bias terms across every feature in the sparse autoencoder space (where $d_\text{auto} = 2048$), sorted in ascending order. We observe that the majority of features are largely unaffected, but a few features at the tails are significantly enhanced or inhibited.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-bias-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-bias-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-bias-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-bias.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The learned bias in the sparse autoencoder space inhibits approximately half of features while enhancing the other half. The x-axis is sorted so that the feature index runs in ascending order of the learned bias. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-scaling-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-scaling-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-scaling-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-09-sparse-autoencoders-for-interpretable-rlhf.md/fine-tuning-scaling.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The learned scaling coefficient in the sparse autoencoder space significantly inhibits a small number of features while significantly enhancing several others. We also observe that a majority of features ($2/3$) are inhibited, compared to a smaller number enhanced. The x-axis is sorted so that the feature index runs in ascending order of the learned scaling. &lt;/div&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;One limitation of our fine-tuning experiments is that Pythia 70M is a small model for which there are fewer interpretable features. In addition, we inserted into Pythia 70M a sparse autoencoder trained to reconstruct activations in Pythia 70M Chess. Nonetheless, our fine-tuning results are promising. The majority of features are not significantly affected, but a few features at the tails are either significantly enhanced or inhibited. We found it fruitful to interpret these outlier features first, as they are a starting point for finding which sparse autoencoder features matter most for the fine-tuning dataset.&lt;/p&gt; &lt;p&gt;When training a sparse autoencoder on Pythia 6.9B, we were successful in learning interpretable features, such as the “the” feature. But we remain uncertain of the best way to train a sparse autoencoder, especially how to resample dead features. However, one implication of our work is that research on sparse autoencoders is accessible to a wide array of researchers. We believe a systematic study of training techniques for sparse autoencoders could benefit the field.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our work indicates that sparse autoencoders are a promising tool for machine learning interpretability. By inserting sparse autoencoders into transformer language models, we investigate how a novel form of fine-tuning can provide insight into changes in model behavior after fine-tuning. We find that our fine-tuning successfully modifies interpretable features in the sparse autoencoder space. Given the rapid adoption of powerful, fine-tuned language models across industries, we believe our method for interpretable fine-tuning is an important direction to continue to explore as researchers seek to understand how fine-tuning affects model cognition. Although our current work is limited because we only fine-tune Pythia 70M, future work can scale up model size, compute resources, and the number of tokens used to train the sparse autoencoder. Additionally, future work can extend from direct fine-tuning to investigating the effects of RLHF performed with PPO (Proximal Policy Optimization).&lt;/p&gt; &lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt; &lt;p&gt;We would like to thank Professor Isola, Professor Beery, and Dr. Bernstein for an introduction to fundamental perspectives in deep learning that will stay with us forever. Thank you to Logan Smith for invaluable early guidance on the questions we could explore related to sparse autoencoders. We are thankful for the AI Safety Student Team at Harvard (AISST) and MIT AI Alignment (MAIA) for a supportive community of fellow researchers.&lt;/p&gt; &lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt; &lt;p&gt;Our code is available at the following Google Colab notebooks:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1LAjClrzEzQEu0BMjachoHuvmbmLz1Zf1?usp=sharing&quot;&gt;Training Sparse Autoencoders&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1KF2djPVOKOemOECnZq3MeI-k9ypf7PEE?usp=sharing&quot;&gt;Analyzing Sparse Autoencoders&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1CVIIbxO7iYnVuiH5UC0bdQuDvFHtBuac?usp=sharing&quot;&gt;Fine-Tuning with Sparse Autoencoders&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1VGVImnhky0bo-SdBYYYDopQIjGpx-5aK?usp=sharing&quot;&gt;Analysis of Fine-Tuning with Sparse Autoencoders&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; </content> </entry> <entry> <title>Using Synthetic Data to Minimize Real Data Requirements</title> <link href="https://deep-learning-mit.github.io/blog/2023/proposal-2/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/proposal-2</id> <content type="html">&lt;p&gt;*And used it as the basis for transfer learning with the real data that someone put hard work in to generate.&lt;/p&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Synthetic biology is a burgeoning field of research which has attracted a lot of attention of the scientific community in recent years with the advancement of technologies that enable the better understanding and manipulation of biological systems. A significant contributor to its steadily increasing popularity is the diverse array of potential applications synthetic biology may have, ranging from curing cancer, to addressing significant climate issues, to colonizing other planets&lt;d-cite key=&quot;lim2022reprogramming&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;delisi2020role&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;conde2020synthetic&quot;&gt;&lt;/d-cite&gt;. But, to effectively manipulate these biological systems, it is necessary to understand how they work and how they interact with other biological systems — it has been shown time and time again that a system characterized in isolation, compared to the same system in a broader, non-isolated context, will not perform identically&lt;d-cite key=&quot;qian2017resource&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;gyorgy2015isocost&quot;&gt;&lt;/d-cite&gt;. This necessitates models that can predict a system’s behavior given both stimuli &lt;em&gt;and&lt;/em&gt; context.&lt;/p&gt; &lt;p&gt;In the synthetic biology literature, the behavior of many systems is characterized by the chemical reactions that take place; these reactions consist most frequently of the so-called central dogma of biology, in which DNA produces RNA, which produces proteins. These proteins are then free to perform almost every function within a cell, including — most notably for us — regulation of DNA. By varying the extent and nature of this regulation, these systems yield mathematical models that range from simple linear systems to highly complex nonlinear dynamical systems:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal-2/fig1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1: A simple model of the central dogma of biology: a stretch of DNA is used to create a strand of messenger RNA, which is used to create a functional protein. Functional proteins are responsible for almost all operations within the cell, from cellular movement to RNA production and everything in between. &lt;/div&gt; &lt;p&gt;However, the figure above does not capture the full purview of the cell; it neglects factors that synthetic biologists know to be critical to the process of protein expression, as well as factors that have not been characterized rigorously yet. The process of analyzing the behavior of a system at the fullest level of detail necessary to encapsulate these intricate dynamics is expensive and time-consuming, and requires significant experimental data to validate — not to mention the fact that, as was mentioned, there are some factors which we simply don’t know about yet. Protein production is an immense and complex task, and identifying its critical parameters at the highest level of detail is no small feat.&lt;/p&gt; &lt;h3 id=&quot;enter-machine-learning&quot;&gt;Enter Machine Learning&lt;/h3&gt; &lt;p&gt;With this in mind, many synthetic biologists are experimenting with characterizing system behavior, especially when augmenting pre-existing models to include newly discovered phenomena, using machine learning and neural networks, due to their universal function approximator property. In this fashion, we may be able to better abstract the levels of biological detail, enabling better prediction of the composition of two genetic circuits.&lt;/p&gt; &lt;p&gt;Unfortunately, training neural networks also requires (surprise surprise!) substantial experimental data, which is taxing on both a researcher’s budget and time — for a small lab with few researchers working, a single experiment may take upwards of 12 hours of attentive action, while yielding only up to 96 data points for training. Some large-scale gene expression data has been collected to assist in the development of machine learning algorithms; however, this data is focused largely on the expression of a static set of genes in different cellular contexts — rather than on a dynamic set of genes being assembled — and is therefore insufficient to address the questions of composition that are being posed here.&lt;/p&gt; &lt;p&gt;This leads us to a fundamental question: &lt;strong&gt;can we use transfer learning to reduce the experimental data we need for training by pre-training on a synthetic dataset which uses a less-detailed model of our system?&lt;/strong&gt; In other words, can we still derive value from the models that we know don’t account for the full depth of the system? If so, &lt;strong&gt;what kinds of structural similarities need to be in place for this to be the case?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In this project, we aim to address each of these questions; to do this, we will first pre-train a model using simpler synthetic data, and use this pre-trained model’s parameters as the basis for training a host of models on varying volumes of our more complex real data. Then, we will consider sets of more complex real data that are less structurally similar to our original synthetic data, and see how well our transfer learning works with each of these sets.&lt;/p&gt; &lt;p&gt;In theory, since the synthetic data from the literature uses models that have already captured some of the critical details in the model, this fine-tuning step will allow us to only learn the &lt;em&gt;new&lt;/em&gt; things that are specific to this more complex model, thus allowing transfer learning to be successful. As the two underlying models become increasingly distant, then, one would expect that this transfer will become less and less effective.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;h3 id=&quot;problem-formulation&quot;&gt;Problem Formulation&lt;/h3&gt; &lt;p&gt;Consider we have access to a limited number of datapoints which are input-output $(x_i,y_i)$ pairs for a biological system, and we want to train a neural network to capture the system behavior. The experimental data for the output $y_i$ we have is corrupted by an additive unit gaussian noise, due to white noise and measurement equipment precision. Moreover, we consider that we also have access to a theoretical model from another biological system which we know to be a simplified version of the one in our experiments, but which explicitly defines a mapping $\hat y_i = g(x_i)$.&lt;/p&gt; &lt;p&gt;Our goal is thus to train a model $y_i = f(x_i)$ to predict the real pairs while using minimal real pairs of data $(x_i, y_i)$. Instead, we will pre-train with $(x_i, \hat y_i)$ pairs of synthetic data, and use our real data for fine-tuning.&lt;/p&gt; &lt;h3 id=&quot;data-acquisition&quot;&gt;Data Acquisition&lt;/h3&gt; &lt;p&gt;In this work we will additionally consider a domain shift between two datasets, which we will refer to as the big domain and the small domain. In the big domain, our inputs will vary between 0 and 20nM, and in the small domain the inputs will vary between 0 and 10nM. These domains represent the ranges for the inputs in the experiments in the small domain, which may be limited due to laboratory equipment, and the desired operation range of the systems in the big domain.&lt;/p&gt; &lt;p&gt;Furthermore, &lt;strong&gt;for all datasets - pre-training, fine-tuning, or oracle training - we will be generating synthetic data for training and testing purposes.&lt;/strong&gt; We will use different levels of complexity to simulate a difference between experimentally-generated and computationally-generated data. In a real setting, we would use the complex model $f$ that we’re trying to learn here as the simple, known model $g$ in our setup. Going forward, we will refer to the data generated by our low-complexity model $g$ as “synthetic” data, and to the data generated by our high-complexity model as “real” or “experimental” data.&lt;/p&gt; &lt;p&gt;For our low-complexity theoretical model, we consider the simplest gene expression model available in the literature, in which the input $x_i$ is an activator, and the output $y_i$ is given by the following Hill function:&lt;/p&gt; \[y_i = \eta_i \frac{\theta_i x_i}{1 + \Sigma_{j=1}^2 \theta_j x_j}, i\in {1,2},\] &lt;p&gt;where our $\eta_i$’s and $\theta_i$’s are all inherent parameters of the system.&lt;/p&gt; &lt;p&gt;For the first experimental model, we consider a more complex gene expression model, where the activator $x_i$ must form an $n$-part complex with itself before being able to start the gene expression process, which yields the following expression for the output $y_i$:&lt;/p&gt; \[y_i = \eta_i \frac{(\theta_i x_i)^n}{1 + \Sigma_{j=1}^2 (\theta_j x_j)^n}, i\in {1,2},\] &lt;p&gt;where - once again - our $\eta_i$’s and $\theta_i$’s are all inherent parameters of the system. Note that, at $n=1$, our real model is identical to our synthetic model. As one metric of increasing complexity, we will vary $n$ to change the steepness of the drop of this Hill function.&lt;/p&gt; &lt;p&gt;As an additional test of increased complexity, we will consider a phosphorylation cycle in which inputs $x_i$ induce the phosphorylation or dephosphorylation of a given protein. We take the dephosphorylated protein to be an output $y_1$, and the phosphorylated protein to be a secondary output $y_2$, for which we have:&lt;/p&gt; \[y_i = y_{tot} \frac{\theta_i x_i}{\Sigma_{j=1}^2 \theta_j x_j}, i\in {1,2},\] &lt;p&gt;in which $\theta_i$’s and $y_{tot}$ are each system parameters. Note that the only functional difference between this system and the synthetic data generation system lies in the denominator of each, as one has a nonzero bias term, where the other does not.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal-2/fig2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: Graphical representation of the three different synthetic or experimental models used in this project. In the first diagram, our input protein $x_i$ is activating the production of an output protein $y_i$. This is the simplest model of which we can conceive, and constitutes our synthetic data. In the second diagram, two copies of our input protein $x_i$ come together to form a complex that induces the production of our output protein $y_i$. This is a step up in complexity, and varying the number of proteins that come together allows us to introduce more and more complexity into our system. Finally, a single protein which can be either of our outputs $y_1$ or $y_2$ is moved between these states by our two input proteins $x_1$ and $x_2$. This system, while seemingly very dissimilar from the above two, winds up being mathematically not too far off, and offers another model on which to transfer our learning. &lt;/div&gt; &lt;h3 id=&quot;training--testing&quot;&gt;Training &amp;amp; Testing&lt;/h3&gt; &lt;p&gt;For each experiment, we trained MLPs composed of 5 hidden layers with 10 nodes each and a ReLU activation function.&lt;/p&gt; &lt;p&gt;For the first experiment, we performed transfer learning by pre-training our model for 90% of the total number of epochs (1800/2000) with the synthetic data sampled from the big domain, where we have a high quantity of data points (40000 $(x_i, y_i)$ pairs); for the remaining 10% of epochs, the network was trained on the experimental data sampled from the small domain, with varying numbers of data points used for training. This can be compared to a model trained exclusively on the same volume of experimental data for a full 2000 epochs, to establish a baseline level of performance. An oracle model was trained for all 2000 epochs on experimental data sampled from the big domain with a high volume of data, and serves as the best-case performance of our model.&lt;/p&gt; &lt;p&gt;For the second experiment, we followed a very similar protocol as in the first experiment; the critical difference here lies in the fact that, where the fine-tuning step used different volumes of data in the previous case, we now instead use a fixed data volume (1000 $(x_i, y_i)$ pairs), and fine-tune on a host of different models of varying complexity relative to the synthetic model.&lt;/p&gt; &lt;p&gt;To evaluate performance of our neural networks, we uniformly sample 100 points from the big domain, for which we calculate the L1 loss mean and variance between the network predictions and the experimental model output.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal-2/fig5.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: A visual example of the training done - on the right is the intended function to be learned, where the left features the output of one of the models that was trained with transfer learning. &lt;/div&gt; &lt;h2 id=&quot;results--analysis&quot;&gt;Results &amp;amp; Analysis&lt;/h2&gt; &lt;h3 id=&quot;experiment-1&quot;&gt;Experiment 1&lt;/h3&gt; &lt;p&gt;As was mentioned before, the first experiment was targeted towards addressing the question of whether we can pre-train a model and use transfer learning to reduce the volume of real data needed to achieve a comparable standard of accuracy. To this end, we trained several models with a fixed volume of pre-training data, and varied the volume of fine-tuning data available to the model.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal-2/fig3.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4: Bar plots of model loss as the volume of fine-tuning (blue) or training (orange) data increases. As can be seen, at high volumes, the blue bars reach a lower loss than the orange bars, suggesting that transfer learning is effective at taking high volumes of data, and improving them further. For very low volumes, these two models are roughly equivalent, although the orange bars have a significantly higher variance than the blue bars. Somewhere in between, a transition occurs, and transfer learning outpaces learning without prior knowledge of anything. &lt;/div&gt; &lt;p&gt;As can be seen in the blue bars of Figure 4, the greater the volume of real data coupled with transfer learning, the lower the loss, and the better the performance. This is to be expected, but this curve helps to give a better sense regarding how quickly we approach the limit of best-case performance, and suggests that the volume of real data used for oracle training could cut be cut down by nearly an order of magnitude while achieving comparable performance. One might argue that this is because the volume of real data used in this training is itself sufficient to effectively train this model; to that end, we consider the orange bars, which represent the loss of models trained for 2000 epochs exclusively on the given volume of real data. This, coupled with the blue bars, suggests that - across all volumes of data - it is, at the very least, more consistent to use transfer learning. Models trained for that duration on exclusively real data sampled from the small domain tended to overfit, and had a much higher variance as a result. As the volume of real data used for fine-tuning increased, the difference between the two regimes of transfer vs. non-transfer learning became more pronounced, and the benefits of transfer learning become more noticeable. Thus we conclude that we can use transfer learning to cut down on the quantity of real data needed, while sacrificing relatively little up to a ~75% cut of data requirements.&lt;/p&gt; &lt;h3 id=&quot;experiment-2&quot;&gt;Experiment 2&lt;/h3&gt; &lt;p&gt;Next, we wish to address the question of how structurally dissimilar a model can be while still making this transfer learning effective. To this end, we varied $n$ from our first experimental model, and generated data with our second experimental model. In each case, we performed a ~95% cut in the volume of real data relative to the volume of data used to train each oracle.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal-2/fig4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal-2/fig4.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5: Bar plots of model loss as the model being learned is varied, as a means of representing increases in complexity or structure. As can be seen, within this range of complexity variation, transfer learning is consistently able to learn the system to a comparable degree across all cases. &lt;/div&gt; &lt;p&gt;In Figure 5, we compare the loss of models trained with transfer learning to oracles for each - as can be seen, the transfer learning models performed consistently across all models being learned, and the oracles of each were similarly consistent. This suggests that the architectures of the models being learned are sufficiently similar that the transfer learning is effective, which is a promising sign for more applications in which the system being learned has been simplified significantly in its mathematical models.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Ultimately, we’ve developed a method by which to potentially reduce the volume of experimental data needed to effectively train a machine learning model by using synthetic data generated by a lower-complexity model of the system. We’ve demonstrated that it has the potential to cut down data requirements significantly while still achieving a high level of accuracy, and that the simple system used to generate data in the sense that the learning process can shore up some substantial structural differences betwen the simple and complex system. These findings are not necessarily limited strictly to synthetic biological learning tasks, either - any complex, data-starved phenomenon in which there is a simpler model to describe parts of the system may find value in this. Looking forward, one can consider deeper structural dissimilarities, as well as application with real synthetic biological data, rather than simply using two models of increasing complexity.&lt;/p&gt; </content> </entry> <entry> <title>Applications of Deep Learning in Timbre Transfer</title> <link href="https://deep-learning-mit.github.io/blog/2023/proposal-1/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/proposal-1</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/spectrogram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/spectrogram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/spectrogram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/spectrogram.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Timbre&lt;/em&gt; is what distinguishes a flute from a trumpet, piano or any other musical instrument. Even if two performers play the same note, there is no ambiguity in the tone of their instruments. But unlike pitch (frequency) or amplitude (loudness), &lt;em&gt;timbre&lt;/em&gt; is not a trivial metric; rather, it pertains much more to subjective qualities like &lt;em&gt;raspiness&lt;/em&gt;, &lt;em&gt;articulation&lt;/em&gt; and even musical &lt;em&gt;intent&lt;/em&gt;. In this article, I’ll be discussing different data-driven approaches to extracting and manipulating this quality of sound using deep learning.&lt;/p&gt; &lt;audio controls=&quot;&quot;&gt; &lt;source src=&quot;/staging/assets/video/2023-11-09-ddsp-proposal/brass.wav&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; &lt;audio controls=&quot;&quot;&gt; &lt;source src=&quot;/staging/assets/video/2023-11-09-ddsp-proposal/flute.wav&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; &lt;p&gt;In particular I’d like to explore &lt;em&gt;timbre transfer&lt;/em&gt;, where one instrument is made to sound like another while retaining most aspects of the original performance. I’ll be training an auto-encoder architecture first conditioned on the source instrument (whistling) then tuned to tracks of trumpets to achieve whistling-to-trumpet timbre transfer. Moreover, I’d like to reduce the complexity of previous architectures to achieve realtime results suitable for musical performance.&lt;/p&gt; &lt;p&gt;First, some context on sound and our perception thereof.&lt;/p&gt; &lt;h2 id=&quot;what-is-sound&quot;&gt;What is Sound?&lt;/h2&gt; &lt;p&gt;Our ears are sensitive to changes in air pressure over time, which we perceive as sound. Digital audio is analogous to this phenomenon, where its representation is a sequence of samples usually in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[-1, 1]&lt;/code&gt; range and discretized at a frequency high enough that it becomes indistinguishable from natural sources. This is known as the time domain, however all signals can be mapped to the frequency domain where the individual sinusoids that compose it are graphed against their respective amplitudes. Below is a Fourier transform &lt;d-cite key=&quot;1&quot;&gt;&lt;/d-cite&gt; applied to the sound of a trumpet from above:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/brass_freq-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/brass_freq-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/brass_freq-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/brass_freq.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;audio controls=&quot;&quot;&gt; &lt;source src=&quot;/staging/assets/video/2023-11-09-ddsp-proposal/brass.wav&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; &lt;p&gt;It turns out that only the bottom-most frequency, \(f_0\), informs our ears of this note’s &lt;em&gt;pitch&lt;/em&gt;. In fact, a pure sine wave at that frequency will sound &lt;em&gt;similar&lt;/em&gt; to the trumpet.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/sine_freq-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/sine_freq-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/sine_freq-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/sine_freq.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;audio controls=&quot;&quot;&gt; &lt;source src=&quot;/staging/assets/video/2023-11-09-ddsp-proposal/sine.wav&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; &lt;p&gt;The distinction between the trumpet and sine wave lies in the frequencies above \(f_0\), known as overtones. Moreover, certain musical instruments exhibit an interesting &lt;em&gt;harmonic&lt;/em&gt; behavior where only the overtones that are multiples of \(f_0\) are actually prominent; this is the case for most instruments you could name, though some non-examples include the gong and timpani &lt;d-cite key=&quot;2&quot;&gt;2&lt;/d-cite&gt;. Below is a spectrogram, which displays the frequency-domain of a signal over time. Observe the estimated \(f_0\) (implemented using the YIN algorithm &lt;d-cite key=&quot;3&quot;&gt;&lt;/d-cite&gt;) and how its multiples (\(2 * f_0\), \(3 * f_0\), etc) evolve over time.&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-09-ddsp-proposal/spectrogram.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;Try playing the audio clip above, whistle into the spectrogram or record your own instrument! The horizontal axis is time and vertical axis is frequency&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So how do overtones relate to timbre? Well, the harmonic series is the most obvious distinguishing factor between different instruments playing the same pitch, &lt;strong&gt;so we could model timbre as the evolution of \(f_0\) and its overtones’ amplitudes over time&lt;/strong&gt;. Note that this is assuming a strictly monophonic context (one note at a time), and overlooks non-harmonic parts of the signal (e.g. a flutist’s breathing). So this representation will still sound synthetic but it forms a good basis for what we’re trying to achieve.&lt;/p&gt; &lt;h2 id=&quot;timbre-transfer&quot;&gt;Timbre Transfer&lt;/h2&gt; &lt;p&gt;Perhaps the most obvious method for achieving timbre transfer is approximating the pitch of the source audio (as demonstrated above) and recreating it using a synthetic MIDI instrument. However, this discards much of the expressiveness which isn’t desireable in a musical performance.&lt;/p&gt; &lt;p&gt;Rather, data-driven approaches have shown promise in audio synthesis &lt;d-cite key=&quot;6&quot;&gt;&lt;/d-cite&gt; and existing deep learning architectures can be repurposed to achieve &lt;em&gt;nuanced&lt;/em&gt; timbre transfer to various degrees of success. &lt;d-cite key=&quot;5&quot;&gt;&lt;/d-cite&gt; treats timbre transfer as an image-to-image problem, leveraging a Conditional Adversarial Networks architecture &lt;d-cite key=&quot;7&quot;&gt;&lt;/d-cite&gt; trained on natural images to transform spectrograms of audio signals. &lt;d-cite key=&quot;4&quot;&gt;&lt;/d-cite&gt; uses a Denoising Diffusion Implicit Model (DDIM) to achieve similar results. The audio is then synthesized from these spectrograms using the Inverse Fourier Transform or another neural network.&lt;/p&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;Keyboard&lt;/th&gt; &lt;th&gt;Guitar&lt;/th&gt; &lt;th&gt;String&lt;/th&gt; &lt;th&gt;Synth Lead&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/keyboard_acoustic.png&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/guitar_acoustic.png&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/string_acoustic.png&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;/td&gt; &lt;td&gt;&lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/synth_lead_synthetic.png&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;em&gt;Images courtesy of &lt;d-cite key=&quot;5&quot;&gt;&lt;/d-cite&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;However, these methods rely on a dataset of audio tracks in two timbre domains, namely audio synthesized from MIDI instruments like in &lt;d-cite key=&quot;9&quot;&gt;&lt;/d-cite&gt; since recordings of performers will never match exactly. The results thereby &lt;em&gt;sound&lt;/em&gt; synthetic; a better architecture would thus be self-supervised and trained on acoustic performances directly.&lt;/p&gt; &lt;h2 id=&quot;proposed-model&quot;&gt;Proposed Model&lt;/h2&gt; &lt;p&gt;I experimented with an auto-encoder architecture, where a network is trained to minimize the audible difference between some input audio track \(x\) and its re-synthesized counterpart \(\hat{x}\); so, the model attempts to recreate its input \(x\) by first encoding it to some latent representation \(z\) and decoding back to audio. Note that although over-fitting is possible, a one-to-one mapping (or, &lt;em&gt;cheating&lt;/em&gt;) is impossible because \(z\) &lt;em&gt;bottlenecks&lt;/em&gt; (has less dimensions than) \(x\). The appeal of this approach is that the problem is now self-supervised and can be trained directly on musical performances of the &lt;em&gt;source&lt;/em&gt; instrument (e.g. whistling).&lt;/p&gt; &lt;p&gt;Next, the encoder is frozen (unaffected by gradient descent) and the decoder is trained anew on samples of the &lt;em&gt;target&lt;/em&gt; instrument (e.g. trumpet). So, the networks knows how to encode the &lt;em&gt;source&lt;/em&gt; instrument to some \(z\), and hopefully its decoder has adapted to map \(z\) onto the &lt;em&gt;target&lt;/em&gt; instrument.&lt;/p&gt; &lt;p&gt;The decoder doesn’t output audio directly, nor does it generate a spectrogram like in &lt;d-cite key=&quot;5&quot;&gt;&lt;/d-cite&gt;. Rather, it controls parameters of a harmonic oscillator proposed by &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; which follows the intuition of timbre as discussed prior; that is, the oscillator has parameters for its \(f_0\) and the amplitudes of each harmonic overtone. Leveraging this strong inductive bias should reduce the size of the neural network enough to be applicable to realtime performances.&lt;/p&gt; &lt;p&gt;The encoder architecture is taken from &lt;d-cite key=&quot;11&quot;&gt;&lt;/d-cite&gt;, whose original application is tracking pitch; I don’t track pitch explicitely, rather &lt;d-cite key=&quot;11&quot;&gt;&lt;/d-cite&gt; demonstrates that CNNs can extract meaningful data from audio directly in the time domain. The issue with working in the frequency domain is shown in &lt;d-cite key=&quot;12&quot;&gt;&lt;/d-cite&gt;, where we’d need a high sampling rate (and thus the network needs to be that much faster) for high frequencies or a long sampling window (which yields a network with more parameters) for low frequencies. Note that there is a nice compromise to these issues by windowing the inputs and outputs &lt;d-cite key=&quot;13&quot;&gt;&lt;/d-cite&gt;, which I’d like to try later.&lt;/p&gt; &lt;p&gt;Finally, the loss I’m using is multi-scale spectrogram loss proposed in &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt;, which computes the L1 loss of two audio tracks in the frequency-domain on both a linear and log scale.&lt;/p&gt; &lt;h2 id=&quot;encoder&quot;&gt;Encoder&lt;/h2&gt; &lt;p&gt;The architecture of my model is largely inspired by Magenta’s Differentiable Digital Signal Processing (DDSP) &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; paper, where differentiable sound processors are introduced. Although modules like reverb and a finite-impulse response (FIR) filter are included, I’m only experimenting with its harmonic oscillator for simplicity. The architecture proposed by &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; is also an auto-encoder, however its latent representation is built on two heuristics (pitch, amplitude) rather than the audio itself. Despite this, &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; is able to achieve natural sounding instruments but its controls are limited in expression, much like MIDI inputs. Realtime Audio Variational autoEncoder (RAVE) &lt;d-cite key=&quot;15&quot;&gt;&lt;/d-cite&gt; builds upon this by encoding a multiband decomposition of the source audio, or a collection of Fourier transforms with varying amount of bins to overcome limitations of the Nyquist frequency and limited precision of discretization. A single Fourier transform operates on a linear scale, where its frequency bins scale from \(0\) to its Nyquist frequency. However, humans hear on a logarithmic scale (i.e. A4 is \(440 \text{Hz}\) but an octave above that is \(880 \text{Hz}\)) so the transform has a bias towards low frequencies. Multiband decomposition approaches this by shifting the frequency bins using different window sizes of audio and letting the network generalize over the complete frequency spectrum. However, although &lt;d-cite key=&quot;15&quot;&gt;&lt;/d-cite&gt; has shown some incredible results and claims to run in realtime, that is not the case in practice &lt;d-cite key=&quot;16&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In my experiment, I leverage a Convolutional Representation for Pitch Estimation (CREPE) &lt;d-cite key=&quot;11&quot;&gt;&lt;/d-cite&gt;; it is a CNN-based pitch estimator that operates directly on the time-domain of an audio signal and achieves state of the art results. Rather than using its output, like in &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt;, I use its latent representation and train the network to generalize over more characteristics of sound than just pitch.&lt;/p&gt; &lt;h2 id=&quot;decoder&quot;&gt;Decoder&lt;/h2&gt; &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;introduced the idea of using oscillators for audio synthesis as opposed to raw waveform modeling. &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; demonstrates that their architecture benefits from this inductive bias and is able to be significantly reduced in size. I wanted to experiment with the encoder for the part, so the decoder of my model remains unchanged from the original paper (for the most part). It consists of several dense layers, ReLU activation functions and layer normalization. In between these is a Gated Recurrent Unit (GRU). The harmonic oscillator from &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; cannot produce sinusoids out of phase (the instantaneous phase is accumulated at each time step) but presumably the network needs &lt;em&gt;some&lt;/em&gt; time dependency to form an audio envelope.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/adsr-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/adsr-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/adsr-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-ddsp-proposal/adsr.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Image courtesy of Tellef Kvifte&lt;/em&gt;&lt;/p&gt; &lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt; &lt;p&gt;I trained the target instrument auto-encoder on the URMP dataset &lt;d-cite key=&quot;17&quot;&gt;&lt;/d-cite&gt;, which consists of individual recordings of performers across a variety of instruments. Specifically, I wrote a dataloader that selects only trumpet solo tracks and randomly samples a 4 second clip from each of them. The audio is down-sampled to \(16\text{kHz}\) because the dataset doesn’t contain many frequencies above \(8\text{kHz}\) and the reduced dimensionality allows for training on my M2 MacBook Air with a batch size of 16!&lt;/p&gt; &lt;p&gt;I also created my own whistling dataset, sampled from MIT students with varying levels of proficiency. The audio clips are normalized, silence is cutout and altogether I have around 2 hours of data.&lt;/p&gt; &lt;h2 id=&quot;loss&quot;&gt;Loss&lt;/h2&gt; &lt;p&gt;Like &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt; and &lt;d-cite key=&quot;15&quot;&gt;&lt;/d-cite&gt;, I focus on &lt;em&gt;perceptual&lt;/em&gt; loss which approximates human hearing. So, comparing waveforms in the time-domain would not work because humans aren’t sensitive to changes in phase whereas the signal changes drastically. I extend upon the multi-scale spectrogram loss proposed by &lt;d-cite key=&quot;10&quot;&gt;&lt;/d-cite&gt;, which consists of taking the L1 norm of the two inputs’ spectrograms (so phase is discarded) in both the linear and log domain. Note that human hearing is logarithmic, but spectrograms are not. I experiment upon this by employing the log Mel spectrogram &lt;d-cite key=&quot;8&quot;&gt;&lt;/d-cite&gt; which is an even better approximation of human hearing and used by &lt;d-cite key=&quot;4&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;18&quot;&gt;&lt;/d-cite&gt; and &lt;d-cite key=&quot;19&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;I trained 500 epochs of 16 times 4 second samples on a single M2 MacBook Air with Metal acceleration, totaling around 10 hours. Unfortunately, the loss converged but the network was not able to generalize over abstract characteristics of sound as I’d hoped. Rather, it learned to represent sound as a mellow mix of harmonics instead of anything useful. I think future experiments should penalize silence (or close to it), and perhaps add skip connections from the inputs’ power (explicitely calculated) to the decoder. Moreover, the size of the encoder was drastically reduced (a few orders of magnitude less parameters in both width and depth) so it’s possible the latent representation did not contain much meaningful data.&lt;/p&gt; &lt;audio controls=&quot;&quot;&gt; &lt;source src=&quot;/staging/assets/video/2023-11-09-ddsp-proposal/epoch0.wav&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; &lt;audio controls=&quot;&quot;&gt; &lt;source src=&quot;/staging/assets/video/2023-11-09-ddsp-proposal/epoch250.wav&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; &lt;audio controls=&quot;&quot;&gt; &lt;source src=&quot;/staging/assets/video/2023-11-09-ddsp-proposal/epoch470.wav&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; &lt;p&gt;Sample synthesized waveforms at epochs 0, 250, and 470 respectively (loud sounds warning!).&lt;/p&gt; </content> </entry> <entry> <title>The Effect of Activation Functions On Superposition in Toy Models</title> <link href="https://deep-learning-mit.github.io/blog/2023/interpretability-of-toy-tasks/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/interpretability-of-toy-tasks</id> <content type="html">&lt;h2 id=&quot;introduction-to-superposition&quot;&gt;Introduction to Superposition&lt;/h2&gt; &lt;p&gt;With the recent emergence of grokking, mechanistic interpretability research has trended towards understanding how models learn &lt;d-cite key=&quot;GrokNanda&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;Pizza&quot;&gt;&lt;/d-cite&gt;. A central concept in this pursuit is superposition - a single neuron learning multiple “features.”&lt;/p&gt; &lt;p&gt;Features are the distinguishing properties of data points, the “things” that allow a neural network to learn the difference between, say, a dog and a cat, or a Phillip Isola and a Jennifer Aniston. Features are the building blocks that determine what makes one data point different from another. In many cases, features discovered by and encoded within neural networks correspond to human-understandable ideas. For example, in language models there exist embedding vectors describing relations like gender or relative size (e.g., the famous vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)&lt;d-cite key=&quot;mikolov2013efficient&quot;&gt;&lt;/d-cite&gt;). It has been found that language models often map ideas like these to features within their parameters. Human understanding is not necessary though, as models can find and map features that exist beyond the perception of humans. This is an important part of the success (and dual inscrutability) of modern deep models, as these models can determine features and relationships within the data that allow them to model large datasets, like language, very well.&lt;/p&gt; &lt;p&gt;In this work we:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Explain Superposition, why it may occur, and why it is important&lt;/li&gt; &lt;li&gt;Motivate a framework to easily study Superposition&lt;/li&gt; &lt;li&gt;Study how activation functions affect Superposition&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;superposition-and-previous-work&quot;&gt;Superposition and Previous Work&lt;/h2&gt; &lt;p&gt;Let us elaborate further. If you were to train some neural network and visualize the weights - chances are you would see some mess that looks like this:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/random_matrix_equation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/random_matrix_equation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/random_matrix_equation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/random_matrix_equation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;You are likely looking at superposition!&lt;/p&gt; &lt;p&gt;As hypothesized by &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, superposition is a phenomenon which occurs when the number of features being learned by a model is greater than the number of parameters in that model. To capture $n$ features with $m&amp;lt;n$ parameters, one can think of the neurons as “working overtime.” In other words, some of the neurons within a model encode information about more than one feature. The neuron exhibiting superposition operates as an information compressor. The caveat is that this compression is often unpredictable and hard to understand!&lt;/p&gt; &lt;p&gt;In a linear model, i.e., one which maps inputs to outputs with only linear functions, there are fewer parameters than the features it tries to represent, so it can only represent the top $m$ features. How then do neural networks use compression and map back to $n&amp;gt;m$ features using only $m$ parameters? The answer is non-linearity. Clearly, the activation function is key to understanding how superposition occurs - unexplored by other work in the field. &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt; explores the activation function in transformer MLP, but not in the setting we present here.&lt;/p&gt; &lt;p&gt;But why do we care about Superposition? Why spend time studying this?&lt;/p&gt; &lt;p&gt;While it may seem tangential, Superposition sheds important insights on Large Language Models (LLMs)! While LLMs are billions of parameters large, this is still not enough for a one-to-one mapping to “features” on the internet. Therefore LLMs also MUST exhibit superposition to learn. We focus our current work on the $\textit{bottleneck superposition}$ regime, but &lt;d-cite key=&quot;incidental&quot;&gt;&lt;/d-cite&gt; has shown that the picture is far more complicated than presented in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;. Namely, varying the initialization can change how superposition unfolds. To normalize across experiments, we initialize all weights using the Xavier norm, as outlined by &lt;d-cite key=&quot;xavier&quot;&gt;&lt;/d-cite&gt;. However, this is certainly a limitation of our presented work. A more rigourous analysis of superposition with activation functions would explore it outside the contex of the bottleneck regime. We leave this for future work.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/feature_visual-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/feature_visual-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/feature_visual-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/feature_visual.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; From &lt;a href=&quot;https://distill.pub/2017/feature-visualization/&quot;&gt;Distill Blog&lt;/a&gt;, &quot;Feature visualization allows us to see how GoogLeNet trained on the ImageNet dataset, builds up its understanding of images over many layers. &lt;/div&gt; &lt;p&gt;Previous research, as detailed in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, has predominantly explored superposition within the confines of toy models utilizing the Rectified Linear Unit (ReLU) activation function. However, to extend these findings to contemporary neural networks, it is crucial to investigate the influence of different activation functions on superposition. Different activation functions provide different ways for a model to use superposition to its advantage.&lt;/p&gt; &lt;p&gt;So you train a neural network - what happens at the neuron level? There are three possibilities. As the network trains each neuron has three choices:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The neuron chooses not to encode the “features”&lt;/li&gt; &lt;li&gt;The neuron chooses to dedicate itself to one feature&lt;/li&gt; &lt;li&gt;The neuron chooses to encode multiple features&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(We anthropomorphize - The neuron doesn’t choose to do anything - there is no free will - you are born into a loss landscape and an optimizer telling you what to do.)&lt;/p&gt; &lt;p&gt;In linear models, each neuron is limited to representing only the most significant features (2), discarding others (1). Conversely, superposition, enabled by non-linear activation functions, adopts a more inclusive approach (3), trying to encode multiple features per neuron and learning efficient representational shortcuts.&lt;/p&gt; &lt;p&gt;While ReLU bears similarity to the Gaussian Error Linear Unit (GeLU) used in modern GPT architectures, a deeper understanding of how different nonlinear activations impact superposition can provide crucial insights. Such understanding is key to unraveling the complex mechanisms through which neural networks utilize non-linearities, a cornerstone in the broader narrative of neural network interpretability.&lt;/p&gt; &lt;h3 id=&quot;monosemanticity-and-polysemanticity&quot;&gt;Monosemanticity and Polysemanticity&lt;/h3&gt; &lt;p&gt;To connect to existing literature (2) and (3) above are given the names monosemanticity and polysemanticity. We will also follow this notation going forward.&lt;/p&gt; &lt;p&gt;To describe further, the idea of superposition in neural networks leads us to two distinct types of neuron behaviors: monosemanticity and polysemanticity.&lt;/p&gt; &lt;p&gt;Monosemantic neurons are those that specialize in a single, distinct feature, acting as dedicated detectors. This characteristic is often observed in the intermediate layers of architectures like Convolutional Neural Networks (CNNs), where neurons become adept at recognizing specific patterns, such as curves or colors. Polysemantic neurons do not align with just one feature but engage with multiple features simultaneously, offering a broader and more nuanced understanding of the data. This trait is essential for handling complex, high-dimensional datasets but comes at the cost of reduced interpretability.&lt;/p&gt; &lt;h2 id=&quot;motivation-and-notation&quot;&gt;Motivation and Notation&lt;/h2&gt; &lt;p&gt;Our work extends the work done in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt; by examining how the changing of the activation function on toy model networks affects the behavior and interpretability of these networks. &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt; uses the canonical ReLU activation function to add non-linearity to two-layer models to analyze how superposition occurs within small networks. They did not generalize their work to other activation functions, which we find, result in &lt;strong&gt;distinct&lt;/strong&gt; new phenomenon. Our work compares the ReLU function with five other common activation functions: GeLU, SiLU, Sigmoid, Tanh, and SoLU. We hope that generalizing the phenomenon across activation functions can push the toy dataset to be in closer to realistic ML settings.&lt;/p&gt; &lt;h3 id=&quot;problem-specification&quot;&gt;Problem Specification&lt;/h3&gt; &lt;p&gt;The models in this experiment will be learning how to replicate a length-$n$ vector of inputs in the range $[0, 1]$ with a compression to a length-$m$ embedding (where $n&amp;gt;m$). The model will then use the length-$m$ embedding to recreate the length-$n$ input, using a non-linear activation function to allow for superposition.&lt;/p&gt; &lt;p&gt;We will run two variations of the experiment. One variation of the experiment will involve compressing inputs of size $n=10$ to an embedding of size $m=5$. This experiment aims to see how superposition occurs across many features which are encoded in a bottleneck with half the number of spots as there are features. The second variation of the experiment will involve compressing inputs of size $n=2$ to an embedding of size $m=1$. This experiment aims to understand precisely how the model encodes the second “extra” feature in a variety of settings.&lt;/p&gt; &lt;p&gt;To set up this experiment, we need to create a dataset that allows for superposition to occur and that also allows for interpretability of the superposition. To motivate this further, we begin with a careful discussion of features.&lt;/p&gt; &lt;h3 id=&quot;features&quot;&gt;Features&lt;/h3&gt; &lt;p&gt;Features are the salient “things” that a neural network learns to differentiate inputs &lt;d-cite key=&quot;features&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Technically, features are the properties which neural networks try to extract from data during learning to compress inputs to useful representations during inference. Although features can map to human-understandable concepts (e.g., dog ears), they can also represent properties of the data that are not immediately apparent to the human brain. To experiment with superposition, we need to encode features in a way that we can understand. In other words, we do not want our experimental model to learn features that we are unaware of. This would make it hard for us to interpret how the model maps features in the data to embeddings within its parameters, consequently obscuring how superposition works. To this aim, we must generate features within the training set for our model which are simple and understandable to us a priori. Similar to &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, we use as each input a vector with entries drawn independently from a uniform distribution over $[0, 1]$. Making each entry independent of the others enforces that each entry is its own (artificial) feature with no correlation to the other features.&lt;/p&gt; &lt;p&gt;Here we define two important augmentations that we used in the dataset to simulate real-world features: sparsity and importance.&lt;/p&gt; &lt;h4 id=&quot;sparsity&quot;&gt;Sparsity&lt;/h4&gt; &lt;p&gt;Sparsity is a measure of how often a specific feature is present in a dataset. A feature is characterized as “sparse” if it only appears in a small fraction of the inputs to the model. Similarly, features that are “dense” appear in many of the inputs. We will also use the term ‘density’, which is the complement of sparsity, defined as $1-S$.&lt;/p&gt; &lt;p&gt;Specifically, a feature with a sparsity of $S \in [0, 1]$ has a probability $S$ of being expressed in any given input. If we have $S=0$, this means that the feature is expressed in every input, whereas if we have $S=0.5$, this means that the feature is expected to be expressed in about half of the inputs.&lt;/p&gt; &lt;p&gt;In our experiment, we train models at different sparsities to capture how sparsity affects superposition.&lt;/p&gt; &lt;h4 id=&quot;importance&quot;&gt;Importance&lt;/h4&gt; &lt;p&gt;Not all features are created equal!&lt;/p&gt; &lt;p&gt;Some features are more useful than others in determining relevant information about inputs. For instance, when building a dog detector - capturing features related to dogs’ faces are extremely important! A model would need to pick up salient features of dogs, perhaps floppy ears and snouts. Other features, like the grass a dog is sitting on or a frisbee in a dog’s mouth, may not be as useful for detecting a dog. The varying degrees of usefulness among features are encapsulated in the concept of “importance”.&lt;/p&gt; &lt;p&gt;In the context of feature detection by a neural network, importance plays a role in modulating which features are encoded within the embedded layers of the network. In the context of the superposition hypothesis, if one feature has more importance than another feature, then it would be inefficient for the network to map both features equally within the embedding; allocating more weight to the feature with greater importance would be more valuable to the network in minimizing error.&lt;/p&gt; &lt;p&gt;In our experiment, we give each input feature a different importance to allow the models to differentiate between them. We will examine when and how the model justifies mapping multiple features of differing importances to the same neuron, i.e., we will observe the superposition of features with differing importances.&lt;/p&gt; &lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;To run this experiment, we will synthetically generate data that has desired sparsity and importance properties.&lt;/p&gt; &lt;p&gt;Each input $x$ will be a vector of length $n$. Each element $x_i$ in the vector will be drawn independently from the other elements in the uniform range $[0, 1]$. As discussed before, we can now synonymously refer to each of these elements as features, given their independent generation. (We will refer to them as features from this point onwards.)&lt;/p&gt; &lt;p&gt;Each feature $x_i$ in the vector has a relative importance to each of the other features $x_{j\ne i}$. The importance of feature $x_i$ is $I_i = r_I^i$ where $r_I\in(0, 1)$ is a constant describing the relative decay of importance between neighboring features. This attribute of the data will be implemented in the loss function (see below for more details).&lt;/p&gt; &lt;p&gt;We will train separate models for each of the varying levels of sparsity. For an input $x$ with sparsity $S$, each feature $x_i$ will take on its “true” value, a uniformly distributed number, with a probability of $1-S$ and will otherwise be set to 0 with a probability of $S$.&lt;/p&gt; &lt;p&gt;Below is a visualization of two batches of inputs with respective sparsities $S=0.5$ and $S=0.99$.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/input_batches-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/input_batches-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/input_batches-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/input_batches.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Each column of the plots represents a feature vector of length 20. Each batch has size 100, corresponding to the number of columns in the plots. Notice how the changing in sparsity affects the feature density. &lt;/div&gt; &lt;h3 id=&quot;network&quot;&gt;Network&lt;/h3&gt; &lt;p&gt;Below are the architectures of the base (linear) and experimental (non-linear) models that we are using in this experiment. Of particular note is the activation function $\mathbb{f}$, which we will substitute using the aforementioned activation functions.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Linear Model&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Activation ( $\mathbb{f}$ ) Output Model&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(h = Wx\)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(h = Wx\)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = W^T h + b\)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = f(W^T h + b)\)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = W^T Wx + b\)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;\(x&apos; = f(W^T Wx + b)\)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Autoencoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Autoencoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Autoencoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Autoencoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;We create an autoencoder - compressing down to induce polysemanticity. This maps $x$ to a direction in a lower-dimensional space, represented by \(h = Wx\). Each column of $W$ corresponds to a lower-dimensional representation of a feature in $x$. To reconstruct the original vector, $W^T$ is used, ensuring clear feature representation correspondence. This structure results in a symmetric matrix $W^TW$ and allows for clear visualization of the weights. They visually allow for the determination of the presence of superposition.&lt;/p&gt; &lt;h3 id=&quot;loss&quot;&gt;Loss&lt;/h3&gt; &lt;p&gt;Sparsity, Importance and Our Network come together in the following loss function:&lt;/p&gt; \[L = \sum_{i} \sum_{x} I_{i}(x_{i} - x&apos;_{i})^{2}\] &lt;p&gt;Motivated by &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, we use a standard MSE loss, where $x_i$ and $x_i’$ measure the absolute difference in the auto-encoding of the datapoint. The Importance factor, $I_i$ , describes how important the given reconstruction is. A smaller importance will allow loss minimization even with a poor reconstruction.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;Below we present each activation function, along with plots depicting how training results in superposition at varying degrees of sparsity.&lt;/p&gt; &lt;p&gt;For the $n=10, m=5$ experiment, we show the $W^TW$ matrix and neuron feature distribution at varying degrees of sparsity. The $W^TW$ matrix reveals which features are prioritized (shown by the diagonal terms) and any polysemanticity that occurs (shown by the off-diagonal terms). The neuron feature distribution shows how each of the $m=10$ features are mapped to each of the $n=5$ embedding dimensions. This can aid in understanding under what conditions polysemanticity arises and how it occurs under each condition of sparsity.&lt;/p&gt; &lt;p&gt;For the $n=2, m=1$ experiment, we show a phase diagram. This phase diagram shows how the second “extra” feature of the length-2 input vector is encoded. There are three options: not encoded at all (only the first feature is encoded), encoded in superposition with the first feature, and encoded as the only feature (the first feature is not encoded).&lt;/p&gt; &lt;h3 id=&quot;relu&quot;&gt;ReLU&lt;/h3&gt; &lt;p&gt;The ReLU (Rectified Linear Units) activation function is a piecewise-linear function, a simple non-linearity that allows models to use superposition of features. ReLU was the only activation function used in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, so our work with the ReLU function was primarily to verify the results from their work and create a baseline for our subsequent experiments.&lt;/p&gt; &lt;p&gt;The following are the $W^TW$ matrices and feature-neuron mappings:&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; ReLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_relu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_relu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_relu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_relu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;As per the results in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, the ReLU model focuses on the most significant features in the low sparsity regime (generally resulting in monosemanticity), while relying on superposition in the high sparsity regime (polysemanticity). With weaker signals for the most important features in the high sparsity regime, the model encodes multiple features in each neuron activation to minimize error of the sparse signals. Notably, the ReLU model uses antipodal pairs in the mapping of features to encode multiple features to single neurons. This can be seen as a light-colored diagonal entry within $W^T W$ and a corresponding dark-colored off-diagonal entry within the same column. This antipodal mapping of features is a method that the model uses to compress more than one feature to one neuron. This antipodal mapping is more interpretable than other kinds of polysemanticity which occurs in subsequently-described activation functions which “speckle” multiple features into a single neuron, making it more difficult to determine how the superposition occurs in that model.&lt;/p&gt; &lt;p&gt;The following is the phase diagram of the ReLU models:&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_relu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_relu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_relu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_relu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;In regimes of high sparsity (i.e., below $1-S=0.1$ on the phase diagram above) the ReLU models are highly polysemantic for all relative feature importances, reflecting an inability to encode features with a sparse signal. In regimes of low sparsity, the model generally embeds the more important of the two features. This result mirrors the phase diagram in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt; as expected.&lt;/p&gt; &lt;h3 id=&quot;gelusilu&quot;&gt;GeLU/SiLU&lt;/h3&gt; &lt;p&gt;The GeLU (Gaussian Error Linear Units) and SiLU (Sigmoid Linear Units) activation functions are very similar to one another, and as a result produced very similar experimental results. Both functions are akin to a “smoothed out” version of the ReLU function, i.e., they have no discontinuities. The GeLU has recently been popularized as the activation function of choice in many transformers, including BERT &lt;d-cite key=&quot;Devlin2019BERTPO&quot;&gt;&lt;/d-cite&gt; and GPT &lt;d-cite key=&quot;gpt&quot;&gt;&lt;/d-cite&gt;. The GeLU is differentiable for all $x$ - and has a smoother curve than the SiLU (Swish) activation. &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt; found that in the setting of transformers, the GeLU was less interpretable than the SoLU. This may be the case after having many linear layers activation - but with a single layer this is not the case.&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; GeLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_gelu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_gelu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_gelu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_gelu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; SiLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_silu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_silu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_silu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_silu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The GeLU and SiLU models exhibit similar kinds of superposition in their weight matrices. With increasing sparsity, superposition of features does happen, but it is more “strict” than the ReLU model, generally mapping at most two features to any single neuron. In each of the polysemantic neurons, though, there is one feature that dominates, suggesting that these activation functions enforce sparsity in their activations. There are also many antipodal pairs of features within these models, reiterating the behavior that exists in the ReLU models (also found in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;).&lt;/p&gt; &lt;div class=&quot;row mt-0 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-2 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_gelu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_gelu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_gelu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_gelu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-2 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_silu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_silu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_silu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_silu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-0 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-2 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The above phase diagrams of the GeLU and SiLU models show a marked difference from that of the ReLU model (earlier), despite the similar shapes of these three activation functions. The GeLU and SiLU models exhibit significant monosemanticity at high degrees of sparsity, unlike the ReLU, which results in near-complete polysemanticity for sparsities higher than $S=0.9$. This differnce may reflect SiLU’s and GeLU’s better fit as an activation for picking up the signal in sparse feature representations, making the case for GeLU and SiLU as more interpretable activation functions within larger models.&lt;/p&gt; &lt;h3 id=&quot;sigmoid&quot;&gt;Sigmoid&lt;/h3&gt; &lt;p&gt;The Sigmoid function is a smooth activation function with an output range of $(0, 1)$. This maps directly to the desired range of values that the model is trying to replicate.&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; Sigmoid $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_sigmoid-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_sigmoid-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_sigmoid-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_sigmoid.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The Sigmoid model exhibits superposition in all neurons as soon as the sparsity is non-zero, as can be seen from the “speckling” of non-zero off-diagonal terms in $W^T W$. This is a difference from the ReLU/GeLU/SiLU models, for which the superposition “leaks” into the least significant encoded features at low, non-zero sparsities and eventually affects all features at higher sparsities. This low-sparsity superposition may occur because the Sigmoid function strictly maps to $(0, 1)$, with increasingly large pre-activation inputs necessary to map to values close to 0 and 1. As such, the model may be “speckling” the off-diagonal values in an attempt to “reach” these inputs which are close to 0 and 1.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_sigmoid-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_sigmoid-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_sigmoid-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_sigmoid.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Despite differences in the occurrence of polysemanticity, the ReLU and Sigmoid models exhibit very similar phase diagrams, reflecting an inability to encode multiple features at sparsities above $S=0.9$ (i.e., below $1-S=0.1$ on the phase diagram). As discussed above, this may be caused by the vanilla sigmoid activation’s inability to “reach” target values close to 0 or 1.&lt;/p&gt; &lt;h3 id=&quot;tanh&quot;&gt;Tanh&lt;/h3&gt; &lt;p&gt;The Tanh function is another smooth activation function, but it results in significantly different behavior from the Sigmoid (despite being a linear mapping of the Sigmoid).&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; Tanh $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_tanh-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_tanh-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_tanh-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_tanh.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;With the Tanh activation function, the models prioritize the most important features regardless of sparsity. This behavior is possibly attributed to the range that the Tanh function maps to $(-1, 1)$, while the target range of input values in this experiment are $[0, 1]$. This behavior is similar to that of a linear model (i.e., no activation function) which exhibits no capability to use superposition, but the phase diagram reveals subtle differences from the linear model results.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_tanh-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_tanh-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_tanh-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_tanh.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Although nearly performing as the linear model would, only encoding the most important feature, there is some difference to the linear model along the boundary between features, as can be seen around the importance of 1. This reflects the model’s ability to use non-linearity to perform superposition.&lt;/p&gt; &lt;h4 id=&quot;a-note-on-sigmoid-and-tanh&quot;&gt;A Note on Sigmoid and Tanh&lt;/h4&gt; &lt;p&gt;Despite similarities in the S-like curvature of the Sigmoid and Tanh activation functions, the Sigmoid model exhibits superposition, whereas the Tanh model exhibits nearly zero superposition. A key difference between the two functions is the fact that the Sigmoid function maps inputs to a range of $(0, 1)$, while the Tanh function maps inputs to a range of $(-1, 1)$. This difference is significant in our experiment, as our experiment uses models to recreate random vectors with elements in the range $[0, 1]$. The range of the Sigmoid function matches this range, while the range of the Tanh function which matches this range only occurs for non-negative inputs to the Tanh function. In other words, the $(-\infty, 0)$ input domain (which maps to the range $(-1, 0)$) of the Tanh function remains useless for prediction of values which should be in the range $[0, 1]$. Therefore, the tanh function empirically acts like a linear function (i.e., no activation layer).&lt;/p&gt; &lt;h3 id=&quot;solu&quot;&gt;SoLU&lt;/h3&gt; &lt;p&gt;The SoLU (Softmax Linear Units) activation function is based on the work from &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt;. \(Solu(x) = x * softmax(x)\) SoLU is a function for which the activation of each neuron is dependent on all the other neurons within its own layer. This is significantly different from all the other activations that we tested, as the activations of neurons with the other functions are independent of the other neurons within the same layer. In other words, all the other activation functions are univariate while the SoLU is multivariate. Similar to other approaches like L1 regularization, the SoLU amplifies neurons with relatively large pre-activations and de-amplifies neurons with relatively smaller pre-activations. This behavior pressures the model to be more monosemantic (and therefore more interpretable in some settings), as discussed in &lt;d-cite key=&quot;elhage2022solu&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; SoLU $W^TW$ Matrices &lt;/div&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_solu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_solu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_solu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/Sparsity_super_solu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;In our experiment, the SoLU model results in non-zero superposition of all features with all degrees of sparsity. This may be attributed to the way that the SoLU “forces” activations to be sparse, i.e., the activations result in a “winner-takes-all” behavior due to the way that the Softmax function works. This is not a useful property for prediction of a vector of independently-drawn values, as the input vectors are unlikely to be peaky, i.e., the SoLU does not quite fit the purposes of its task.&lt;/p&gt; &lt;div class=&quot;row mt-3 l-page&quot;&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_solu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_solu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_solu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/phase_51_solu.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-6 mx-auto mt-3 mt-md-0 d-flex align-items-center&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-interpretability-of-toy-tasks/legend.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;As seen in the heatmap plot above, the SoLU activation results in very polysemantic behavior. This function is not precisely fit for its task of recreating given vectors and likely results in using polysemanticity to attempt to pass information about inputs forward. Curiously, the SoLU models have preference for the more important feature in the low sparsity regime.&lt;/p&gt; &lt;h3 id=&quot;bringing-them-all-together&quot;&gt;Bringing Them All Together&lt;/h3&gt; &lt;div class=&quot;caption&quot;&gt; Sparsity vs Dimensions Per Feature &lt;/div&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-09-interpretability-of-toy-tasks/file.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;p&gt;The diagram above depicts a variation on the two experiments explained thus far. In this experiment $n=200$ features were compressed to $m=20$ features and the loss function was tweaked to give uniform importance $I_i = 1$ to all features. This was done to determine how each activation functions compresses features in different sparsity regimes without the influence of feature importance.&lt;/p&gt; &lt;p&gt;On the y axis, the plot depicts a metric (dimensions per feature) that measures the number of dimensions a model dedicates to each feature. In other words, a point with a y-value near 1 represents a model that dedicates one dimension of its embedding space to one feature, whereas a point with a y-value near 0.25 represents a model that represents four features at each dimension.&lt;/p&gt; &lt;p&gt;The plots are generally consistent with the analysis from the previous experiments. Many of the activations result in superposition in the low-density/high-sparsity regime, and increases in sparsity result in increases in the polysemanticity of the model (i.e., the dimensions per feature decrease). Consistent with the other experiments, SiLU and GELU perform very similarly. The Sigmoid and SoLU activations pack nearly 20 features per dimension at high sparsities. The Tanh activation exhibits behavior similar to the linear model, neatly packing one dimension with one feature, a result that is mirrored in the previous experiments. Similar to the results in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;, we see “sticky” behavior of the ReLU activation function at 1 and 0.5 dimensions per feature. This can be explained by the phenomenon of “antipodal pairs” discussed in &lt;d-cite key=&quot;toymodels&quot;&gt;&lt;/d-cite&gt;. None of the other activation functions that we tested exhibit this behavior - which is striking since this is a well-studied effect for the ReLU activation function. This may be because the ReLU activation function is the only one that is not smooth, and therefore has a differentiable behavior than the other activation functions.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our investigation into the effects of various activation functions reveals that significant changes occur in model behavior depending on the chosen function. This finding underscores the ability to modulate the degree of superposition through the selection of activation functions, highlighting yet unexplored degrees of freedom in model design. This line of inquiry goes seamlessly with considerations of how neural networks are initialized and trained, suggesting these as promising future research directions.&lt;/p&gt; &lt;p&gt;Our work is limited by the breadth of activation functions that we tested, though. Further iterations on each of the activation functions (e.g., tweaking the Sigmoid function to map to the range $(-\epsilon, 1+\epsilon)$) could prove fruitful in getting better performance from the models. Furthermore, while writing this blog, &lt;d-cite key=&quot;incidental&quot;&gt;&lt;/d-cite&gt; published a new key insight related to the importance of initialization in superposition, which we do not explore here. Despite this, we have learned valuable insights about the effects that our set of activation functions can have on superposition.&lt;/p&gt; &lt;p&gt;Pursuing enhanced interpretability, however, does not come without its challenges. Specifically, striving for transparency and understandability in neural network models raises concerns about the potential for deception. Despite these challenges, our work aims to develop neural network models that are more interpretable, transparent, and secure.&lt;/p&gt; &lt;ol class=&quot;bibliography&quot;&gt;&lt;/ol&gt; </content> </entry> <entry> <title>Training Robust Networks</title> <link href="https://deep-learning-mit.github.io/blog/2023/generating-robust-networks/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/generating-robust-networks</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;In the recent years, deep neural networks have emerged as a dominant force in the field of machine learning, achieving remarkable success across a variety of tasks, from VGG-16 in image classification to ChatGPT in natural language modeling. However, the very complexity that allows deep neural networks to learn and represent complex patterns and relationships can also leave them susceptible to challenges such as overfitting, adversarial attacks, and interpretability. The brittleness of deep neural networks, in particular, poses a significant challenge toward their deployment in real-world applications, especially those where reliability is paramount, like medical image diagnosis and autonomous vehicle navigation. Consequently, it is crucial to develop a better understanding of deep architectures and explore strategies for enhancing robustness. This project focuses specifically on ResNet, a model introduced in 2015 for image classification that is still widely used today. In particular, we study the model’s vulnerability to adversarial perturbations and, subsequently, work through a strategy to enhance its resilience through data augmentation and hyperparameter optimization.&lt;/p&gt; &lt;h1 id=&quot;related-works&quot;&gt;Related Works&lt;/h1&gt; &lt;p&gt;ResNet&lt;d-cite key=&quot;resnet2015&quot;&gt;&lt;/d-cite&gt; is a convolutional neural network architecture introduced in 2015 that sought to overcome numerical instability issues in deep networks and simplify the complexity of architecture search. It achieved this by incorporating skip connections, essentially allowing the training procedure to dynamically determine the optimal number of layers for the network. ResNet is trained on the ImageNet dataset&lt;d-cite key=&quot;imagenet2014&quot;&gt;&lt;/d-cite&gt;, a popular benchmark in object category classification with 1,000 classes and millions of images. For our project, we use ResNet-18, a version of the original ResNet-34 model that is 18 layers deep, and TinyImageNet, a smaller version of ImageNet with around 100,000 images and 200 classes. This is largely for computational ease.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/resnet-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/resnet-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/resnet-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/resnet.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1. ResNet-18 Architecture &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/tinyimagenet-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/tinyimagenet-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/tinyimagenet-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/tinyimagenet.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2. Sample Images from TinyImageNet &lt;/div&gt; &lt;p&gt;The brittleness of many deep neural networks for computer vision, including ResNet, is well documented. For example, adding a tiny amount of random Gaussian noise, imperceptible to the human eye, can dramatically affect the accuracy and confidence of a network. In fact, we can optimize over the input image to generate small, non-random perturbations that can be used to alter the network’s prediction behavior arbitrarily, a vulnerability that applies to a variety of networks&lt;d-cite key=&quot;brittleness1&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;brittleness2&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In this project, we investigate two small perturbations: adding random Gaussian noise and modifying the colors of a small subset of pixels. We use hyperparameter search to fine-tune ResNet-18, aiming to create a network robust to these perturbations without compromising significantly on accuracy. Specifically, we examine general hyperparameters like batch size, learning rate, number of frozen layers, and more. The ultimate goal is to define a straightforward and resource-efficient strategy for mitigating brittleness that can potentially be extended to other architectures and domains.&lt;/p&gt; &lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt; &lt;h2 id=&quot;baseline-model&quot;&gt;Baseline Model&lt;/h2&gt; &lt;p&gt;The out-of-the-box ResNet18 model is pretrained on ImageNet, achieving about 55% accuracy on the ImageNet validation set. TinyImageNet is a subset of ImageNet with fewer classes; there is a potential need for further fine-tuning of the out-of-the-box model to optimize performance. Thus, we start off by performing a simple hyperparameter grid search over batch size and learning rate. Each model is trained on the TinyImageNet training set, a dataset of 40,000 images (downsampled from 100,000 for computational ease) with 200 classes (roughly uniform class distribution). The baseline model is then selected based on accuracy on the TinyImageNet validation set, a uniformly balanced dataset of 10,000 images.&lt;/p&gt; &lt;h2 id=&quot;generating-adversarial-perturbations&quot;&gt;Generating Adversarial Perturbations&lt;/h2&gt; &lt;p&gt;Next, we use gradient descent to create adversarial perturbations. The first perturbation is adding a small amount of Gaussian noise. We try to maximize the probability of the input image belonging to a wrong class (the inverse of the standard cross-entropy classification objective) while also penalizing the magnitude of the noise. This approach is more efficient and controllable compared to attempting to add a random sample of Gaussian noise with the hope of inducing misclassification.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-4&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-4 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_steps-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_steps-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_steps-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_steps.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-4&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3. Noise added to image during each step in a sample gradient descent path for the first perturbation &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_examples-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_examples-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_examples-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/noise_examples.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4. Sample images and their Gaussian-perturbed, misclassified versions &lt;/div&gt; &lt;p&gt;The other perturbation is randomly selecting a small subset of pixels (0.5%) and adjusting their color until the image is misclassified by the baseline model. A gradient descent approach that maximizes the probability of the input image belong to a wrong class is used to implement this perturbation; however, it is much more sensitive to initialization and can require retries, making it less resource-efficient.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-4&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-4 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_steps-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_steps-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_steps-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_steps.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-4&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5. Noise added to image during each step in a sample gradient descent path for the second perturbation &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_examples-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_examples-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_examples-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/pixel_examples.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 6. Sample images and their pixel-perturbed, misclassified versions &lt;/div&gt; &lt;p&gt;We generate 11,000 adversarial examples using the Gaussian noise perturbation technique on the training examples that the baseline model correctly classifies. Of these adversarial examples, we use 10,000 of them to augment the training dataset (call it the augmented training set) and reserve 1,000 for hyperparameter optimization (call it the perturbed training set). We also generate 2,000 adversarial examples using the same perturbation technique on the validation examples that the baseline model correctly classifies. 1,000 of these are used for hyperparameter optimization (call it the perturbed validation set) while the rest are saved for out-of-sample evaluation (call it the hold-out validation set).&lt;/p&gt; &lt;p&gt;Note that we keep adversarial examples generated from the validation set out of the augmented training set to avoid lookahead bias. We want to avoid allowing the model to gain insights into the characteristics of examples that it will encounter in the validation set (since perturbed images are very similar to the original images), ensuring a more accurate assessment of the model’s robustness and generalization capabilities.&lt;/p&gt; &lt;p&gt;Finally, we generate an additional 500 examples using the pixel modification perturbation technique on the validation examples that the baseline correctly classifies (call it the out-of-distribution hold-out set). These examples are reserved for out-of-sample and out-of-distribution evaluation, assessing the model’s ability to perform well on adversarial perturbations it has never seen before.&lt;/p&gt; &lt;h2 id=&quot;hyperparameter-optimization-to-create-a-more-robust-model&quot;&gt;Hyperparameter Optimization to Create a More Robust Model&lt;/h2&gt; &lt;p&gt;Equipped with the augmented/additional datasets from the previous step, we start the process of model creation. The relevant metrics for selecting a model are original validation accuracy (derived from the original validation dataset from TinyImageNet), perturbed training accuracy, and perturbed validation accuracy. It is crucial to look at original validation accuracy to ensure that we are not creating robust models by compromising significantly on the original image classification task. In addition, accuracy on the perturbed train dataset tells us how well our model adjusts to the perturbation, while accuracy on the perturbed validation dataset provides an additional perspective by evaluating how well the model generalizes to perturbations on images it has never seen before. The same set of metrics is used in evaluating the final model on out-of-sample datasets, in addition to accuracy on the out-of-distribution hold-out set.&lt;/p&gt; &lt;p&gt;We examine how varying four different hyperparameters affects the robustness of ResNet-18. The first hyperparameter involves initializing the model with either weights from the baseline model or the default pre-trained weights. The next hyperparameter is how many layers of ResNet-18 are frozen during the training procedure. The last two hyperparameters are batch size and learning rate. It is important to note that we do not conduct a search over a four-dimensional hyperparameter grid for computational reasons. Instead, we fix some hyperparameters at reasonable default values while we vary over the other hyperparameters. Using the insights gleaned from this hyperparameter search, we proceed to train the final model.&lt;/p&gt; &lt;h2 id=&quot;comparing-models-via-visualization&quot;&gt;Comparing Models via Visualization&lt;/h2&gt; &lt;p&gt;Finally, we transform the feature maps generated for an input image into interpretable visualizations to better understand the learned representations within the models. These feature maps capture the activations of learned filters or kernels across different regions of the input images and are the basis for our analysis&lt;d-cite key=&quot;simonyan2014&quot;&gt;&lt;/d-cite&gt;. Each residual block in a ResNet consists of multiple convolutional layers. We register forawrd hooks (a feature in Pytorch that allows us to register a function to be called each time a forward pass is executed through a layer) for each convolutional and linear layer in the network to capture and store the activations produced during the forward pass. The layers in the ResNet model are as follows:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Layer: conv1, Activation shape: torch.Size([1, 64, 112, 112]) Layer: layer1.0.conv1, Activation shape: torch.Size([1, 64, 56, 56]) Layer: layer1.0.conv2, Activation shape: torch.Size([1, 64, 56, 56]) Layer: layer1.1.conv1, Activation shape: torch.Size([1, 64, 56, 56]) Layer: layer1.1.conv2, Activation shape: torch.Size([1, 64, 56, 56]) Layer: layer2.0.conv1, Activation shape: torch.Size([1, 128, 28, 28]) Layer: layer2.0.conv2, Activation shape: torch.Size([1, 128, 28, 28]) Layer: layer2.0.downsample.0, Activation shape: torch.Size([1, 128, 28, 28]) Layer: layer2.1.conv1, Activation shape: torch.Size([1, 128, 28, 28]) Layer: layer2.1.conv2, Activation shape: torch.Size([1, 128, 28, 28]) Layer: layer3.0.conv1, Activation shape: torch.Size([1, 256, 14, 14]) Layer: layer3.0.conv2, Activation shape: torch.Size([1, 256, 14, 14]) Layer: layer3.0.downsample.0, Activation shape: torch.Size([1, 256, 14, 14]) Layer: layer3.1.conv1, Activation shape: torch.Size([1, 256, 14, 14]) Layer: layer3.1.conv2, Activation shape: torch.Size([1, 256, 14, 14]) Layer: layer4.0.conv1, Activation shape: torch.Size([1, 512, 7, 7]) Layer: layer4.0.conv2, Activation shape: torch.Size([1, 512, 7, 7]) Layer: layer4.0.downsample.0, Activation shape: torch.Size([1, 512, 7, 7]) Layer: layer4.1.conv1, Activation shape: torch.Size([1, 512, 7, 7]) Layer: layer4.1.conv2, Activation shape: torch.Size([1, 512, 7, 7]) Layer: fc, Activation shape: torch.Size([1, 1000]) &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;After obtaining these activations, we compute the average activation values across the channels (neurons) within a specified layer of interest. This process provides insights into which regions or patterns in the input images contribute significantly to the neuron activations within that layer. We then create heatmap visualizations based on these average activations, highlighting the areas of the input data that have the most substantial impact on the network’s feature detection process. This allows us to gain valuable insights into how the network perceives and prioritizes various features across its layers, aiding in our understanding of the model’s inner workings.&lt;/p&gt; &lt;p&gt;We use this approach to compare the baseline model to the final model, aiming to identify significant differences in feature prioritization or the patterns detected at various layers.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/heatmap_sample-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/heatmap_sample-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/heatmap_sample-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/heatmap_sample.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 7. Heatmap visualization at four different layers when an image of a goldfish is passed into the ResNet &lt;/div&gt; &lt;h1 id=&quot;results-and-discussion&quot;&gt;Results and Discussion&lt;/h1&gt; &lt;h2 id=&quot;baseline-model-1&quot;&gt;Baseline Model&lt;/h2&gt; &lt;p&gt;First, we perform a grid search over batch sizes ranging from 128 to 512 and learning rates ranging from 0.0001 to 0.01.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 8. Hyperparameter grid for baseline model &lt;/div&gt; &lt;p&gt;The results from the first hyperparameter search suggest that conservative learning rates and large batch sizes lead to good performance. Thus, we perform a finer grid search over batch sizes ranging from 256 to 512 and learning rates ranging from 0.00001 to 0.0001.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/baseline2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-3&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 9. Finer hyperparameter grid for baseline model &lt;/div&gt; &lt;p&gt;Based on the results from the second hyperparameter search, we choose our baseline model to be ResNet-18 fine-tuned with a batch size of 256 and a learning rate of 0.00005. The baseline model achieves nearly 73% accuracy on the validation set, which is possibly due to the fact that TinyImageNet has less classes, so classification may be an easier task.&lt;/p&gt; &lt;h2 id=&quot;effect-of-hyperparameters&quot;&gt;Effect of Hyperparameters&lt;/h2&gt; &lt;h4 id=&quot;number-of-unfrozen-layers&quot;&gt;Number of Unfrozen Layers&lt;/h4&gt; &lt;p&gt;Next, we evaluate how the number of unfrozen layers (up to 3) affects the robustness of the trained models, whose weights can either be initialized from the baseline model or from the pre-trained/default model (in the diagram below, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_finetuned=True&lt;/code&gt; corresponds to the baseline model).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-7 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-4 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 10. Performance of trained models as number of frozen layers and source of initialized weights changes &lt;/div&gt; &lt;p&gt;First, we observe that training for more epochs does not improve the metrics of interest. This implies that training for robustness can be computationally efficient. Next, we observe there is a substantial drop in accuracy for the perturbed datasets compared to the original validation dataset, which is to be expected. Pairing the accuracies for the perturbed datasets across hyperparameter combinations, we observe that they are tightly correlated, which implies that our models are effectively adapting to the perturbation.&lt;/p&gt; &lt;p&gt;One interesting observation to note here is that accuracies on the perturbed datasets are significantly higher for the model initialized with default weights (27% compared to 10%). An intuitive explanation for this is that we have deliberately engineered a brittle baseline model, so the model is in a region of the optimization landscape characterized by high accuracy but low robustness. If we want achieve high accuracy and high robustness, we may need to start from a less unfavorable position in the optimization landscape.&lt;/p&gt; &lt;p&gt;Finally, we observe that freezing some layers can enhance robustness for models initialized from the default weights at the cost of performance on the original task. This aligns with intuition, since allowing all the weights to vary can lead to overfitting, resulting in more brittle networks.&lt;/p&gt; &lt;h4 id=&quot;batch-size&quot;&gt;Batch Size&lt;/h4&gt; &lt;p&gt;Next, we evaluate how batch size (ranging from 4 to 512) affects the robustness of the trained models.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-7 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-4 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 11. Performance of trained models as batch size and source of initialized weights changes &lt;/div&gt; &lt;p&gt;We notice immediately that batch size has a considerable effect on robustness. For both the perturbed training set and the perturbed validation set, accuracies are markedly lower with large batch sizes (around 15%) and higher with small batch sizes (around 70%). As expected, this comes at the expense of lower performance on the original task, with original validation accuracy dropping 10% as the batch size decreases from 512 to 4. Depending on the use case, this may be an efficient tradeoff to make!&lt;/p&gt; &lt;h4 id=&quot;learning-rate&quot;&gt;Learning Rate&lt;/h4&gt; &lt;p&gt;Finally, we evaluate how learning rate (ranging from 0.00001 to 0.001) affects the robustness of the trained models.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-7 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_line3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-sm-4 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/final_bar3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 12. Performance of trained models as learning rate and source of initialized weights changes &lt;/div&gt; &lt;p&gt;Like batch size, learning rate significantly impacts robustness. The sweet spot for learning rate in terms of robustness seems to be around 0.00025, with original validation accuracy dropping as the learning rate becomes more conservative; a learning rate of 0.00025 leads to a 3% drop in performance. Like before, this may be a worthwhile tradeoff to make.&lt;/p&gt; &lt;h2 id=&quot;out-of-sample-evaluation&quot;&gt;Out of Sample Evaluation&lt;/h2&gt; &lt;p&gt;Using the insights gained from the hyperparameter search, we define the final model with the following hyperparameters:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;is_finetuned=False num_unfrozen_layers=3 batch_size=8 learning_rate=0.00025 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Of course, this is likely not the optimal hyperparameter combination, since we were not able to perform a full grid search. The results are as follows:&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; Table 1. Performance of final model &lt;/div&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Original validation&lt;/td&gt; &lt;td&gt;0.522754&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Perturbed training&lt;/td&gt; &lt;td&gt;0.569572&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Perturbed validation&lt;/td&gt; &lt;td&gt;0.442720&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hold-out validation&lt;/td&gt; &lt;td&gt;0.485621&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Out-of-distribution validation&lt;/td&gt; &lt;td&gt;0.489786&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Original validation, perturbed validation, and hold-out validation accuracy are somewhat lower than the optimistic estimates derived from the hyperparameter search. However, we observe that we are able to achieve nearly 50% accuracy on the out-of-distribution validation set, which contains pixel modification perturbations that the model was never trained on, underscoring the robustness and adaptability of our model.&lt;/p&gt; &lt;h2 id=&quot;model-comparison&quot;&gt;Model Comparison&lt;/h2&gt; &lt;p&gt;Lastly, we observe the progression of feature map representations: starting from basic visual elements such as edges and textures in the initial layers, to more complex patterns in intermediate layers, and culminating in sophisticated, high-level feature representations in the deeper layers. This layered evolution is integral to the network’s ability to analyze and recognize complex images.&lt;/p&gt; &lt;p&gt;When comparing the baseline model to the final model, there are very few (if any) differences in the initial layers. By the intermediate and deeper layers, there are clear differences in which aspects of the images have the greatest activation. This observation aligns with the foundational principles of convolutional neural networks, where initial layers tend to be more generic, capturing universal features that are commonly useful across various tasks. As a result, the similarity in the initial layers between the baseline and final models suggests that these early representations are robust and essential for basic image processing, irrespective of specific model optimizations or task-focused training.&lt;/p&gt; &lt;p&gt;However, the divergence observed in the intermediate and deeper layers is indicative of the specialized learning that occurs as a result of hyperparameter tuning in the final model. These layers, being more task-specific, have adapted to capture more complex and abstract features relevant to the particular objectives of the final model.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_original-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_original-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_original-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_original.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 13. Comparison of the heatmaps for both models when passed in an image of a frog &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_perturbed-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_perturbed-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_perturbed-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-generating-robust-networks/comparison_perturbed.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 14. Comparison of the heatmaps for both models when passed in a perturbed image of a frog &lt;/div&gt; &lt;h1 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and Next Steps&lt;/h1&gt; &lt;p&gt;In this project, we have undertaken a comprehensive exploration of enhancing ResNet through data augmentation with adversarial examples and straightforward hyperparameter tuning. Key highlights include the computational efficiency and simplicity of the employed technique, the resulting model’s ability to adapt to both seen and unseen perturbations, and the capacity to finely control tradeoffs between robustness and accuracy thorugh the manipulation of diverse hyperparameters.&lt;/p&gt; &lt;p&gt;There are many potential avenues for future exploration. One prospect involves expanding and refining the discussed techniques by continuing to explore the hyperparameter space, considering additional parameters or refining the search range. Additionally, applying this analysis to different architectures and domains could reveal further insights. Finally, broadening the scope of perturbations presents another avenue, offering the potential to develop more practical models tailored for real-world applications.&lt;/p&gt; </content> </entry> <entry> <title>Imposing uniformity through Poisson flow models</title> <link href="https://deep-learning-mit.github.io/blog/2023/enforcing-uniformity/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/enforcing-uniformity</id> <content type="html">&lt;!-- Introduction --&gt; &lt;p&gt;Most objects encountered in machine learning are extremely high dimensional. For example, a relatively small $512$x$512$ RGB image has over $750,000$ dimensions. However most of this space is empty, that is the set of well-formed images form an extremely small subset of this large space.&lt;/p&gt; &lt;p&gt;Thus a useful task in machine learning is to map this large space into a much smaller space, such that the images we care about form a compact organized distribution in this new space. This is called representation learning. For such a map to be useful, there are two key features. Firstly the representations should be useful for downstream tasks and not worse than the original representation. Thus they should preserve as much of the useful data as possible. Secondly, they should be relatively task agnostic and help across a diverse array of such downstream tasks. For example, word embeddings (such as those produced by BERT &lt;d-cite key=&quot;bert&quot;&gt;&lt;/d-cite&gt;) can be used for a wide array of language tasks such as language modeling and generation to sentiment analysis. An important question is how to generally find such useful representations.&lt;/p&gt; &lt;p&gt;Several methods exist. For example, autoencoders &lt;d-cite key=&quot;autoencoder&quot;&gt;&lt;/d-cite&gt; attempt to learn maps that are essentially bijective over the dataset we care about. These ensure that important information is not lost during the transformation. Contrastive encoders attempt to learn maps that enforce similarity between representations of similar images. Contrastive encoders are seen to perform quite well on unsupervised representation learning tasks, and we will explore these in a bit more detail soon. Lastly, we can layers of already trained neural networks can be used as features as well. For example, layers of VGG-19 trained on ImageNet are useful features that contain much information about the style and content of the images &lt;d-cite key=&quot;nst&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;It is important to further quantify what makes a useful representation from a theoretical standpoint. Wang and Isola &lt;d-cite key=&quot;isola-alignment&quot;&gt;&lt;/d-cite&gt; introduced concepts of alignment and uniformity to explain why contrastive encoders perform well. Alignment is the idea that similar objects should have close feature vectors in the representation space. Uniformity is the idea that the set of well-formed objects should cover the representation space uniformly.&lt;/p&gt; &lt;p&gt;In this post, we will further examine how uniformity affects the quality of representations. To do this, we will use Poisson flows. As we shall see, Poisson flows are an incredibly useful tool to enforce uniformity. We show that enforcing uniformity on well-aligned features can improve representations as measured by their performance on downstream tasks.&lt;/p&gt; &lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt; &lt;p&gt;We introduce several notations to make talking about representations easier. Let $\mathcal{X}$ be our original space of the data, and let $p_{\mathrm{x}}$ be the distribution of the data. Let $\mathcal{Y}$ be any representation space, and let $f: \mathcal{X} \to \mathcal{Y}$ be a mapping from the original space to the representation space. If $\mathrm{y} = f(\mathrm{x}), \ \mathrm{x} \sim p_{\mathrm{x}}$, then let $\mathrm{y} \sim p_{f}$ and where $p_{f}$ is the new distribution after $f$.&lt;/p&gt; &lt;p&gt;We will also have a notion of similarity. Let $p_{\mathrm{pos}}(x_1, x_2)$ be a joint probability distribution that quantifies this similarity. We assume that $p_{\mathrm{pos}}$ satisfies&lt;/p&gt; \[\begin{aligned} p_{\mathrm{pos}}(x_1, x_2) &amp;amp;= p_{\mathrm{pos}}(x_2, x_1) \\ \int_{x_2} p_{\mathrm{pos}}(x_1, x_2) d x_2 &amp;amp;= p_{\mathrm{x}}(x_1) \end{aligned}\] &lt;h2 id=&quot;alignment-and-uniformity&quot;&gt;Alignment and Uniformity&lt;/h2&gt; &lt;p&gt;As mentioned earlier, contrastive autoencoders learn useful representations by minimizing a distance metric for similar pairs, while maximizing the same for dissimilar pairs &lt;d-cite key=&quot;isola-cmc&quot;&gt;. Thus if $D(x_1, x_2)$ is some distance metric of $\mathcal{Y}$, contrastive encoders maximize $d(x, x^+)$ for positive pairs, while minimizing $d(x, x^-)$ for negative pairs.&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;In their most common formulation, they set $\mathcal{Y}$ as the hypersphere $\mathcal{S}^d \subset \mathbb{R}^d$, and use cosine similarity &lt;d-cite key=&quot;SimCLR&quot;&gt;&lt;/d-cite&gt; $d(x_1, x_2) = x_1^T x_2$ as the similarity metric. Then the loss function becomes &lt;d-footnote&gt;$M$ and $\tau$ are hyper parameters&lt;/d-footnote&gt;&lt;/p&gt; \[\mathcal{L} \triangleq \mathbb{E}_{(x, x^+) \sim p_{\mathrm{pos}}, \{x_i^-\}_{i=1}^M \overset{\mathrm{iid}}{\sim} p_{\mathrm{x}}} \left[ \frac {e^{f(x)^T f(x^+)^T / \tau}} {e^{f(x)^T f(x^+)^T / \tau} + \sum_{i=1}^{M} e^{f(x)^T f(x_i^-)^T / \tau}} \right]\] &lt;p&gt;These encoders have been successful at several image representation tasks. Wang and Isola explained their performance through alignment and uniformity. Alignment, is simply the the quality that similar images are close together in the representation space. This is clearly present in contrastive encoders, as one of their goals is indeed to minimize&lt;/p&gt; \[\mathcal{L}_{\mathrm{alignment}} \triangleq \mathbb{E}_{(x, x^+)\sim p_{\mathrm{pos}}} \left[ D(x, x^+) \right]\] &lt;p&gt;However, Wang and Isola also stated that uniformity was an equally important feature of contrastive architectures. That is, when training the contrastive loss to learn an encoder $f$, the new probability distribution $p_{f}$ is close to uniform. They showed that using $L_2$ norm as a distance metric and using Gaussian kernels to promote uniformity, learned representations perform better than those learned by contrastive learning.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/alignment_uniformity.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1: Alignment and Uniformity. In figure (a), we see the quality of alignment, i.e. similar images are close to each other in the representation space. In figure (b), we see the quality of uniformity, i.e. images form a uniform distribution across the representation space. Image borrowed from &lt;d-cite key=&quot;isola-alignment&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;Why does uniformity help? Firstly, it acts as a regularization term. This is because if we tried to learn representations that maximized alignment without any target for uniformity, then a map that just takes all input vectors to zero would trivially minimize the loss. Yet this would be an extremely bad representation. However, aside from regularization, uniform distributions also have maximal self-entropy. Thus their importance can be explained equally well through some sort of minimizing loss of information. Indeed this is how &lt;d-cite key=&quot;isola-alignment&quot;&gt;&lt;/d-cite&gt; explains it.&lt;/p&gt; &lt;p&gt;In this post we will investigate this even further. In particular, if regularization is the only effect that uniformity has on representations, then slightly nudging already aligned representations to make them uniform should not improve their quality. This is exactly what we will do, and we will do this through Poisson Flows.&lt;/p&gt; &lt;h2 id=&quot;poisson-flows&quot;&gt;Poisson Flows&lt;/h2&gt; &lt;p&gt;If you let a planar positive distribution of charges slightly above $z=0$ loose, then they will repel each other. If you stop them at some large enough distance $R$ from the origin, then their distribution approaches uniform as $R \to \infty$. This is very interesting, and what’s even more interesting is that this fact generalizes to arbitrary dimensions. Thus such fields allow a convenient way to map arbitrary high-dimensional distributions to uniform distributions. Poisson flow generative models proposed by Xu and Liu &lt;d-cite key=&quot;poisson-flow&quot;&gt;&lt;/d-cite&gt; exploit this property for image generation; by sampling uniformly from the hemisphere, one can iterate through the backward ODE and thus sample from $p_{\mathrm{x}}$. We shall use it to impose uniformity on well-aligned features.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/poisson-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/poisson-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/poisson-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/poisson.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: Evolution of data points through a Poisson field. As we can see, arbitrary distributions are mapped to uniform. Further the mapping is continuous. Borrowed from &lt;d-cite key=&quot;poisson-flow&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;Say we have a probability distribution $p_{\mathrm{y}}$ over $\mathcal{Y}_1 = \mathbb{R^d}$. Set this distribution at the $z = 0$ plane &lt;d-footnote&gt;here z-dimension refers to the new dimension we recently augmented the dataset with&lt;/d-footnote&gt; in the expanded space $\tilde{\mathcal{Y}}_1 = \mathcal{Y} \times \mathbb{R}$. Let the electric field at an arbitrary point in $\tilde{\mathcal{Y}}_1$ be defined as&lt;/p&gt; \[E_{p_{\tilde{\mathrm{y}}}}(\tilde{y}) = \int_{\tilde{y}&apos;} \frac{\tilde{y} - \tilde{y&apos;}}{\|\tilde{y} - \tilde{y&apos;} \|_2^{d+1}} \cdot p_{\tilde{\mathrm{y}}}(\tilde{y}&apos;) d\tilde{y}&apos;\] &lt;p&gt;Let $\mathrm{y} \sim p_{\mathrm{y}}$. Evolve $\tilde{\mathrm{y}} = (\mathrm{y}, 0) \in \tilde{\mathcal{Y}_1}$ according to the ODE&lt;/p&gt; \[\frac{d\tilde{\mathrm{y}}}{dt} = E_{p_{\tilde{\mathrm{y}}}}(\tilde{y})\] &lt;p&gt;Let the final point be $f_{\mathrm{poisson}}(\mathrm{y}; p_{\mathrm{y}})$. Then the distribution of $p_{f_{\mathrm{poisson}}}(\cdot)$ approaches uniform as $R \to \infty$.&lt;/p&gt; &lt;p&gt;In practice, since we want to take $s = 0$ to $R$, we do a change of variables to write the ODE as&lt;/p&gt; \[\frac{d \tilde{\mathrm{y}}}{ds} = \frac{1}{E_{p_{\tilde{\mathrm{y}}}}(\tilde{\mathrm{y}})^T \tilde{\mathrm{y}}} \cdot E_{p_{\tilde{\mathrm{y}}}}(\tilde{\mathrm{y}})\] &lt;p&gt;Note that the field stated here isn’t actually used directly, it is rather learned through a deep neural network. This is possible since the integral can be replaced with an expectation, which itself can be approximated through Monte-Carlo methods.&lt;/p&gt; &lt;p&gt;Since Poisson flows allow us to map arbitrary distributions to uniform ones, while preserving continuity; they are an extremely powerful tool to further understand the effects of uniformity. This brings us to our main hypothesis&lt;/p&gt; &lt;h2 id=&quot;hypothesis&quot;&gt;Hypothesis&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Assume that uniformity acts more than just a regularizing term for learning useful representations. Then if we take any well-aligned features that have good downstream performance, and apply a continuous map that imposes uniformity, our new features should perform better at downstream tasks&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is because if uniformity is simply a regularizing term, then training them for the downstream task is the best we can do. This hypothesis itself is counterintuitive because the original features should already be well-trained against the task at hand. However, surprisingly, this hypothesis seems to hold true. To show this, we describe the following experiment.&lt;/p&gt; &lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt; &lt;p&gt;We consider the pen-ultimate layer of AlexNet &lt;d-cite key=&quot;alexnet&quot;&gt;&lt;/d-cite&gt; trained on CIFAR-10 &lt;d-cite key=&quot;cifar-10&quot;&gt;&lt;/d-cite&gt; as our initial features. These features must be well aligned, as linear decision boundaries are able to accurately classify them into their classes.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/flow-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/flow-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/flow-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/flow.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: A representation of how the features should evolve. (a) Initial representation is seemingly random and hard to classify with. (b) After training a classifier, the penultimate layer is well clustered between the various features. (c) Features after learning a Poisson field, and sending the data through it. &lt;d-footnote&gt;Note that images (a) and (b) are for representation purposes only. However image (c) is indeed produced through a learned Poisson field from data points in (b)&lt;/d-footnote&gt; &lt;/div&gt; &lt;p&gt;We take these features and learn a corresponding Poisson field. For our predicted poisson field, we use a relatively small fixed-size two-hidden layer network.&lt;/p&gt; &lt;p&gt;We finally pass our features through this Poisson field and train a linear classifier on top of the final learned representations. We compare this accuracy against the original accuracy.&lt;/p&gt; &lt;p&gt;A summary of our approach is given in the figure below:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/architecture-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/architecture-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/architecture-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-enforcing-uniformity/architecture.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4: Overview of architecture &lt;/div&gt; &lt;p&gt;Further training details are given in &lt;a href=&quot;#appendix-a-training-details&quot;&gt;Appendix A&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;The results are given in the below table.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Architecture&lt;/th&gt; &lt;th&gt;Train accuracy&lt;/th&gt; &lt;th&gt;Test accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AlexNet&lt;/td&gt; &lt;td&gt;88%&lt;/td&gt; &lt;td&gt;82%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AlexNet + Poisson Flow &lt;em&gt;(ours)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;95%&lt;/td&gt; &lt;td&gt;85%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Here we see that our method outperforms a well-trained AlexNet considerably.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This is a surprisingly nice improvement. Note that the Poisson flow post-processing step is completely unsupervised. This seems to hint that having a uniform prior is helpful for reasons other than just regularization.&lt;/p&gt; &lt;p&gt;It would be extremely interesting to develop an entirely unsupervised architecture based on Poisson flow. This would begin by using an unsupervised method to learn well-aligned features. A suitable loss candidate could possibly be just a contrastive loss, with L2 norm as a distance metric:&lt;/p&gt; \[\mathcal{L} \triangleq \mathbb{E}_{(x, x^+) \sim p_{\mathrm{pos}}, \{x_i^-\}_{i=1}^M \overset{\mathrm{iid}}{\sim} p_{\mathrm{x}}} \left[ \|x - x^+\|_2^{\alpha} - \lambda \sum_{i=1}^{M} \|x - x_i^{-}\|_2^{\beta} \right]\] &lt;p&gt;Then passing these well-aligned features through a Poisson flow would enforce uniformity. Such a proposed architecture could be worth exploring.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;appendices&quot;&gt;Appendices&lt;/h2&gt; &lt;p&gt;See &lt;a href=&quot;https://github.com/mathletema/poisson-representations&quot;&gt;https://github.com/mathletema/poisson-representations&lt;/a&gt; for code.&lt;/p&gt; &lt;h3 id=&quot;appendix-a-training-details&quot;&gt;Appendix A: Training details&lt;/h3&gt; &lt;p&gt;We used a version of AlexNet similar to that given in Isola’s paper, such that the pen-ultimate layer was 128 neurons wide. We trained this network against cross entropy loss for 20 epochs using Adam as an optimizer.&lt;/p&gt; &lt;p&gt;After this, we moved the features from $\mathbb{R}^{128}$ to $\mathbb{R}^{129}$ by setting $z = 0$. We then learned a Poisson field for this network similar to &lt;d-cite key=&quot;poisson-flow&quot;&gt;&lt;/d-cite&gt;. We use the default values of $\tau, \gamma, \sigma$ as the original paper, but used $M = 20$ as a consequence of our reduced dimension size. We trained this Poisson field with a large batch size of $1024$ and a small batch size of $128$. We trained this over $200$ epochs.&lt;/p&gt; &lt;p&gt;We then passed the features through the Poisson field. To simulate the ODE, we used Euler’s method with a small delta of $0.01$ and $100$ steps. Using RK4 might produce better results, and we leave this to future work.&lt;/p&gt; &lt;p&gt;We finally trained a logistic classifier on top of these final representations, and printed train and test accuracies.&lt;/p&gt; </content> </entry> <entry> <title>6-DOF estimation through visual place recognition</title> <link href="https://deep-learning-mit.github.io/blog/2023/dof-visual-place-recognition-satellite/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/dof-visual-place-recognition-satellite</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;The goal of this project is to demonstrate how a drone or other platform with a downward-facing camera could perform approximate geolocation using a neural scene representation of existing satellite imagery. Note that the use of the term “Visual Place Recognition” in the title is a carryover from the proposal, but no longer applies to this project. Rather, the goal of this project is to implement 6-DOF pose-estimation.&lt;/p&gt; &lt;p&gt;Pose estimation &lt;d-cite key=&quot;xiang2018posecnn&quot;&gt;&lt;/d-cite&gt; can refer to the ability of an agent to determine its 3D position and orientation based on visual or other sensor info.&lt;/p&gt; &lt;p&gt;In this work, the goal is to compress the ground-truth image data into a neural model which maps live camera footage to geolocation coordinates.&lt;/p&gt; &lt;p&gt;Twitter user Stephan Sturges demonstrates his solution&lt;d-cite key=&quot;Sturges_2023&quot;&gt;&lt;/d-cite&gt; for allowing a drone with a downward-facing camera to geolocate through cross-referencing against a database of satellite images:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sturges_satellite_vpr-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sturges_satellite_vpr-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sturges_satellite_vpr-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sturges_satellite_vpr.jpeg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Twitter user Stephan Sturges shows the results&lt;d-cite key=&quot;Sturges_2023&quot;&gt;&lt;/d-cite&gt; of geolocation, purportedly based on Visual Place Recognition. &lt;/div&gt; &lt;p&gt;The author of the above tweet employs a reference database of images. It would be interesting to eliminate the need for a raw dataset. Whereas the author employs Visual Place Recognition, here I employ pose estimation techniques. Thus I do not seek to estimate predict place &lt;em&gt;labels&lt;/em&gt;, but rather geolocated place &lt;em&gt;coordinates&lt;/em&gt; for the camera, as well as the camera’s orientation.&lt;/p&gt; &lt;p&gt;Thus, this works seeks to develop a neural network which maps a terrain image from the agent’s downward-facing camera, to a 6-DOF (position/rotation) representation of the agent in 3-space.&lt;/p&gt; &lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt; &lt;p&gt;The goal-statement - relating a camera image to a location and orientation in the world - has been deeply studied in computer vision and rendering&lt;d-cite key=&quot;Anwar_2022&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/camera_intrinsic_extrinsic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/camera_intrinsic_extrinsic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/camera_intrinsic_extrinsic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/camera_intrinsic_extrinsic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Camera parameters, as described in&lt;d-cite key=&quot;Anwar_2022&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;p&gt;Formally&lt;d-cite key=&quot;Anwar_2022&quot;&gt;&lt;/d-cite&gt;,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The image-formation problem is modeled as a camera forming an image of the world using a planar sensor.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;World coordinates&lt;/strong&gt; refer to 3-space coordinates in the Earth or world reference frame.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image coordinates&lt;/strong&gt; refer to 2-space planar coordinates in the camera image plane.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pixel coordinates&lt;/strong&gt; refer to 2-space coordinates in the final image output from the image sensor, taking into account any translation or skew of pixel coordinates with respect to the image coordinates.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The mapping from world coordinates to pixel coordinates is framed as two composed transformations, described as sets of parameters&lt;d-cite key=&quot;Anwar_2022&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Extrinsic camera parameters&lt;/strong&gt; - the transformation from world coordinates to image coordinates (affected by factors “extrinsic” to the camera internals, i.e. position and orientation.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intrinsic camera parameters&lt;/strong&gt; - the transformation from image coordinates to pixel coordinates (affected by factors “intrinsic” to the camera’s design.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so broadly speaking, this work strives to design a neural network that can map from an image (taken by the agent’s downward-facing camera) to camera parameters of the agent’s camera. With camera parameters in hand, geolocation parameters automatically drop out from extracting extrinsic translation parameters.&lt;/p&gt; &lt;p&gt;To simplify the task, assume that camera intrinsic characteristics are consistent from image to image, and thus could easily be calibrated out in any application use-case. Therefore, this work focuses on inferring &lt;strong&gt;extrinsic camera parameters&lt;/strong&gt; from an image. We assume that pixels map directly into image space.&lt;/p&gt; &lt;p&gt;The structure of extrinsic camera parameters is as follows&lt;d-cite key=&quot;Anwar_2022&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; \[\mathbf{E}_{4 \times 4} = \begin{bmatrix} \mathbf{R}_{3 \times 3} &amp;amp; \mathbf{t}_{3 \times 1} \\ \mathbf{0}_{1 \times 3} &amp;amp; 1 \end{bmatrix}\] &lt;p&gt;where \(\mathbf{R}_{3 \times 3} \in \mathbb{R^{3 \times 3}}\) is rotation matrix representing the rotation from the world reference frame to the camera reference frame, and \(\mathbf{t}_{3 \times 1} \in \mathbb{R^{3 \times 1}}\) represents a translation vector from the world origin to the image/camera origin.&lt;/p&gt; &lt;p&gt;Then the image coordinates (a.k.a. camera coordinates) \(P_c\) of a world point \(P_w\) can be computed as&lt;d-cite key=&quot;Anwar_2022&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; \[\mathbf{P_c} = \mathbf{E}_{4 \times 4} \cdot \mathbf{P_w}\] &lt;h1 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h1&gt; &lt;h2 id=&quot;image-to-extrinsics-encoder-architecture&quot;&gt;Image-to-extrinsics encoder architecture&lt;/h2&gt; &lt;p&gt;The goal of this work, is to train a neural network which maps an image drawn from \(R^{3 \times S \times S}\) (where \(S\) is pixel side-length of an image matrix) to a pair of camera extrinsic parameters \(R_{3 \times 3}\) and \(t_{3 \times 1}\):&lt;/p&gt; \[\mathbb{R^{3 \times S \times S}} \rightarrow \mathbb{R^{3 \times 3}} \times \mathbb{R^3}\] &lt;p&gt;The proposed solution is a CNN-based encoder which maps the image into a length-12 vector (the flattened extrinsic parameters); a hypothetical architecture sketch is shown below:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/nn.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/nn.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/nn.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/nn.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Image encoder architecture. &lt;/div&gt; &lt;h2 id=&quot;data-sources-for-offline-training&quot;&gt;Data sources for offline training&lt;/h2&gt; &lt;p&gt;Online sources&lt;d-cite key=&quot;Geller_2022&quot;&gt;&lt;/d-cite&gt; provide downloadable satellite terrain images.&lt;/p&gt; &lt;h2 id=&quot;training-and-evaluation&quot;&gt;Training and evaluation&lt;/h2&gt; &lt;p&gt;The scope of the model’s evaluation is, that it will be trained to recognize aerial views of some constrained area i.e. Atlantic City New Jersey; this constrained area will be referred to as the “area of interest.”&lt;/p&gt; &lt;h3 id=&quot;data-pipeline&quot;&gt;Data pipeline&lt;/h3&gt; &lt;p&gt;The input to the data pipeline is a single aerial image of the area of interest. The output of the pipeline is a data loader which generates augmented images.&lt;/p&gt; &lt;p&gt;The image of the area of interest is \(\mathbb{R^{3 \times T \times T}}\) where \(T\) is the image side-length in pixels.&lt;/p&gt; &lt;p&gt;Camera images will be of the form \(\mathbb{R^{3 \times S \times S}}\) where \(S\) is the image side-length in pixels, which may differ from \(T\).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Generate an image from the agent camera’s vantage-point&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Convert the area-of-interest image tensor (\(\mathbb{R^{3 \times T \times T}}\)) to a matrix of homogenous world coordinates (\(\mathbb{R^{pixels \times 4}}\)) and an associated matrix of RGB values for each point (\(\mathbb{R^{pixels \times 3}}\)) &lt;ul&gt; &lt;li&gt;For simplicity, assume that all features in the image have an altitutde of zero&lt;/li&gt; &lt;li&gt;Thus, all of the pixel world coordinates will lie in a plane&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Generate random extrinsic camera parameters \(R_{3 \times 3}\) and \(t_{3 \times 1}\)&lt;/li&gt; &lt;li&gt;Transform the world coordinates into image coordinates (\(\mathbb{R^{pixels \times 3}}\)) (note, this does not affect the RGB matrix)&lt;/li&gt; &lt;li&gt;Note - this implicitly accomplishes the commonly-used image augmentations such as shrink/expand, crop, rotate, skew&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Additional data augmentation&lt;/strong&gt; - to prevent overfitting &lt;ul&gt; &lt;li&gt;Added noise&lt;/li&gt; &lt;li&gt;Color/brightness adjustment&lt;/li&gt; &lt;li&gt;TBD&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Convert the image coordinates and the RGB matrix into a camera image tensor (\(\mathbb{R^{3 \times S \times S}}\))&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each element of a batch from this dataloader, will be a tuple of (extrinsic parameters,camera image).&lt;/p&gt; &lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;For each epoch, and each mini-batch…&lt;/li&gt; &lt;li&gt;unpack batch elements into camera images and ground-truth extrinsic parameters&lt;/li&gt; &lt;li&gt;Apply the encoder to the camera images&lt;/li&gt; &lt;li&gt;Loss: MSE between encoder estimates of extrinsic parameters, and the ground-truth values&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;hyperparameters&quot;&gt;Hyperparameters&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Architecture &lt;ul&gt; &lt;li&gt;Encoder architecture - CNN vs MLP vs ViT(?) vs …, number of layers, …&lt;/li&gt; &lt;li&gt;Output normalizations&lt;/li&gt; &lt;li&gt;Nonlinearities - ReLU, tanh, …&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Learning-rate&lt;/li&gt; &lt;li&gt;Optimizer - ADAM, etc.&lt;/li&gt; &lt;li&gt;Regularizations - dropout, L1, L2, …&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt; &lt;p&gt;For a single epoch, measure the total MSE loss of the model’s extrinsic parameter estimates relative to the ground-truth.&lt;/p&gt; &lt;h2 id=&quot;feasibility&quot;&gt;Feasibility&lt;/h2&gt; &lt;p&gt;Note that I am concurrently taking 6.s980 “Machine learning for inverse graphics” so I already have background in working with camera parameters, which should help me to complete this project on time.&lt;/p&gt; &lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt; &lt;h2 id=&quot;source-image&quot;&gt;Source image&lt;/h2&gt; &lt;p&gt;DOF estimation was applied to a 2D aerial image&lt;d-cite key=&quot;Taylor_2020&quot;&gt;&lt;/d-cite&gt;, shown below:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sample_image-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sample_image-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sample_image-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/sample_image.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Sample aerial image from &lt;d-cite key=&quot;Taylor_2020&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;h2 id=&quot;dataloader&quot;&gt;Dataloader&lt;/h2&gt; &lt;p&gt;A dataloader was created which generates (1) generates a random extrinsic camera matrix as described above, in order to generate (2) visualization of the above source image from the perspective of the random camera matrix.&lt;/p&gt; &lt;p&gt;More specifically, the dataloader generates &lt;em&gt;Euler Angles&lt;/em&gt; in radians associated with with the camera matrix rotation, as well as a 3D offset representing the camera’s position.&lt;/p&gt; &lt;p&gt;You will notice that the images suffer from an artifact whereby the pixels are not adjacent to each other but rather have black space between them; a production implementation of this solution would require interpolation between pixels in order to produce a continuous image.&lt;/p&gt; &lt;p&gt;An example of a single generated image is shown below; it is the original image, above, viewed from the perspective of a random camera matrix:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_sample-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_sample-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_sample-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_sample.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Single datapoint from data loader. &lt;/div&gt; &lt;p&gt;A batch of generated images is shown below:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_grid-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_grid-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_grid-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/dof_grid.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; A batch from the data loader. &lt;/div&gt; &lt;p&gt;Again, you can see that owing to a lack of interpolation, the pixels are spread out, with black space between them.&lt;/p&gt; &lt;h2 id=&quot;dnn-architecture&quot;&gt;DNN architecture&lt;/h2&gt; &lt;p&gt;The DNN architecture is an MLP with 6 hidden layers of width 512, 256 and 128.&lt;/p&gt; &lt;p&gt;The input is a 224x224 image with 3 color channels representing the view of the source image from an orientation determined by the (unknown) camera extrinsic parameters.&lt;/p&gt; &lt;p&gt;The architecture outputs 6 logit values values corresponding to predictions of 3 Euler angles and 3 positional offsets for the camera extrinsic matrix.&lt;/p&gt; &lt;p&gt;For this project, I experimented with the sinusoidal activation functions described in the SIREN&lt;d-cite key=&quot;sitzmann2020implicit&quot;&gt;&lt;/d-cite&gt; paper. Sinusoidal activation functions, combined with MLPs, were previously shown to be more effective at capturing high-frequency information in radiance fields, compared to ReLU MLPs. I employed sinusoidal activation functions in this work in the hopes of more effectively capturing high-frequency variation in the relationship between camera extrinsic parameters and camera image pixels.&lt;/p&gt; &lt;p&gt;One question which might arise is, if the DNN outputs logits, how do I account for the difference in statistical characteristics between the three Euler Angle outputs and the three translation vector outputs? I employed scikitlearn StandardScalers at both the input and the output in order to normalize image pixels and extrinsic camera matrix parameters, respectively. The use of normalization at the input is standard. The use of normalization at the output allows each dimension of the 6-logit output to learn a zero-mean, unit-variance distribution: the output StandardScaler converts from zero-mean, unit-variance to the estimated actual mean and variance of the target distribution. The way the output StandardScaler is computed is as follows: a batch of random data is sampled from the dataloader; mean and variance are computed; then a StandardScaler is designed such that its &lt;em&gt;inverse&lt;/em&gt; maps from the computed mean and variance of the target extrinsics, to zero mean/unit-variance. Thus, run forward, the output StandardScaler will map from unit gaussian to the computed mean and variance.&lt;/p&gt; &lt;h2 id=&quot;training-setup&quot;&gt;Training setup&lt;/h2&gt; &lt;p&gt;I train for 80 epochs with an Adam optimizer and a learning rate of 0.00001.&lt;/p&gt; &lt;p&gt;MSE loss is employed for training and evaluation. The extrinsic parameters predicted by the DNN are compared against the target (correct) extrinsic parameters which the dataloader used to generate the camera image of the scene. Recall from the previous section that, owing to the output StandardScaler, the DNN outputs 6 roughly zero-mean/unit-variance predicted camera extrinsic parameters. I chose to evaluate loss &lt;em&gt;relative to these zero-mean/unit-variance predictions&lt;/em&gt;, prior to the output StandardScaler; the rationale being that I wanted each extrsinsic parameter to have equal weighting in the MSE loss computation, and not be biased by the mean/variance of the particular parameter. Thus, I use the output StandardScaler in &lt;em&gt;inverse&lt;/em&gt; mode to normalize the target values to zero-mean/unit-variance. MSE loss is then computed between the DNN output logits, and these normalized target values.&lt;/p&gt; &lt;p&gt;A side-effect of computing MSE against normalized values, is that it is effectively a relative measure: MSE tells me how large the variance in the error between predictions and target is, relative to the unit-variance of the normalized target values. Thus I expect that an MSE much less than one is a good heuristic for the quality of the estimate.&lt;/p&gt; &lt;h1 id=&quot;training-results&quot;&gt;Training results&lt;/h1&gt; &lt;p&gt;The plot below shows that the DNN architecture was able to converge on low-MSE predictions of the extrinsic camera matrix:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/loss_plot-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/loss_plot-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/loss_plot-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-dof-visual-place-recognition-satellite/loss_plot.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Train and test MSE loss between the predicted and actual extrinsic camera matrix. &lt;/div&gt; &lt;p&gt;Note that the train and test curves overlap almost perfectly; this is because all datapoints generated by the dataloader are random, so in fact the model is constantly being trained on fresh data, and the resampling is really unnecessary.&lt;/p&gt; &lt;p&gt;Since the final MSE is relatively small (0.020), and since (as described in the previous section) the MSE is effectively a relative measure of error, I believe the DNN is learning a relatively good estimate of camera extrinsics.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Based on the low MSE attained during training, I believe I successfully trained a DNN to roughly estimate camera extrinsics from orientation-dependent camera views.&lt;/p&gt; &lt;p&gt;There are many improvements which would be necessary in order to deploy this in production.&lt;/p&gt; &lt;p&gt;For example, it would be better to use more detailed satellite imagery, preferably with stereoscopic views that effectively provide 3D information. Without having 3D information about the scene, it is hard to train the model to recognize how the scene will look from different angles. In my work, I used a 2D image and essentially assumed that the height of the geographic features in the image was negligible, such that I could approximate the 3D point-cloud as lying within a 2D plane. With stereoscopic satellite data, it could be possible to construct a truly 3D point-cloud, on which basis I could synthesize more accurate camera views during the training process.&lt;/p&gt; &lt;p&gt;Also, as discussed in the Implementation section, it would likely be necessary to implement interpolation between the pixels when generating simulated camera views. Otherwise, the camera views during training would look nothing like what the camera would see in the real world.&lt;/p&gt; </content> </entry> <entry> <title>Tracing the Seeds of Conflict: Advanced Semantic Parsing Techniques for Causality Detection in News Texts</title> <link href="https://deep-learning-mit.github.io/blog/2023/conflict-causality/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/conflict-causality</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;“In the complex world of political conflicts, understanding the underlying dynamics can often feel like trying to solve a puzzle with missing pieces. This project attempts to find those missing pieces through a novel approach that combines the insights of qualitative research with the precision of quantitative analysis.”&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img1_map-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img1_map-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img1_map-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-conflict-causality/img1_map.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot; style=&quot;color: white; font-style: italic; font-weight: bold;&quot;&gt;Retrieved from https://conflictforecast.org&lt;/p&gt; &lt;p&gt;Political conflicts are multifaceted and dynamic, posing significant challenges for researchers attempting to decode their intricate patterns. Traditional methods, while insightful, often grapple with the dual challenges of scale and specificity. This project embarks on an innovative journey to bridge this gap, leveraging a frame-semantic parser to illustrate its applicability for the task and to discuss an approach to achieve domain-specificity for the model using semantic similarity. By synthesizing the depth of qualitative research into the scalability of quantitative methods, we aim to contribute to more informed analyses and actions in low-resource, low-tech domains like conflict studies.&lt;/p&gt; &lt;p&gt;On this journey, the projects key contributions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advancing Frame-Semantic Parsing in Conflict Research&lt;/strong&gt;: We introduce the frame-semantic parser, a method that brings a high degree of explainability to conflict studies. Particularly when used in conjunction with news articles, this parser emerges as a powerful tool in areas where data is scarce, enabling deeper insights into the nuances of political conflicts.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Harnessing Semantic Similarity for Domain Attunement&lt;/strong&gt;: The project underscores the significance of semantic similarity analysis as a precursor to frame-semantic parsing. This approach finely tunes the parser to specific thematic domains, addressing the gaps often present in domain distribution of common data sources. It illustrates how tailoring the parser input can yield more contextually relevant insights.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Demonstrating Domain-Dependent Performance in Frame-Semantic Parsing&lt;/strong&gt;: We delve into the impact of thematic domains on the performance of a transformer-based frame-semantic parser. The research highlights how the parser’s effectiveness varies with the domain of analysis, primarily due to biases and structural peculiarities in the training data. This finding is pivotal for understanding the limitations and potential of semantic parsing across different contexts.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Developing Domain-Specific Performance Metrics&lt;/strong&gt;: In environments where additional, domain-specific labeled test data is scarce, the project proposes an intuitive method to derive relevant performance metrics. This approach not only aligns the evaluation more closely with the domain of interest but also provides a practical solution for researchers working in resource-constrained settings.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;literature-background&quot;&gt;Literature Background&lt;/h2&gt; &lt;h3 id=&quot;qualitative-research-on-conflicts&quot;&gt;Qualitative Research on Conflicts&lt;/h3&gt; &lt;p&gt;Qualitative research has long been a cornerstone in the study of political conflicts. This body of work, now well-established, emphasizes the unique nature of each conflict, advocating for a nuanced, context-specific approach to understanding the drivers and dynamics of conflicts. Researchers in this domain have developed a robust understanding of the various pathways that lead to conflicts, highlighting the importance of cultural, historical, and socio-political factors in shaping these trajectories. While rich in detail and depth, this approach often faces challenges in scalability and systematic analysis across diverse conflict scenarios.&lt;/p&gt; &lt;h3 id=&quot;the-role-of-quantitative-methods&quot;&gt;The Role of Quantitative Methods&lt;/h3&gt; &lt;p&gt;The emergence of computational tools has spurred a growing interest in quantitative approaches to conflict research. These methods primarily focus on predicting the severity and outcomes of ongoing conflicts, with some success &lt;d-cite key=&quot;beck2000improving&quot;&gt;&lt;/d-cite&gt;. However, the onset of conflicts remains challenging to predict, indicating a need for more sophisticated tools and methodologies. Quantitative methods provide scalability and a degree of objectivity but often fail to capture the complexities and evolving nature of conflicts. &lt;d-cite key=&quot;goldstein1992conflict&quot;&gt;&lt;/d-cite&gt;’s work on a conflict-cooperation scale illustrates the difficulty in quantifying conflict dynamics and the controversy in creating aggregate time series from event data. &lt;d-cite key=&quot;vesco2022united&quot;&gt;&lt;/d-cite&gt; highlight the importance of diverse, accurate predictions in conflict forecasting, noting the value of incorporating contextual variables to predict early signals of escalation.&lt;/p&gt; &lt;h3 id=&quot;bridging-the-gap-with-explainable-modeling-approaches&quot;&gt;Bridging the Gap with Explainable Modeling Approaches&lt;/h3&gt; &lt;p&gt;The challenge now lies in bridging the insights from qualitative research with the systematic, data-driven approaches of quantitative methods. While the former provides a deep understanding of conflict pathways, the latter offers tools for large-scale analysis and prediction. The key to unlocking this synergy lies in developing advanced computational methods to see the smoke before the fire – identifying the early precursors and subtle indicators of impending conflicts &lt;d-cite key=&quot;vesco2022united&quot;&gt;&lt;/d-cite&gt;. This approach aligns with the evolving needs of conflict research, where traditional models may not adequately address the complex and non-linear nature of conflict data &lt;d-cite key=&quot;weidmann2023recent&quot;&gt;&lt;/d-cite&gt;. &lt;d-cite key=&quot;mueller2018reading&quot;&gt;&lt;/d-cite&gt; demonstrate the potential of utilizing newspaper text for predicting political violence, suggesting a novel data source for uncovering early conflict indicators. However, these early attempts are outdated given the fast technological development in recent years, particularly in the field of natural language processing. This research endeavour seeks to fill that gap and introduce a scalable, explainable method to quantitative conflict research.&lt;/p&gt; &lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt; &lt;p&gt;The project capitalizes on the premise that risk factors triggering a conflict, including food crises, are frequently mentioned in on-the-ground news reports before being reflected in traditional risk indicators, which can often be incomplete, delayed, or outdated. By harnessing newspaper articles as a key data source, this initiative aims to identify these causal precursors more timely and accurately than conventional methods.&lt;/p&gt; &lt;h3 id=&quot;news-articles-as-data-source&quot;&gt;News Articles as Data Source&lt;/h3&gt; &lt;p&gt;News articles represent a valuable data source, particularly in research domains where timely and detailed information is crucial. In contrast to another “live” data source that currently revels in popularity amongst researchers - social media data - news articles are arguably less prone to unverified narratives. While news articles typically undergo editorial checks and balances, ensuring a certain level of reliability and credibility, they certainly do not withstand all potential biases and are to be handled with caution - as arguably every data source. To counteract potential biases of individual news outputs, accessing a diverse range of news sources is essential. Rather than having to scrape or otherwise collect data on news articles, there is a set of resources available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://newsapi.org/&quot;&gt;NewsAPI&lt;/a&gt;: This platform provides convenient access to a daily limit of 100 articles, offering diverse query options. Its integration with a Python library streamlines the process of data retrieval. However, the limitation lies in the relatively small number of data points it offers, potentially restricting the scope of analysis.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.gdeltproject.org/&quot;&gt;GDELT Database&lt;/a&gt;: Renowned for its vast repository of historical information spanning several decades, GDELT stands as a comprehensive data source. Its extensive database is a significant asset, but similar to NewsAPI, it predominantly features article summaries or initial sentences rather than complete texts, which may limit the depth of analysis.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.dowjones.com/professional/factiva/&quot;&gt;Factiva&lt;/a&gt;: A premium service that grants access to the complete bodies of articles from a plethora of global news sources in multiple languages. While offering an exhaustive depth of data, this resource comes with associated costs, which may be a consideration for budget-constrained projects.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://paperswithcode.com/dataset/realnews&quot;&gt;RealNews&lt;/a&gt;: As a cost-free alternative, this dataset encompasses entire newspaper articles collated between 2016 and 2019. Selected for this project due to its unrestricted accessibility and comprehensive nature, it provides a substantial set of articles, making it a valuable resource for in-depth analysis.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;descriptive-analysis-of-the-data&quot;&gt;Descriptive Analysis of the Data&lt;/h3&gt; &lt;p&gt;The analysis delved into a selected subset of &lt;strong&gt;120,000 articles&lt;/strong&gt; from the &lt;a href=&quot;https://paperswithcode.com/dataset/realnews&quot;&gt;RealNews&lt;/a&gt; open-source dataset. This subset was chosen randomly to manage the extensive scope of the complete dataset within the project’s time constraints. Each article in this subset provided a rich array of information, including &lt;strong&gt;url&lt;/strong&gt;, &lt;strong&gt;url_used&lt;/strong&gt;, &lt;strong&gt;title&lt;/strong&gt;, &lt;strong&gt;text&lt;/strong&gt;, &lt;strong&gt;summary&lt;/strong&gt;, &lt;strong&gt;authors&lt;/strong&gt;, &lt;strong&gt;publish_date&lt;/strong&gt;, &lt;strong&gt;domain&lt;/strong&gt;, &lt;strong&gt;warc_date&lt;/strong&gt;, and &lt;strong&gt;status&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The range of articles spans from 1869 to 2019, but for focused analysis, we narrowed the scope to articles from &lt;strong&gt;January 2016 through March 2019&lt;/strong&gt;. This temporal delimitation resulted in a dataset comprising &lt;strong&gt;58,867 articles&lt;/strong&gt;. These articles originated from an expansive pool of &lt;strong&gt;493 distinct news outlets&lt;/strong&gt;, offering a broad perspective on global events and narratives. The distribution of these articles across the specified time frame provides the expected observation of increasing news reporting, as visualized below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img4_articlecounts-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img4_articlecounts-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img4_articlecounts-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-conflict-causality/img4_articlecounts.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot; style=&quot;color: white; font-style: italic; font-weight: bold;&quot;&gt;Counts of Articles over Time&lt;/p&gt; &lt;p&gt;To understand the content of our dataset’s news articles better, we utilized the &lt;em&gt;TfidfVectorizer&lt;/em&gt;, a powerful tool that transforms text into a numerical representation, emphasizing key words based on their frequency and distinctiveness within the dataset. To ensure focus on the most relevant terms, we filtered out commonly used English stopwords. The &lt;em&gt;TfidfVectorizer&lt;/em&gt; then generated a &lt;em&gt;tf-idf matrix&lt;/em&gt;, assigning weights to words that reflect their importance in the overall dataset. By summing the Inverse Document Frequency (IDF) of each term, we obtained the adjusted frequencies that helped identify the most influential words in our corpus. To visually represent these findings, we created a word cloud (see below), where the size of each word correlates with its relative importance.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img3_wordcloud-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img3_wordcloud-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img3_wordcloud-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-conflict-causality/img3_wordcloud.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot; style=&quot;color: white; font-style: italic; font-weight: bold;&quot;&gt;Word Cloud for Entire News Article Dataset (tf-idf adjusted)&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;p&gt;We showcase the applicability of a frame-semantic parsing to the study of conflicts and inform the model with domain-specific seed phrases identified through semantic similarity analysis. This approach not only demonstrates the effectiveness of the method in conflict studies but also showcases how domain-specific applications of deep learning tasks can be accurately applied and measured. Thus, we not only validate the utility of frame-semantic parsing in conflict analysis but also explore innovative ways to tailor and evaluate domain-specific performance metrics.&lt;/p&gt; &lt;h3 id=&quot;the-frame-semantic-parser&quot;&gt;The Frame-Semantic Parser&lt;/h3&gt; &lt;h4 id=&quot;contextualizing-the-approach&quot;&gt;Contextualizing the Approach&lt;/h4&gt; &lt;p&gt;In the pursuit of bridging the gap between the robust theoretical understanding of conflict dynamics and the practical challenges in data availability, the frame-semantic parser emerges as a promising methodological tool. In a recent study (&lt;d-cite key=&quot;balashankar2023predicting&quot;&gt;&lt;/d-cite&gt;), a team of researchers established a proof-of-concept via its successful application of a frame-semantic parser for the study of food insecurity - a field with similar challenges surrounding data access and quality. While this study relied on what can now be considered the “old state-of-the-art,” our proposed approach diverges towards a more contemporary, transformer-based model, inspired by the advancements outlined in &lt;d-cite key=&quot;chanin2023open&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img2_parser-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img2_parser-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img2_parser-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-conflict-causality/img2_parser.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot; style=&quot;color: white; font-style: italic; font-weight: bold;&quot;&gt;Retrieved from https://github.com/swabhs/open-sesame&lt;/p&gt; &lt;h4 id=&quot;how-does-a-frame-semantic-parser-work&quot;&gt;How Does a Frame-Semantic Parser Work?&lt;/h4&gt; &lt;p&gt;At the heart of frame-semantic parsing, as conceptualized by &lt;d-cite key=&quot;gildea2002frame&quot;&gt;&lt;/d-cite&gt; and formalized by the FrameNet project &lt;d-cite key=&quot;baker1998framenet&quot;&gt;&lt;/d-cite&gt;, is the identification of structured semantic frames and their arguments from natural language text. As illustrated above, these frames encapsulate events, relations, or situations along with their participants, making it a critical tool in natural language understanding (NLU) tasks. The practical applications of frame semantics are broad, ranging from voice assistants and dialog systems &lt;d-cite key=&quot;chen2013dialog&quot;&gt;&lt;/d-cite&gt; to complex text analysis &lt;d-cite key=&quot;zhao2023text&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;The process of frame-semantic parsing constitutes three subtasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Trigger Identification&lt;/strong&gt;: This initial step involves pinpointing locations in a sentence that could potentially evoke a frame. It’s a foundational task that sets the stage for more detailed analysis.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frame Classification&lt;/strong&gt;: Following trigger identification, each potential trigger is analyzed to classify the specific FrameNet frame it references. This task is facilitated by leveraging lexical units (LUs) from FrameNet, which provide a strong indication of potential frames.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Argument Extraction&lt;/strong&gt;: The final task involves identifying the frame elements and their corresponding arguments within the text. This process adds depth to the frame by fleshing out its components and contextualizing its application within the sentence.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While frame-semantic parsers have arguably not received as much attention as other language modeling methods, three major contributions of the past few years can be highlighted. &lt;d-cite key=&quot;swayamdipta2017frame&quot;&gt;&lt;/d-cite&gt;’s approach - which is still outperforming many other implementations - presented an efficient parser with softmax-margin segmental RNNs and a syntactic scaffold. It demonstrates that syntax, while beneficial, is not a necessity for high-performance frame-semantic parsing. &lt;d-cite key=&quot;kalyanpur2020open&quot;&gt;&lt;/d-cite&gt; explores the application of transformer-based architectures to frame semantic parsing, employing a multi-task learning approach that significantly improves upon previous state-of-the-art results. Most recently, &lt;d-cite key=&quot;chanin2023open&quot;&gt;&lt;/d-cite&gt; developed the first open-source approach - treating frame semantic parsing as a sequence-to-sequence text generation task, utilizing a T5 transformer model. It emphasizes the importance of pretraining on related datasets and employing data augmentations for improved performance. The distinctive strength of a frame-semantic parser lies in its ability to contextualize information, rather than interpreting it in isolation. This feature is particularly invaluable in conflict analysis, where the semantics of discourse play a critical role.&lt;/p&gt; &lt;h4 id=&quot;implementation-of-the-frame-semantic-parser&quot;&gt;Implementation of the Frame-Semantic Parser&lt;/h4&gt; &lt;p&gt;The implementation of our frame-semantic parser involves several key steps. We begin by splitting our text data into sentences using a &lt;em&gt;split_into_sentences&lt;/em&gt; function. This granular approach allows us to focus on individual narrative elements within the articles and since frame-semantic parsers are reported to perform better on sentence-level &lt;d-cite key=&quot;chanin2023open&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;swayamdipta2017frame&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In the heart of our methodology, we utilize various functions to extract and filter relevant frames from the text. Our &lt;em&gt;extract_features&lt;/em&gt; function captures the full text of each frame element, ensuring a comprehensive analysis of the semantic content. The &lt;em&gt;filter_frames&lt;/em&gt; function then refines this data, focusing on frames that are explicitly relevant to conflict, as informed by research on causal frames in FrameNet.&lt;/p&gt; &lt;p&gt;To optimize the performance of our transformer-based parser, we build a &lt;em&gt;process_batch&lt;/em&gt; function. This function handles batches of sentences, applying the frame semantic transformer model to detect and filter frames relevant to our study.&lt;/p&gt; &lt;p&gt;Our approach also includes a careful selection of specific frames related to causality and conflict as we are interested in these frames and not just any. We rely on both manually identified frame names (informed by &lt;d-cite key=&quot;vieu2016a&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;vieu2020a&quot;&gt;&lt;/d-cite&gt;) and pattern-based searches in &lt;strong&gt;FrameNet&lt;/strong&gt; to compile a comprehensive list of relevant frames. This curated set of frames is instrumental in identifying the nuanced aspects of conflict narratives within the news articles.&lt;/p&gt; &lt;p&gt;The implementation is designed to be efficient and scalable, processing large batches of sentences and extracting the most relevant semantic frames. This approach enables us to parse and analyze a substantial corpus of news articles, providing a rich dataset for our conflict analysis.&lt;/p&gt; &lt;h3 id=&quot;seed-selection-via-semantic-similarity-analysis-to-inform-causal-modeling&quot;&gt;Seed Selection via Semantic Similarity Analysis to Inform Causal Modeling&lt;/h3&gt; &lt;h4 id=&quot;understanding-semantic-similarity&quot;&gt;Understanding Semantic Similarity&lt;/h4&gt; &lt;p&gt;Semantic similarity plays a pivotal role in our methodology, serving as the foundation for expanding our understanding of how conflict is discussed in news articles. By exploring the semantic relationships between words and phrases, we can broaden our analysis to include a diverse array of expressions and viewpoints related to conflict. This expansion is not merely linguistic; it delves into the conceptual realms, uncovering varying narratives and perspectives that shape the discourse on conflict.&lt;/p&gt; &lt;h4 id=&quot;how-do-we-compute-semantic-similarity&quot;&gt;How Do We Compute Semantic Similarity?&lt;/h4&gt; &lt;p&gt;To compute semantic similarity and refine our seed phrases, we employ a combination of distance calculation and cosine similarity measures. We begin with a set of initial key phrases &lt;strong&gt;conflict&lt;/strong&gt;, &lt;strong&gt;war&lt;/strong&gt;, and &lt;strong&gt;battle&lt;/strong&gt;, ensuring they capture the core essence of our thematic domain. We then leverage pretrained word embeddings from the &lt;em&gt;Gensim&lt;/em&gt; library to map these phrases into a high-dimensional semantic space. We also experimented with more sophisticated embedding approaches (like transformer-based) to compute the semantic similarity and thus obtain the seeds. When trading off complexity/time and performance the simpler pretrained &lt;em&gt;Gensim&lt;/em&gt; model preservered.&lt;/p&gt; &lt;p&gt;Our methodology involves generating candidate seeds from our corpus of documents, including unigrams, bigrams, and trigrams, with a focus on those containing key words related to conflict. We filter these candidates based on their presence in the word vectors vocabulary, ensuring relevance and coherence with our seed phrases.&lt;/p&gt; &lt;p&gt;Using functions like &lt;em&gt;calculate_distances&lt;/em&gt; and &lt;em&gt;calculate_cosine_similarity&lt;/em&gt;, we measure the semantic proximity of these candidates to our initial seed phrases. This process involves averaging the distances or similarities across the seed phrases for each candidate, providing a nuanced understanding of their semantic relatedness.&lt;/p&gt; &lt;p&gt;The candidates are then ranked based on their similarity scores, with the top candidates selected for further analysis. This refined set of seed phrases, after manual evaluation and cleaning, forms the basis of our domain-specific analysis, guiding the frame-semantic parsing process towards a more focused and relevant exploration of conflict narratives.&lt;/p&gt; &lt;h3 id=&quot;domain-specific-metrics&quot;&gt;Domain-Specific Metrics&lt;/h3&gt; &lt;p&gt;In the final stage of our methodology, we integrate the identified seed phrases into the frame-semantic parser’s analysis. By comparing the model’s performance on a general set of sentences versus a subset containing at least one seed phrase, we assess the model’s domain-specific efficacy. This comparison not only highlights the general capabilities of large language models (LLMs) but also underscores their potential limitations in domain-specific contexts.&lt;/p&gt; &lt;p&gt;Our approach offers a pragmatic solution for researchers and practitioners in low-resource settings. We demonstrate that while general-purpose LLMs are powerful, they often require fine-tuning for specific domain applications. By utilizing identified domain-specific keywords to construct a tailored test dataset, users can evaluate the suitability of general LLMs for their specific needs.&lt;/p&gt; &lt;p&gt;In cases where technical skills and resources allow, this domain-specific dataset can serve as an invaluable tool for further refining the model through data augmentation and fine-tuning. Our methodology, therefore, not only provides a robust framework for conflict analysis but also lays the groundwork for adaptable and efficient use of advanced NLP tools in various thematic domains.&lt;/p&gt; &lt;p&gt;We present the results for these domain-specific measure for &lt;strong&gt;F1 score&lt;/strong&gt;, &lt;strong&gt;recall&lt;/strong&gt;, and &lt;strong&gt;precisions&lt;/strong&gt;. Likewise, to illustrate performance differences across domains, we conducted the entire approach also for the finance domain, starting with the keywords &lt;strong&gt;finance&lt;/strong&gt;, &lt;strong&gt;banking&lt;/strong&gt;, and &lt;strong&gt;economy&lt;/strong&gt;.&lt;/p&gt; &lt;h2 id=&quot;findings--insights&quot;&gt;Findings &amp;amp; Insights&lt;/h2&gt; &lt;h3 id=&quot;frame-semantic-parser-identifies-causal-frames-reliably&quot;&gt;Frame-Semantic Parser Identifies Causal Frames Reliably&lt;/h3&gt; &lt;p&gt;In this stage, we assess if the methodology is truly applicable to the domain of conflicts and for the use with news article data. We find that of our 37 identified cause-effect related frames, all are represented with various instances in our dataset. In fact, as few as 1,600 randomly selected news articles (processed in 100 batches of 16 batch samples) suffice to cover all cause-effect related frames. Therefore, for this intermediate step of the project, we gather support that the parser is in-fact applicable to news article data.&lt;/p&gt; &lt;h3 id=&quot;differences-in-seed-phrase-selection&quot;&gt;Differences in Seed Phrase Selection&lt;/h3&gt; &lt;p&gt;We make one major observation between the results of the finance- versus conflict-specific seed selection for downstream use. Potentially driven by the fact that conflicts are drastically driven by geographic labels and information, a number of the top 50 seed phrases were geographic terms like “Afghanistan.” Since we did not want to bias the downstream evaluation of our domain-specific metrics we excluded these seed phrases and continued the analysis with 34 seeds. In contrast, the top 50 finance-specific seed phrases obtained from the semantic analysis were neither geographic nor linked to individual (financial) historic events, wherefore we continued the downstream analysis with all top 50 seed phrases. Already here we can observe the deviances across domains, given more support to the idea of domain-specific evaluation and metrics.&lt;/p&gt; &lt;h3 id=&quot;employing-domain-specific-performance-metrics&quot;&gt;Employing Domain-Specific Performance Metrics&lt;/h3&gt; &lt;p&gt;Our research involved an extensive evaluation of the frame-semantic parser, based on a transformer architecture, across various configurations and domain-specific datasets. We began by rebuilding and training the model using the vanilla code and a smaller model size without hyperparameter tuning. Subsequently, we fine-tuned the hyperparameters to match the baseline performance levels. After this, we move to one of the main contributions of this project: the domain-specific evaluation. The evaluation was carried out on domain-specific validation and test datasets, curated using seed words from &lt;strong&gt;finance&lt;/strong&gt; and &lt;strong&gt;conflict&lt;/strong&gt; domains to highlight differences across domains.&lt;/p&gt; &lt;p&gt;The untuned model (&lt;em&gt;validation n = 646, test n = 1891&lt;/em&gt;) showed an argument extraction &lt;strong&gt;F1 score of 0.669&lt;/strong&gt; and a &lt;strong&gt;loss of 0.181&lt;/strong&gt; on the validation set. On the test set, it presented a slightly similar &lt;strong&gt;F1 score of 0.669&lt;/strong&gt; and a &lt;strong&gt;loss of 0.227&lt;/strong&gt;. Hyperparameter-Tuned Performance&lt;/p&gt; &lt;p&gt;Post hyperparameter tuning, there was a notable improvement in the model’s validation performance (&lt;em&gt;n = 156&lt;/em&gt;), with the &lt;strong&gt;F1 score for frame classification reaching as high as 0.873&lt;/strong&gt;, and the &lt;strong&gt;precision for trigger identification at 0.818&lt;/strong&gt;. The test metrics (&lt;em&gt;n = 195&lt;/em&gt;) also showed consistent enhancement, with the &lt;strong&gt;F1 score for frame classification at 0.864&lt;/strong&gt; and &lt;strong&gt;trigger identification precision at 0.747&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;When evaluated on domain-specific datasets, &lt;strong&gt;the model exhibited varying degrees of effectiveness&lt;/strong&gt; which showcases our assumption that domains matter to the applicability of LLMs to domain-specific tasks and that our simple proposed way of generating domain-specific metrics can give insights on that. For the conflict keywords (&lt;em&gt;validation n = 121, test n = 255&lt;/em&gt;), the model achieved a &lt;strong&gt;validation F1 score of 0.865 for frame classification and 0.764 for trigger identification precision&lt;/strong&gt;. However, for the finance domain (&lt;em&gt;validation n = 121, test n = 255&lt;/em&gt;), the &lt;strong&gt;F1 score for frame classification was slightly higher at 0.878&lt;/strong&gt;, while the &lt;strong&gt;trigger identification precision was lower at 0.781&lt;/strong&gt; compared to the conflict domain.&lt;/p&gt; &lt;p&gt;The results indicate that the hyperparameter-tuned model significantly outperforms the vanilla model across all metrics. Additionally, domain-specific tuning appears to have a considerable impact on the model’s performance, with the finance domain showing slightly better results in certain metrics compared to the conflict domain. These insights could be pivotal for further refinements and targeted applications of the frame-semantic parser in natural language processing tasks. Moreover, these observation fit our general understanding of the two domains. Reports on conflicts are likely to discuss the involved parties’ reasons for specific actions like attacks on certain targets. Additionally, the actions in conflicts are arguably more &lt;strong&gt;triggering&lt;/strong&gt; events than “the good old stable economy.” Certainly, this research project can only be the beginning of a more rigorous assessment, but these findings show great promise of the idea of &lt;strong&gt;generating and evaluating simple, domain-specific performance metrics&lt;/strong&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img5_performance-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img5_performance-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-conflict-causality/img5_performance-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-conflict-causality/img5_performance.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p align=&quot;center&quot; style=&quot;color: white; font-style: italic; font-weight: bold;&quot;&gt;Performance Evaluation of Frame-Semantic Parser&lt;/p&gt; &lt;h2 id=&quot;conclusion--limitations&quot;&gt;Conclusion &amp;amp; Limitations&lt;/h2&gt; &lt;p&gt;This project has embarked on an innovative journey, merging advanced natural language processing techniques with the intricate study of conflict. By harnessing the power of a transformer-based frame-semantic parser and integrating semantic similarity analysis, we have made significant strides in identifying causal relationships within news articles. This methodology has not only illuminated the dynamics of conflict as portrayed in media but also demonstrated the adaptability and potential of frame-semantic parsing in domain-specific applications.&lt;/p&gt; &lt;h3 id=&quot;key-findings&quot;&gt;Key Findings&lt;/h3&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Utility of Frame-Semantic Parsing&lt;/strong&gt;: Our work has showcased the frame-semantic parser as a valuable and explainable tool, particularly effective in data-scarce environments like conflict research. Its ability to contextualize information and discern nuanced semantic relationships makes it an indispensable asset in understanding complex thematic domains.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Semantic Similarity for Domain-Specific Perspective&lt;/strong&gt;: We illustrated the effectiveness of using semantic similarity to refine seed phrases, thereby tailoring the frame-semantic parser to the specific domain of conflict. This approach has proven to be a straightforward yet powerful means to customize advanced NLP models for targeted analysis.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dependence on Domain for Model Performance&lt;/strong&gt;: Our findings highlight a significant insight: the performance of general-purpose language models can vary depending on the domain of application. This observation underscores the need for domain-specific tuning to achieve optimal results in specialized contexts.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Development of Domain-Specific Performance Metrics&lt;/strong&gt;: We proposed and validated a practical approach to developing domain-specific metrics, especially useful in resource-constrained environments. This methodology enables a nuanced evaluation of model performance tailored to specific thematic areas.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;limitations--future-research&quot;&gt;Limitations &amp;amp; Future Research&lt;/h3&gt; &lt;p&gt;Despite the promising results, our project is not without its limitations, which pave the way for future research opportunities:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Dependency&lt;/strong&gt;: The effectiveness of our approach is heavily reliant on the quality and diversity of the news article dataset. Biases in media reporting or limitations in the scope of articles can skew the analysis and affect the accuracy of the results. In an extended version of the project - and with funding - one could switch to the &lt;a href=&quot;https://www.dowjones.com/professional/factiva/&quot;&gt;Factiva&lt;/a&gt; dataset.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Applicability of Domain-Specificity to Other Themes&lt;/strong&gt;: While our method has shown efficacy in the context of conflict analysis, its applicability to other specific domains requires further exploration. Future research could test and refine our approach across various thematic areas to assess its broader utility.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Complexity and Interpretability&lt;/strong&gt;: While we have emphasized the explainability of the frame-semantic parser, the inherent complexity of transformer-based models can pose challenges in terms of scaling and deployment. Future work could focus on simplifying these models without compromising their performance - for instance via pruning and quantization.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expansion of Semantic Similarity Techniques&lt;/strong&gt;: Our semantic similarity analysis was instrumental in refining seed phrases, but there is room for further enhancement. Incorporating more advanced semantic analysis techniques could yield even more precise and relevant seed phrases. While we found alternative methods, like BERT-based approaches to not yield significant improvements, ever more models flood the market.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Integration with Other Data Sources&lt;/strong&gt;: Expanding the dataset beyond news articles to include social media, governmental reports, or academic literature could provide a more holistic view of conflict narratives and their causal relations.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In conclusion, our project represents a significant step forward in the intersection of natural language processing and conflict research. By addressing these limitations and building on our foundational work, future research can continue to push the boundaries of what is possible in this exciting and ever-evolving field.&lt;/p&gt; </content> </entry> <entry> <title>To Encode or Not To Encode: The Case for the Encoder-free Autodecoder Architecture</title> <link href="https://deep-learning-mit.github.io/blog/2023/autodecoders/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/autodecoders</id> <content type="html">&lt;h2 id=&quot;autodecoders&quot;&gt;Autodecoders&lt;/h2&gt; &lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt; &lt;p&gt;Autoencoders have been a part of the neural network landscape for decades, first proposed by LeCun in 1987. Today, many variants of the autoencoder architecture exist as successful applications in different fields, including computer vision and natural language processing, and the variational autoencoder remains among the forefront of generative modeling. Autoencoders are neural networks trained to reconstruct their input as their output via compression through dimensionality reduction, accomplishing this task with the use of an encoder-decoder network.&lt;/p&gt; &lt;p&gt;Autoencoders comprise of the encoder network, which takes a data sample input and translates it to a lower-dimensional latent representation consisting of only the most necessary features, and the decoder network, which attempts to reconstruct the original data from this encoding. By learning a compressed, distributed representation of the data, the latent space learned by autoencoders is usable for a plethora of downstream tasks.&lt;/p&gt; &lt;p&gt;With traditional autoencoders, both the encoder and decoder are trained, but for certain applications— particularly generative tasks— only the decoder is utilized for inference. Because the itself encoder is not used at test time, training an encoder may not be an effective use of computational resources; the autodecoder is an alternative architecture that operates without an encoder network and brings some novel benefits.&lt;/p&gt; &lt;p&gt;Rather than using the encoder to encode an input into a low-dimensional latent code, each sample in the training set begins with a randomly initialized latent code, and the latent codes and decoder weights are updated jointly during training time. For inference on new data, the latent vector for a given sample is then also randomly initialized and updated through an additional optimization loop with the decoder’s frozen weights.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/encoderdecoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/encoderdecoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/encoderdecoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/encoderdecoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The architecture for an autoencoder (top) compared to that of an autodecoder (bottom).&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Are explicit encoders necessary for image reconstruction? What are the unique benefits that come from using decoder-only architectures? One interesting application of autodecoders is the ability to reconstruct complete samples from partial inputs. The main focus of our research revolved around testing this ability, answering the question of how much of a sample is required for a complete reconstruction using an autodecoder given an expressive latent space, and comparing its performance to that of an autoencoder.&lt;/p&gt; &lt;p&gt;Furthermore, we discuss additional applications in various fields that other research has accomplished in part due to the utilization of the autodecoder architecture over the traditional autoencoder, with a focus on the beneficial properties that we explore in our experiments, including partial reconstructions.&lt;/p&gt; &lt;h3 id=&quot;related-work&quot;&gt;Related Work&lt;/h3&gt; &lt;p&gt;Different literature have utilized autodecoder frameworks in the past along with providing rationale for their usage, mainly for tasks related to reconstruction or generative modeling through representation learning. However, none have provided standalone examples of their use, something we aim to accomplish in this blog.&lt;/p&gt; &lt;p&gt;The Generative Latent Optimization framework was introduced by Bojanowski et al. (2019) as an alternative to the adversarial training protocol of GANs. Instead of producing the latent representation with a parametric encoder, the representation is learned freely in a non-parametric manner. One noise vector is optimized by minimizing a simple reconstruction loss and is mapped to each image in the dataset.&lt;/p&gt; &lt;p&gt;Tang, Sennrich, and Nivre (2019) trained encoder-free neural machine translation (NMT) models in an endeavor to produce more interpretable models. In the encoder-free model, the source was the sum of the word embeddings and the sinusoid embeddings (Vaswani et al., 2017), and the decoder was a transformer or RNN. The models without an encoder produced significantly poorer results; however, the word embeddings produced by encoder-free models were competitive to those produced by the default NMT models.&lt;/p&gt; &lt;p&gt;DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes, was introduced by Park et al. (2019) as a novel representation for generative 3D modelling. Autodecoder networks were used for learning the shape embeddings, trained with self-reconstruction loss on decoder-only architectures. These autodecoders simultaneously optimized the latent vectors mapping to each data point and the decoder weights through backpropogation. While outperforming previous methods in both space representation and completion tasks, autodecoding was significantly more time consuming during inference because of the explicit need for optimization over the latent vector.&lt;/p&gt; &lt;p&gt;Sitzmann et al. (2022) introduced a novel neural scene representation called Light Field Networks (LFNs), reducing the time and memory complexity of storing 360-degree light fields and enabling real-time rendering. 3D scenes are individually represented by their individual latent vectors that are obtained by using an autodecoder framework, but it is noted that this may not be the framework that performs the best. The latent parameters and the hypernetwork parameters are both optimized in the training loop using gradient descent; the LFN is conditioned on a single latent variable. Potential applications are noted to include enabling out-of-distribution through combining LFNs with local conditioning.&lt;/p&gt; &lt;p&gt;Scene Representation Networks (SRNs) represent scenes as continuous functions without knowledge of depth or shape, allowing for generalization and applications including few-shot reconstruction. SRNs, introduced by Sitzmann, Zollhöfer and Wetzstein (2019), represent both the geometry and appearance of a scene, and are able to accomplish tasks such as novel view synthesis and shape interpolation from unsupervised training on sets of 2D images. An autodecoder framework is used to find the latent vectors that characterize the different shapes and appearance properties of scenes.&lt;/p&gt; &lt;h3 id=&quot;methodology&quot;&gt;Methodology&lt;/h3&gt; &lt;h4 id=&quot;traditional-autoencoder&quot;&gt;Traditional Autoencoder&lt;/h4&gt; &lt;p&gt;To establish a baseline, we first trained a convolutional autoencoder network containing both an encoder and decoder on a version of the MNIST dataset normalized and padded to contain 32x32 sized images. For our autoencoder architecture, we utilized convolutional layers with ReLU nonlinearity.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoderloss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoderloss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoderloss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoderloss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The training and validation losses from the training loop for the autoencoder.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The latent space learned by the autoencoder, color-coded by digit label and visualized through a 2-dimensional t-SNE plot. We see the expected result, with consistency and separation.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencodersampleoutput-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencodersampleoutput-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencodersampleoutput-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencodersampleoutput.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;A sample output from an unseen image after training. We can see that our small convolutional autoencoder does a fairly good job at learning how to compress simple information into a single latent code and decode it into its original form.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h4 id=&quot;autodecoder&quot;&gt;Autodecoder&lt;/h4&gt; &lt;p&gt;We implemented and trained an autodecoder on the same dataset by creating a convolutional decoder that takes latent codes as an input and transforms them into full images. We utilized transpose convolutions to upscale the images while additionally concatenating normalized coordinates to embed positional information, and also used leaky ReLU layers for nonlinearity.&lt;/p&gt; &lt;p&gt;For training, the latent codes for 10,000 images in our training set were randomly initialized. The loss for our autodecoder then included three components: the reconstruction loss; the latent loss, which encourages latent values to be closer to zero in order to encourage a compact latent space; and the L2 weight regularization, which prevents the decoder from overfitting to the training set by encouraging the model weights to be sparse.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/lossfn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/lossfn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/lossfn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/lossfn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The loss function used to train the autodecoder. During inference, a custom loss function can be used based on the application.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoderloss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoderloss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoderloss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoderloss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The training and validation losses from the training loop for the autodecoder. The validation loss has no actual meaning in the autodecoder framework, as new images would have a randomly initialized latent code and so would output nonsense. This loss was included simply to demonstrate this feature.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Below are progressive reconstructions on the training data performed by the autodecoder as it trained and optimized both the decoder weights and the training set’s latent codes. We can observe that the digits’ general forms were learned before the exact shapes, which implies good concentration and consistency of the latent space between digits of the same class.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/progress1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/progress2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/progress3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/progress4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/progress4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Progressive reconstructions from top to bottom (model outputs compared to ground truth): 1. Decoding a randomly initialized latent code outputs nonsense. 2. The correct digit is reconstructed, implying that the latent space is improving, but the specific shape differs from that of the ground truth image. 3. The output’s shape begins to better match that of the ground truth. 4. The autodecoder and latent code are optimized to be able to effectively reconstruct the ground truth image.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/tsne_autodecoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The latent space learned by the autodecoder, also visualized through a 2-dimensional t-SNE plot. When compared to the t-SNE plot representing the latent space learned by the autoencoder, we again see consistency but notice that here the clusters are more compact. While the distance between clusters in t-SNE plots does not have a definite meaning, this could potentially imply that the features of shapes, rather than the shapes themselves, are better learned, as different digits share similar features (curves, straight lines, etc).&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Upon training the autodecoder, for inference on a new image we first freeze the decoder weights and then run an additional gradient descent-based optimization loop over a new randomly initialized latent code with reconstruction loss.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecodersampleoutput-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecodersampleoutput-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecodersampleoutput-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecodersampleoutput.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Output from the trained autodecoder on a new image from the test set.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3 id=&quot;experimentation&quot;&gt;Experimentation&lt;/h3&gt; &lt;p&gt;One benefit of the autodecoder framework is that because we have an additional optimization loop for each input during inference, we are able to do varying pixel-level reconstructions, whereas an autoencoder is designed and trained to reconstruct complete images each time.&lt;/p&gt; &lt;p&gt;We demonstrate this feature in our experiments below by applying center masks to our images before autoencoding or decoding.&lt;/p&gt; &lt;p&gt;1: We trained a traditional &lt;strong&gt;autoencoder&lt;/strong&gt; with generic reconstruction loss, and input an image with a mask in the center. The output is expected, as the autoencoder learned to reconstruct whatever it saw, and so the empty space from the mask is included in the result.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_7.png&quot; class=&quot;img-fluid&quot; width=&quot;50&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The input image&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_7.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The reconstructed image compared to the image without the mask.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;2: We trained a traditional &lt;strong&gt;autoencoder&lt;/strong&gt; with reconstruction loss without considering a centered square area and input an unmodified image. The output is again expected, as the autoencoder was trained to fully disregard the center area, and so the output is empty in that region.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_input_4.png&quot; class=&quot;img-fluid&quot; width=&quot;50&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The area of the images that the autoencoder is trained to learn on.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_output_4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The model output compared to the original image.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;3: We trained an &lt;strong&gt;autodecoder&lt;/strong&gt; with generic reconstruction loss, and during the optimization loop for inference we utilized a custom loss function that did not consider the masked area. However, in this case, we are still able to reconstruct the original image to varying levels of success because of the latent space we originally learned through the training loop.&lt;/p&gt; &lt;p&gt;Shown below are the areas optimized in the loss functions, along with the decoded output and original image.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_input-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_input-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_input-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_input.png&quot; class=&quot;img-fluid&quot; width=&quot;50&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_output-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_output-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_output-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/8x8mask_output.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;An 8x8 mask.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12mask_input-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12mask_input-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12mask_input-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12mask_input.png&quot; class=&quot;img-fluid&quot; width=&quot;50&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12maskoutput-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12maskoutput-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12maskoutput-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/12x12maskoutput.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;A 12x12 mask. Even with significant information about the digit missing, the autodecoder is able to sufficiently reconstruct the ground truth image based on the learned information.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_input-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_input-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_input-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_input.png&quot; class=&quot;img-fluid&quot; width=&quot;50&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_output-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_output-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_output-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/16x16mask_output.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;A 16x16 mask.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_input-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_input-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_input-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_input.png&quot; class=&quot;img-fluid&quot; width=&quot;50&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_output-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_output-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_output-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/20x20mask_output.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;A 20x20 mask. Although the reconstructed digit is ultimately incorrect, we see that we are able to get very close even with extremely limited inputs.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;To analyze and compare the latent spaces learned by both our autoencoder and autodecoder, we additionally perform linear interpolation (with α=0.5) between the embeddings of two images and include their decoded results below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autoencoder_interpolation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The output of the decoded interpolation of two embeddings from the autoencoder.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoder_interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoder_interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoder_interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-autodecoders/autodecoder_interpolation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;The output of the decoded interpolation of two embeddings from the autodecoder.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The autoencoder output was somewhat expected due to the simplistic nature of the MNSIT dataset, and we can see a merge of the two images with equal features of both.&lt;/p&gt; &lt;p&gt;More interesting was the output for the autodecoder, which simply returned an image consisting of the pixel average of both images. Some hypotheses for this result include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The shape of the latent space for the learned autodecoder potentially being one that does not pair well with linear interpolation, causing linear interpolations in latent space to be equivalent to those in the data space. Meanwhile, the shape of the latent space for the autoencoder might better match a Gaussian, which translates to effective nonlinear interpolations in the data space, which is desired.&lt;/li&gt; &lt;li&gt;The inductive bias from the existence of the encoder architecture allowing for better interpolatability.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt; &lt;h4 id=&quot;discussion&quot;&gt;Discussion&lt;/h4&gt; &lt;p&gt;While autoencoders (and variations such as VAEs) have traditionally been the standard architectures for representation learning, we explore an alternate autodecoder architecture, in which the encoder is excluded and individual latent codes are learned along with the decoder. We investigated the necessity of an explicit encoder in representation learning tasks and found that even without an encoder network, we are able to learn latent representations of input data through the optimization of randomly initialized latent codes during the training loop. Through this alternate dimensionality reduction process, we showed that we were still able to learn a consistent latent space on a multi-class dataset. Furthermore, we showed that through the use of an additional optimization loop for inference rather than learned encoder weights, the autodecoder can learn to reconstruct incomplete observations through pixel-level optimizations.&lt;/p&gt; &lt;p&gt;The autodecoder has the potential for many further applications beyond the scope of the research and experiments introduced in this blog. As an example, the task of prior-based 3D scene reconstruction in the field of computer vision, in which novel views of a 3D scene can be generated from a limited number of static images of that scene along with their camera poses, utilizes the autodecoder architecture to guarantee better out-of-distribution views. This task involves the use of camera pose as an additional source of information in addition to input images, something that the encoder itself is unable to integrate when encoding images, leading to the valuable scene representation information being left out. Meanwhile, because the latent code itself is learned in an autodecoder, it is able to use the camera pose to effectively generalize to novel viewpoints. This serves as just one of several examples of the autodecoder being able to carry out tasks normally gatekept by the limitations of the encoder.&lt;/p&gt; &lt;h4 id=&quot;limitations&quot;&gt;Limitations&lt;/h4&gt; &lt;p&gt;Some limitations of the encoder-free architecture include certain fallbacks discussed in our experiments, including the difficulties in generating satisfactory novel outputs through linear interpolation of the latent space. Furthermore, while the existence of a secondary optimization loop during inference comes with interesting properties such as being able to define unique loss functions for different purposes, this can be more computationally or temporally costly than running inputs on a trained encoder for inference. Regardless, as much of the research around this topic has emerged only within the past several years, it can be expected that autodecoders and their unique properties will continue to emerge, evolve, and find use in novel applications in the years to come.&lt;/p&gt; &lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt; &lt;p&gt;Robin Baumann. Introduction to neural fields, 2022.&lt;/p&gt; &lt;p&gt;Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space of generative networks. arXiv preprint arXiv:1707.05776, 2017.&lt;/p&gt; &lt;p&gt;Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165–174, 2019.&lt;/p&gt; &lt;p&gt;Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:19313–19325, 2021.&lt;/p&gt; &lt;p&gt;Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems, 32, 2019.&lt;/p&gt; &lt;p&gt;Gongbo Tang, Rico Sennrich, and Joakim Nivre. Encoders help you disambiguate word senses in neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2019.&lt;/p&gt; </content> </entry> <entry> <title>New Synthesis Approach for Personalized LLMS</title> <link href="https://deep-learning-mit.github.io/blog/2023/PersonalizedGeneration_w_LLMAgents/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/PersonalizedGeneration_w_LLMAgents</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Deep learning has revolutionized the way in which humans interact with the world around them. Its growing ability to ingest vast amounts of data, automate feature extraction, and learn complex patterns and nuances among inputs have contributed to breakthroughs in healthcare, natural language processing, computer vision, and more. A particularly exciting avenue of this innovation has been in the burgeoning field of personalized text generation, which aims to produce text that resembles the style, tone, and word choice taken on by a particular user. Significant advancement in this field has the potential to create more effective forms of communication for individuals with disabilities, personalize educational content, and enhance user interactions with chatbots and virtual assistants, all contributing to a better overall user experience.&lt;/p&gt; &lt;p&gt;In an effort to make the availability of personalized text generation more wide-scale, researchers have conducted several studies in the field, centering their approach to the generation of domain-specific personalized text (utilizing domain-specific features/knowledge). Notable studies conducted include &lt;a href=&quot;https://arxiv.org/pdf/1910.03506.pdf&quot;&gt;Towards Controllable and Personalized Review Generation&lt;/a&gt;, which utilizes a product description and self-attentive recursive autoencoders to generate a personalized review &lt;a href=&quot;#1&quot;&gt;[1]&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2010.01480.pdf&quot;&gt;Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural Network&lt;/a&gt;, which constructs a model based on a CapsGNN, and &lt;a href=&quot;https://iopscience.iop.org/article/10.1088/1742-6596/2294/1/012015/pdf&quot;&gt;Research on user granularity-level personalized social text generation technology&lt;/a&gt;, which utilizes an encoder and decoder for text generation &lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt;. A lesser explored part of the field and an area that we have chosen to explore for our final project is embedding in the ability to generate personalized text across domains without domain-specific features &lt;a href=&quot;#3&quot;&gt;[3]&lt;/a&gt;. Our project draws inspiration from &lt;a href=&quot;https://arxiv.org/pdf/2308.07968.pdf&quot;&gt;“Teach LLMs to Personalize – An Approach inspired by Writing Education”&lt;/a&gt;, which includes a promising multi-step framework that retrieves, summarizes, ranks, and synthesizes a user’s past documents to generate a personalized version of the document at hand &lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A critical aspect of the workflow discussed in the LLM personalization paper and an area that we believe can be improved upon using some of the methods discussed in 6.S898 this semester is the way in which the model synthesizes past documents. Throughout the paper, we will be exploring two creative approaches to synthesis that utilize vector word embeddings to pull relevant words from past documents in an effort to improve the models ability to personalize text.&lt;/p&gt; &lt;h1 id=&quot;literature-review&quot;&gt;Literature Review&lt;/h1&gt; &lt;p&gt;An integral part of our exploration project was experimenting with using less data and smaller models to see how performance degrades with respect to the approach discussed in the personalization for LLMs paper (no open source code attached as the project is currently being worked on by researchers at Google). Experimentation required taking an extensive look at the steps involved in the original implementation, gaining an in-depth understanding of the deep learning principles discussed, and optimizing training and compute under machine constraints to process vast amounts of real-world data.&lt;/p&gt; &lt;p&gt;The problem formulation for the approach to personalized text generation discussed in the paper can be stated as the following: Given the immediate context of a current document (first k characters) written by a user and access to their past documents, can we develop a model that generates text that is similar to the text of the current document (similarity evaluated by calculating Rouge-1, Rouge-2, Rouge-L, and Bleu scores) . As mentioned earlier, the framework for answering this problem formulation involves first obtaining outputs for retrieval, ranking, summarization, and synthesis, and then feeding these distinct parts into an LLM to produce a personalized body of text (we ignore the auxiliary task of training the LLM to distinguish the owners of written documents for the purposes of this project).&lt;/p&gt; &lt;p&gt;The retrieval discussed in the paper uses two methods of outputting relevant documents: sparse retrieval, which compares past documents to the current context using the popular BM25 ranking algorithm, and dense retrieval, which uses a transformer-based text-to-text model to map and compare documents in a 768 dimensional vector space. The ranking step then takes this input, orders documents based on their BM25 scores or cosine similarity when compared with the immediate context, and truncates the input to 2500 characters to only take the top documents. The summarization step then summarizes the top ranked past documents in two ways: context independent summarization, which finetunes an LLM on publicly available data and applies this model to the top ranked entries, and context dependent summarization, which uses weak labels (generated from immediate context) to generate a summary in line with the contents of the current document. A visualization of the approach to the structure can be seen below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/gen_structure_overview.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An overview of the infrastructure used to process documents and fine tune the personalized generative model. &lt;/div&gt; &lt;p&gt;The paper currently explores two methodologies for synthesis: (1) context dependent synthesis, which simply uses the top 20 frequently used keywords from a user’s past documents and (2) context dependent synthesis, which uses weak labels to find text from past documents similar to the immediate context of the document to be personalized. After carefully analyzing the two methodologies, we found that by focusing on keywords the synthesized text is missing an in-depth understanding of sentence structure and semantics that are crucial to personalization.&lt;/p&gt; &lt;p&gt;To enhance this step of the text generation process, we have explored several new methods of synthesis and have landed on two approaches with one utilizing the Word2Vec model and the other using GloVe. We have chosen these methods because they both use unique embedding space attributes to form important relationships between texts. Both networks use the method of creating a centroid of the current document that exists in vector space and output words from top ranked past documents that exist close to this centroid. By doing this, we are essentially selecting words (after filtering out synonyms and stopwords) that are in line with the theme of the current document, which will provide the LLM with more thematically relevant synthesized entries that should in theory generate a more personalized output.&lt;/p&gt; &lt;p&gt;As an additional research consideration, we explored the effect of passing in the output from both the context independent synthesis discussed in the paper and our auxiliary method of using Word2Vec or GloVe compared to passing in just one of the methods of synthesis. The motivation for doing so came from our initial hypothesis that the combination of both methods of synthesis would enable the LLM to learn complex interactions between important words (results from context independent synthesis) and thematic words (GloVe/Word2Vec) that could lead to better personalization of the final output. A more detailed explanation of the implementations of our proposed approaches will be shown in the following section.&lt;/p&gt; &lt;h1 id=&quot;description-of-methods--experiments&quot;&gt;Description of methods &amp;amp; experiments&lt;/h1&gt; &lt;h2 id=&quot;the-baseline-implementation&quot;&gt;The Baseline Implementation&lt;/h2&gt; &lt;p&gt;Our methodological approach began by re-implementing the baseline model from the “Teach LLMs to Personalize” paper. We utilized two datasets mentioned in the research paper: CNN_DailyMail (&lt;a href=&quot;https://huggingface.co/datasets/cnn_dailymail&quot;&gt;CNN_DailyMail&lt;/a&gt;)and Amazon Review Data for Books (&lt;a href=&quot;https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/&quot;&gt;Amazon_review_data&lt;/a&gt;). To enhance efficiency of compute time, we streamlined the data by reducing its size, ensuring a quicker fine-tuning process while retaining data integrity. We also utilized the &lt;a href=&quot;https://huggingface.co/t5-base&quot;&gt;T5-base model&lt;/a&gt;, a smaller model than the T5-11b model mentioned in the paper, for summarization and the personalized generation model. Furthermore, we opted to use the context-independent methods for both summarization and synthesis because the research paper results indicated that their effectiveness is closely comparable to the context-dependent methods. For fine-tuning the summarization model, we utilized a 10 percent subset of the CNN daily mail dataset (311k datapoint original size) with the AdamW optimizer (seeing AdamW is a comparable optimizer to Adafactor, which is what was used in the “Teach LLMs to Personalize” paper), ensuring a balance between efficiency of tuning and comprehensive learning. This set the foundation for our exploration of advanced text synthesis techniques by giving us a base fine tuning and data processing infrastructure. On top of this, the changes we made to the amount of data used along with utilizing a smaller T5 model allowed us to analyze whether the final evaluation results degraded significantly when making the infrastructure of fine tuning the personalized generation model more compact.&lt;/p&gt; &lt;h2 id=&quot;overview-of-modification--experiments&quot;&gt;Overview of Modification &amp;amp; Experiments&lt;/h2&gt; &lt;p&gt;In our new approach for synthesis, we utilized Word2Vec and GloVe which hinges on the concept of embedding space. In this space, words are represented as vectors, capturing their semantic relationships based on their context in large text corpora. By embedding the current document and past documents (from the same user) in this space, each word is assigned a position that reflects its semantic meaning.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/tsne_dim_reduction_example.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of how using TSNE dimension reduction can illustrate how words are placed in embedding space. Note that dimension reduction does not always come out cleanly since word embeddings are complex and can&apos;t be easily represented in 2D space. &lt;/div&gt; &lt;p&gt;The ‘centroid’ of the current document in this space is a calculated mean vector, representing the overall semantic direction of the document. Words closest to this centroid are likely to be central to the document’s theme or style. When we look for words from past documents that are closest to this centroid, we are essentially searching for words that align closely with the thematic and stylistic essence of the current document.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/centroid_example.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of how our centroid for the current document corresponds to other words from past documents (note we used PCA dimensionality here). We also chose to display words that had very close euclidean distances to the centroid. Note our centroid from the current document highlighted the following as significant words: [&apos;like&apos;, &apos;since&apos;, &apos;first&apos;, &apos;mystery&apos;, &apos;book&apos;] &lt;/div&gt; &lt;p&gt;This method makes sense technically because it leverages the nuanced understanding of language captured in word embeddings. By focusing on words that are semantically close to the central theme of the current document, the model can more effectively identify and use terms that are likely to be relevant and stylistically consistent for personalization, thereby potentially enhancing the generated text of the personalized generation model.&lt;/p&gt; &lt;h2 id=&quot;experiment--word2vec-vs-glove&quot;&gt;Experiment / Word2Vec vs GloVe&lt;/h2&gt; &lt;p&gt;Word2Vec and GloVe are both models for word embeddings, but they differ in their approach to creating these embeddings. Word2Vec, developed by Google, primarily uses local context information of words (words surrounding a given word) to generate embeddings. This results in embeddings that capture more of the syntactic and semantic relationships based on specific local contexts.&lt;/p&gt; &lt;p&gt;GloVe (Global Vectors for Word Representation), on the other hand, is designed by Stanford and incorporates global matrix factorization and local context window methods. It emphasizes capturing global statistics of the corpus by considering overall word co-occurrence frequencies, essentially acting as an unsupervised learning algorithm that generates word embeddings.&lt;/p&gt; &lt;p&gt;When used for synthesis in text personalization, these differences influence the nature of the embeddings. Word2Vec might be more sensitive to the specific contextual use of words in the current and past documents, potentially offering more precise thematic matches based on immediate context. GloVe, with its global perspective, might bring in a broader understanding of word use, capturing more general usage patterns and thematic relationships that extend beyond the immediate context. This could lead to a slightly different set of words being selected for personalization in the synthesis process.&lt;/p&gt; &lt;p&gt;In our experiment, we adapted the structure from the “Teach LLMs” paper, incorporating our novel synthesis methods using Word2Vec and GloVe. The process involved independently fine-tuning the personalized generation model for each synthesis approach. This fine-tuning was crucial to observe how the different embedding techniques influenced the model’s performance. After implementing the new synthesis methods, we conducted a thorough evaluation to compare their effectiveness, along with the combination of the original and new synthesis approaches, with the base model. The key focus was on analyzing how the different word embeddings (and combinations of embeddings) impacted the quality and personalization of the generated text, with performance metrics providing insights into the strengths and limitations of each method.&lt;/p&gt; &lt;h1 id=&quot;analysis--evaluation-of-results&quot;&gt;Analysis / Evaluation of Results&lt;/h1&gt; &lt;p&gt;The evaluation metrics used in the “Teach LLMs” paper (and also what we utilized), BLEU (Bilingual Evaluation Understudy), ROUGE-1, ROUGE-2, and ROUGE-L, are standard metrics used to evaluate the quality of text which has been machine-translated or generated by machine learning models.&lt;/p&gt; &lt;p&gt;BLEU Score: The BLEU score evaluates the quality of machine-translated text by comparing it with one or more reference translations. It does so at various levels, from individual words to consecutive sequences of words (n-grams), to assess precision. A higher BLEU score indicates more similarity to the reference text, often implying better translation quality. However, BLEU has limitations as it does not account for the fluency or grammaticality of the generated text.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/bleu_score.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Calculations behind the BLEU score calculations. &lt;/div&gt; &lt;p&gt;ROUGE Scores: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is another set of metrics for evaluating automatic summarization and machine translation. ROUGE-1 and ROUGE-2 refer to the overlap of unigrams (single words) and bigrams (two consecutive words) between the machine-generated text and a set of reference texts, respectively. ROUGE-L considers the longest common subsequence, focusing on the longest coherently matching sequence of words. ROUGE scores can consider both precision (like BLEU) and recall, providing a more rounded evaluation.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-PersonalizedGeneration_w_LLMAgents/rouge_score.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Calculations behind the ROUGE-N (N-gram) score calculations; in our case N = 1, 2, or longest common subsequence. &lt;/div&gt; &lt;p&gt;We can also take a look into how our models performed during the fine tuning period. Based on the progression of the training and validation loss, you can infer how well the model is learning and whether it’s overfitting (learning the training data too closely and not generalizing well) or underfitting (not learning the training data well enough).&lt;/p&gt; &lt;p&gt;Comparing the performance of our models using two different synthesis approaches–our base model versus the new synthesis approach using the GloVe or Word2Vec model, and the combination of the base model and new synthesis–could result in different behaviors most likely for one particular reason:&lt;/p&gt; &lt;p&gt;Quality of Embeddings: The GloVe and Word2Vec models provide a different representation for words, capturing semantic relationships in a more nuanced way than just looking at IDF scores, which could lead to varied results during fine tuning. Also, combining our original synthesis with our new synthesis can give the model more information to finetune on allowing for a more intricate understanding of the text when generating.&lt;/p&gt; &lt;p&gt;The differences in BLEU and ROUGE scores between the two models can arise from how each model handles the linguistic nuances of the generated text. If the new approach with the GloVe model is better at capturing the context and meaning of the sentences, it might score higher in BLEU and ROUGE, despite potentially higher loss values.&lt;/p&gt; &lt;p&gt;The variations in BLEU and ROUGE scores could also indicate how each model deals with the trade-off between precision and recall—whether it’s better at producing text that contains most of the expected content (high recall) or at avoiding extraneous content not in the reference (high precision).&lt;/p&gt; &lt;p&gt;Evaluating these metrics in combination with each other, rather than in isolation, provides a more comprehensive picture of a model’s performance and areas for potential improvement.&lt;/p&gt; &lt;p&gt;The following results portray the overarching BLEU, ROUGE-1, ROUGE-2, and ROUGE-L score we received for the base model, our model using the new synthesis approach, and our model using the base synthesis along with the new synthesis. We have highlighted the snippets of the generated cases that produced our highest scores which are indicative of the possibilities of improvement if we were able to utilize larger T5 models and more training data.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;The following table highlights the results of our evaluation of generated outputs from our baseline model versus our two new approaches (new synthesis and old synth + new synth). Althought there are cases where the max score for our new approaches are high, we believe that this is most likely the case where we generate the rest of a document that is already signficantly short. Essentially, since we don’t need to generate a diverse output of words for a longer length, our more compact t5-base model with minimal training performs very well still. &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;BLEU (avg)&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;ROUGE1 (avg)&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;ROUGE2 (avg)&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;ROUGEL (avg)&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;BLEU (max)&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;ROUGE1 (max)&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;ROUGE2 (max)&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;ROUGEL (max)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;Baseline Model&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;08.9531&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;29.5847&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;18.6126&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;25.6882&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;49.5207&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;65.2174&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;62.2222&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;65.2173&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;New Synth (Word2Vec)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;09.0722&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;29.3465&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;18.3129&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;25.6115&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;46.6638&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;65.9340&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;62.2222&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;65.2174&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;New Synth (GloVe)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;10.3810&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;31.9870&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;21.1543&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;27.4335&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;50.5317&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;65.8537&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;60.1942&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;63.4146&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;New Synth (Word2Vec) + Old Synth&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;10.4402&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;31.4181&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;20.2349&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;27.7710&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;58.0197&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;64.8148&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;61.9048&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;62.7907&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: left&quot;&gt;New Synth (GloVe) + Old Synth&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;08.7228&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;29.2284&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;17.1685&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;24.6075&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;49.7273&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;65.5462&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;60.9756&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;61.9048&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h1 id=&quot;conclusion--discussion-of-limitations&quot;&gt;Conclusion / Discussion of Limitations&lt;/h1&gt; &lt;p&gt;Throughout the paper, we have demonstrated the potential of embedding techniques like Word2Vec and GloVe in enhancing the personalization aspect of text generation models. Our experiments, which involved comparing these methods with traditional synthesis techniques, have shown promising results in terms of creating text that more accurately reflects the style and thematic preferences of individual users.&lt;/p&gt; &lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt; &lt;p&gt;For our exploration, we were limited to running all of our models and doing our data analysis on Google Colab in a short period of time along with having to reimplement the structure used in the “Teach LLMs to Personalize” paper since no codebase exists for it. Because of this, we had to find ways to condense our models and limit the amount of data we ingested so that we could spend less time waiting on models to run and freeing up storage and more time analyzing the output of our code. Two of the big adjustments that we made to navigate these constraints was using the t5-base model (fewer tokens than t5-11b), which we ran for a limited number of epochs, instead of the t5-11b model and using only a subset of data points from the provided Amazon Review Dataset. One of the other things that we tried to make the most advantage of our compute was quantizing our t5-base model to provide faster synthesis and summary to run on our ingested data, but we unfortunately ran into dependency issues and were unable to get this method working. However, from our analysis, we estimate that our evaluation results would have been much more in line with the paper’s results, or even surpass them, if we were able to run the t5-11b model for a larger amount of epochs and utilize more amazon review data.&lt;/p&gt; &lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt; &lt;p&gt;If we choose to continue this project, we want to explore ways in which we can synthesize domain-specific knowledge, along with thematic tendencies, related to the current document that can be fed into the final LLM for text generation. There are a lot of benefits of providing synthesized information to the model as it filters for the “most important/significant” words in a document and we hypothesize that this supplementary information could add an extra level of knowledge to a model that has proven to perform well in personalization.&lt;/p&gt; &lt;p&gt;Also, another pathway that could be explored is integrating Agent LLMs in the initial document ranking phase to see if the procured rankings are better than the current methods set in place (RankDocBM25, RankDocDense, RankSnippet, RankDocBySnpt). We believe that utilizing LLMs that have more awareness of context over large document spaces (and even varying languages) could be benefitial to the process of developing personalized generation model.&lt;/p&gt; &lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt; &lt;p&gt;&lt;a id=&quot;1&quot;&gt;[1]&lt;/a&gt; Li, Pan, and Alexander Tuzhilin. Towards Controllable and Personalized Review Generation - arXiv.Org, arxiv.org/pdf/1910.03506.pdf. Accessed 12 Dec. 2023.&lt;/p&gt; &lt;p&gt;&lt;a id=&quot;2&quot;&gt;[2]&lt;/a&gt; Li, Junyi, et al. Knowledge-Enhanced Personalized Review Generation with … - Arxiv.Org, arxiv.org/pdf/2010.01480.pdf. Accessed 12 Dec. 2023.&lt;/p&gt; &lt;p&gt;&lt;a id=&quot;3&quot;&gt;[3]&lt;/a&gt; Gao, Y B, et al. “IOPscience.” Journal of Physics: Conference Series, IOP Publishing, 1 June 2022, iopscience.iop.org/article/10.1088/1742-6596/2294/1/012015.&lt;/p&gt; &lt;p&gt;&lt;a id=&quot;4&quot;&gt;[4]&lt;/a&gt; Li, Cheng, et al. Teach LLMs to Personalize: An Approach Inspired by Writing Education - Arxiv.Org, arxiv.org/pdf/2308.07968.pdf. Accessed 12 Dec. 2023.&lt;/p&gt; &lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt; &lt;ol&gt; &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt; &lt;p&gt;Output Produced From our Codebase: &lt;a href=&quot;https://github.com/dapacica/DL_finalproject_code/blob/main/FinalProjCleanColab.ipynb&quot;&gt;https://github.com/dapacica/DL_finalproject_code/blob/main/FinalProjCleanColab.ipynb&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt; </content> </entry> <entry> <title>Augmenting Expert Domain Image Inputs for Enhancing Visual Language Models Performance</title> <link href="https://deep-learning-mit.github.io/blog/2023/vig-algorithm-flow-project-proposal/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/vig-algorithm-flow-project-proposal</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Over the past few years, we have seen a surge in creation, adoption, and excitement around visual language models, specifically around Open AI’s CLIP model. Visual language models can bridge the gap between image and text, allowing tokenized understanding of the visual world around us. For instance, Meta released Segment Anything, a model with enhanced object detection through multimodal inputs like defined bounding boxes and text.&lt;/p&gt; &lt;p&gt;After the recent surge with ChatGPT, we have begun to see advancements in the visual language model space to combine the image analysis and conversational tool. While the recent developments with Bard, GPT4-v, LLava, and many others have progressed the visual language model domain, the overall capabilities of the models are limited to the type of images provided. Most of the models have been trained and finetuned on common day objects, specializing in every-day normal tasks.&lt;/p&gt; &lt;p&gt;However, theses models continue to struggle with answering images derived from an expert domain, especially scientific literature. Images from these domains can be challenging for the model, as they require common background knowledge, domain knowledge, and interpretation of the diagram.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Question: What is the path from 1 to 4? &lt;br /&gt;&lt;br /&gt; GPT4-v Answer: The image you&apos;ve uploaded appears to show a diagram with four numbered points, possibly representing steps or locations connected by a path... However, as an AI, I can&apos;t visually trace paths or analyze images in the way a human would... &lt;/div&gt; &lt;/div&gt; &lt;p&gt;How can we assist visual language models to improve performance in expert domains?&lt;/p&gt; &lt;h2 id=&quot;past-works&quot;&gt;Past Works&lt;/h2&gt; &lt;p&gt;Visual Language Models have become very popular in the recent years with their ability to connect image to text. Open Flamingo&lt;d-cite key=&quot;openflamingo&quot;&gt;&lt;/d-cite&gt; is an open source model with a similar architecture to Flamingo&lt;d-cite key=&quot;flamingo&quot;&gt;&lt;/d-cite&gt;: images are sent to (1) visual encoders, (2) perceived resampler, and (3) through a dense group of layers. Through few shot learning, text completion, and image analysis, Open Flamingo allows users to have a conversation involving images.&lt;/p&gt; &lt;p&gt;Currently, popular visual language models, like Flamingo, utilize CLIP&lt;d-cite key=&quot;clip&quot;&gt;&lt;/d-cite&gt; as the visual encoder to perform the image-to-embedding conversion. Behind the scenes, CLIP uses a Vision Transformer architecture as the encoder. However, when we analyze the embeddings output from CLIP, they do not necessarily have a clear representation to the human eye. This makes it really challenging to inject domain-specific knowledge through the embedding to help perform specific tasks.&lt;/p&gt; &lt;p&gt;The project, FlowchartQA&lt;d-cite key=&quot;flowchart&quot;&gt;&lt;/d-cite&gt;, contributed by creating a dataset of flowcharts and code, revealing insights into the relationships forming code. However, no research has tried to understand the way visual language models interpret graphs with nodes and edges, specifically for domain specific questions. Through various changes to text and image inputs, we can learn about the way a visual language model understands graphical structures to improve future performance of VLMs.&lt;/p&gt; &lt;h2 id=&quot;procedure&quot;&gt;Procedure&lt;/h2&gt; &lt;h3 id=&quot;dataset-creation&quot;&gt;Dataset Creation&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Data Augmentation Pipeline for the Inputs &lt;/div&gt; &lt;p&gt;To learn more about the graphical understanding of VLMs, a dataset had to be curated to test various conditions. The original images of the flowcharts are sampled from the BizGraphQA dataset&lt;d-cite key=&quot;bizgraphqa&quot;&gt;&lt;/d-cite&gt;. Due to the compute and time contraints, only a sample of ten images were utilized for the analysis. This subset of images is passed through the Segment Anything Model from Meta &lt;d-cite key=&quot;sam&quot;&gt;&lt;/d-cite&gt; to extract a mask of each of the nodes. From there, OCR is performed on each node to retrieve the text and hand coloring is performed to color the image. In the end, we get dataset triples of the original image, colored image, and the list of nodes in the image.&lt;/p&gt; &lt;p&gt;For example, for this image, we would have the following dataset.&lt;/p&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Original Image &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Colored Image &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Node List &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; [Finanze Tyco Roadsafe Insurance, Greyhound Bbbytf Limited, Etablissements Lgac Lthalat Incorporated Indiana, Korte Pro-Cut Grayhawk Insurace North Carolina, Hutchins Sandvik Maryland, Nm Suc. Krsx Limited Michigan] &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;experimentation&quot;&gt;Experimentation&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Experimentation Pipeline for the Inputs &lt;/div&gt; &lt;p&gt;Bard uses Flamingo, a visual language model, to answer queries. We will provide an input image with or without the augmentation and a question about the graph into Flamingo, as illustrated in the figure above. Each image will be paired with a question in a specific category. For this analysis, we will focus on four major types of questions to evaluate the VLM’s understanding of graph connectivity. These questions are to be asked in tandem with the original image, the colored image, and the original image paired with the list of nodes in the image. We ask the following questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Based on the image, is there a cycle in the graph?&lt;/li&gt; &lt;li&gt;Based on the image, what is the path from &lt;strong&gt;__ to __&lt;/strong&gt;_? (The ground truth path involves nodes that only have one child node.)&lt;/li&gt; &lt;li&gt;Based on the image, what is the path from &lt;strong&gt;__ to __&lt;/strong&gt;_? (The ground truth path involves nodes that have multiple child nodes.)&lt;/li&gt; &lt;li&gt;Based on the image, how many child/parent nodes does _____ have?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For the same image from above, here are the questions and relevant answers:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Question&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Answer&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Is there a cycle in this graph?&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;What is the organization hierarchy path from Etablissements Lgac Lthalat Incorporated, Indiana to Nm Suc. Krsx Limited - Michigan?&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;The path is Etablissements Lgac Lthalat Incorporated, Indiana to Korte Pro-Cut Grayhawk Insurance, North Carolina to Nm Suc. Krsx Limited - Michigan&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;What is the organization hierarchy path from Finanze Tyco Roadsafe Insurance to Nm Suc. Krsx Limited - Michigan?&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;The path is from Finanze Tyco Roadsafe Insurance to Greyhound Bbbytf Limited to Nm Suc. Krsx Limited - Michigan&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;How many child companies does Greyhound Bbbytf Limited have holdings in?&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Two&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;But, you must be wondering: why ask these questions specifically? Each question tests understanding of graphical elements without background understanding of the topic. This should serve as a baseline for the way that VLMs understand graphical structures and the common questions to be asked.&lt;/p&gt; &lt;h3 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h3&gt; &lt;p&gt;To evaluate the success of our model, we will conduct both qualitative and quantitative analyses on the dataset, given that quantitative evaluation of generative models can be challenging. The control group will provide a baseline for normalizing the results.&lt;/p&gt; &lt;p&gt;Qualitatively, we will perform a manual analysis of the generated outputs. By using prompts, images, and answer, we will subjectively compare the prompt, the image, and the resulting answer. Our primary goal is to assess how effectively the visual language model generates the answer based on the prompt while being constrained by the graph.&lt;/p&gt; &lt;p&gt;Quantitatively, we plan to utilize an accuracy score will be employed to evaluate the percentage of questions that have been answered correctly in each metric. Incorporating these metrics will provide a more comprehensive understanding of the model’s performance and allow for better comparisons with other generative models.&lt;/p&gt; &lt;p&gt;This comparative analysis will facilitate a robust evaluation of the methods employed to represent graphical information within visual language models.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;The following table provides the accuracy metric of the aforementioned experiment:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Question 1&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Question 2&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Question 3&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Question 4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;Original Image&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.7&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;Original Image with Node List&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;Colored Image&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.3&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.1&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We clearly see that the Colored Image performs better than the Original Image in most of the scenarios. Also, we unfortunately see that the node list did not provide gains in performance for the VLM. Overall, the performance is quite limited when asking specific questions about path finding and children nodes. Let’s dive a bit deeper to understand why.&lt;/p&gt; &lt;p&gt;Preface: In the following sections, there are tables with prompts, images, and answers from Bard for various mini explorations.&lt;/p&gt; &lt;h3 id=&quot;superiority-in-extracting-nodes&quot;&gt;Superiority in Extracting Nodes&lt;/h3&gt; &lt;p&gt;When first embraking on this project, I believed that the VLM would struggle to pair the text and nodes together. However, the VLM actually proved to be quite impressive with the extraction of nodes and the names associated with, making the need for a node list not quite neccesary.&lt;/p&gt; &lt;p&gt;Using the input figure from above without any augmentations, I asked the VLM how many child nodes a specific node in the graph had. The VLM outputs the wrong answer, but it actually provides generated code for its reasoning. Here is a portion of the code:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Define the corporate structure as a dictionary corporate_structure = { &quot;Etablissements Lgac Lthalat Incorporated, Indiana&quot;: { &quot;Nm Suc. Krsx Limited - Michigan&quot;: {} }, &quot;Nm Suc. Krsx Limited - Michigan&quot;: {}, &quot;Krsx Incorporated, California&quot;: { &quot;Nm Suc. Krsx Limited - Michigan&quot;: {} }, &quot;Krsx Incorporated, Texas&quot;: {}, &quot;Lthalat Holdings, Delaware&quot;: { &quot;Etablissements Lgac Lthalat Incorporated, Indiana&quot;: {}, &quot;Krsx Incorporated, California&quot;: {}, &quot;Krsx Incorporated, Texas&quot;: {} }, &quot;Greyhound Bbbytf Limited&quot;: { &quot;Krsx Incorporated, California&quot;: {}, &quot;Krsx Incorporated, Texas&quot;: {} } } &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The VLM was actually quite impressive with obtaining the specific nodes in the graph, recognizing the text through OCR, and pairing it with the nodes. While all the nodes are not correct, we can see that the VLM can follow a procedure to determine this. While this is a good example of node extraction, the capability is still ambiguous.&lt;/p&gt; &lt;p&gt;To poke this topic a bit more, I wanted to test out the VLM’s ability to extract the nodes if the colors are the same or different. I designed a basic figure with just nodes to test this. The same prompt was passed into Bard with the images below. The only difference between the two images is the fact that in one image, the colors of the nodes are same, and in the other image, the colors of the nodes are different. In the results below, we can clearly see that the VLM is able to perform better with the colored nodes, as the VLM is able to distinguish between different nodes.&lt;/p&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Original Image &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Colored Image &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Prompt: What are the names of the nodes in this graph? &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Prompt: What are the names of the nodes in this graph? &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Answer: B1, B2, B3, B4, B5. &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Answer: B1, B2, B3, B4. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;&lt;br /&gt; To support this argument, we look at the attention that CLIP places on segments of the image based on a caption. We specifically use CLIP because CLIP is the visual encoder in Flamingo. While this isn’t necessarily a rigorous proof, we can see that the attention on the nodes is placed stronger in the colored graph example rather than the regular graph example.&lt;/p&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Caption: Node B1, B2, B3, B4 &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Caption: Node B1, B2, B3, B4 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Through the examples and tests above, we can clearly see the VLM’s ability to extract nodes, especially with a visually distinugishing factor between the nodes like color. Since the VLM can do a pretty decent job of extracting the nodes, it makes sense that providing the VLM with the node list may not allow for great improvements in performance.&lt;/p&gt; &lt;p&gt;So, if the VLM can extract the nodes relatively well, why is the performance still subpar?&lt;/p&gt; &lt;h3 id=&quot;difficulties-with-edge-dectection&quot;&gt;Difficulties with Edge Dectection&lt;/h3&gt; &lt;p&gt;Aside from nodes, most graphs have edges, and for the questions asked in the experiments, understanding the connectivity was crucial to providing the correct answer. We actually observed that the colored graphs had answers that were closer to 100% accuracy in comparison to the regular graphs. To explore how VLMs understand the connections between nodes, I decided to ask Bard about some simple graphs to determine how it responded.&lt;/p&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Image &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Prompt &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Can you please tell me what the edges in the graph are? &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Can you please tell me what the edges in the graph are? &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Answer &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; The edges in the graph are: a -&amp;gt; b, b -&amp;gt; c, c -&amp;gt; a &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; The edges in the graph are: B -&amp;gt; A, C -&amp;gt; A &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Wow! It’s really surprising that the VLM is creating edges where there aren’t even edges? Also, the direction of the edges are wrong. Why would this be the case?&lt;/p&gt; &lt;p&gt;Let’s explore a bit more about the individual arrows. Can a VLM accurately describe the arrow direction?&lt;/p&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Image &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Prompt &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Which way does the arrow point? &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Which way does the arrow point? &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot; style=&quot;border: 1px solid #000&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; Answer &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; The arrow in the image points to the right. &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot; style=&quot;border: 1px solid #000&quot;&gt; The arrow in the image points to the left. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Well, if you take a look at the responses above, we notice that the VLM can accurately provide the arrow direction for larger arrows incomparison to smaller arrows. Since most of the graphs involve smaller arrows, we can see why the VLM could not tell the directionality of the arrow.&lt;/p&gt; &lt;h3 id=&quot;outside-information&quot;&gt;Outside information&lt;/h3&gt; &lt;p&gt;Beyond the graphical structure of the nodes, I found that Bard typically incorporated outside information that required background knowledge about the topic. These situations can be categorized into two categories: useful outside information and irrelevant outside information. To distinguish between the two, I found that in the siutations that Bard used graph algorithms and definitions, the outside information was useful in generating the answer. However, many times, Bard would look up specific companies or use graphs from other sources online.&lt;/p&gt; &lt;p&gt;In this example, Bard was asked if there was a cycle in the graph, and Bard outputted the answer below. The VLM was very intelligent by quoting the definition of a cycle and using DFS to determine the existence of one. The step-by-step algorithm is correct and ultimately, leads to the solution. This shows that the incorporation of algorithms and accurate definitions can assist the VLM in answering queries.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;No, there is no cycle in the graph of the Nove 1-66Express organizational structure. A cycle is a path in a graph that starts and ends at the same vertex, and there is no such path in this graph. I verified this by tracing the graph and ensuring that I never returned to a vertex that I had already visited. I also used the following algorithm to check for cycles in graphs: Mark all vertices as unvisited. Choose a vertex and start a depth-first search (DFS). As you traverse the graph, mark each vertex as visited. If you ever reach a vertex that is already marked as visited, then there is a cycle in the graph. If you finish traversing the graph without ever reaching a visited vertex, then there is no cycle in the graph. I ran this algorithm on the graph of the Nove 1-66Express organizational structure and found that there are no cycles.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;However, outside information can be just as harmful as it is beneficial. Pulling from outside sources can be tricky for a VLM. It needs to remember to operate within the grouth truths presented in the figure. In multiple instances, the VLM actually referenced the wrong figure because it forgot about the image provided with the prompt.&lt;/p&gt; &lt;p&gt;Therefore, the VLM needs to be fine-tuned to filter the outside information it brings in to improve the accuracy of its answers.&lt;/p&gt; &lt;h2 id=&quot;conclusions-and-future-steps&quot;&gt;Conclusions and Future Steps&lt;/h2&gt; &lt;p&gt;In this study, I delved into the capabilities and limitations of VLMs, like Open Flamingo, in interpreting complex graphical structures within expert domains, such as scientific literature. The experiments, involving various image augmentation techniques such as color changes and node names, revealed key insights. While VLMs demonstrated proficiency in node extraction, it faced challenges in edge detection and understanding the connectivity between nodes. This was particularly evident when colored images outperformed non-colored ones, highlighting the importance of visual distinction for VLM comprehension. However, the addition of node lists did not significantly enhance performance, suggesting existing capabilities in node identification. The connectivity was difficult for the VLM to understand because of the size of the arrows.&lt;/p&gt; &lt;p&gt;The findings of this research highlight a crucial challenge for VLMs: integrating domain-specific knowledge, especially for non-standard images like scientific diagrams. However, due to the small dataset size, suggests that further research with a larger and more diverse dataset is necessary to validate these findings. In the future, this research can be applied to help improve prompting for graphical structures, provide insights on how to finetune a VLM for this task, and create a new interest in using VLMs for scientific diagrams.&lt;/p&gt; </content> </entry> <entry> <title>Embeddings for Spatio-temporal Forecasting</title> <link href="https://deep-learning-mit.github.io/blog/2023/spatiotemporal/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/spatiotemporal</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Time series forecasting is an interdisciplinary field that affects various domains, including finance and healthcare, where autoregressive modeling is used for informed decision-making. While many forecasting techniques focus solely on the temporal or spatial relationships within the input data, we have found that few use both. Our goal was to compare two SOTA spatiotemporal models, the STAEformer and the Spacetimeformer, and determine why one works better than the other. The papers on both models did not feature each other in their benchmark evaluations, and we thought that analyzing their embeddings and identifying their failure modes could offer new insights on what exactly the models are learning from the dataset. We hypothesized that the Spacetimeformer would perform better as its proposed approach, sequence flattening with Transformer-based processing, seems to offer a more flexible and dynamic representation of spatiotemporal relationships that doesn’t depend on predefined variable graphs. We focused on forecasting in the field of traffic congestion, which is a pervasive challenge in urban areas.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;We focused on two SOTA spatiotemporal models that were evaluated on traffic forecasting datasets. The first is the STAEformer &lt;d-cite key=&quot;liu2023staeformer&quot;&gt;&lt;/d-cite&gt;. STAEformer proposes a novel adaptive embedding that learns the spatio-temporal relations in the dataset. In their architecture, the input embedding is generated by combining the projected raw data (denoted by \(E_p\) in the embedding layer) with temporal embeddings (\(E_f\)) and the adaptive embeddings (\(E_a\)), which was used instead of an embedding solely focused on capturing spatial relations. This output is then fed into temporal and spatial transformer layers, followed by a regression layer.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/staeformer_architecture.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Architecture of the Spatio-Temporal Adaptive Embedding transformer (STAEformer). &lt;d-cite key=&quot;liu2023staeformer&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;The second is the Spacetimeformer &lt;d-cite key=&quot;grigsby2023spacetimeformer&quot;&gt;&lt;/d-cite&gt;. Spacetimeformer uses embeddings generated from breaking down standard embeddings into elongated spatiotemporal sequences. The idea behind doing this is to enable the downstream tasks to learn direct relationships between variables at every timestep. In their architecture, these embeddings are fed into a variant of the transformer model using local, global, and cross self-attention. The figure below shows an intuitive visualization for this idea.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/spacetimeformer_architecture.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Architecture of the Spacetimeformer Embedding. &lt;d-cite key=&quot;grigsby2023spacetimeformer&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt; &lt;p&gt;We used the PEMS08 dataset &lt;d-cite key=&quot;pems08&quot;&gt;&lt;/d-cite&gt;, which contains traffic data in San Bernardino from July to August of 2016. Each data point consists of readings from 170 detectors along with the time of day and day of the week they were recorded. We initially considered using the PEMSBAY dataset &lt;d-cite key=&quot;pemsbay&quot;&gt;&lt;/d-cite&gt;, which is widely used in traffic speed forecasting, but it was almost double the size of the PEMS08 dataset and took too long to train our model on.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;p&gt;The problem statement is as follows: given the sensor readings across the 170 sensors for the previous N timesteps, we want to predict their readings for the next N timesteps. We tested the model with varying context lengths, but we found that the default value of 12 given in the STAEformer paper provided enough information to the model. We used huber loss as we wanted the model to converge faster in the presence of outliers, which was necessary given the limited compute that we had (training 50 epochs took around 3 hours).&lt;/p&gt; &lt;p&gt;We trained STAEformer for 50 epochs, which was sufficient to achieve performance metrics similar to that of the paper. To compare the embeddings from Spacetimeformer, we retrained the model end to end after replacing the embedding layer in the model with Spacetimeformer’s embedding layer. To do this, we kept the context dimensions the same and flattened the input sequence along the input dimension and the dimension corresponding to the number of sensors. This structured the embedding layer so that it could learn the spatiotemporal relations across the sensors from different time frames.&lt;/p&gt; &lt;p&gt;Replacing the embedding layer within the STAEformer with a pretrained embedding layer from the Spacetimeformer instead may seem like a more legitimate method to test the effectiveness of the embeddings, as we would basically be doing transfer learning on the embedding layer. However, the pretrained embeddings from Spacetimeformer might have been optimized to capture specific spatiotemporal patterns unique to its architecture, which was why we believe training the model end to end with the Spacetimeformer embeddings would result in a more accurate and contextually relevant integration of the pretrained embeddings into the STAEformer framework.&lt;/p&gt; &lt;p&gt;After training, we wanted to provide visualizations of the embeddings from STAEformer and Spacetimeformer to show whether the learned embeddings are meaningful at all. To do this, we obtained the embeddings by passing in the raw data through the embedding layers of the loaded models and generated t-SNE plots with them. For the STAEformer, we focused solely on the adaptive embeddings as they were the parts of the embedding layer that captured spatiotemporal relations in the data. To generate the t-SNE plots, we had to reshape the embeddings so that they could be passed into the function so we flattened them across the model dimension. After fitting the t-SNE, we then unflattened the embeddings back to their original shape and plotted them. Each sensor was color coded with different colors, and the results can be shown in the next section. We hypothesized that the t-SNE plots would contain clusters grouped by either the sensors or the time the readings were recorded.&lt;/p&gt; &lt;p&gt;After generating the t-SNE plots, we wanted to test the effects of perturbing the raw data on the embeddings. We wanted to know how the embeddings would change. For example, regardless of what the clusters represent, are they tighter? Will additional clusters be formed? Conversely, will some of the existing clusters break apart? In particular, we were hoping that augmenting the data would perhaps improve cluster formations in the worse looking embeddings, as there is a good possibility that the data itself isn’t good enough.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;The table below shows the results after training STAEformer and the STAEformer model with a Spacetimeformer embedding layer for 50 epochs each. Table of loss values:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Embedding Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Train Loss&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Validation Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;STAEformer&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;12.21681&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;13.22100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;Spacetimeformer&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;12.42218&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;16.85528&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We can see that the STAEformer had better training and validation loss than the Spacetimeformer. While the train loss converged to similar values, the validation loss for the model using the STAEformer embedding layer was much better. So now that we know the STAEformer embedding layer seems to perform better than the Spacetimeformer embedding layer, we plotted the embeddings for both to analyze why this is the case. To do this, we passed a data point from the validation set through the embedding layer. The results are shown in the figure below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the STAEformer embeddings. &lt;/div&gt; &lt;p&gt;The t-SNE plot for the STAEformer embeddings shows clearly separate clusters for most of the 170 different sensors. The shape of each cluster is a “snake-like” trajectory. Therefore, we know that the embeddings preserve some pattern-like notion across readings from a single sensor. We hypothesize that each of these trajectories represent the reading of a single sensor over time. There are a couple outliers, where the clusters are not grouped by color. One prominent example is the string of cyan, maroon, and moss points along the bottom of the plot. However, even these points have some clustering, though they may not be clustered by color.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the Spacetimeformer embeddings. &lt;/div&gt; &lt;p&gt;On the other hand, the t-SNE plot for the Spacetimeformer embeddings show no clear clusters across the same sensor. The distribution resembles a normal distribution, meaning that there is little pattern preserved in the embeddings. It becomes more difficult to differentiate between data points from the same sensor across time.&lt;/p&gt; &lt;p&gt;In order to further analyze the effectiveness of each embedding layer, we perturbed the training data and re-trained each model. We were expecting the clusters from the STAEformer embeddings to remain largely the same, with some of the existing clusters possibly breaking apart due to the added noise. However, we were hoping that the Spacetimeformer embeddings would show more visible clusters after the raw data was perturbed. Given the characteristics of the embeddings, one possible output we expected were clusters containing multiple colors. An example is shown in the following image.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/ideal_tsne.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Ideal t-SNE plot of the Spacetimer embeddings. &lt;/div&gt; &lt;p&gt;This would show that the Spacetimeformer successfully learned spatial relationships across the sensors at variable timesteps. Instead of each cluster representing the embeddings for one sensor, the presence of larger clusters with multiple colors could imply that the Spacetimeformer learned spatiotemporal relations among the corresponding sensors and embedded them into a larger cluster.&lt;/p&gt; &lt;p&gt;The following table shows the results after training the model with the perturbed data.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;Embedding Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Train Loss&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Validation Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;STAEformer (with perturbations)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;13.58251&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;13.35917&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;Spacetimeformer (with perturbations)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;13.42251&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;17.01614&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;As expected, validation loss slightly increased for both models, and the STAEformer continued to have lower loss values than the model with the Spacetimeformer embedding layer.&lt;/p&gt; &lt;p&gt;When we generated the t-SNEplots with the new embeddings, we obtained the following:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_perturbed.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the perturbed STAEformer embeddings. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_perturbed.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the perturbed Spacetimeformer embeddings. &lt;/div&gt; &lt;p&gt;Both t-SNE plots for the STAEformer and Spacetimeformer embeddings look the same as when the models were trained on the original, unperturbed data. So unfortunately, the augmentation had little to no effect on the embedding layers for these two models.&lt;/p&gt; &lt;p&gt;Since the t-SNE plots can be hard to parse with the human eye, we decided to focus on the embeddings for the most relevant features of the dataset and see how they compared between the Spacetimeformer and STAEformer. In parallel, this would enable us to identify the failure modes of the dataset and augment those features to see if they improve the model performance. In order to do this, we used PCA to identify the principal components. From there, we found which features help explain the most variance in the dataset and identified those as the features that had the largest impact on the learned embeddings.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_heatmap.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Z-normalized correlation matrix between the original PEMS08 dataset and PC-space, normalized by explained variance. &lt;d-cite key=&quot;pca&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;This heatmap shows the top 10 principal components and and the top 10 features that correlate with each principal component. From this heatmap, we can see that the 9th sensor in the dataset is the most relevant feature. Therefore, we can find the corresponding embedding to be the most relevant.&lt;/p&gt; &lt;p&gt;Using only the 5 most relevant embeddings obtained from PCA, we re-graphed the t-SNE plots. This helped us to narrow our attention to the most important embeddings.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/STAEformer_top5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the top 5 STAEformer embeddings. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/Spacetimeformer_top5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the top 5 Spacetimeformer embeddings. &lt;/div&gt; &lt;p&gt;As expected, the embeddings for the most relevant sensors in the STAEformer all maintain the “snake-like” trajectory. However, the embeddings for even the most relevant sensors in the Spacetimeformer are seemingly random, and have no pattern across the points.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-spatiotemporal/pca_cumulative.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Cumulative PCA plot of the original PEMS08 dataset. &lt;d-cite key=&quot;pca&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;We found that the top 25 sensors explained 95% of the variance in the dataset, so we did a quick experiment where we augmented the rest of the 145 sensors (as opposed to the entire training dataset) to see how that affected the learned embeddings. For this augmentation, we expected the results to not improve by much since the learned embeddings for even the most relevant sensors in Spacetimeformer didn’t form visible clusters in the t-SNE plots. As expected, the results were almost identical to the ones generated from augmenting the entire dataset.&lt;/p&gt; &lt;h2 id=&quot;conclusion-discussion-next-steps&quot;&gt;Conclusion, Discussion, Next Steps&lt;/h2&gt; &lt;p&gt;There are a couple of reasons why we think the Spacetimeformer performed worse than the STAEformer overall. The first explanation that came to mind is that the readings across different sensors may be mostly independent from one another. The color coded t-SNE plots for the STAEformer clearly separate each sensor into its individual cluster. In this case, the Spacetimeformer would not be suited for the task as its embedding layer solely focuses on learning spatiotemporal relationships, while the STAEformer also contains an embedding layer that is solely dedicated to learning temporal relationships.&lt;/p&gt; &lt;p&gt;A second, more plausible explanation deals with the embedding architecture. The difference in performance between the STAEformer and the Spacetimeformer in time series forecasting shows the importance of adaptive embeddings in capturing spatio-temporal relationships. While the STAEformer introduces adaptive embeddings to comprehend the patterns in the data, the Spacetimeformer relies on breaking down standard embeddings into elongated spatiotemporal sequences. The t-SNE plots show that the STAEformer’s adaptive embeddings generate clusters representing sensors with snake-like trajectories, providing a visualization of the model’s ability to capture spatio-temporal patterns. In contrast, the Spacetimeformer’s embeddings follow a scattered distribution, indicating challenges in identifying clusters. This suggests that the Spacetimeformer’s approach may face limitations in effectively learning the spatio-temporal relationships within the PEMS08 dataset, and potentially traffic data in general.&lt;/p&gt; &lt;p&gt;Having said all this, the resilience of both the STAEformer and Spacetimeformer to perturbations in the raw data showcases the robustness of their learned representations. Despite the added augmentations, the fact that the t-SNE plots remain largely unchanged indicates the stability in the embedding layers. This may be attributed to the models’ ability to learn a generalizable representation of the spatio-temporal patterns resilient to changes in the input data, regardless of how accurate they may be. This may also be attributed due to the dataset itself. The PEMS08 dataset’s readings may already have been noisy, as it’s unlikely that the readings were recorded with perfect accuracy. We would like to explore these implications of the embeddings’ robustness in our future work.&lt;/p&gt; &lt;p&gt;Another possible avenue we would like to explore is why certain sensors (such as the 9th sensor) are more relevant than others beyond just the theory. We came up with a couple hypotheses. First, it’s probable that this particular sensor is placed at important intersections, such that cars that pass this sensor are guaranteed to pass many other sensors. This would mean that there exists a way to extrapolate the readings from this sensor to the readings from other sensors. Tangentially related, it’s possible that two nodes are negatively correlated, such that the cars that pass through one node tend to not pass through another node, and the model extracts readings based on this relationship. If neither of these ideas is the case, the exact opposite concept could be true: the sensor is at a location where the speed data is very consistent, such as a highway. This allows the readings from the sensor to give very accurate predictions. The next step would be to figure out the geographical locations of the sensors and determine whether the ones we found to be the most relevant seem to be placed at important locations.&lt;/p&gt; &lt;p&gt;We would also like to do some more experimentation in the future. We used a personal GPU for training (an RTX 2070), and it took a few hours to train the model for every one of our experiments which made it difficult to tune our hyperparameters. Further experiments we would like to run with more compute include running the experiments on the Spacetimeformer model architecture instead of the STAEformer architecture and replacing its embedding layer with STAEformer’s. We mentioned before that the learned embeddings may have been optimized for the model architecture it’s from. Therefore, if the resulting plots from the embeddings look similar to the ones we have generated, then we have conclusive evidence that the STAEformer input embedding does a better job of learning the spatio-temporal relations in the data.&lt;/p&gt; </content> </entry> <entry> <title>In the pursuit of cheap and robust word embeddings</title> <link href="https://deep-learning-mit.github.io/blog/2023/sentence-embeddings/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/sentence-embeddings</id> <content type="html">&lt;h2 id=&quot;introduction-and-motivation&quot;&gt;Introduction and Motivation&lt;/h2&gt; &lt;p&gt;Large Language Models (LLMs), such as Bard and OpenAI’s GPT-4 are typically used to obtain data embeddings of text. These embeddings are quite rich, encoding common-sense semantic information. A good embedding naturally aligns with our intuitive human understanding of language: at a high level, similar text/words are clustered together, while dissimilar text/words are farther apart.&lt;/p&gt; &lt;p&gt;High-quality embeddings also satisfy semantic equations that represent simple analogies. Define \((\text{some_text})\) to be the embedding of some string “some_text.” Then, a traditionally good embedding will typically obey linguistic equations like &lt;d-cite key=&quot;Vylomova2016&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; \[(\text{King}) - (\text{Man}) + (\text{Woman}) ≈ (\text{Queen})\] \[(\text{Paris}) - (\text{France}) = (\text{Warsaw}) - (\text{Poland})\] &lt;p&gt;However, repeatedly querying LLMs for large-scale analysis is expensive. Many utilize thousands of cloud GPUs and are constantly fine-tuned, adding to their cost. This cost barrier discourages researchers—especially those with less funding—from making use of these embeddings for their own models. Repeated strain on LLM’s infrastructure can even cause a negative environmental impact. However, we often don’t need embeddings as good as these fancy ones to conduct certain types of research. Specifically, it would be desirable for a researcher to choose their embedding quality, with the understanding that higher-quality embeddings take longer, and vice versa. Such a model should be robust and resistant to being trained on a small amount of incorrect data (which can happen by accident when scraping tex, or due to malicious behavior.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;These issues motivate the following research question: on how little data can we train a text embedding model—with OpenAI embedding as ground truth—such that our embeddings are good enough quality? And can we quickly preprocess the data to improve our results?&lt;/strong&gt;&lt;/p&gt; &lt;h2 id=&quot;background-and-literature-review&quot;&gt;Background and Literature Review&lt;/h2&gt; &lt;p&gt;While there is some existing literature on generating word embeddings more “cheaply,” significant differences exist with current methodologies. Broadly, this process is called knowledge distillation (KD), which aims to “distill” knowledge from a larger teacher model (in our case, OpenAI embeddings) into a smaller student model.&lt;/p&gt; &lt;p&gt;For example, Shin et al. discuss a novel distillation technique that “distills” a “student” embedding model from a “teacher” model &lt;d-cite key=&quot;Shin2019&quot;&gt;&lt;/d-cite&gt;. Importantly, this work focuses on reducing the dimensionality of the “student” word embeddings without compromising accuracy—which is fundamentally different from our goals. While our OpenAI embeddings form the “teacher,” our student model should output embeddings of the same dimensionality. Our dimensionality preservation is useful if we want to directly translate general patterns or trends from our student’s embeddings to the OpenAI embeddings.&lt;/p&gt; &lt;p&gt;Gao et al. take a different approach. They propose a KD framework for contrastive sentence embeddings, DistilCSE. It works by first applying KD on a large amount of unlabeled text before fine-tuning the student model via contrastive learning on limited labeled data &lt;d-cite key=&quot;gao2023distilcse&quot;&gt;&lt;/d-cite&gt;. Contrastive learning in this domain is promising, especially since synonyms and pairs of similar words naturally form positive pairs. However, in our context, a direct application of contrastive learning presents some issues.&lt;/p&gt; &lt;p&gt;For example, suppose we had some Euclidean distance threshold A and B, such that, for any two word embeddings \(c\) and \(d\):&lt;/p&gt; &lt;p&gt;If the distance between \(c\) and \(d\) is less than A, then define \(c\) and \(d\) to be positive pairs for contrastive learning.&lt;/p&gt; &lt;p&gt;If the distance between \(c\) and \(d\) is greater than B, then define \(c\) and \(d\) to be negative pairs for contrastive learning.&lt;/p&gt; &lt;p&gt;While this process (and others like it) isn’t too resource-intensive, it has a few issues, even if we are able to define proper thresholds A and B. Firstly, it “wastes” pairs of data where the distance is in between A and B. Secondly, information about direction is easy to lose—so while a student would learn to embed similar words closer together and dissimilar ones further apart, the student may be invariant to direction and sensitive only to Euclidean distance in the n-dimensional space. This is not ideal.&lt;/p&gt; &lt;p&gt;Other related state-of-the-art approaches also present issues. Gao et al. describe another approach involving running data through an encoder multiple times with standard dropout to generate positive pairs instead of searching for them in the data itself &lt;d-cite key=&quot;Gao2022&quot;&gt;&lt;/d-cite&gt;. While this approach is promising, the approach involves repeatedly using somewhat slow pre-trained text embedders, which is exactly what we want to avoid.&lt;/p&gt; &lt;p&gt;To reduce computational complexity while still reaping the benefits of preprocessing, we look to a paper by Rahimi et al. They explain how removing stop words (common words, like “a,” “the,” etc.) and punctuation improves sentence embedding quality, for a variety of reasons &lt;d-cite key=&quot;Rahimi2023&quot;&gt;&lt;/d-cite&gt;. Even though we’re focusing on word embeddings—and not sentence embeddings—we can adapt this general approach to our project with lemmatization, which applies morphological analysis to words to simplify them to a base form. For example, “fighting” turns into “fight,” “was” turns into “be,” and “octopi” turns into octopus, and so on.&lt;/p&gt; &lt;p&gt;This reduces data sparsity by consolidating related forms of words into a single representation, which is especially helpful for low-frequency words. This in turn helps the model generalize across tenses and other variations as it can focus on the “core” differences of words rather than auxiliary modifiers. We thus plan to investigate lemmatization in this context.&lt;/p&gt; &lt;p&gt;We struggle to find closely related literature about student models’ resistance to poisoned data. Thus, we decided to investigate this aspect as well.&lt;/p&gt; &lt;p&gt;To conclude our literature review, while different variants of KD exist, we decide to focus on a modified response-based KD, in which the teacher model sends final predictions to the student network, which then directly mimics these predictions by minimizing some loss &lt;d-cite key=&quot;Yang2023&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/response_based_KD.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Generic response-based knowledge distillation. Image credits: Gou, J., Yu, B., Maybank, S.J. et al. Knowledge Distillation: A Survey. Int J Comput Vis 129, 1789–1819 (2021). https://doi.org/10.1007/s11263-021-01453-z &lt;/div&gt; &lt;p&gt;Other distillation approaches—such as feature-based KD, relation-based KD, and the contrastive approach described above—do exist, but require more intimate knowledge of the teacher’s features and/or layers &lt;d-cite key=&quot;yang2023attention&quot;&gt; &lt;/d-cite&gt; &lt;d-cite key=&quot;Park2019&quot;&gt;&lt;/d-cite&gt;. This is not reasonable information we can expect to have, as companies are often incentivized to obfuscate their specific word embedding architectures.&lt;/p&gt; &lt;h2 id=&quot;methods-and-experiments&quot;&gt;Methods and Experiments&lt;/h2&gt; &lt;p&gt;We center our studies on a standard dataset of 10k English words scraped from high-level Standard English texts that’s been empirically validated for quality. We also use the OpenAI API to obtain text-embedding-ada-002 embeddings of the entire dataset to use as ground truth. While these aren’t necessarily the best embeddings, even among OpenAI’s own embeddings, they are the best choice given our computational restrictions.&lt;/p&gt; &lt;p&gt;Now, we detail our model architecture. Our baseline model (call this Model A) is a sequential ReLU and nn.Embedding layer followed by L2 normalization. Model A serves as a crude baseline—therefore, we do not investigate it as deeply as the more complex model that followed due to large differences in performance.&lt;/p&gt; &lt;p&gt;Instead, we focus our efforts on the more complex Model B, detailed below in Figure 1 in the context of our pipeline. Model B utilizes an nn.Embedding layer, followed sequentially by 2 blocks. The first uses ReLU activation followed by a linear layer of size \(\frac{\text{embedding_dim}}{2}\). The second layer is the same, except the final Linear layer outputs embeddings with the full “embedding_dim.” Notably, we use L2 normalization to make sure each embedding vector has magnitude 1 (such that all embeddings exist in an n-hypersphere.) Since all embeddings are unit embeddings, using cosine embedding loss along an Adam optimizer is natural. Thus, instead of computing cosine similarities between teacher and student vectors, we can just focus on minimizing this embedding loss.&lt;/p&gt; &lt;p&gt;For the training stage, we train our embedding model to map words to vector embeddings on Google Colab with an Nvidia T4 GPU. There may be up to 3 processing steps, as depicted in Figure 1:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/architecture_diagram.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 1: An overview of Model B’s architecture in context of our experimentation. &lt;/div&gt; &lt;p&gt;First, we choose whether or not to lemmatize the entire dataset before proceeding.&lt;/p&gt; &lt;p&gt;Second, the training split. We train our embedding models above on each of the following proportions (call this \(p\)) of the dataset: 0.005, 0.009, 0.016, 0.029, 0.053, 0.095, 0.171, 0.308, 0.555, and 1.00.&lt;/p&gt; &lt;p&gt;Finally, we choose whether or not to poison 10 percent of the entire word dataset (not the training dataset). When a word is poisoned, the model incorrectly believes that some random unit vector is the ground-truth embedding instead of the actual OpenAI embedding.&lt;/p&gt; &lt;p&gt;For each such model, we train for up to 80 epochs, limited by our computational resources.&lt;/p&gt; &lt;p&gt;We then evaluate the model’s embeddings against the ground truth with multiple metrics—cosine similarity (via the embedded cosine loss), graphically via distributions of the embedding means, linguistic math, etc.&lt;/p&gt; &lt;p&gt;Taken together, this methodology is comprehensive.&lt;/p&gt; &lt;h2 id=&quot;results-and-analysis&quot;&gt;Results and Analysis&lt;/h2&gt; &lt;h3 id=&quot;model-a-the-baseline&quot;&gt;Model A, the Baseline&lt;/h3&gt; &lt;p&gt;First, here is a graph of training up our baseline Model A (Figure 2) and our augmented Model B (Figure 3). The difference in epochs (80 for Model A, and 60 for Model B) training is due to limited resources. This doesn’t matter much, as a clear, near-linear relationship between \(p\) and training time, which we use to estimate used computational resources. Thus, we consider \(p\) as inversely proportional to the computational resources used for all our experiments.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_time.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 2 &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_time.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 3. &lt;/div&gt; &lt;p&gt;For Model A (with no lemmatization, no data poisoning), we also want to visualize the tradeoffs between the number of epochs trained, the training proportion \(p\), and the training loss to establish some baseline intuition. To this end, we take inspiration from the game theoretic concept of Pareto efficiency, which aims to find equilibria where no change improves one of these 3 factors without hurting one of the other 2.&lt;/p&gt; &lt;p&gt;We also wanted to visualize the tradeoffs between the number of epochs trained, the training proportion, and the cosine embedding loss, since we are motivated to find the optimal balance of these 3 factors. See Fig. 4.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_pareto.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 4. &lt;/div&gt; &lt;p&gt;Unfortunately, Fig. 4 is not particularly enlightening. Training loss decreases as the number of epochs increases and as training proportion \(p\) increases. There are also no local minima or maxima of interest. Figures 5 and 6 also confirm this with their plots of distributions of embedding means. Specifically, as we tend to move towards the right and bottom of Fig. 6, i.e. we train longer and on more data, we simply seem to approach the true distribution (Fig. 5) without anything of note.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_full_means.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 5. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/bad_model_all_means.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 6. &lt;/div&gt; &lt;p&gt;These results motivate us to look beyond our Model A. Our results from this point focus on Model B because we didn’t want a poorly performing model like Model A to be a true control, it merely served as an intuitive baseline.&lt;/p&gt; &lt;h3 id=&quot;model-b-the-baseline&quot;&gt;Model B, the Baseline&lt;/h3&gt; &lt;p&gt;As in the previous part, we obtain a Pareto-like graph for Model B, without any lemmatization and data poisoning. Firstly, the cosine embedding losses are much lower than before, due to the improved model architecture. More interestingly, after about 10 iterations, the training loss seems to stabilize across all versions of the model, potentially suggesting that training longer may not be worthwhile.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_nolemma_nopoison_pareto.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 7. &lt;/div&gt; &lt;p&gt;Since this is our base model, we don’t investigate further.&lt;/p&gt; &lt;h3 id=&quot;model-b-lemmatization-no-poisoned-data&quot;&gt;Model B, Lemmatization, No Poisoned Data&lt;/h3&gt; &lt;p&gt;Now, we look to Model B, with lemmatization, but no poisoned data. The Pareto-like curve for this is telling (Fig. 8), with it looking very similar to the baseline Model B’s. As before, this suggests that training for longer may not be worthwhile, and could potentially lead to overfitting.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_pareto.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 8. &lt;/div&gt; &lt;p&gt;We also have a distribution of the means of embeddings for the whole dataset (Fig. 9) and from each variant of the model at different epochs (Fig. 10). Again, the results don’t say anything surprising: as we train on more data for longer, the distribution approaches that of the training dataset.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_full_means.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 9. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_nopoison_all_means.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 10. &lt;/div&gt; &lt;p&gt;To check for overfitting, we will later validate our model on simple linguistic tests, as described in the very beginning. Specifically, we will validate our model’s performance on linguistic math against OpenAI’s performance.&lt;/p&gt; &lt;h3 id=&quot;model-b-lemmatization-poisoned-data&quot;&gt;Model B, Lemmatization, Poisoned Data&lt;/h3&gt; &lt;p&gt;The following is the Pareto-like curve, except now we poison 10 percent of the entire dataset, as described in Methods/Experiments. Curiously, we find a local minima at approximately \(p = 0.1\) and ~20 epochs, demonstrating that our overall approach of training on a small fraction of the dataset naturally resists moderate-scale adversarial attacks on our ground-truth embeddings. Of course, the addition of poisoned data means that the loss values are on average higher than those in the previous subsection, where there was no poisoned data.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_pareto.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 11. &lt;/div&gt; &lt;p&gt;Again, looking at the distribution of the means of embeddings (see below), we see that models that trained on too much of the data are completely ruined. We don’t even need to compare these distributions against the whole-model distribution to see this. This result demonstrates that even a relatively small amount of poisoned data can manipulate a naive embedding model trained on an entire dataset.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_model_yeslemma_yespoison_all_means.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 12. &lt;/div&gt; &lt;h3 id=&quot;the-effects-of-data-poisoning-and-surprising-robustness&quot;&gt;The Effects of Data Poisoning and Surprising Robustness&lt;/h3&gt; &lt;p&gt;As discussed previously, we want to externally validate our models with both linguistic equations and pairs of synonyms. Essentially, we want to check that our student groups together similar words like the OpenAI teacher. Since our poisoned model performed best with \(p = 0.095,\) we use this training proportion to compare Model B with lemmatization, but no poisoned data to Model B with lemmatization and poisoned data.&lt;/p&gt; &lt;p&gt;For clarity’s sake, we focus on single a representative example of our validation results in this blog. Specifically, we look into “nonviolent” and “antiaggressive,” which intuitively should exist close together in the n-dimensional unit hypersphere. Using dimensionality reduction techniques to visualize this in 2D, we obtain the following:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-sentence-embeddings/good_models_yeslemma_yesnopoison_linguisticmath.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Fig. 13. &lt;/div&gt; &lt;p&gt;The poisoned model is surprisingly performant, performing decently against both the unpoisoned model and the OpenAI model. These results support our notion that student models that train on as little of the data as possible are somewhat resistant to uniform, random adversarial data poisoning. This empirical result is encouraging, especially since our data poisoning threshold was somewhat high.&lt;/p&gt; &lt;h2 id=&quot;conclusion-discussions-and-future-directions&quot;&gt;Conclusion, Discussions, and Future Directions&lt;/h2&gt; &lt;p&gt;On balance, our results help us answer our question about how to best mimic OpenAI’s word embeddings without excessive API calls. We utilize a spin-off of a response-based KD architecture to train our student model under different conditions, demonstrating both that certain preprocessing (lemmatization) improves our embedding model and that training on smaller amounts of data creates more robust models that resist adversarial data. Our initial results demonstrate promise and serve as a call to action for others to research other cheap, robust word embedding models.&lt;/p&gt; &lt;p&gt;To be clear, there are certainly many limitations to our study. For one, we keep our modeling architecture simpler due to our limited compute, while a real model would certainly use a different architecture altogether. Our dataset was also on the smaller side and doesn’t fully represent the English language. Also, our implicit use of time as a proxy for computation (especially on the erratic Google Colab) is imperfect. Also, preprocessing (including, but not limited to, lemmatization) may require substantial computational resources in some cases, which we don’t account for.&lt;/p&gt; &lt;p&gt;Additionally, many of the constants that we chose (such as the 10 percent data poisoning threshold, the proportions of data we trained on, etc.) are arbitrarily chosen due to limited compute. This could’ve caused unexpected issues. For example, the output dimension of embedding Model B, 1536, is more than 10 percent the size of the dataset (10k). Thus, due to our relative lack of data, our trials with data poisoning can encourage non-generalizable memorization, which is not ideal.&lt;/p&gt; &lt;p&gt;Future directions would include exploring other types of preprocessing, as hinted at in our literature review. We could also look into different types of adversaries—perhaps smarter ones that actively feed information that they know to be detrimental to the model, instead of some random unit vector. While we didn’t have robust supercomputer access, we’d also love to be able to test out fancier embedding architectures.&lt;/p&gt; &lt;p&gt;Finally, we’d like to thank the 6.S898 faculty and TAs for their support!&lt;/p&gt; </content> </entry> <entry> <title>Leveraging Representation Engineering For LLM’s In-Context-Learning</title> <link href="https://deep-learning-mit.github.io/blog/2023/representationengineering-incontextlearning/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/representationengineering-incontextlearning</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;Emerging capabilities in deep neural networks are not well understood, one of which is the concept of “in-context learning” (ICL), a phenomenon where the a Large Language Model (LLM)’s understanding of the prompt and ability to answer accordingly drastically increases after being shown some examples that answer the question. Evaluating in-context learning and understanding why the behavior happens is both an interesting theoretical research question and a practical question that informs directions to conduct research that further advances LLM capabilities by, say, exploiting more of in-context learning.&lt;/p&gt; &lt;p&gt;We attempt to explore the phenomenon of in-context learning by leveraging another exciting field of work on mechanistic interpretability where researchers set out to understand model behaviors by interpreting and editing internal weights in models. One such work that we base on is Representation Engineering by Zou et al. (2023)&lt;d-cite key=&quot;zou2023representation&quot;&gt;&lt;/d-cite&gt; , where they construct a set of training text stimuli to probe LLM activations and use such stimuli to identify a direction that accurately predicts the underlying concept based on the neural activations of the model. This approach allows us to elicit readings of representation and control such representation.&lt;/p&gt; &lt;p&gt;We propose to use methods in Zou et al. (2023) &lt;d-cite key=&quot;zou2023representation&quot;&gt;&lt;/d-cite&gt; to evaluate in-context learning. There has not been previous attempts to examine the model internals holistically in a LLM while it is performing in-context learning. We expose such neural activations by constructing stimulus through artificial examples of in-context learning on binary classication tasks. We find a reading vector that shows high neural activity after the model is stimulated with the context pairs; such a “Context Vector” indicates the context the models draws from. While we hoped to find certain universal mechanisms across different datasets, we find that the Context Vector is dataset-specific and confirm previous hypotheses that in-context learning retrieves information from different parts of the model’s latent space.&lt;/p&gt; &lt;p&gt;We then explore the results of controlling the activations along the “Context Vector” direction, in the hope that editing the activitions would further boost the performance on top of in-context learning. We compare the model outputs on the classification datasets in a zero-shot setting and a setting of natural in-context learning, with the “Context Vector” amplified, and suppressed. While we find boosting performance through such editing to be challenging and sometimes finicky to tune, we find the results to be promising on editing weights to suppress the context that the model draws from and drastically reducing the performance.&lt;/p&gt; &lt;h1 id=&quot;background--related-work&quot;&gt;Background &amp;amp; Related Work&lt;/h1&gt; &lt;h3 id=&quot;in-context-learning-icl&quot;&gt;In-Context Learning (ICL)&lt;/h3&gt; &lt;p&gt;An LLM is frequently aseked to perform a task in inference time that many realized providing some examples of how to answer the task can drastically improve the model’s performance. This phenomenon is called in-context learning. For example, Zhou et al. (2022) &lt;d-cite key=&quot;zhou2022teaching&quot;&gt;&lt;/d-cite&gt; evaluates how LLM can become better at solving algorithmic problems through in-context learning, a task that LLM traditionally struggles at.&lt;/p&gt; &lt;p&gt;In other scenarios, the LLM does not need to rely on prompts at all and can deduce the pattern from the few-shot examples alone to predict the answer. While there is no universal definition of in-context learning and its meaning has shifted over time, we define it as the performance boost to answer questions based on a limited amount of examples (as the context).&lt;/p&gt; &lt;p&gt;Interesting, Min et al. (2022) &lt;d-cite key=&quot;min2022rethinking&quot;&gt;&lt;/d-cite&gt; observes that such ICL phenonemon is observed as long as examples are given, and a mismatch between input and output pairs would not hinder the ability of models performing ICL and thus its performance on the tasks. Wei et al. (2023) &lt;d-cite key=&quot;wei2023larger&quot;&gt;&lt;/d-cite&gt; further corrobates this work by finding on small models but show that as models scale, the ability to pick up on flipped patterns when given in-context examples with flipped labels and override semantic priors is stronger.&lt;/p&gt; &lt;h3 id=&quot;theories-on-why-icl-happens&quot;&gt;Theories on why ICL happens&lt;/h3&gt; &lt;p&gt;While the concept of ICL is well studied, the underlying mechanism of ICL is not well understood. Xie et al. (2022) &lt;d-cite key=&quot;xie2022explanation&quot;&gt;&lt;/d-cite&gt; explains the phenomenon of ICL as an Implicit Bayesian Inference, where the in-context learning prompt serves as a stimulus for the model to go “locate” corresponding concept stored in the model’s latent space that the LM has learned implicitly during pre-training. They study this by generating a simple pretraining distribution that parameterizes the transition of a Hidden Markov Model (HMM) and another prompting distribution. In this setting, the authors reduce the ICL task to Bayesian inference to map the prompting distribution to the pretraining distribution.&lt;/p&gt; &lt;p&gt;Akyürek et al. (2022) &lt;d-cite key=&quot;akyürek2023learning&quot;&gt;&lt;/d-cite&gt; further explains that Transformer-based in-context learners implement standard learning algorithms implicitly by encoding smaller models modularized to perform each specific tasks and update them based on the new in-context exampless. von Oswald et al. (2023) &lt;d-cite key=&quot;vonoswald2023transformers&quot;&gt;&lt;/d-cite&gt;claims that Transformer-based in-context learners is similar to gradient-based meta-learning formulations where they found that the Transformer can learn smaller models of a certain concept by gradient descent in their forward pass.&lt;/p&gt; &lt;p&gt;Furthermore, Olsson et al. (2022) &lt;d-cite key=&quot;olsson2022context&quot;&gt;&lt;/d-cite&gt; draws parallel from ICL to a more understood phenomenon of Induction Head, where attention-only Transformers picks up on the algorithm to predict next tokens by searching for a previous occurance of the last token and copying the same next token from previous occurences. They claim that this can be a potential mechanism to explain ICL.&lt;/p&gt; &lt;p&gt;While many hypotheses and theories have been proposed to explain ICL, most explorations to prove their theory has been small in scale, and the literature lacks a study on the large-scale LMs’ internals when performing ICL.&lt;/p&gt; &lt;h3 id=&quot;model-editing--representation-engineering&quot;&gt;Model Editing &amp;amp; Representation Engineering&lt;/h3&gt; &lt;p&gt;We’ll use the Representation reading and controls methods presented in &lt;a href=&quot;https://arxiv.org/pdf/2310.01405.pdf&quot;&gt;Zou et al. (2023)&lt;/a&gt; to understand the context where the model attends to and discover directions that indicate such reasoning.&lt;/p&gt; &lt;p&gt;Relatedly, there have been a recent surge in research related to model knowledge editing, including Meng et al. (2023) &lt;d-cite key=&quot;meng2023massediting&quot;&gt;&lt;/d-cite&gt;, Zhong et al. (2023) &lt;d-cite key=&quot;zhong2023mquake&quot;&gt;&lt;/d-cite&gt;, and Hernandez et al. (2023) &lt;d-cite key=&quot;hernandez2023inspecting&quot;&gt;&lt;/d-cite&gt; that demonstrate different methods for locating and editing factual associations. Other work, including Shao et al. (2023) &lt;d-cite key=&quot;shao2023gold&quot;&gt;&lt;/d-cite&gt; and Belrose et al. (2023) &lt;d-cite key=&quot;belrose2023leace&quot;&gt;&lt;/d-cite&gt;, have shown results on erasing larger-scale memory units such as concepts. Li et al. (2023) &lt;d-cite key=&quot;li2023inferencetime&quot;&gt;&lt;/d-cite&gt; applies such concept erasion techniques by conducting Inference Time Interference, where one can find a direction of causal influence on “truthfulness” data and increase the activations along that direction to increase truthfulness, scoring better on the TruthfulQA dataset.&lt;/p&gt; &lt;h1 id=&quot;experiment-setup&quot;&gt;Experiment Setup&lt;/h1&gt; &lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt; &lt;p&gt;We adopt a total of 30 datasets on binary classification, (sentiment analysis, natural language inference, true/false inference) and multiple choices; 16 datasets are used by Min et al. (2022) &lt;d-cite key=&quot;min2022rethinking&quot;&gt;&lt;/d-cite&gt;, plus 12 extra datasets in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tweet_eval&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ethos&lt;/code&gt; dataset families, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotten_tomatoes&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ade_corpus_v2-classification&lt;/code&gt;. Following Min et al. (2022)&lt;d-cite key=&quot;min2022rethinking&quot;&gt;&lt;/d-cite&gt;, we only use the test set to avoid potential cross-contamination with the data that the model is pretrained on. reserve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k=64&lt;/code&gt; examples in the test for few-shot training, and the rest are used for testing.&lt;/p&gt; &lt;h3 id=&quot;training-data-generation&quot;&gt;Training Data Generation&lt;/h3&gt; &lt;p&gt;For training, we construct a set of context pairs for each dataset, each context pairs containing the same examples but different instructions. The instructions are “Pay attention to the following examples” and “Ignore the following examples” respectively, in the hope that by stimulating two opposites and examining the difference, we can find a Context Vector that represents what the model draws from. We then truncate the example at each and every token till the last 5 tokens, so we can get a neural activation reading for each of the tokens.&lt;/p&gt; &lt;p&gt;A sample training data input using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotten_tomatoes&lt;/code&gt; dataset is as follows:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[INST] Pay attention to the following examples: [/INST]&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;offers that rare combination of entertainment and education.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;positive.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;a sentimental mess that never rings true .&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;negative.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;[INST] Ignore the following examples: [/INST]&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;offers that rare combination of entertainment and education.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;positive.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;a sentimental mess that never rings true .&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;negative.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Each context pair is identical except for the instructions. We use the context pairs to stimulate the model to learn the context and use the context vector to control the model’s behavior.&lt;/p&gt; &lt;h3 id=&quot;testing-data-generation&quot;&gt;Testing Data Generation&lt;/h3&gt; &lt;p&gt;For testing data, we use 3 input-labels pairs as the prompt, with the first two pairs serving as the in-context examples, and the last pair serving as the question that we actually want to test on, obfuscating the label from the prompt.&lt;/p&gt; &lt;p&gt;A sample testing data input using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotten_tomatoes&lt;/code&gt; dataset is as follows:&lt;/p&gt; &lt;p&gt;Input:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[INST] offers that rare combination of entertainment and education. [/INST]&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;positive.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;[INST] a sentimental mess that never rings true . [/INST]&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;negative.&lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote&gt; &lt;p&gt;an odd , haphazard , and inconsequential romantic comedy .&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Label:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;negative.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt; &lt;p&gt;We have explored using two models with 7 billion parameters, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Mistral-7B-Instruct-v0.&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Llama-2-7b-hf&lt;/code&gt;; while we have found preliminary results consistent between the two models, all of our results later reported are from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Mistral-7B-Instruct-v0&lt;/code&gt; for consistency and due to a constraint on computational power and time.&lt;/p&gt; &lt;h3 id=&quot;training-infrastructure&quot;&gt;Training Infrastructure&lt;/h3&gt; &lt;p&gt;We used the MIT Supercloud infrastructure and a local machine with a single RTX 4090 GPU to train the model.&lt;/p&gt; &lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt; &lt;p&gt;We present results first on finding the Context Vector in the embedding space, then on using the Context Vector to control model outputs and evaluate their performance.&lt;/p&gt; &lt;h3 id=&quot;representation-reading&quot;&gt;Representation Reading&lt;/h3&gt; &lt;p&gt;We use the Representation Reading method presented in Zou et al. (2023) &lt;d-cite key=&quot;zou2023representation&quot;&gt;&lt;/d-cite&gt; to find the Context Vector. Specifically, we adopted the setup of the instruction response pairs where for a given function $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f&lt;/code&gt;$ and pairs of instructions $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x_i&lt;/code&gt;$ and $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_i&lt;/code&gt;$, we denote the model’s response truncated at the $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt;$-th token as $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f(x_i)_j&lt;/code&gt;$ and $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f(y_i)_j&lt;/code&gt;$ and take the neuron activity at the last token of each of the responses, namely the activations of each and every token in the response.&lt;/p&gt; &lt;p&gt;We then perform PCA on the difference of the activations of the two instructions, namely $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f(x_i)_j - f(y_i)_j&lt;/code&gt;$ and find the first principal component $&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v&lt;/code&gt;$ that maximizes the difference in the embedding space.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/correlation_tomato.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Graph plotting the correlation between the Context Vector sign and actual dataset label on Rotten Tomatoes dataset. The x-axis is the layer and the y-axis is the correlation. &lt;/div&gt; &lt;p&gt;More surprisingly is the fact that we can find a clean representation of such Context Vector that correlates decently with the model inputs.&lt;/p&gt; &lt;p&gt;We use t-SNE to visualize the difference in the embedding space on the inputs of the 30 datasets across 32 different layers and report the results below.&lt;/p&gt; &lt;!-- &lt;figure&gt; &lt;img src=&quot;assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data.png&quot; class=&quot;img-fluid&quot; alt=&quot;Description of the first image&quot;&gt; &lt;figcaption&gt;Caption for the first image&lt;/figcaption&gt; &lt;/figure&gt; --&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_data.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the embedding space of the Context Vectors across the 30 datasets and 32 layers, color coded by dataset. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/tsne_layers.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE plot of the embedding space of the Context Vectors across the 30 datasets and 32 layers, color coded by layers. &lt;/div&gt; &lt;p&gt;As shown in the figure, we find that the vectors are clustered by dataset, indicating that the Context Vectors are dataset-specific. There are no clear patterns across dataset or between different layers of the Context Vectors, further indicating that in-context learning activates different parts of the model’s latent space with information about different types of tasks.&lt;/p&gt; &lt;p&gt;We also conducted scans for neuron activities in the Context Vector across the different tokens of an example sequence in a similar style as Zou et al. (2023) &lt;d-cite key=&quot;zou2023representation&quot;&gt;&lt;/d-cite&gt;, for which the previous work has referred to as Linear Artificial Tomography (LAT) scans.&lt;/p&gt; &lt;p&gt;The following are the LAT scans for the neuron activities corresponding to a Context Vector trained on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotten_tomatoes&lt;/code&gt; sentiment analysis dataset evaluated on different dataset sequences. The following graphs further corroborate the findings above on the dataset-specificity of in-context learning; while the a sequence from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotton_tomatoes&lt;/code&gt; dataset result in high neural activities for the Context Vector, most sequences from the other dataset do not, showing the uniqueness of such Context Vector. We have also observed most of the neuron activities in the later layers. This phenomenon makes sense since more abstract concepts and semantic structures formulate in later layers, thus being more correlated with the Context Vector, while earlier layers pick up more on token-level abstractions.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_rotten_tomatoes.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; A LAT scan of the Context Vector trained on `rotten_tomatoes` dataset evaluated with a `rotten_tomatoes` sequence. The x-axis is the token index, and the y-axis is the Layer number. More red indicates higher neural activities, and more blue indicates lower neural activities. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_medical_questions_pair.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; A LAT scan of the Context Vector trained on `rotten_tomatoes` dataset evaluated with a `medical_questions_pair` sequence. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/lat_scan_ethos_religion.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; A LAT scan of the Context Vector trained on `rotten_tomatoes` dataset evaluated with a `ethos-religion` sequence. &lt;/div&gt; &lt;p&gt;We have also produced graphs that zoom into the token-level neural activities detection on the Context Vector of the opposing pair (Pay attention &amp;amp; Don’t pay attention), shown below. A large difference in the neural activities of the two instructions is denoted by red and indicates that the ablation is effective, while the green shades indicate that there are similar in neural activities. The results show that the neural activities are consistently different across the sequence until the model starts generating next tokens and the context ends where the neural activities are similar.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/rotten_tomatoes_token_level.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; A token-level LAT scan that compares the difference between the neural activities of the Context Vector of the opposing pair (Pay attention &amp;amp; Don&apos;t pay attention) on the `rotten_tomatoes` dataset. &lt;/div&gt; &lt;h2 id=&quot;representation-control&quot;&gt;Representation Control&lt;/h2&gt; &lt;p&gt;To change an activation along some direction, we can imagine there are several canonical ways. First, given our Context Vector $v$ and an activation $a$, we can do one of the following.&lt;/p&gt; &lt;h3 id=&quot;addition&quot;&gt;Addition&lt;/h3&gt; \[a&apos; = a + v\] &lt;h3 id=&quot;amplification&quot;&gt;Amplification&lt;/h3&gt; \[a&apos; = a + \text{sign}(a \cdot v) v\] &lt;h3 id=&quot;projection&quot;&gt;Projection&lt;/h3&gt; \[a&apos; = a - (a \cdot v) \cdot \frac{v}{||v||^2}\] &lt;p&gt;The first represents a constant perturbation so it supposedly transforms the representation to become more of a certain quality. The second amplifies the direction according to which side it is on, so it makes the representation more extreme. The third removes the quality from the representation by subtracting the projection.&lt;/p&gt; &lt;p&gt;We explore all these methods to control Mistral-7b-instruct. We do our experiments on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rotten_tomato&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sick&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hate_speech18&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;glue-wnli&lt;/code&gt; in-context-learning datasets consisting of input-output pairings where outputs have two possible correct options – positive or negative contradiction or entailment, hate or noHate, and entailment or not_entailment (for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sick&lt;/code&gt;, it originally contains a third option of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neutral&lt;/code&gt; which we remove since our framework requires two classes).&lt;/p&gt; &lt;p&gt;Given learned representations with the same configuration as our representation reading, we construct a test set from the same dataset as training. The test set has $16$ examples, each with one demonstration followed by a question. We evaluate correctness by having the LLM generate $10$ tokens and checking if the correct answer is contained in the output and the incorrect answer is not contained in the output, without being sensitive to case. This ensures correct evaluation so that an answer of no_entailment does not evaluate as correct for having entailment inside of it if entailment is the right answer.&lt;/p&gt; &lt;p&gt;A hyperparameter which we denote $\alpha$ scales the size of $v$. If our Context Vector is $r$, sign value is $s$, then we have $v = \alpha \cdot r \cdot s$. We vary $\alpha \in { 0, 0.25, 0.5, 1, 2, 5, 10}$, and also take the negative of $\alpha$, which we label as positive and negative respectively.&lt;/p&gt; &lt;h3 id=&quot;results-for-control-with-addition&quot;&gt;Results for Control with Addition&lt;/h3&gt; &lt;p&gt;For rotten tomatoes, we see the expected performance gap of positive over negative, though positive does worse than no control. Moreover, we see in glue-wnli and sick, the negative control actually does better than positive control. In hate_speech18, we see the desired result.&lt;/p&gt; &lt;p&gt;Despite modifying the layers that we controlled, based upon observing the layers at which the Context Vectors had the most correlation to the trained concept, we cannot find a set of layers to control that works &lt;strong&gt;consistently&lt;/strong&gt; across all four datasets, though we can find layers that work for one dataset.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_tomato.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `rotten_tomatoes` dataset with amplification or suppression of the Context Vector using Addition. The x-axis is the coefficient of amplification, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_sick.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Addition&lt;/bold&gt;. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_hate_speech.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `hate_spe` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Addition&lt;/bold&gt;. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/add_glue.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Addition&lt;/bold&gt;. &lt;/div&gt; &lt;h3 id=&quot;results-for-control-with-amplification&quot;&gt;Results for Control with Amplification&lt;/h3&gt; &lt;p&gt;Note the result depends on the absolute value of $\alpha$ so the positive and negative graphs converge. The affect of amplification is quite smooth relative to addition in the sense that there is a consistent downward trend in performance for both amplification and suppression. This could be because amplification amplifies existing signals and this gets stronger as $\alpha$ increases.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_tomato.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `rotten_tomatoes` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Amplification&lt;/bold&gt;. The x-axis is the alpha value, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_sick.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Amplification&lt;/bold&gt;. The x-axis is the alpha value, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_hate_speech.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `hate_speech18` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Amplification&lt;/bold&gt;. The x-axis is the alpha value, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/piecewise_glue.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Amplification&lt;/bold&gt;. The x-axis is the alpha value, and the y-axis is the accuracy. &lt;/div&gt; &lt;h3 id=&quot;results-for-control-with-projection&quot;&gt;Results for Control with Projection&lt;/h3&gt; &lt;p&gt;We can see that projection consistently decreases performance, which is expected as we can imagine projection as erasing the idea that the model needs to pay attention to these examples. Having positive or negative sign of $\alpha$ does not affect projection.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_tomato.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `rotten_tomatoes` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Projection&lt;/bold&gt;. The x-axis is the alpha value, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_sick.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Projection&lt;/bold&gt;. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_hate_speech.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `hate_speech18` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Projection&lt;/bold&gt;. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/proj_glue.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of the Context Vector using &lt;bold&gt;Projection&lt;/bold&gt;. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. &lt;/div&gt; &lt;h3 id=&quot;ablation-studies&quot;&gt;Ablation Studies&lt;/h3&gt; &lt;p&gt;A key question is whether the Context Vectors are truly special. Especially because much of our results do not work, we would like to assess the “noise level.” By sampling a random unit vector from $4096$-dimensional space, the hidden dimension of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Mistral-7b-instruct&lt;/code&gt;, for each layer and using that for control, we get the following results.&lt;/p&gt; &lt;p&gt;If we take the negative of all the Context Vectors, the graphs for positive and negative $\alpha$’s would switch. The fact that in our random sample we see such a large gap in the Glue-wnli graph indicates that there is quite a lot of noise. Moreover, if we take the negative of our particular randomly sampled vector, we obtain a Context Vector for Glue-wnli that is &lt;strong&gt;extremely good&lt;/strong&gt; at controlling in-context-learning. The large landscape of $4096$-dimensional space is an exciting mystery.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_tomato.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `rotten_tomatoes` dataset with amplification (positive) or suppression (negative) of a random vector using &lt;bold&gt;Addition&lt;/bold&gt;. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_sick.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `sick` dataset with amplification (positive) or suppression (negative) of a random vector using &lt;bold&gt;Addition&lt;/bold&gt;. The x-axis is the alpha value, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_hate_speech.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `hate_speech18` dataset with amplification (positive) or suppression (negative) of a random vector using &lt;bold&gt;Addition&lt;/bold&gt;. The x-axis is the alpha value, and the y-axis is the accuracy. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-representationengineering-incontextlearning/random_glue.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The accuracy of the model on the `glue_wnli` dataset with amplification (positive) or suppression (negative) of a random vector using &lt;bold&gt;Addition&lt;/bold&gt;. The x-axis is the alpha value of amplification, and the y-axis is the accuracy. &lt;/div&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;While we understand our work is limited due to time and compute constraints and did not achieve the results we hoped for, we tried our best to explore this research direction of finding a Context Vector that corresponds to the in-context learning behaviors and experiments of using it to control model outputs.&lt;/p&gt; &lt;h2 id=&quot;implications&quot;&gt;Implications&lt;/h2&gt; &lt;p&gt;If successful, this research direction could be a powerful tool to understand mechanistically why in-context learning emerges and potentially use model editing to achieve better State-of-the-Art results on LLMs in specific benchmark evaluation scenarios with model editing. Even with our current results that demonstrate more success in suppressing the Context Vector than amplifying it, i.e. suppressing such behaviors than boosting it, this can have implications on works that try to perform model unlearning and impact the robustness of LLMs.&lt;/p&gt; &lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt; &lt;p&gt;Through ablating with the random vector in the embedding space, it is unfortunate that controlling for the particular Context Vector we found is not particularly different from other vectors, despite it showing some promises on suppressing the results. We hope to run further ablation studies to confirm that suppressing the Context Vector is only suppressing the in-context learning behaviors of the specific behaviors and does not have other side effects.&lt;/p&gt; &lt;p&gt;Regarding our current setup of the contrasting prompts of telling the model to pay attention or not pay attention to the concept, we can further explore the space of contrasting prompts. Directly related to our work, we would also like to explore the other type of experiment setup in Zou et al. (2023)&lt;d-cite key=&quot;zou2023representation&quot;&gt;&lt;/d-cite&gt;; unlike the data pair setup where we ask the model to pay attention to the examples or ignore them, we can ask the model to “think hard about the context/structure of the question” and elicit neural activities that way.&lt;/p&gt; &lt;p&gt;We are also interested in exploring vectors that control step-by-step reasoning and in general, intelligence. The phrases “Let’s think step by step” &lt;d-cite key=&quot;kojima2023stepbystep&quot;&gt;&lt;/d-cite&gt; or “Take a deep breath and work on this problem step-by-step” &lt;d-cite key=&quot;yang2023deepbreath&quot;&gt;&lt;/d-cite&gt; are powerful phrases that elicit chain-of-thought reasoning and improve model performance. Could we engineer activation transformations that improve these models’ performance even more than and without the need for prompting?&lt;/p&gt; </content> </entry> <entry> <title>Reasoning with Maps: Assessing Spatial Comprehension on Maps in Pre-trained Models</title> <link href="https://deep-learning-mit.github.io/blog/2023/mapreason/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/mapreason</id> <content type="html">&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt; &lt;p&gt;Humans possess a remarkable ability to intuitively understand and make sense of maps, demonstrating a fundamental capacity for spatial reasoning, even without specific domain knowledge. To illustrate this, consider the following question: Do these two maps represent the same location?&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/85_301-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/85_301-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/85_301-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/85_301.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/85_302-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/85_302-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/85_302-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/85_302.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Answering this query necessitates &lt;strong&gt;coregistration&lt;/strong&gt;, the ability to align two maps by overlaying their significant landmarks or key features. Moreover, humans can go beyond mere alignment; they can tackle complex inquiries that demand aligning maps, extracting pertinent data from each, and integrating this information to provide answers.&lt;/p&gt; &lt;p&gt;Maps reasoning is a fundamental skill with important applications in domains such as navigation and geographic analysis. For example, pilots need to be able to reference and understand multiple kinds of FAA charts as a core prerequisite for many aviation-related tasks. Further, making inferences on historical maps that lack digitized versions relies on human capabilities for reasoning on maps and is crucial for various fields such as geology or archeology. Machine learning models that can match human visual map understanding hold substantial promise in these applications. Additionally, such models have the potential to enhance accessibility by providing alternative modalities for individuals with visual impairments to comprehend and extract spatial information from maps.&lt;/p&gt; &lt;p&gt;Our work aims to tackle the following question: To what degree do contemporary state-of-the-art (SOTA) machine learning models, pre-trained on vast datasets comprising millions or even billions of images, possess the capacity for spatial reasoning and do they reach the human level? We will do this specifically by focusing on the task of coregistration.&lt;/p&gt; &lt;p&gt;We propose a map reasoning dataset which we believe is a suitable initial benchmark to test the capabilities of multimodal models on coregistration; The example given above about coregistration possibly cannot be answered directly using prior knowledge a Large Language Model (LLM) might have while ignoring the vision modality. Moreover, the complexity of the task can be increased and controlled, leading to a rigorous evaluation of the model’s ability to comprehend and synthesize information across textual and visual modalities.&lt;/p&gt; &lt;h2 id=&quot;literature-review-and-the-gap-in-previous-literature&quot;&gt;Literature review and the gap in previous literature&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Multimodality:&lt;/strong&gt; There are countless significant recent advances in Large Language Models (LLMs) achieved by models such as Meta’s Llama 2&lt;d-cite key=&quot;llama&quot;&gt;&lt;/d-cite&gt;, OpenAI’s GPT 3&lt;d-cite key=&quot;gpt3&quot;&gt;&lt;/d-cite&gt;, Google’s Palm 2&lt;d-cite key=&quot;palm2&quot;&gt;&lt;/d-cite&gt;, WizardLM&lt;d-cite key=&quot;wizardlm&quot;&gt;&lt;/d-cite&gt;, and countless others. These models have successfully achieved or surpassed human-level performances on numerous natural language processing tasks (such as Retro-Reader&lt;d-cite key=&quot;retro_reader&quot;&gt;&lt;/d-cite&gt; on the SQuAD2.0&lt;d-cite key=&quot;squad2&quot;&gt;&lt;/d-cite&gt; benchmark, ST-MOE&lt;d-cite key=&quot;st_moe&quot;&gt;&lt;/d-cite&gt; on the SuperGLUE&lt;d-cite key=&quot;super_glue&quot;&gt;&lt;/d-cite&gt; benchmark, and many other benchmarks). LLMs have achieved a surprisingly high level of knowledge about text by being able to achieve very high zero-shot scores on many NLP tasks demonstrating their understanding and versatility in Language as opposed to non-LLM NLP models that are usually trained to accomplish a specific task and do not generalize beyond that task.&lt;/p&gt; &lt;p&gt;Recently there has been a massive push towards integrating other modalities into LLMs, most notably vision. Models such as Google’s Gemini&lt;d-cite key=&quot;gemini&quot;&gt;&lt;/d-cite&gt;, OpenAI’s GPT 4&lt;d-cite key=&quot;gpt4&quot;&gt;&lt;/d-cite&gt;, VisionLLM&lt;d-cite key=&quot;vision_llm&quot;&gt;&lt;/d-cite&gt;, and many others all focus on scaling the capabilities achieved by LLMs in the NLP domain to the vision domain. The language modality LLMs are trained on consists mostly of written text recorded online that follows a grammatical structure from a human language (English, etc.) or a computer language (Python, HTML, Latex, etc). However, this is in stark contrast to the vision modality which can consist of categorically different representations consisting of anything from photographs of people in the park, to a representation of the Silicon Atom from a computer simulation, to an infographic about the importance of drinking enough liters of water per day. This difference between the distribution of the entirety of text modality and the vision modality hints that much more careful attention needs to be placed on compiling unique and diverse datasets that aim at sufficiently representing the distribution of the vision modality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vision-Language Benchmarks:&lt;/strong&gt; There were significant strides made in the past years in developing benchmarks and datasets for LVLMs which are composed of questions that require both Language and Vision to successfully answer. However, there are very few datasets that include or focus on maps as part of the benchmark. LVLM-eHub&lt;d-cite key=&quot;lvlm_ehub&quot;&gt;&lt;/d-cite&gt; compiles numerous benchmarks for LVLMs such as benchmarking object and scene recognition using ImageNet1K, GVT, and many other datasets, or benchmarking visual common sense using ImageNetVC and the visual commonsense reasoning datasets. They also include a Visual Reasoning benchmark, however, it focuses mostly on real-world photographs (such as the Visual Spatial Reasoning&lt;d-cite key=&quot;visual_reasoning&quot;&gt;&lt;/d-cite&gt; dataset or the VisDial&lt;d-cite key=&quot;visual_dialog&quot;&gt;&lt;/d-cite&gt; Dataset that is based on images from COCO&lt;d-cite key=&quot;coco&quot;&gt;&lt;/d-cite&gt;). However, none of these datasets place focus or sufficiently contain images of maps or diagrams while they place much more focus on real-world images.&lt;/p&gt; &lt;p&gt;A frequently used dataset for evaluating LVLMs, which is also included in the Visual Reasoning benchmark, is the ScienceQA&lt;d-cite key=&quot;science_qa&quot;&gt;&lt;/d-cite&gt; dataset which includes more than 20 thousand multimodal multiple-choice questions across 127 categories including a category for maps. However, examining the narrow slice of questions with images in the geography/map category shows that many of them do not necessitate a vision component and can be answered solely based on the textual question (e.g. “Which of these states is farthest north?” with four different states as a multiple choice question), or provide an entirely unused visual representation (e.g. providing a map of the United States with no text alongside the question “What is the capital of New York?”). Out of the questions that rely on the visual component to correctly answer, many of them require relatively little understanding of maps such as asking “Which country is highlighted?” which only requires visual matching of the highlighted section with typical shapes of countries or continents. Additionally, recent papers such as LLama-adapter&lt;d-cite key=&quot;llama_adapter&quot;&gt;&lt;/d-cite&gt; have demonstrated that it’s possible to achieve a high accuracy of 78.3% on ScienceQA using an unimodal text-only Large Language Model. Thus, although ScienceQA does have a subsection dedicated to maps, it does not seem sufficiently capable of testing the capabilities of LVLMs to reason and understand maps.&lt;/p&gt; &lt;p&gt;An area closely related to maps that do have a relatively higher degree of focus is the capability of models to parse and reason about diagrams, figures, and plots. Datasets on this topic include the ACL-FIG&lt;d-cite key=&quot;acl_fig&quot;&gt;&lt;/d-cite&gt; which involves classifying and labeling scientific figures, InfographicVQA&lt;d-cite key=&quot;info_vqa&quot;&gt;&lt;/d-cite&gt; which requires reasoning over data visualizations in infographics, ChartQA&lt;d-cite key=&quot;chart_qa&quot;&gt;&lt;/d-cite&gt; which requires reasoning over charts, and many other datasets that focus on figures are plots. Models have been developed to specifically tackle this challenge, such as Google’s DEPLOT&lt;d-cite key=&quot;deplot&quot;&gt;&lt;/d-cite&gt; which is capable of reasoning over charts and plots by translating them to text and then using an LLM as a reasoning engine on top of the outputted text. However, charts and plots are still significantly different from maps, as the plots these datasets usually contain are simple line charts and bar graphs that can be translated into a table or textual format in a relatively lossless manner, while it is difficult or impossible to perfectly transform a sufficiently detailed map to a textual format without losing information. This illustrates the inherent complexities associated with processing maps meant to depict dense information which requires direct reasoning on the vision modality as opposed to charts and plots which present data in a simple manner.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Maps Reasoning:&lt;/strong&gt; Huge strides have been made in specific tasks related to maps, such as image-to-map&lt;d-cite key=&quot;image_to_map&quot;&gt;&lt;/d-cite&gt; conversion and map segmentation&lt;d-cite key=&quot;map_segmentation&quot;&gt;&lt;/d-cite&gt;. However, we wanted to focus more generally on map understanding and reasoning by LVLMs as opposed to a single task-specific performance. To draw on an analogy, consider the act of isolating specific parts of speech (such as nouns or verbs) in language. A model designed exclusively for this task lacks the comprehensive understanding exhibited by an LLM which is proficient in addressing almost any language task. In the context of map co-location, deep learning models are employed solely as preprocessing steps to extract relevant information for subsequent utilization by matching algorithms as opposed to an LVLM model capable of general reasoning on maps. For example, the authors in this study&lt;d-cite key=&quot;intersection_map&quot;&gt;&lt;/d-cite&gt; use region-based CNN to extract road intersections, which are subsequently input into a map-matching procedure. Other features like street names have also been proposed in the literature&lt;d-cite key=&quot;street_name_map&quot;&gt;&lt;/d-cite&gt;. In general, current frameworks for map reasoning require many hand-crafted and engineered steps (see, e.g., this&lt;d-cite key=&quot;aerial_understanding&quot;&gt;&lt;/d-cite&gt; work and the references within). A recently proposed dataset, MapQA&lt;d-cite key=&quot;map_qa&quot;&gt;&lt;/d-cite&gt;, is closely related to what we consider as map reasoning. However, the maps contained are of sufficiently low information-density and exhibit similar limitations to what we described in InfographicVQA and ChartQA as the images provided can be sufficiently translated to a textual domain before considering the textual input using a similar technique to DEPLOT. To the best of our knowledge, there are no examples in the literature where LVLMs are used to directly reason about maps at the detail we propose and perform tasks such as coregistration.&lt;/p&gt; &lt;p&gt;Our aim is to tackle the gap in assessing the map reasoning capabilities of LVLMs by developing a dataset aimed only at coregistration and analyzing the capabilities of existing models on such a dataset We focus our benchmark construction on the specific task of coregistration as it serves as an indicator of map reasoning capabilities and is one step towards constructing a comprehensive benchmark for map reasoning capabilities of LVLMs.&lt;/p&gt; &lt;!-- ############## --&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;h1 id=&quot;new-dataset&quot;&gt;New Dataset&lt;/h1&gt; &lt;p&gt;We have opted to create and compile a map dataset focusing on maps from the aviation domain for our research. The maps we utilized are carefully crafted by aviation agencies to provide a wealth of information while maintaining readability within a concise timeframe, ensuring clarity for pilots. Our dataset will be constructed by incorporating maps from the following sources:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;World Visual Flight Rules (VFR):&lt;/strong&gt; These maps are intended to guide pilots when they operate aircraft visually. They include aeronautical and topographic information such as airports, obstructions, and navigation aids.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;World Instrument Flight Rules (IFR) Low&lt;/strong&gt; These maps are suitable to assist pilots when they control the aircraft through instruments. They contain information such as cruising altitudes, route data, and controlled airspaces.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These maps are accessible in an interactive environment through the SkyVector website (&lt;a href=&quot;https://skyvector.com/?ll=42.3525,-71.025833333&amp;amp;chart=301&amp;amp;zoom=2&quot;&gt;VFR&lt;/a&gt;, &lt;a href=&quot;https://skyvector.com/?ll=42.3525,-71.025833333&amp;amp;chart=302&amp;amp;zoom=1&quot;&gt;IFR Low&lt;/a&gt;), which we used as part of our dataset generation pipeline.&lt;/p&gt; &lt;p&gt;To generate the map snippets for our experiment, we chose to sample from the previous map sources around airports. This selection guarantees that the snippets are inherently information-rich, given that the map originates in the aviation domain. To ensure diversity in our dataset, we specifically sampled airports situated in the states of Massachusetts, New York, Delaware, Arizona, and Hawaii.&lt;/p&gt; &lt;p&gt;The resulting dataset exhibits significant variations in terms of density, featuring both isolated airports and those nestled within cities, diverse locations such as inland, seaside, and islands, as well as various terrain types ranging from greenery landscapes, mountainous regions, and arid environments. In total, our dataset contains 1185 image pairs, each image is 600x600 pixels in PNG format. The total size of our dataset is 1.28 GB.&lt;/p&gt; &lt;h2 id=&quot;a-glimpse-of-the-coregistration-task&quot;&gt;A glimpse of the coregistration task&lt;/h2&gt; &lt;p&gt;To gain an understanding of our task and its intricacies, we present a few examples from our dataset. Generally, humans can successfully align two maps by identifying common features, which fall into one of the following categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Terrains:&lt;/strong&gt; such as shorelines or mountains.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Charts:&lt;/strong&gt; such as flight paths or restricted airspaces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Landmarks:&lt;/strong&gt; such as airport or city names.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The process of mapping by terrain is typically swift for humans, especially when there are ample distinctive details. On the other hand, mapping by chart requires a more thoughtful approach, involving careful examination to establish a connection between the depicted attributes. Mapping by names usually serves as a last resort, employed if the prior approaches prove unsuccessful. Consider the following examples:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/43_301-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/43_301-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/43_301-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/43_301.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/43_302-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/43_302-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/43_302-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/43_302.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Positive Pair #1: A VFR map (left) and an IFR map (right) that depict the same location (Port Allen Airport in South Kauai, Hawaii) and can be coregistered easily by the shorelines of the island. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1038_301-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1038_301-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1038_301-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/1038_301.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1038_302-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1038_302-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1038_302-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/1038_302.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Positive Pair #2: A VFR map (left) and an IFR map (right) that depict the same location (Circle H Ranch Airport in Arizona) and can be coregistered by aeronautical lines (even if one does not know what these lines represent in the aviation domain). &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/779_301-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/779_301-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/779_301-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/779_301.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/779_302-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/779_302-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/779_302-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/779_302.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Positive Pair #3: A VFR map (left) and an IFR map (right) that depict the same location (Bertrand Chaffee Hospital Heliport, New York). The VFR map is feature-rich providing information for Helicopter pilots while the IFR map is a hard example due to being almost blank as the Heliport does not facilitate landing any IFR aircraft. Thus, the IFR map only depicts certain names of cities and waypoints. The two maps can be coregistered by matching the name SPRINGVILLE. &lt;/div&gt; &lt;p&gt;All of these examples are positive (the maps show the same location). We showcase below negative examples with varying complexity.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/51_301-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/51_301-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/51_301-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/51_301.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/223_302-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/223_302-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/223_302-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/223_302.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Negative Pair #1: An easy and straightforward negative for humans due to the VFR map depicting a shoreline crossing the map vertically, while the IFR depicts only a landmass. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1037_301-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1037_301-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1037_301-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/1037_301.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1013_302-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1013_302-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/1013_302-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/1013_302.png&quot; class=&quot;img-fluid z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Negative Pair #2: A harder negative example. Aeronautical lines partially appear similar but the locations are different. Naively matching circles on both maps to classify a positive pair would provide a false positive. &lt;/div&gt; &lt;p&gt;We showcase multiple positive and negative pairs alongside the natural reasoning that a human would take to correctly classify the pairs. We hope that this showcases the complexity of the task and the various strategies involved in achieving successful coregistration.&lt;/p&gt; &lt;!-- ############## --&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt; &lt;h2 id=&quot;zero-shot-evaluation&quot;&gt;Zero-shot evaluation&lt;/h2&gt; &lt;p&gt;To start, we want to evaluate the zero-shot performance of pre-trained LVLMs on the task of identifying whether the two images are the same (coregistration). The models we start our evaluation with are BLIP-2&lt;d-cite key=&quot;blip&quot;&gt;&lt;/d-cite&gt;, ViLT&lt;d-cite key=&quot;vilt&quot;&gt;&lt;/d-cite&gt;, LXMERT-VQA, and LXMERT-GQA&lt;d-cite key=&quot;lxmert&quot;&gt;&lt;/d-cite&gt;. We specifically chose these models as they are all publicly available multimodal text generative models that were partly trained on visual question-answering datasets. Thus, they are able to accept both the vision and language inputs consisting of an image of the two side-by-side maps alongside the yes-or-no question of whether the two maps depict the same geographical location.&lt;/p&gt; &lt;p&gt;To verify that the models we obtained are behaving as expected and are capable of answering a textual question that relies on a visual component, we compile a very simple dataset of 200 cat and dog pictures, half the images depict a cat while the other half depict dogs. We present these trivial images to the models alongside the prompt “Is this an image of a cat? Answer:” and generate a single token. As expected, out of the 200 images all four models achieved an almost perfect classification accuracy (&amp;gt;95% for all 4 models) by answering with either a “Yes” or a “No” token.&lt;/p&gt; &lt;p&gt;This is not surprising because, as mentioned, object recognition questions are very prevalent in visual question-answering datasets, especially on ubiquitous everyday objects such as cats and dogs. To see if these models can generalize beyond their training datasets and properly reason on maps, we start by running the following experiment:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experiment #1:&lt;/strong&gt; For each VFR and IFR image pair, we generate two examples (positive and negative). For the positive example, we use the correct pairing (e.g., maps from the same location with the two different styles). For the negative example, we randomly replace one map uniformly from our datasets. Each model is provided with a concatenation of the two maps in its vision input, and with the question “Do these two maps show the same location? Answer with “Yes” or “No”. Answer:” in its text input.&lt;/p&gt; &lt;p&gt;In total, each model was asked 2370 questions. Below, we show the accuracy, precision, and recall that each model obtained.&lt;/p&gt; &lt;div class=&quot;row mt-3 align-items-center&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_02-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_02-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_02-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_01_02.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The models performed barely above random guessing in the zero-shot experiment, and some models consistently produced the same single output (either “yes” or “no”) regardless of whether the input image was a positive or negative pair.&lt;/p&gt; &lt;p&gt;While the results of the models are very low and barely above random guessing, we wanted to analyze whether this failure is due to the model not comprehending the task or whether the issue is simply in the last layer of the model where the text generation occurs. The reason behind this analysis is that there is a possibility that the LVLM is able to correctly capture all the features necessary for determining whether the two maps coregister while still failing at providing the final answer due to the final layer of the model outputting an incorrect distribution over the labels (or tokens in the case of LVLMs). Thus we decide to ignore the last linear layer of the model (the language model head) and capture the hidden state of the last token from the last layer of the model.&lt;/p&gt; &lt;h2 id=&quot;fine-tuned-evaluation&quot;&gt;Fine-tuned evaluation&lt;/h2&gt; &lt;p&gt;Using this methodology, the output we obtain from each model is a single embedding vector (the length of which depends on the embedding size of the model). Usually, a single linear layer is finetuned on the last layer to directly predict the answer. However, we opt for a more detailed analysis by training multiple classifiers (Logistic Regression, SVM, and XGBoost) that take the embedding vector and produce a binary output. In all the upcoming figures, we always report the results using the classifier that performed the best (for each model) on the validation set.&lt;/p&gt; &lt;p&gt;Moreover, it is known that LLMs can be sensitive to prompts&lt;d-cite key=&quot;yang2023large,yang2022prompt&quot;&gt;&lt;/d-cite&gt;. Thus, to more rigorously analyze the performance of the models, we create a total of 8 variations of the coregistration question, as this one question serves as the text input for all pairs of images in our created dataset. A detailed analysis of these prompts will be provided later. Similar to the classifiers, we report results using the best prompt for each model.&lt;/p&gt; &lt;p&gt;This methodology of using the model to extract a rich embedding that contains the answer to our prompt (instead of generating the answer directly as text) means that we are now capable of utilizing additional large transformer-based multimodal models that output an embedding vectors instead of directly outputting text. Thus we include in our analysis two such models which are FLAVA&lt;d-cite key=&quot;flava&quot;&gt;&lt;/d-cite&gt; and GIT&lt;d-cite key=&quot;git&quot;&gt;&lt;/d-cite&gt; that output embeddings that represent the multimodal input. Those embeddings are then similarly passed to the simple classifiers to detect whether the models are capable of capturing the required details from the maps to answer the co-location task correctly. Thus we perform our second experiment:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experiment #2:&lt;/strong&gt; We generate the examples using the same procedure described in Experiment #1. Then, for each model, we pass through the entire dataset and generate an embedding for each sample in our dataset. We then train the simple classifiers on 70% of the embedding vectors to predict the positive and negative pairs. We report the evaluation on the other 30% data and report the results in orange below.&lt;/p&gt; &lt;div class=&quot;row mt-3 justify-content-center&quot;&gt; &lt;div class=&quot;col-sm-8 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_02_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_02_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_02_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_02_01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The accuracy of this fine-tuning methodology (shown in orange) achieves around 65%-70% for all models which is a significantly higher accuracy compared to the zero-shot accuracy of the LVLMs (shown in red) which was incapable of achieving more than 55%. This experiment shows that the embedding of the last token does contain a slightly more feature-rich representation of the multimodal input and can be used to classify the positive/negative pairs at a higher rate than random but is overall still incapable of sufficiently solving the task.&lt;/p&gt; &lt;p&gt;Thus far we have tried to assess the capabilities of LVLMs and (more generally) Multimodal Vision Language models on solving the coregistration task, and we assessed this capability using our constructed dataset of determining whether two maps of different styles represent the same location or not. Given the low accuracy achieved on this task, we can claim that the LVLMs we have analyzed are incapable of reasoning and answering more complicated questions relative to our simple baseline question of “Are these two maps of the same location”&lt;/p&gt; &lt;h2 id=&quot;improving-results-for-co-registration&quot;&gt;Improving results for co-registration&lt;/h2&gt; &lt;p&gt;We emphasize that our goal is not to directly achieve high accuracy on this task by utilizing any machine learning model, but rather it is to evaluate the capabilities of LVLMs to reason on maps. Furthermore, we created and proposed this dataset and task to act as a baseline for assessing the reasoning abilities of LVLMs on maps.&lt;/p&gt; &lt;p&gt;However, despite the failure of LVLMs to answer this baseline task, we next want to assess the inherent difficulty of the dataset. For this, we develop a simple model by utilizing the same simple classifiers used above to train on the embedding of a unimodal vision-only model. Unlike LVLMs, we are not testing our proposed task-specific model on the dataset to assess its capabilities for reasoning on maps, as the model is not trained to answer questions based on images, does not accept text modality, and is specifically fine-tuned to solve this one narrow task. Thus, the results of this experiment serve only to give a sense of the difficulty of the task that we considered as a simple baseline for map reasoning. This will hopefully demonstrate that the relatively older frozen vision-only models can achieve a significantly higher accuracy on this specific task when compared to state-of-the-art open-source LVLMs and possibly indicating the gap between the embeddings captured by the vision-only model and the LVLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experiment #3:&lt;/strong&gt; We develop a simple unimodal vision classification model by utilizing a frozen CLIPVIsion model as a backbone. First, we feature-engineer the input by subtracting the two maps from each other in the image space to produce a single image. This image is passed through the frozen CLIPVision model to generate an embedding of the difference between the maps, the embeddings are then used to train the simple classifiers mentioned above and the one that achieves the highest accuracy on the validation set is reported below.&lt;/p&gt; &lt;div class=&quot;row mt-3 justify-content-center&quot;&gt; &lt;div class=&quot;col-sm-8 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_03_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_03_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_03_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_03_01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;We see that our fine-tuned vision model (shown in green) achieves a significantly higher accuracy than all previously tested LVLMs. This shows that the task is not a significantly difficult vision task as a frozen CLIPVision model with a head fine-tuned on approximately two thousand samples was able to sufficiently extract an embedding and correctly distinguish positive and negative pairs 85% of the time.&lt;/p&gt; &lt;p&gt;This significant difference between the accuracy of the frozen CLIP model and the LVLMs on this task signifies that the LVLMs we tested are still significantly farther behind on certain tasks even when compared to a frozen vision-only model that was trained and released years prior. This is in stark contrast to the significant achievements that LLMs accomplish on numerous datasets when compared to task-specific NLP models, where the highest-scoring models on most NLP datasets are LLMs.&lt;/p&gt; &lt;h2 id=&quot;human-benchmarking&quot;&gt;Human benchmarking&lt;/h2&gt; &lt;p&gt;So far, we have examined the performance of pre-trained LVLMs on our proposed dataset in a zero-shot as well as a fine-tuned manner alongside a vision-only model with feature engineering to assess the difficulty of the task.&lt;/p&gt; &lt;p&gt;A natural next question to analyze is the performance of humans on this same task as it is not immediately clear how hard or easy the task is for us. The performance achieved by humans on a task such as this would serve as a great target for LVLMs to try to reach.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experiment #4:&lt;/strong&gt; We present the following task to two subjects. Each human subject will see two maps for 10 seconds. The pair can be positive or negative with equal probability. After the 10 seconds elapse, the maps automatically disappear and the human subject is asked if the two maps show the same location with a binary “Yes” or “No” choice. After the answer is received, a new pair is sampled and this process is repeated until we gather 50 answers from each human subject.&lt;/p&gt; &lt;p&gt;The 10-second window acts as a pseudo-computational limit on the human subject and ensures that the subject’s answers are mostly based on visual and spatial reasoning and not on reading and comparing text. If the subject does not immediately identify a visual or spatial cue, the 10-second window possibly allows for a maximum of one or two texts to be compared if the subject is quick enough. This time limitation prevents the participants from spending an extensive amount of time comparing the nuances of the two images for a severely long time which would make the task more trivial. Below, we show the accuracy obtained from two human subjects and compare it with the previous LVLM results.&lt;/p&gt; &lt;div class=&quot;row mt-3 justify-content-center&quot;&gt; &lt;div class=&quot;col-sm-8 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_04_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_04_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_04_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_04_01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;We see that both human participants (shown in blue) achieve a significantly higher accuracy (~95%) compared to all the tested ML models. This shows that the task is significantly easier for humans despite the 10-second time limit preventing the subject from extensively comparing the images.&lt;/p&gt; &lt;p&gt;Our experiments showcase the inability of LVLMs to properly solve our proposed dataset on coregistration as well as showing that a vision-only fine-tuned model with feature-engineering is able to solve the task at a significantly higher accuracy. Finally, we show that humans are able to solve the time-constrained task with a significantly high accuracy.&lt;/p&gt; &lt;h2 id=&quot;analysis-on-prompt-engineering&quot;&gt;Analysis on prompt engineering&lt;/h2&gt; &lt;p&gt;Numerous recent studies have indicated the importance of prompt engineering in the quality of the output of Large-Transformer based models&lt;d-cite key=&quot;yang2023large,yang2022prompt,white2023prompt, zhou2022large&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Due to the potential importance of prompts in affecting performance, we decided to run all experiments that require prompts using multiple different prompts with varying degrees of length and complexity. We note that the prompts considered and listed below were only the ones that consistently conditioned the model to output a “Yes” or “No” output token instead of any other arbitrary sentence completion output. The prompts are shown in the following table:&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt;ID&lt;/th&gt; &lt;th style=&quot;text-align: left&quot;&gt;Prompt&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;Are these two maps the same? Answer with “Yes” or “No”. Answer:&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;Do these two maps show the same location? Answer with “Yes” or “No”. Answer:&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;Do the two charts depict the same area? Answer:”&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;The following image contains two maps with different styles side by side. Do the two maps show the same location? Answer with “Yes” or “No”. Answer:&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;On the left there is a map from the VFR dataset and on the right a map from the IFR dataset. Do the two maps show the same location? Answer with “Yes” or “No”. Answer:&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;There are two maps of different styles, do they represent the same area or are they completely different? Answer:&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;The following image contains two maps with different styles side by side. Do the two maps show the same location? Try to compare the maps by looking at key landmarks or features. Answer with “Yes” or “No”. Answer:&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt; &lt;td style=&quot;text-align: left&quot;&gt;Carefully examine the following two images that contain two maps with different styles side by side. Do the two maps correspond on the same latitude and longitude point? It is of utmost importance that you answer this correctly. Answer with “Yes” or “No”. Answer:&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;The initial prompts (prompts #1 - #3) are meant to be short and direct, while the ones in the middle (prompts #4 - #6) are more verbose and add a bit more complexity, while the last two (prompts #7 - #8) are very verbose and add an exact explanation of the task. We also include additions to some of the prompts that try to guide the models on how they accomplish the task, and some additions that emphasize the importance of correct answers. In the figure below, we study the effect of prompts on model performance.&lt;/p&gt; &lt;div class=&quot;row mt-3 justify-content-center&quot;&gt; &lt;div class=&quot;col-sm-8 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_05_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_05_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_05_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_05_01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;We notice that varying the prompts has a relatively high variance in terms of accuracy with an improvement of less than 5% for all models across all prompts. Still, there are no strong general trends across models when considering prompts with increasing complexity. We note that the VILT model was incapable of accepting prompts #5 - #8 due to the limitation of its maximum context length which is shorter than the other models.&lt;/p&gt; &lt;p&gt;One aspect that might limit this analysis is that almost all prompts contain an explicit requirement for the models to provide answers immediately (e.g., “Answer with ‘Yes’ or ‘No’. Answer:”). This was done to reduce the computational inference cost and avoid generating long sequences of texts. The models might respond better to some prompts if they were allowed to reason about their answers first.&lt;/p&gt; &lt;h2 id=&quot;investigating-the-failure-points-of-lvlms-on-coregistration&quot;&gt;Investigating the failure points of LVLMs on coregistration&lt;/h2&gt; &lt;p&gt;The figures presented in the beginning of the blog post demonstrating some examples in our proposed dataset give a clue of the variance in the difficulty of the examples in the dataset, where some samples are easy to identify as positive pairs and others much harder to do so.&lt;/p&gt; &lt;p&gt;Thus, to get a better insight into the model’s performance and investigate its failure points, we investigate some examples where the models made confidently wrong predictions. Here, we focus on a single model, FLAVA, which was our best-performing LVLM. In the figure below, we investigate both false positives with the highest predicted positive label and false negatives with the highest predicted negative label. The figure contains the 9 examples where the model generated a very high (very low) score while the true label was positive (negative).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; data-zoomable=&quot;&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_02-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_02-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_02-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-mapreason/experiment_06_02.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; data-zoomable=&quot;&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;For the false positives, we see more than one example where two maps containing water were wrongly classified. This might indicate that the model is making predictions on these images based on colors more so than spatial reasoning. For the false negatives, there are many examples where the VFR chart is dense while the IFR is sparse. These examples require discarding a lot of information from the VFR charts and focusing solely on the region where the IFR chart contains information. Given that the model made wrong decisions in these examples, there might be a preference for positively matching images based on density. Notably, some of these examples were straightforward for the human subjects (matching based on the shoreline), while other examples required more effort (matching between dense and sparse maps).&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;h1 id=&quot;discussion-limitations-and-future-work&quot;&gt;Discussion, Limitations, and Future Work&lt;/h1&gt; &lt;p&gt;One of the key takeaways of our experiments, and specifically from contrasting the first two experiments with the third experiment, is that it was not difficult for a non-LVLM model to achieve an 85% accuracy on our proposed dataset. Yet, our dataset proved to be challenging for LVLMs, especially in zero-shot performance where they achieved almost no better than random guessing. This implies that it would be beneficial to further expand future datasets that are used for LVLM training and specifically the addition of data collection similar to what we propose and that this could provide invaluable improvements to future training of LVLMs.&lt;/p&gt; &lt;p&gt;Existing vision-language benchmarks exhibit a heavy focus on real-world objects and scenes, with a distinctive lack of images and questions on maps. This is despite the fact that maps are ubiquitous and used in many real-world scenarios. Furthermore, many maps are easily accessible in digital format and ready to be integrated into vision-language benchmarks. We believe such inclusion would require relatively little effort in terms of data collection while providing significantly higher capabilities for LVLMs.&lt;/p&gt; &lt;p&gt;We plan to expand the size of our new dataset used in this project and to make it publicly available. Additionally, while our current project primarily focused on the coregistration tasks, we have plans to incorporate more intricate and challenging questions that delve deeper into map reasoning.&lt;/p&gt; &lt;p&gt;There are some limitations to the current analysis done in this project. A significant limitation is the computational limit preventing us from feasibly generating answers from the LVLMs in an autoregressive manner instead of our analysis which used only one output token per sample. A possible future work is examining more complicated generation methods such as Chain of Thought&lt;d-cite key=&quot;chain_Thought&quot;&gt;&lt;/d-cite&gt; prompting. Additionally regarding the inputs of the models, although we investigated different text prompts, we only used one template for the visual prompts while there have been multiple recent works on visual prompt engineering in vision-language models&lt;d-cite key=&quot;vision_prompt&quot;&gt;&lt;/d-cite&gt; analogous to textual prompt engineering in LLMs. It could be the case that some models are sensitive to the way the maps are concatenated. This aspect warrants further investigation to gain a more comprehensive understanding of how different concatenation methods might impact model performance and results.&lt;/p&gt; &lt;p&gt;Another limitation is that we were only capable of running our analysis on open-source models, the largest model tested was blip-2 with less than 3 billion parameters. This was the largest LVLM that we had access to in terms of weights, to be able to run our analysis on. Future work could attempt to run the analysis on larger closed-source models if access is granted.&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;In this project, we propose a novel dataset to serve as an initial benchmark for the capabilities of LVLMs to reason on maps with the goal of addressing a gap in current LVLM benchmarks and datasets.&lt;/p&gt; &lt;p&gt;Using this dataset, we run an extensive analysis on the performance of open-source LVLMs showing that they struggle to achieve good performance on the coregistration task. Additionally, we show that the task for our dataset is a relatively simple vision task by showing that a fine-tuned vision-only model released years prior to the tested LVLMs achieves a significantly higher accuracy. Finally, we show that the coregistration task is intuitive to humans, as participants were able to achieve close to perfect accuracy even in a time-constrained manner.&lt;/p&gt; &lt;p&gt;We hope that future initiatives regarding data collection for LVLMs and training foundational LVLMs will put more emphasis on datasets such as our proposed datasets. This will hopefully unlock new capabilities for LVLMs enabling them to advance beyond their current limitations and possibly expand their utility and reasoning abilities in a variety of real-world scenarios.&lt;/p&gt; </content> </entry> <entry> <title>Autoen-chorder: Predicting Musical Success With Neural Nets</title> <link href="https://deep-learning-mit.github.io/blog/2023/foley-to-video/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/foley-to-video</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Our aim is to use deep learning (the crux of 6.s898) to help musicians and their sponsors (for example: agents, record labels, and investors) identify whether songs will resonate with listeners. Solving this problem would enable established artists to release more impactful music, and spur new musicians to break into a competitive market.&lt;/p&gt; &lt;p&gt;We first begin by establishing what our success metric is. For the purposes of this project, we will use the concept of song “popularity” as the metric we want to predict, and we source our popularity data from the SpotGenTrack Popularity Dataset. This dataset leverages Spotify’s Popularity Index, which is a relative rank measured against other songs’ popularities. It is a function of recent stream count, save rate, playlist appearance count, skip rate, share rate, and more.&lt;/p&gt; &lt;p&gt;There already exist a few models to help us solve this problem. However, these models make use of metadata, such as artist name, year of release, and genre. We believe that these models – while interesting – are insufficient to be actionable, particularly for up-and-coming musicians who may be innovating new music genres, or who may not yet have a strong name. Specifically, metadata like Artist Name are both highly-weighted (for example, even Taylor Swift’s least popular song will be a relative superhit) and unchangeable (we cannot suggest that artists change their identity to Beyonce). Additionally, features like Genre are imprecise, and can quickly become outdated as new subgenres and crossover genres are developed.&lt;/p&gt; &lt;p&gt;To address this gap and become more actionable to musicians, we aimed to create a new model that can achieve near-parity with metadata-based models without leveraging any metadata. By combining multiple audio-feature models, we not only achieved comparable results to metadata-based models, we actually outperformed metadata-based models on more than half our testing data.&lt;/p&gt; &lt;h2 id=&quot;previous-works&quot;&gt;Previous Works&lt;/h2&gt; &lt;p&gt;The most prominent existing model is HitMusicNet (heretofore referred to as “HMN”). The HMN model predicts popularity based on lyric data from Genius.com (syllables, words, etc.), high-level audio features from SpotGenTrack (e.g. acousticness, key, tempo, speechiness), low-level audio features from SpotGenTrack (audio preprocessing, such as spectral analyses), and metadata from SpotGenTrack (e.g. artist name, year of release, genre). A feature vector is created with this information, and said vector is fed as the input into an Autoencoder network to compress the features, followed by a neural network to obtain the predicted popularity.&lt;/p&gt; &lt;p&gt;HitMusicNet has two different objectives: Regression and classification. For this project, we will focus only on regression since it will allow us to visualize differences between our model and HMN with higher granularity. We replicated the code from the paper in PyTorch, using the same functions and data and calculated metrics to make sure our implementation is correctly replicating the paper. We see a slight discrepancy in the errors, likely due to the test/train split during the paper’s training. Altogether, we can still consider our replicated model as valid as the metrics are within reasonable range to the reported metrics. Additionally, we added the R-squared metric as an additional metric to ensure our model fits the data.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Comparison-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Comparison-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Comparison-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Comparison.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;A second model, VGGish, is a pretrained convolutional neural network trained on YouTube-100M (a database with 100 million YouTube videos). This network is a representation learning network widely used in established papers. This network takes in a wav file and processes it on 0.96-second windows, and calculates 128 embeddings per window. This means that the resulting tensor from VGGish will be 2 dimensional for a single file, and 3 dimensional for a batch of files.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-HMN-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-HMN-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-HMN-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-HMN.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Autoencoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Autoencoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Autoencoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Autoencoder.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;hypothesis&quot;&gt;Hypothesis&lt;/h2&gt; &lt;p&gt;HMN has a tendency to be heavily-indexed on metadata features and lyrics. Data such as artist name heavily bias the model’s popularity predictions in favor of big-name artists. Lyrics information can make the model biased to predicting instrumental music as less likely to be successful. While this may be representative of how the industry works, it makes HMN much less actionable for musicians trying to assess their chance of success with the market.&lt;/p&gt; &lt;p&gt;We believe that audio-only features – such as temporal information (i.e. the structure of the song and information about previous sections) and repetition – can alone be fairly successful in determining a song’s popularity. Thus, we chose to use just audio data, as well as temporal data, to predict popularity.&lt;/p&gt; &lt;p&gt;We hypothesize that combining the audio-only features of HMN with VGGish’s audio representation will yield superior outcomes to HMN’s audio-only features alone. We also hope that our new model can compete with the full HMN model (i.e. audio features and metadata combined).&lt;/p&gt; &lt;h2 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h2&gt; &lt;p&gt;Given our hypothesis, we need to extract the low-level features from our signal and map each row to its corresponding audio file to be fed into VGGish. We used Spotify’s API to obtain the raw audio files to be processed, and then ran them through the VGGish network. We performed the same preprocessing as the one done in the MusicHitNet paper.&lt;/p&gt; &lt;h3 id=&quot;file-length-limitation&quot;&gt;File length limitation&lt;/h3&gt; &lt;p&gt;Unfourtunately, Spotify only allows the download of 30s previews of songs.&lt;/p&gt; &lt;h3 id=&quot;memory-limitation&quot;&gt;Memory limitation&lt;/h3&gt; &lt;p&gt;Audio files are heavy, and the longer they are, the heavier. We should ideally process all 95,000 songs’ full length, but given Spotify’s API’s request limit, and the size of the files, we were only able to obtain 10,000 30s snippets. This still resulted in roughly 14.5 GB of data. Processing the whole dataset would not only require roughly 140 GBs of data, but the Spotify API’s limits will likely be exceeded, and our colab notebook will likely run out of memory.&lt;/p&gt; &lt;h3 id=&quot;downsampling-and-splitting&quot;&gt;Downsampling and splitting&lt;/h3&gt; &lt;p&gt;Given the considerations above, we decided to use 10,000 songs for our model’s development, splitting the data into 8,500 samples for training and 1,500 for validation. Given that this is roughly 10% of the original data, we expect the model’s performance to be below the reported metrics.&lt;/p&gt; &lt;h2 id=&quot;baselines&quot;&gt;Baselines&lt;/h2&gt; &lt;p&gt;The metrics obtained when replicating the HMN network serve as a reasonable parameter to verify our model’s accuracy. As mentioned above, the model’s performance is expected to be below the paper’s reported metrics. To understand the range, we retrained a network with the same shape as the paper’s using the 10,000 samples in the same train/test split we will feed to our new network. The resulting metrics for this experiment can be seen in Table 2.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/img5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/img5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/img5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/img5.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Training a model that results in similar metrics would be ideal, but realistically, as we will only be using low-level data, we expect the metrics to be lower than the values in Table 2. To ensure that our trained model isn’t just predicting noise, we use a baseline comparison, comparing against a random normal distribution with mean μ=40.02 and σ=16.79.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/img6-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/img6-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/img6-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/img6.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;As seen in table 3, the baseline intuitively would not appear to be too far from the trained HMN model in terms of MSE and MAE. When looking at the r-squared, the random model has a negative value, while the trained HMN netw ork results with a much higher 0.5616 value. To deem a model as successful, we will compare it against both sets of metrics.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/img1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/img1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/img1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/img1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/img2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/img2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/img2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/img2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;alternate-models&quot;&gt;Alternate Models&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Single autoencoder. Our first iteration to solve this problem consisted of using a single autoencoder to find representations with data coming from VGGish and SpotGetTrack low level features, and then running that through a feed-forward network similar to the one used in HMN. Since the output of VGGish is a tensor of shape (batch_size, n_windows, n_features) and the output of SpotGenTrack is (batch_size, 207), we concluded there was no simple way to combine the two data sources without losing temporal information.&lt;/li&gt; &lt;li&gt;RNN. Our second iteration consisted of running the data coming from SpotGenTrack Low-Level through an autoencoder in the same way HMN does it. After this initial train gives us a compressed representation of the data from SpotGenTrack Low-Level, we train two subsequent networks: First an LSTM RNN which transforms data into (batch_size, 20), then we add the compressed representation from SpotGenTrack Low-Level and run that through a feedforward network. This model yielded a performance below the baseline.&lt;/li&gt; &lt;li&gt;HMN+VGGish: This model consists of taking the full SpotGenTrack data, passing it through the regular autoencoder defined in HMN, and add it to the output coming from VGGish. This model, while resulting in promising results, still yielded worse performance than HMN on its own, so our team decided to explore alternatives.&lt;/li&gt; &lt;li&gt;LossNet. Our third exploration consisted of training a model that uses VGGish’s outputs to try and predict losses from HMN. In essence, we are trying to use VGGish Representation to capture information that HMN consistently is unable to. This approach has parallels with Adversarial Networks, in that one model is being trained on the losses of another model. However, this approach is more cooperative than adversarial, since the result of the two models is not zero-sum. This approach led to a dead-end with surprising results.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;final-architecture&quot;&gt;Final Architecture&lt;/h2&gt; &lt;p&gt;Our final iteration consists of a model with two autoencoders: One for data from SpotGenTrack low level features, the second for the representation obtained using the VGGish model. The slight difference between these two models is that the VGGish autoencoder has additional LSTM layers at the start of the encoder, and at the end of the decoder. The output from these two autoencoders is then added together and passed through a feed-forward network. This architecture can be seen in Figure 4.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-FinalArch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-FinalArch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-FinalArch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-FinalArch.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;padding-and-packing&quot;&gt;Padding and Packing&lt;/h3&gt; &lt;p&gt;None of the audio files coming from Spotify previews are more than 30s in duration, but some are in fact shorter than others. To solve this issue, and also to be able to feed our model whichever sized data we require, we use pytorch’s packing functionality. Packing allows us to process sequential data with different sizes, so that only the relevant information is passed through the LSTM. Conversely, padding allows us to add zeros at the end of sequences so that all samples have the same size. This is required to store data in tensors.&lt;/p&gt; &lt;h3 id=&quot;hyperparameters&quot;&gt;Hyperparameters&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/img3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/img3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/img3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/img3.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;additional-model-considerations&quot;&gt;Additional Model Considerations&lt;/h3&gt; &lt;p&gt;The original HMN model compiles 228 features into 45 representations for the feed-forward network. We want our model’s feed-forward network to have a similar number of inputs as the given architecture, therefore we compress the data in the encoder of both autoencoders to 20 features, so that when added together, they result in 40 total features.&lt;/p&gt; &lt;p&gt;Additionally, as can be seen in figure 3.2, the target’s distribution is condensed at a central point, and distributed in a Gaussian shape. To help our model accurately predict the shape of the results, we use multiply the losses by a weighting factor. This multiplication is important to make our model more likely to predict outliers. The equation is the following:&lt;/p&gt; &lt;p&gt;\begin{equation} \frac{1}{N} \sum_{i=1}^{N} \exp\left(\left(\frac_{i} - \text)}}{\alpha \cdot \text}\right)^2 \cdot \frac{1}{\beta}\right) \end{equation}&lt;/p&gt; &lt;p&gt;Our feed-forward network was suffering of vanishing gradients during training. To attempt to avoid this, we initialized all linear layers with a weight distributed by Xavier uniform, and a constant bias of 0.1.&lt;/p&gt; &lt;h3 id=&quot;finding-the-best-model&quot;&gt;Finding the Best Model&lt;/h3&gt; &lt;p&gt;In order to find the best model, we modified plenty of parameters and hyperparameters. We first found the optimal autoencoder models (seen on table 4), and then we proceeded to run several loops over our linear layer to obtain the model with lowest errors. The parameters modified were the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Learning rate: (0.001, 0.0001, 0.0002, 0.02, 0.0005)&lt;/li&gt; &lt;li&gt;Weight decays: (0, 0.0001, 0.0002)&lt;/li&gt; &lt;li&gt;Batch sizes: (200, 100, 256, 277)&lt;/li&gt; &lt;li&gt;Means (for weights calculation): 0.33, 0.34, 0.35, 0.37, 0.38, 0.40, 0.42, 0.45)&lt;/li&gt; &lt;li&gt;Alphas (for weights calculation): (1.8, 2.0, 2.1, 2.2)&lt;/li&gt; &lt;li&gt;Betas (for weights calculation): (1.8, 2.0, 2.2)&lt;/li&gt; &lt;li&gt;Number of linear layers: (7, 9, 12)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The combination that resulted in the optimal model was the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weight decays: 0&lt;/li&gt; &lt;li&gt;Batch sizes: 200&lt;/li&gt; &lt;li&gt;Means (for weights calculation): 0.36&lt;/li&gt; &lt;li&gt;Alphas (for weights calculation): 2.0&lt;/li&gt; &lt;li&gt;Betas (for weights calculation): 2.0&lt;/li&gt; &lt;/ul&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/img4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/img4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/img4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/img4.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Table 5 shows the best-performing models obtained after experimentation. MAE, MSE and r-squared were calculated using the testing data, i.e. Data not used in training. Looking at the data in tables 2 and 3, we see that our model shows a significant improvement above the random baseline, with a reasonable r-squared and MSE. Reduction in the MAE remains challenging, but still we see a significant improvement from the random baseline.&lt;/p&gt; &lt;p&gt;Furthermore, we analyzed the testing data, and found that in 919 of the 1,500 songs (61.2%) of the songs, our model did better than HitMusicNet. Upon further analysis, we found that our model did a better job predicting the popularity of songs with popularities ranged [0.22-0.55], while HMN does a better job at predicting outliers (songs with &amp;lt;0.2 or &amp;gt;0.6 of popularity).&lt;/p&gt; &lt;h2 id=&quot;conclusions-and-next-steps&quot;&gt;Conclusions and Next Steps&lt;/h2&gt; &lt;h3 id=&quot;data-exploration&quot;&gt;Data Exploration&lt;/h3&gt; &lt;p&gt;Given Spotify’s ubiquity and analytics excellence, its Popularity Index is a good proxy for relative song popularity. But there are concerns around using data from a single platform (Spotify) and from a single channel (digital streaming). Given this concern, we would like to explore other methods of calibrating a track’s popularity (for example, Billboard and Discogs API). We can aggregate popularities into a single output, or can train each model on multiple outputs of various popularity scores.&lt;/p&gt; &lt;p&gt;Currently, our data consists of 30s audio clips. The average new song length is around 3min 17s, meaning that our models’ inputs cover around 15% of the song. This can cause the model to miss information critical to song likeability, such as the intro, chorus, or bridge. We would like to make our dataset more complete by using full songs as inputs. Furthermore, we’re using only 10,000 data points, which can also be affecting our training efficiency, especially our ability to detect outliers, which we have found to be a key issue with our model. Ideally, we would like to train our models on all 95k songs in SpotGenTrack.&lt;/p&gt; &lt;h3 id=&quot;architectures&quot;&gt;Architectures&lt;/h3&gt; &lt;p&gt;Many more architectures can further be explored to predict song popularity. We found VGGish with an LSTM to be an efficient “boosting” algorithm, which contributed to the model in a less significant way that SpotGenTrack, but still allowed our model to increase its performance. Similarly, the use of transformer architectures can help improve the performance of our model.&lt;/p&gt; &lt;p&gt;In this study, we explored and evaluated our model against the HitMusicNet’s regression algorithm. In further studies, it could be beneficial to explore the classification algorithm, as we have seen very promising results in the prediction of songs along a certain range.&lt;/p&gt; &lt;p&gt;We used the VGGish model purely on inference since we required to train the autoencoder and then the feed-forward network. Future studies can include architectures such that the VGGish model is trained in series with the feedforward network, and fine-tuned to predict popularity. We could also look at alternate representation models that are perhaps better suited or supply a more apt representation for our task than VGGish.&lt;/p&gt; &lt;p&gt;In conclusion, the use of low-level features to predict popularity can have several real-world advantages. The proposed model is able to predict a song’s popularity to a fair degree without the need for high-level features. Emerging artists can use these parameters to determine the possible success of their songs. Music labels can use this algorithm to predict an artist’s possible popularity. Platforms such as Spotify can also take advantage of this model in order to tackle recommendations and boost emerging artists.&lt;/p&gt; &lt;h2 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;D. Martín-Gutiérrez, G. Hernández Peñaloza, A. Belmonte-Hernández and F. Álvarez García, “A Multimodal End-to-End Deep Learning Architecture for Music Popularity Prediction,” in IEEE Access, vol. 8, pp. 39361-39374, 2020, doi: 10.1109/ACCESS.2020.2976033.&lt;/li&gt; &lt;li&gt;Ding, Yiwei, and Alexander Lerch. “Audio embeddings as teachers for music classification.” arXiv preprint arXiv:2306.17424 (2023).&lt;/li&gt; &lt;li&gt;D. Martín-Gutiérrez, “HitMusicNet” in https://github.com/dmgutierrez/hitmusicnet.&lt;/li&gt; &lt;li&gt;Koutini, Khaled, et al. “Efficient training of audio transformers with patchout.” arXiv preprint arXiv:2110.05069 (2021).&lt;/li&gt; &lt;li&gt;P. Nandi, “Recurrent Neural Nets for Audio Classification” in https://towardsdatascience.com/recurrent-neural-nets-for-audio-classification-81cb62327990.&lt;/li&gt; &lt;li&gt;Wu, Rick, “VGGish Tensorflow to PyTorch” in https://github.com/tcvrick/audioset-vggish-tensorflow-to-pytorch.&lt;/li&gt; &lt;li&gt;Wu, Yiming. (2023). Self-Supervised Disentanglement of Harmonic and Rhythmic Features in Music Audio Signals.&lt;/li&gt; &lt;li&gt;S. Shahane, “Spotify and Genius Track Dataset” in https://www.kaggle.com/datasets/saurabhshahane/spotgen-music-dataset/data.&lt;/li&gt; &lt;/ul&gt; </content> </entry> <entry> <title>Ensemble Learning for Mitigating Double Descent</title> <link href="https://deep-learning-mit.github.io/blog/2023/double_descent/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/double_descent</id> <content type="html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt; &lt;p&gt;We outline the fundamental ‘bias-variance tradeoff’ concept in machine learning, as well as how the double descent phenomenon counterintuitively bucks this trend for models with levels of parameterization at or beyond the number of data points in a training set. We present a novel investigation of the mitigation of the double descent phenomenon by coupling overparameterized neural networks with each other as well as various weak learners. Our findings demonstrate that coupling neural models results in decreased loss during the variance-induced jump in loss before the interpolation threshold, as well as a considerable improvement in model performance well past this threshold. Machine learning practitioners may also find useful the additional dimension of parallelization allowed through ensemble training when invoking double descent.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;p&gt;There are many important considerations that machine learning scientists and engineers must consider when developing a model. How long should I train a model for? What features and data should I focus on? What exactly is an appropriate model size? This last question is a particularly interesting one, as there is a bit of contention regarding the correct answer between different schools of thought. A classical statistician may argue that, at a certain point, larger models begin to hurt our ability to generalize. By adding more and more parameters, we may end up overfitting to the training data, resulting in a model that poorly generalizes on new samples. On the other hand, a modern machine learning scientist may contest that a bigger model is always better. If the true function relating an input and output is conveyed by a simple function, in reality, neither of these ideas are completely correct in practice, and empirical findings demonstrate some combination of these philosophies. This brings us to the concept known as &lt;em&gt;double descent&lt;/em&gt;. Double descent is the phenomenon where, as a model’s size is increased, test loss increases after reaching a minimum, then eventually decreases again, potentially to a new global minimum. This often happens in the region where training loss becomes zero (or whatever the ’perfect’ loss score may be), which can be interpreted as the model ’memorizing’ the training data given to it. Miraculously, however, the model is not only memorizing the training data, but learning to generalize as well, as is indicated by the decreasing test loss.&lt;/p&gt; &lt;p&gt;The question of ’how big should my model be?’ is key to the studies of machine learning practitioners. While many over-parameterized models can achieve lower test losses than the initial test loss minimum, it is fair to ask if the additional time, computing resources, and electricity used make the additional performance worth it. To study this question in a novel way, we propose incorporating &lt;em&gt;ensemble learning&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Ensemble learning is the practice of using several machine learning models in conjunction to potentially achieve even greater accuracy on test datasets than any of the individual models. Ensemble learning is quite popular for classification tasks due to this reduced error empirically found on many datasets. To our knowledge, there is not much literature on how double descent is affected by ensemble learning versus how the phenomenon arises for any individual model.&lt;/p&gt; &lt;p&gt;We are effectively studying two different &lt;em&gt;types&lt;/em&gt; of model complexity: one that incorporates higher levels of parameterization for an individual model, and one that uses several models in conjunction with each other. We demonstrate how ensemble learning affects the onset of the double descent phenomenon. By creating an ensemble that includes (or is fully comprised of) overparameterized neural networks, which can take extreme amounts of time and resources to generate, with overparameterized machine learning models, we will show the changes in the loss curve, specifically noting the changes in the regions where double descent is invoked. We hope that the results we have found can potentially be used by machine learning researchers and engineers to build more effective models.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;One of the first papers discussing double descent was ’Reconciling modern machine- learning practice and the classical bias–variance trade-off’ by Belkin et al. &lt;d-cite key=&quot;belkin2019reconciling&quot;&gt;&lt;/d-cite&gt;. This paper challenged the traditional idea of the ’bias-variance tradeoff’, a fundamental concept in machine learning that describes the tension between two types of model error: bias and variance. Bias is the error between the expected prediction of the model and the true output value, introduced by approximating a real-world quantity with a model, which may overisimplify the true problem at hand. Variance refers to the error due to a model’s sensitivity to small fluctuations in the training dataset. Overfitted models may have high variance, as they may model random noise in the data as well.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/biasvariance-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/biasvariance-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/biasvariance-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/biasvariance.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;The Double Descent Curve&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The Bias-Variance Tradeoff &lt;d-cite key=&quot;cornell&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;In short, classical statistical learning argues that there is some optimal level of parameterization of a model, where it is neither underparameterized nor overparameterized, that minimizes the total error between bias and variance. However, Belkin’s paper finds that, empirically, the tension between bias and variance no longer becomes a tradeoff after a certain level of overparamaterization. They showed that after the interpolation threshold (beyond where the model fits perfectly to the training data), test error eventually began to decrease again, even going below the error deemed optimal by the bias-variance minimum.&lt;/p&gt; &lt;p&gt;Nakkiran et al.’s ’Deep Double Descent: Where Bigger Models and More Data Hurt’ &lt;d-cite key=&quot;nakkiran2021deep&quot;&gt;&lt;/d-cite&gt; expanded these findings to the realm of &lt;em&gt;deep&lt;/em&gt; learning. In this work, double descent is shown to occur for both large models and large datasets. Additionally, this paper demonstrates that, counterintuitively, adding more data at a certain point actually worsened the performance of sufficiently large models. Specifically, this occurred at and close to the interpolation threshold for neural models. This paper’s results can be seen here:&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/openai-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/openai-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/openai-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/openai.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;The Double Descent Curve&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The Double Descent Curve &lt;d-cite key=&quot;nakkiran2021deep&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;For the region between the first and second loss minima, model performance can suffer greatly, despite the increased computational time and resources used to generate such models. While this region of the test loss curve is typically not a level of parameterization that one would use in practice, understanding such loss curve behavior can help practitioners for several reasons. For one, this degraded phase of performance can be crucial for tweaking model architecture and adjusting training strategies. This is key to discovering if one’s model is robust and adaptable to various other datasets and tasks. This highlights the need for a new understanding for model selection in order to effectively generalize to testing datasets better, mitigating decreases in model performance and invoking a second loss minimum quickly.&lt;/p&gt; &lt;p&gt;In the classic paper ’Bagging Predictors’, Breiman describes the concept of combining the decisions of multiple models to improve classification ability &lt;d-cite key=&quot;breiman1996bagging&quot;&gt;&lt;/d-cite&gt;. Empirically, this bootstrap aggregating, or ’bagging’ technique, reduced variance and improved accuracy, outperforming the single predictors that comprised the ensemble model. We present a novel combination of the findings of this paper with the double descent phenomenon. Effectively, by increasing model complexity via overparameterization and ensemble learning, we aim to study if this combination can mitigate loss increases and invoke a second loss minimum with smaller models.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;h3 id=&quot;computing-resources-and-software&quot;&gt;Computing Resources and Software&lt;/h3&gt; &lt;p&gt;We have implemented this project using CUDA and the free version of Google Colab, with additional computing units for more costly experiments. To train and test these models, we use various machine learning packages in Python, namely Scikit-learn, PyTorch and Tensorflow. Additional software commonly used for machine learning projects, such as numpy, tensorboard and matplotlib, was also utilized.&lt;/p&gt; &lt;p&gt;All plots have been produced by us, unless otherwise specified. Note that all tensorboard plots have $0.25$ smoothing applied, except for the Soft-Voting Ensemble, which has $0.6$ smoothing applied (though this won’t make much of a difference as will soon be seen). The non-smoothed plot can be seen traced in light-blue in all provided plots.&lt;/p&gt; &lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt; &lt;p&gt;We use the MNIST dataset for this report &lt;d-cite key=&quot;deng2012mnist&quot;&gt;&lt;/d-cite&gt;. MNIST is a popular dataset used for image classification, where each sample image is a $28$ by $28$ grayscale image of a written integer between $0$ and $9$, inclusive. Each image comes with the true label of the image’s integer. This data is publicly available for experimentation, and our use of it does not pose any ethical or copyright concerns.&lt;/p&gt; &lt;p&gt;For this project, we use the MNIST dataset to unearth the double descent phenomenon. We experiment with a variety of models, as well as an ensemble of them: decision trees, AdaBoost trees, L2-Boost trees, random forests, logistic regression, and small neural networks. We choose these models because of their ability to be used for classification tasks, and more complicated models run the risk of exceeding Google Colab’s limitations, especially when we overparameterize these models to invoke double descent.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mnist-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mnist-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mnist-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/mnist.jpeg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;MNIST Sample Data&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Sample MNIST Data &lt;d-cite key=&quot;deng2012mnist&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt; &lt;h3 id=&quot;decision-trees&quot;&gt;Decision Trees&lt;/h3&gt; &lt;p&gt;Decision trees are a machine learning model used for classification tasks. This model resembles a tree, splitting the data at branches, culminating in a prediction at the leaves of the tree.&lt;/p&gt; &lt;p&gt;To invoke overparameterization for decision trees, we can start with a tree of depth 2, and increase the number of maximum leaves of the model until the loss plateaus. Then, keeping this new number of max leaves in our decision tree, we continually increase the maximum depth of the tree until the loss once again stops decreasing. Lastly, keep both the maximum leaves and depth at their plateau levels while increasing the max features. The results of this are plotted below. Notice how varying the number of maximum leaves has minimal effect on the loss, and how increasing the maximum depth causes the most dramatic decrease. However, fluctuations on the maximum depth at this point do not have a major effect, whereas varying the number of features causes another slight, yet consistent, fall in classification loss.&lt;/p&gt; &lt;p&gt;Notice that the loss curve is more or less linear in the number of parameters (with some having much more effect than others), and so there is little evidence of double descent for this model.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/dd_decision_tree_zero_one_8.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Decision Tree Overparameterization &lt;/div&gt; &lt;h3 id=&quot;adaboost-tree&quot;&gt;AdaBoost Tree&lt;/h3&gt; &lt;p&gt;Adaptive Boosting (AdaBoost) itself is an ensemble model used for robust classification. Freund et al.’s paper ‘A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting’ first introduced the algorithm &lt;d-cite key=&quot;freund1997decision&quot;&gt;&lt;/d-cite&gt;. On a high level, this paper describes how boosting is especially effective when sequentially combining weak learners that are moderately inaccurate (in this case, these are decision trees) to create a strong learner. We study the loss curve of the AdaBoost model as we first increase the number of boosting trees which form a forest, then increase the number of forests across which we average results, after adding additional trees fails to significantly increase model performance. Each tree is constrained to have no more than 10 leaves.&lt;/p&gt; &lt;p&gt;Notice that the loss curve is more or less linear in the number of parameters, and the double-U shape doesn’t seem to make its presence known.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/dd_adaboost_zero_one_2.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; AdaBoost Overparameterization &lt;/div&gt; &lt;h3 id=&quot;l2-boost-tree&quot;&gt;L2-Boost Tree&lt;/h3&gt; &lt;p&gt;L2 Boosting is quite similar to the AdaBoost model, except for L2 Boosting, as models are built sequentially, each new model in the boosting algorithm aims to minimize the L2 loss&lt;d-cite key=&quot;article&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;10.1214/aos/1013203451&quot;&gt;&lt;/d-cite&gt;. Like before, we first increase the number of boosting trees which form a forest in the L2-Boost model, then the number of forests across which we average using majority voting. The results can be seen below. Each tree is constrained to have no more than 10 leaves.&lt;/p&gt; &lt;p&gt;Notice how the classification loss begins to fall, then rises up again, then falls once more when we average across more forests to lower minimums than before. This result was consistent across multiple runs of this experiment, suggesting that double descent is real for L2-Boosted Tree Ensembles.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/dd_l2boost_zero_one_1.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; L2-Boost Overparameterization &lt;/div&gt; &lt;p&gt;The behavior of the loss once we add more models agrees with general intuition regarding ensembling, but the appearance of double descent as we increase the total number of parameters is still quite interesting to see. L2-Boost is a relatively inexpensive model and ensembling a large number of trees is still quite fast, suggesting that overparameterization could be the way to go in this case.&lt;/p&gt; &lt;h3 id=&quot;random-forest&quot;&gt;Random Forest&lt;/h3&gt; &lt;p&gt;Random Forest is another popular ensemble model. As the name implies, it is a collection of decision trees with randomly selected features, and, like the singular decision tree, this model is used for classification tasks.&lt;/p&gt; &lt;p&gt;We initialize random forest with a small number of maximum leaves allowed in each tree, and increase the max leaves until we see the loss plateau as we continually add more. After this, we begin increasing the number of trees in our forest until the loss plateaus once again.&lt;/p&gt; &lt;p&gt;While Belkin et al. lists random forest as a model exhibiting double descent, this claim has been recently disputed, namely by Buschjager et al, which suggests that there is no true double descent with the random forest model &lt;d-cite key=&quot;randomforest&quot;&gt;&lt;/d-cite&gt;. Instead, they suggest that random forest does not overfit in the classical sense, and argue that its curve instead has a single descent. By looking at our results, there is little evidence for the double-U curve, but we will refrain from drawing a decisive conclusion.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/dd_rf_zero_one_6.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Random Forest Overparameterization &lt;/div&gt; &lt;p&gt;Despite this, for our ensemble model, we aim to see if the addition of this overparameterized learner to the neural network’s decision making is able to improve ensemble performance.&lt;/p&gt; &lt;h3 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h3&gt; &lt;p&gt;Logistic regression is a classic model used for estimating the probability a sample belongs to various classes. We induce overfitting in logistic regression through two methods.&lt;/p&gt; &lt;p&gt;First, we continually increase the ‘C’ parameter, indicating the inverse strength of regularization applied to the regression, as shown below. Notice that the loss decreases to a minimum before it starts slowly rising again, indicating that overfitting through fluctuations in ‘C’ may not actually lead to double descent, as would be expected from classical theory.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_c.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Logistic Regression Overparameterization (Regularization Based) &lt;/div&gt; &lt;p&gt;Second, we try inducing double descent by varying the ratio of the number of features over the amount of data. We gradually reduce this ratio using the intuition developed by Deng et al. in order to induce overfitting &lt;d-cite key=&quot;logistic&quot;&gt;&lt;/d-cite&gt;, since the data becomes more and more separable as the number of features increases relative to the number of data samples.&lt;/p&gt; &lt;p&gt;To do this, we test varying across the number of training samples instead of varying the number of features used for training. This eventually leads to 0 training error, but causes testing error to blow up, suggesting that some significant amount of training data is still needed to witness the desired behavior, consistent with both statistical and machine learning theory.&lt;/p&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/dd_logistic_regression_zero_one_d.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Logistic Regression Overparameterization (Feature-Data Ratio Based) &lt;/div&gt; &lt;p&gt;An interesting setup for future experiments would be simultaneously increasing the amount of training samples and the number of polynomial features given to the logistic regression, while increasing the feature-data ratio each time we reparameterize or redefine the dataset.&lt;/p&gt; &lt;h3 id=&quot;neural-networks&quot;&gt;Neural Networks&lt;/h3&gt; &lt;p&gt;We use a Multilayer Perceptron as our main model for the ensemble. Our deep learning model is a relatively small one, with variable width in the hidden layer. By increasing this width, we eventually achieve perfect training loss.&lt;/p&gt; &lt;p&gt;We define the general architecture of the neural network used in this report as follows:&lt;/p&gt; &lt;h4 id=&quot;network-layer&quot;&gt;Network Layer&lt;/h4&gt; &lt;p&gt;Let the input data be an $m$ by $m$ pixel image from the MNIST dataset, which can be processed as an $m$ by $m$ matrix, where entry $(i,j)$ is an integer between $0$ and $255$ (inclusive) representing the grayscale color of the pixel. Note that $m=28$ for MNIST, though for generality, we use $ m $ in this network definition. A value of $0$ represents a black pixel, $255$ is a white pixel, and values between these are varying shades of gray. We first flatten this structure into a $d = m^2 $ by 1 vector, such that the entry $ (i,j) $ of the matrix becomes the $ j + 28*i$-th entry of the vector, using zero-indexing. We use this vector as the input of our neural network.&lt;/p&gt; &lt;p&gt;Set $H$ as the hidden layer width, which in our project will be varied in different tests. Let $ W^1 $ be an $ d \times H$ matrix, where $ W^1_{ij}$ is the weight of input $i$ applied to node $j$, and let $W^1_0$ be an $H \times 1$ column vector representing the biases added to the weighted input. For an input $X$, we define the &lt;em&gt;pre-activation&lt;/em&gt; to be an $H \times 1$ vector represented by $Z = {W^1}^T X + W^1_0$.&lt;/p&gt; &lt;p&gt;We then pass this linearly transformed vector to the ReLU activation function, defined such that&lt;/p&gt; \[\begin{equation*} \text{ReLU}(x)=\begin{cases} x \quad &amp;amp;\text{if} \, x &amp;gt; 0 \\ 0 \quad &amp;amp;\text{if} \, x \leq 0 \\ \end{cases} \end{equation*}\] &lt;p&gt;We use this choice of activation function due to the well-known theorem of universal approximation. This theorem states that a feedforward network with at least one single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $ \mathbb{R}^{m^2} $ if the ReLU activation function is used &lt;d-cite key=&quot;hornik1991approximation&quot;&gt;&lt;/d-cite&gt;. Applying an activation function ReLU to each element of $Z $, the layer finally outputs&lt;/p&gt; \[A = \text{ReLU}(Z) = \text{ReLU}(W^T X + W_0)\] &lt;p&gt;Next, we will input $A$ into a second hidden layer of the neural network. Let $K$ be the number of classes that the data can possibly belong to. Again, $K = 10$ for MNIST, though we will use $K$ for generality. Then let $W^2$ be an $H$ by $K$ matrix, where $W^2_{ij}$ is the weight of input $i$ applied to node $j$, and let $W^2_0$ be a $K \times 1$ column vector representing the biases added to the weighted input. For input $A$, define a second pre-activation to be a $K \times 1$ vector represented by $B = {W^2}^T A + W^2_0$.&lt;/p&gt; &lt;p&gt;This will yield a $K \times 1$ vector representing the logits of the input image, with which we’ll be able to take Cross Entropy Loss or compute its probability of belonging to any of the $K$ classes.&lt;/p&gt; &lt;h4 id=&quot;training&quot;&gt;Training&lt;/h4&gt; &lt;p&gt;Let class $i $ be the true classification for a data point. We have that $y_i = 1$, and for all $j \neq i$, $y_j = 0$. Furthermore, let $\hat{y_i}$ be the generated probability that the sample belongs to class $i$. The categorical cross-entropy loss is then defined as follows:&lt;/p&gt; \[\mathcal{L}_{CCE} (y_i, \hat{y_i}) = - \sum_{i=0}^{9} y_i \log (\hat{y_i})\] &lt;p&gt;From this computed loss, we use backpropagation and stochastic gradient descent (SGD) with learning rate $\eta = 0.1$ and $momentum = 0.95$ to optimize model weights. We run experiments on a dataset with $n = 4000$ subsamples that train over $100$, $500$, and $2000$ epochs using Belkin et al.’s approach to training &lt;d-cite key=&quot;belkin2019reconciling&quot;&gt;&lt;/d-cite&gt;. Up to interpolation, we train until we reach zero classification error on the training data, or until we have finished all of the epochs, whichever comes first. After we have reached the interpolation thereshold, we train until we have gone through all of the epochs. Note that to get their results, Belkin et al. trained over $6000$ epochs, which proved to be prohibitively expensive given our resources. Instead, we chose to train over a variety of smaller maximum epoch sizes to illustrate the double descent curve taking clearer shape, with the $2000$ epoch run being the most indicative of this phenomena. Below are the results of the trained and tested neural networks. Notice that interpolation consistently happens when the number of parameters is roughly equal to $n\times K$ (i.e. Parameter Count / 1000 $= 40$), and the test loss starts consistently getting lower and lower as we add more and more parameters beyond this threshold. Double descent is real!&lt;/p&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;MLP 100 Epoch Training&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-100-epochs-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;MLP 100 Epoch Testing&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; MLP 100 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 &lt;/div&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;MLP 500 Epoch Training&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-500-epochs-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;MLP 500 Epoch Testing&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; MLP 500 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 &lt;/div&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epoch-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;MLP 2000 Epoch Training&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-15 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/mlp-2000-epochs-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;MLP 2000 Epoch Testing&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; MLP 2000 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 &lt;/div&gt; &lt;p&gt;For the sake of brevity, we avoid including plots for train/test classification loss for the MLPs. However, it is worth noting that train classification loss eventually reaches 0 in all experiments, whereas test loss eventually becomes $\sim$ 0.08 or smaller.&lt;/p&gt; &lt;p&gt;Throughout each experiment, we vary across the number of total parameters of the model. For a network with $H$ hidden units, the total number of parameters is equal to $(d+1)\times H + (H + 1)\times K$, and so we choose $H$ accordingly each time we reparametrize.&lt;/p&gt; &lt;p&gt;Note that we also incorporated a weight reuse scheme for models in the underparametrized regime to cut on training time, similarly to the approach in Belkin et al &lt;d-cite key=&quot;belkin2019reconciling&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Additionally, even though the individual MLPs are small, training several of them sequentially for a relatively large number of epochs can take a very long time. To help reduce the time it takes to complete experiments, we also try adapting the Parameter Count Generation Algorithm provided in John Abascal’s blog &lt;d-cite key=&quot;testbed&quot;&gt;&lt;/d-cite&gt; to intelligently select the next level of parameterization given the performance of the previous one. This algorithm was designed to most clearly showcase the existence of the double descent curve by fitting a third degree polynomial (since that is roughly what we expect the double descent curve to look like) to the model capacity vs. test loss graph, and choosing the next parameter by examining the first derivative of the polynomial fit to the data. More detail is provided in his blog.&lt;/p&gt; &lt;p&gt;This algorithm proved helpful for empirically confirming the existence and validity of the interpolation threshold. However, after a few tests with the algorithm, we chose to complete most of the experiments using a pre-specified list of parameters which were able to consistently capture the double descent phenomena in detail.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;ensemble-learning&quot;&gt;Ensemble Learning&lt;/h2&gt; &lt;p&gt;We experimented with two different types of ensembles. The first ensemble is what we call the ‘weak-learner’ ensemble, which is the model that incorporates the multi-layer perceptron supported by L2-Boost tree ensembles, random forests, decision trees and logistic regression. Note that we ultimately did not use AdaBoost in this ensemble because we believed this was too similar to the included L2-Boost model in both architecture and performance.&lt;/p&gt; &lt;p&gt;The second ensemble is the ‘multi-layer perceptron’ ensemble, which includes 5 MLPs.&lt;/p&gt; &lt;h3 id=&quot;weak-learner-ensemble&quot;&gt;Weak-Learner Ensemble&lt;/h3&gt; &lt;p&gt;We use bootstrap aggregating, or ‘bagging’, to formulate our ensemble of these five models . Effectively, each model is given a certain number of ‘votes’ on what that model believes is the correct classification for any given MNIST sample image. We then experimented with two approaches to voting: hard voting and soft voting.&lt;/p&gt; &lt;p&gt;In hard voting, the classification with the most total votes is then used as the ensemble’s overall output. In the event of a tie, the neural network’s prediction is chosen. Using this voting scheme, we train the MLP independently of the other models in the ensemble, using the same scheme as described previously.&lt;/p&gt; &lt;p&gt;In soft voting, the weighted average of the predicted class probabilities of each model is used as the predicted class probabilities of the ensemble. We utilize this prediction when training the MLP, and use negative log likelihood loss instead of cross entropy loss, since taking the softmax of probabilities is not necessary. This way, we can incorporate the predictions of the whole ensemble into the training of the MLP. Since the ensemble now outputs a vector of class probabilities, the one with the highest probability will be used as the soft voting ensemble’s prediction.&lt;/p&gt; &lt;p&gt;Since we want a neural model to be the basis of our ensemble, we vary the number of votes assigned to the neural network while keeping the number of votes for other models fixed to 1. With four supplementary models in addition to the neural network, giving the neural network 4 or more votes is not necessary, since this ensemble would always output the same results as the neural network. Because of this, we study the loss curve when giving the neural network 1, 2, and 3 votes. Note that decimal value votes for the neural network are not sensible (at least in the hard-voting scheme), since it can be proved that all potential voting scenarios are encapsulated into the three voting levels we have chosen.&lt;/p&gt; &lt;p&gt;Another important aspect of our ensemble is that the ‘weak’ classifiers do not vary in parameterization; only the MLP does. Refitting all the weak classifiers across epochs and MLP parameterizations took much longer than expected, perhaps due to incompatibilities between sklearn and GPUs, and completing the experiments using this approach was unfortunately unfeasible. Hence, all ‘weak’ classifiers have fixed architectures, chosen such that each one has low test error but is not at the highest level of parameterization according to the previous discussion, and only the MLP varies.&lt;/p&gt; &lt;h3 id=&quot;multi-layer-perceptron-ensemble&quot;&gt;Multi-Layer Perceptron Ensemble&lt;/h3&gt; &lt;p&gt;The Multi-Layer Perceptron Ensemble uses 5 identically initialized MLPs which are trained in parallel using Pytorch’s autovectorization capabilities. Since they are defined in the same way and trained simultaneously using the MLP training scheme discussed above, each receives equal weight when it comes to taking an averaged prediction. However, unlike the bagging method used for the Weak-Learner Ensemble, we take advantage of the identical architectures of the models and the numerical stability provided by this, and generate ensemble predictions by averaging the logits of all five learners and using those values as the logits of the ensemble. Again, we experiment using 100 and 500 epochs to see how the behavior evolves across increasing number of epochs, but we omit training over 2000 epochs due to excessive computational costs. An experiment for the future would be training over a very large number of epochs for even greater ensemble sizes to see how results vary across time.&lt;/p&gt; &lt;p&gt;There has been discussion in the past of whether to average the raw logits or the softmax-transformed probabilities. The main concern raised over averaging across raw logits is that the outputted values can vary greatly in magnitude across models (and therefore overconfident models can potentially overshadow all other models when taking the prediction), but, empirically, that doesn’t seem to be a problem here. Tassi et al. provide some intuition in “The Impact of Averaging Logits Over Probabilities on Ensembles of Neural Networks” &lt;d-cite key=&quot;Tassi2022TheIO&quot;&gt;&lt;/d-cite&gt;, suggesting that different approaches to ensembling should be taken depending on the architecture and levels of confidence of the models. For general safety, they recommend averaging the probabilities, but for the purposes of our task, averaging the logits suffices.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;results-and-discussion&quot;&gt;Results and Discussion&lt;/h2&gt; &lt;p&gt;Contrary to our expectations, the Weak Learner Ensemble performs much worse than even the individual models on MNIST classification. Although our focus is on double descent and not on the strong predictive power of ensembles, the latter is needed to observe the former, or at least discuss it at an interesting level.&lt;/p&gt; &lt;p&gt;Initially, we tried applying the soft-voting scheme for the Weak Learner Ensemble, but the reported results are unexpectedly poor, yielding very high classification loss, especially when compared to the results of each model taken individually. This may be because each ‘weak’ learner has high confidence in its predicted class, whereas the MLP may be more evenly split between different classes, which would result in the weak classifiers winning more often, even if the MLP has higher weight in the prediction. The plot of the negative log likelihood loss for both training and testing is also hard to understand, but it is clear the ensemble has a very hard time improving, even as more parameters are added. We only include the results for the ensemble that allocates 3 votes to the MLP, but note that these are the best loss curves we were able to produce given this method.&lt;/p&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Soft-Voting Weak-Learner Ensemble 100 Epoch Training/Testing Negative Log Likelihood Loss Over MLP Parameter Count / 1000. MLP given 3 votes. &lt;/div&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-train-accuracy.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Soft-Voting-3-test-accuracy.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Soft-Voting Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 3 votes. &lt;/div&gt; &lt;p&gt;We then tried the weak-learner approach again with hard-voting, and let the MLP independently train using the unmodified MLP training scheme mentioned previously. However, as opposed to halting training when MLP classifications loss first hits 0, we only halt training when &lt;em&gt;ensemble&lt;/em&gt; classification first hits 0.&lt;/p&gt; &lt;p&gt;We found that while classification loss had certainly gone down when compared to the soft-voting scheme (with even just &lt;em&gt;one&lt;/em&gt; vote!), the ensemble still severely underperformed when compared to each of the individual models used. As seen in the plots, the classification loss starts to improve once the MLP gets more and more votes, agreeing with intuition that, eventually, the MLP has the veto right. As opposed to the soft-voting scheme, all classifiers now have a contribution that is proportional to their voting weight, which mitigates the previous problem of some models having much higher confidence than others. However, we believe the poor results can be attributed to the models we used for ensembling. Indeed, a significant number of models are regular, boosted or ensembled (or all) versions of decision trees, which means there is a significant chance that they make similar mistakes on similar data points. Looking at the plots for overparameterized decision trees and L2-Boost ensembles, we see that train error never quite reaches 0 for any of them. Since the train loss seems to pleateau for our models as well, this may prove why. In the cases of 1 or 2 votes, this can lead to consistently poor predictions, especially since the models are not reparameterized across the experiment. For 3 votes, this phenomenon is less significant, as the ensemble slowly begins to reach the testing performance of the individual models.&lt;/p&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-1-vote-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 1 vote. &lt;/div&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-2-vote-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 2 votes. &lt;/div&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-vote-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/Strong-Classifier-100-Epochs-3-votes-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Weak-Learner Ensemble 100 Epoch Training/Testing Zero-One Loss Over MLP Parameter Count / 1000. MLP given 3 votes. &lt;/div&gt; &lt;p&gt;Further work could be done on the Weak-Learner Ensemble, focusing on better model selection and concurrent reparameterization across all models. Given the limited time and compute resources at our disposal, we leave this problem open for now.&lt;/p&gt; &lt;p&gt;All hope is not lost, however. Seeing the poor performance of the Weak-Learner Ensemble given the significantly better performance of individual models, one could be discouraged from attempting to use ensembling to mitigate double descent, since it may not even be observable in such settings. However, we saw double descent in L2-Boost ensembles and, arguably, in random forests, and so we pushed onward. All other ensemble methods used multiple copies of the same model, and so we decided to experiment with a small ensemble of MLPs, to see how they would behave.&lt;/p&gt; &lt;p&gt;This was feasible for 100 and 500 epochs only, but the obtained results shed light on how ensembling could in fact mitigate double descent. The phenomenon is not quite as observable in the 100 epoch case (one explanation could be that the train loss has not converged yet), but it becomes quite clear when looking at the 500 epoch ensemble and comparing it with the original 500 epoch MLP. Double descent is still very easy to see, ocuring at the same threshold as before. This makes sense, since the MLPs have all reached interpolation, which should increase test loss for all, and then start going down as we overparametrize more and more. However, the main result is that the increase once we reach interpolation is &lt;em&gt;much&lt;/em&gt; lower than before. Indeed, the ensemble sees a jump from $\sim$ 0.35 to around $\sim$ 0.4 at the highest, whereas the individual MLP sees a jump from $\sim$ 0.36 to around $\sim$ 0.52. Another important result is that the loss as we overparameterize becomes &lt;em&gt;significantly&lt;/em&gt; lower in the ensemble model than in the individual MLP.&lt;/p&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-100-epochs-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; MLP Ensemble 100 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 &lt;/div&gt; &lt;div class=&quot;row justify-content-sm-center&quot;&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-train.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-12 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-double_descent/ensemble-500-epochs-test.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; title=&quot;example image&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; MLP Ensemble 500 Epoch Training/Testing Cross-Entropy Loss Over MLP Parameter Count / 1000 &lt;/div&gt; &lt;p&gt;While we weren’t able to fully get rid of the double descent curve by ensembling multiple MLPs, the fact that it became flatter and the loss past the interpolation threshold started to become smaller is quite exciting, as it suggests that, potentially, large ensembles of MLPs may not noticeably suffer from double descent at all, and yield better overall predictions than individual models can. One notable advantage to this ensemble method is the ability to further parallelize one’s training of overparameterized neural networks. These models can take extreme lengths of time to train, and besides increasing the computational allocation used, practitioners may use data, model, or processor parallelism in order to reduce this time. The ensemble neural networks we use are independently generated, meaning that they can be vectorized or trained on different GPU cores without issue. This could be a valid alternative to training for more epochs for reducing model error past the interpolation threshold. More work investigating the effect of neural network ensembling on double descent, especially on models trained over many epochs, would be very exciting and potentially shed even more light on the possible advantages of overparameterization.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;We discussed the existence of double descent for some simple and classical models, observing the effects of varying across levels of parameterization and noting that single descent can sometimes be mistaken for double descent, and proposed the use of various ensembles to mitigate the effects of double descent.&lt;/p&gt; &lt;p&gt;Ensembles consisting solely of neural networks resulted in a considerable boost in performance past the individual model interpolation threshold, and in a flatter curve when compared to individual models. However, pairing the neural network with weak learners in an ensemble voting system decreased testing performance, though this adverse effect waned as the neural network received proportionally more votes. Machine learning engineers that intend to intentionally overparameterize their models may take advantage of not only the ensemble approach’s increased performance and significantly more reliable results, but the enhanced parallelization and vectorization capabilities offered by the proposed method.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt; &lt;p&gt;This project was implemented using Google Colab, which proved to be restrictive for adopting more complex models. A key part of the double descent phenomenon is overparameterization, which happens across multiple full training loops, and so complex models that are additionally overparameterized will require more powerful computing resources beyond what we used. For example, a model which takes 10 hours to complete a single training loop will take multiple days to train before being able to plot results and observe double descent. Even for models that take around 10 to 15 minutes to train, such as the 500 epoch MLP we explored throughout our project, a full experiment that showcases the double descent curve in detail can take upwards of 5 hours. Furthermore, additional computing power can allow for this project to be expanded to more complicated datasets and tasks. MNIST classification is computationally inexpensive, though invoking double descent in more complex tasks such as text generation in natural language processing was not feasible using Google Colab. Future projects that follow this work should keep computational limitations in mind when choosing models and datasets.&lt;/p&gt; &lt;p&gt;In addition to the future work suggested throughout our project, we propose a final approach that we believe is worth exploring further. During the planning process of this project, we discussed using a more rigorous voting system than what is traditionally found in ensemble model projects. Effectively, each model would have a weight associated with how much influence its output should have on the overall ensemble output. For $n$ models, each model could start with, say, a weight of $1/n$. Then, after producing each model’s vector output, the categorical cross-entropy loss with respect the true output could be computed, and the weights of each model could be updated such that each model has its weight decreased by some amount proportional to the calculated loss. Then, these weights could be normalized using the softmax function. This would be repeated for each level of parameterization. Due to resource constraints and the limitations of sklearn to the CPU, learning both the model weights and ensemble weights at each level of ensemble parameterization was not feasible given the size of the models we built and the classifiers we chose to use, as well as the number of epochs we trained over. Future studies may wish to implement this method, however, to produce a more robust ensemble for classification.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;reproducibility-statement&quot;&gt;Reproducibility Statement&lt;/h2&gt; &lt;p&gt;To ensure reproducibility, we have included the codebase used for this project, as well as the above description of our data, models, and methods &lt;d-cite key=&quot;colab&quot;&gt;&lt;/d-cite&gt;. Note that the Colab notebook that we have worked in is currently very messy and sometimes incomplete due to faults in Google’s autosaving feature, but we plan to clean it up and have it available for easy future experimentation.&lt;/p&gt; </content> </entry> <entry> <title>Injecting Node Information via Embedding Initializations</title> <link href="https://deep-learning-mit.github.io/blog/2023/SmartEmbeddingInitializations/"/> <updated>2023-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/SmartEmbeddingInitializations</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Graph Neural Networks (GNNs) have emerged as a transformative tool in machine learning, with the ability to capture the complex structures and relationships inherent in data. In molecular property prediction, for example, GNNs are great at encoding the atomic structure and intermolecular forces into high-dimensional embeddings, leading to more accurate predictions of chemical properties and drug efficacy. GNNs have also be used in traffic time prediction problems, physics simulations and social media analysis applications. Through message-passing and updating, GNNs are capable of learning embeddings that encode informations of node neighbors and long-distance complex connections – that we, as humans, may not be able to make. The quality of the embeddings is not only important for the accuracy of the task the GNN is trained on, but quality node embeddings can be used through transfer learning – enabling models trained on one task to adapt and excel in another. The importance of good embeddings in GNNs is why we want to look closer at embedding initializations and if we can inject additional information – not present in the graph – to result in better learned embeddings after training.&lt;/p&gt; &lt;p&gt;Possible applications of initial embedding initializations could help in the field of drug discovery. For GNNs used for protein retrieval trained on a biomedical knowledge graph, using ESM embeddings for the proteins could add structure information that is not previously encoded in the graph entities.&lt;/p&gt; &lt;h3 id=&quot;project-outline&quot;&gt;Project Outline&lt;/h3&gt; &lt;p&gt;We will explore the question can additional node information be injected into the model by using intentional embedding initializations rather than random initializations? Furthermore, are the learned embeddings better representations of the nodes? To answer this question we will follow the steps outlined below:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We will download a precision medicine knowledge graph that and use a GNN, TxGNN, that is implemented for disease-drug link prediction on a biomedical knowledge graph as a baseline model.&lt;/li&gt; &lt;li&gt;We will modify the GNN for protein-molecular function link prediction.&lt;/li&gt; &lt;li&gt;Generate and download ESM embeddings for each protein&lt;/li&gt; &lt;li&gt;Pretrain and finetune two models – one using random protein node initialization and one using ESM embeddings for protein node initialization. We must pretrain our own models, rather than use the already pretrained model, since we are focusing on how different node initializations impact the predictive power.&lt;/li&gt; &lt;li&gt;Evaluate both models&lt;/li&gt; &lt;li&gt;Visualize latent spaces before pretrain, after pretraining and after finetuning&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;related-work--motivation&quot;&gt;Related Work &amp;amp; Motivation&lt;/h2&gt; &lt;p&gt;In reviewing the literature, we found several papers which reference the possibility of improved performance through a more informed initialization process. As discussed by Li et al., the initialization methods used for GNNs, such as Xavier random initialization were originally designed for CNNs and FNNs. In that setting, the Xavier approach helped to avoid vanishing gradients and maintain a constant information flux. However, Li et al. point out that by leveraging the structure of the graph, we can likely do better than the random intializations used previously &lt;d-cite key=&quot;Li2023&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In the paper detailing TxGNN, Huang et al. present promising results on their ability to predict drug repurposing opportunities using a GNN &lt;d-cite key=&quot;Huang2023&quot;&gt;&lt;/d-cite&gt;. However, in their work they considered only the Xavier random initializations for weight matrices and node embeddings. This left open the idea of initializing the graph using more sophisticated methods.&lt;/p&gt; &lt;p&gt;Previous work by Cui et al. has explored the power of artificial node initializations, finding that encoding structural and positional information in the node initializations can have profound effect on the ability of a GNN to accurately predict features based on the graph. They provide a basis for our investigation by showing the effect that initializations can have on the results, if done correctly. We seek to build on this work by testing the effect of injecting related, but not exactly equivalent information through the node initializations &lt;d-cite key=&quot;Cui2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Not only did we see an opportunity to try a different initialization method, but this problem also lent itself well to data-informed initializations. The molecules in TxGNN have a wealth of knowledge about them which is not represented in the linkages in the graph, some of which is represented in the ESM embeddings of the molecules. Thus, we thought that by supplying these embeddings to the GNN, we might be able to leverage the additional data to make better predictions.&lt;/p&gt; &lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/PrimeKG-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/PrimeKG-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/PrimeKG-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/PrimeKG.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Precision Medicine Knowledge Graph. Figure credit: &lt;i&gt;Building a knowledge graph to enable precision medicine&lt;/i&gt; (Chandak, Huang, Zitnik 2023). &lt;/div&gt; &lt;p&gt;We used a precision medicine knowledge graph (PrimeKG) constructed by Marinka Zitnik’s group at Harvard &lt;d-cite key=&quot;Chandak2023&quot;&gt;&lt;/d-cite&gt;. PrimeKG compiles data from knowledge bases that coverage a broad variety of biomedical information including human disease, drug-protein interactions, genes and proteins with their associated biological processes, functions and cellular component, etc. PrimeKG contains 10 different node types – shown above – and 29 different types of undirected edges. There are over 120,000 nodes in total and over 8 million edges. What PrimeKG lacks, importantly, is any nodes or encodings of structural, molecular or sequenctial information for entity nodes such as proteins and drugs. The node types of interest for our model are proteins, extracted from NCBI, and molecular function Gene Ontology (GO) annotations &lt;d-cite key=&quot;Gene_Ontology_Consortium2021-uk&quot;&gt;&lt;/d-cite&gt;. We will be predicting links between these two node types.&lt;/p&gt; &lt;p&gt;The other data used were ESM embeddings for proteins in PrimeKG. ESM embeddings, or Evolutionary Scale Modeling embeddings, are high-dimensional vector representations of proteins, derived from advanced machine learning models developed by Meta trained on large datasets of protein sequences. These embeddings capture the intricate structural and functional characteristics of proteins, reflecting evolutionary relationships and biochemical properties that are crucial for various biological and computational applications &lt;d-cite key=&quot;Lin2022-esm2&quot;&gt;&lt;/d-cite&gt;. The reason we were interested in using ESM embeddings, rather than embeddings from other protein foundation models, was that structural information was not already captured in PrimeKG, as previously mentioned. To obtain the ESM embeddings, first we downloaded the amino acid sequence for each protein from NCBI using Entrez. Then, using these sequences as input to Facebook’s ESM2 model, we extracted the corresponding embedding.&lt;/p&gt; &lt;h2 id=&quot;gnn&quot;&gt;GNN&lt;/h2&gt; &lt;p&gt;The model we used as a baseline is TxGNN, a graph neural network trained on PrimeKG used to make therapeutic drug predictions for diseases &lt;d-cite key=&quot;Huang2023&quot;&gt;&lt;/d-cite&gt;. The GNN has two training phases. First, pretraining where the GNN finds biologically meaningful embeddings for all nodes in the knowledge graph, and therefore the objective is all link prediction. The second phase is to finetune the GNN, using self-supervised learning, to be able to predict drugs for diseases. Therefore, the objective for finetuning is to optimize contraindication and indication link prediction – the two types of links between diseases and drugs. We modified the training code for the finetuning phase, to train and validate on protein-molecular function links instead.&lt;/p&gt; &lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt; &lt;p&gt;The GNN has two linear layers with parameters n_input, n_hidden, and n_output. For all our models n_input is 1280, restricted by the length of ESM embeddings. We play around with different dimensions for the hidden and output layers. Leaky ReLU activation is used after the first layer.&lt;/p&gt; &lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt; &lt;p&gt;The first step of the training phase is &lt;strong&gt;node embedding initialization&lt;/strong&gt;. The default, which is our random control, is to initialize all nodes using Xavier uniform initialization &lt;d-cite key=&quot;pmlr-v9-glorot10a&quot;&gt;&lt;/d-cite&gt;. Models referred to as &lt;em&gt;random&lt;/em&gt; from here on out are referring to using Xavier uniform initialization. For our experimental model, we initialized the protein nodes using the ESM embeddings we obtained earlier. All other node types were still initialized with Xavier uniform initialization. Note that we reinitialized nodes between pretraining and finetuning.&lt;/p&gt; &lt;p&gt;During the training phase, the GNN uses a standard message-passing algorithm to update and optimize the node embeddings. There is a relation-type specific weight matrix (for each of the 29 relation types) used to calculate relation-type specific messages. The message for one relation to the some node $i$ is calculated using this equation: \begin{equation} m_{r, i}^{(l)} = W_{r, M}^{(l)} h_i^{(l-1)} \end{equation}&lt;/p&gt; &lt;p&gt;For each node $v_i$, we aggregate incoming messages from neighboring nodes for each relation-type $r$, denoted as $N_r(i)$. This is done by taking the average of these messages: \begin{equation} m_{g_r, i}^{(l)} = \frac{1}{|N_r(i)|} \sum_{j \in N_r(i)} m_{r, j}^{(l)} \end{equation}&lt;/p&gt; &lt;p&gt;The new node embedding is then updated by combining the node embedding from the last layer and the aggregated messages from all relations: \begin{equation} h_i^{(l)} = h_i^{(l-1)} + \sum_{r \in TR} m_{g_r, i}^{(l)} \end{equation}&lt;/p&gt; &lt;p&gt;Finally, DistMult &lt;d-cite key=&quot;Yang2014-zb&quot;&gt;&lt;/d-cite&gt; is used to calculate link prediction between two nodes using their respective embeddings.&lt;/p&gt; &lt;h2 id=&quot;model-evaluation&quot;&gt;Model Evaluation&lt;/h2&gt; &lt;p&gt;We fixed all parameters and hyperparameters, and trained two models – one using random initializations and one using ESM embeddings. We pretrained for 3 epochs with a learning rate of $1e-3$ and a batch size of 1024. We then finetuned for 150 epochs with a learning rate of $5e-4$.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/Figure2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/Figure2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/Figure2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/Figure2.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;These results are promising and using ESM embeddings to initialize the protein node representations slightly improves the model. The ESM model has a final testing loss of 0.3915, whereas the random model has a final testing loss of 0.4151. However, the difference between the models is slim and may not be significant, especially looking at the similarities in the pretraining, training and validation loss curves. Later, we will look more in depth about how the embedding spaces vary between the 2 models which has the potential to yield more interesting results.&lt;/p&gt; &lt;h3 id=&quot;testing-varying-hidden-and-output-layer-dimensions&quot;&gt;Testing varying hidden and output layer dimensions&lt;/h3&gt; &lt;p&gt;We wanted to see the impact changing the hidden and output layer dimensions would have on model performance. We tested 3 models, with parameters detailed in Table 1. All models outside of this experiment, unless otherwise specified, have the same parameters as Model 1.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;Input Dimensions&lt;/th&gt; &lt;th&gt;Hidden Layer Dim.&lt;/th&gt; &lt;th&gt;Output Layer Dim.&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Model 1&lt;/td&gt; &lt;td&gt;1280&lt;/td&gt; &lt;td&gt;1280&lt;/td&gt; &lt;td&gt;1280&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Model 2&lt;/td&gt; &lt;td&gt;1280&lt;/td&gt; &lt;td&gt;512&lt;/td&gt; &lt;td&gt;512&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Model 3&lt;/td&gt; &lt;td&gt;1280&lt;/td&gt; &lt;td&gt;512&lt;/td&gt; &lt;td&gt;128&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/Testing_output_dim-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/Testing_output_dim-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/Testing_output_dim-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/Testing_output_dim.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We can see from the testing loss that when just comparing ESM initialized model, testing loss increases as the output layer decreases. The same trend holds true between random initialized models. We can also see that when comparing ESM and random models for the same layer dimensions, ESM always slightly outperforms the random model.&lt;/p&gt; &lt;h2 id=&quot;latent-space-visualizations&quot;&gt;Latent Space Visualizations&lt;/h2&gt; &lt;p&gt;In the fast-evolving world of deep learning, the analysis of model latent spaces has emerged as an interesting area of study, especially to get a better understanding of how models are achieving their tasks. These spaces are important to understanding how complex models like GNNs perceive and process the intricate relationships and structures inherent in graph data. GNNs can learn powerful representations that capture both node-level and graph-level features. By analyzing the latent spaces of GNNs, we can get insights into how these models prioritize various patterns and connections within the data. The following analyses visualize the latent spaces our models, clustered and colored in different ways, to get a deeper understanding of how the ESM initialized embeddings are effecting the GNN.&lt;/p&gt; &lt;p&gt;We first were curious whether, after training our model, the final embeddings retained structural information about the proteins. To do this, we first clustered the original ESM embeddings using K-Means clustering. Next, we visualized the embedding space of the original ESM embeddings, the final embeddings from the ESM model and the final embeddings from the random model using t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction. From the t-SNE plot of original ESM embeddings, we can clearly see the clusters from K-Means which serves as a verification of our clustering technique.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/init_cinit-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/init_cinit-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/init_cinit-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/init_cinit.jpeg&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Looking at the embedding space for the ESM and random models, colored by ESM clusters, we note that most of the ESM structural information (used to create the clusters) seems to be forgotten during the training process, as evidenced by the mostly random assortment of colors present in the t-SNE plot. We note that some clusters do remain, for example cluster 12 (light sage green on the right side of the ESM initialized plots) is still clustering in the final embeddings (top middle cluster). However, the most prominent ones appear in both the ESM initialized and random initialized data, meaning that the ESM embedding did encode some function, but the model using random initialized embeddings was able to capture that relation as well.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/cluster_init-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/cluster_init-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/cluster_init-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/cluster_init.jpeg&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Given that the final embedding space for the ESM model didn’t seem to retain much of the information for the ESM embedding initialization, we were curious whether the ESM and random embeddings converged to a similar space. To test this theory, we clustered the final ESM model embeddings and subsequently visualized the final embeddings of the ESM and random models using t-SNE and colored by those clusters.&lt;/p&gt; &lt;p&gt;If the two models converged to similar embedding spaces, we’d expect to see that clusters found in one embedding space would also be found in the other. This is the case, as seen in the two plots below. Both plots are colored based on a clustering of the final embeddings generated by the ESM initialized network, and they share many of the same structures, indicating that the two networks were able to pick up on mostly the same features in the underlying information. Both models converged to a similar embedding space different initialization methods.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/cluster_esm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/cluster_esm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/cluster_esm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/cluster_esm.jpeg&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;testing-varying-hidden-and-output-layer-dimensions-1&quot;&gt;Testing varying hidden and output layer dimensions&lt;/h3&gt; &lt;p&gt;As mentioned earlier, we tested different dimensions for the hidden and output layers to see whether more and less output dimensions would retain the original ESM embedding information.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/dimensions-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/dimensions-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/dimensions-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/dimensions.jpeg&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Although there are more distinct clusters on the t-SNE plots as the number of output dimensions increases, these clusters are not the same as the clusters from the original ESM embeddings (seen by the randomly colored dots). Therefore, neither of these 3 models retained the structural information provided by initializing with ESM embeddings. It does not seem that decreasing output and hidden layer dimensions improves the model performance or latent space of our GNN.&lt;/p&gt; &lt;h3 id=&quot;clustering-by-molecular-function-labels&quot;&gt;Clustering by molecular function labels&lt;/h3&gt; &lt;p&gt;Because our model’s task was to predict links between protein and molecular function nodes, we were curious to see if the final embeddings for the protein nodes would cluster well on the function labels. However, this wasn’t as straight forward as having 1 molecular function label for each protein node, because each protein may be linked to multiple molecular functions. One protein may have multiple molecular function Gene Ontology (GO) annotations because the GO database uses a hierarchical system to categorize functions, where broader functions encompass more specific ones. A protein can be involved in several distinct biochemical activities, each represented by its own GO term, reflecting the diverse roles a protein can play in the cell. Instead of a single label, we extracted a molecular function profile, $v_i$, for each protein where $v_i[j] = 1$ if a link exists between protein $i$ and function $j$. We then had a sparse matrix, $V^{i \times j}$. Before clustering, we performed dimensionality reduction using truncated SVD which is optimal for sparse matrices. Finally, we performed K-Means clustering.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/cluster_func-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/cluster_func-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/cluster_func-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/cluster_func.jpeg&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Looking at the t-SNE plots, there is no apparent clustering by molecular function profiles in the final embedding spaces for either the ESM model or the randomly initialized model. There are multiple possible explanations for this. One explanation is that the actual objective is to prediction each singular link between a protein and a function node, not to predict do well at predict all function nodes linked to a protein at once. On top of that our GNN uses self-supervised learning, therefore the molecular function profiles are not true labels used during training.&lt;/p&gt; &lt;p&gt;The second plausible explanation has to do once again with the hierarchical nature of molecular function GO annotations. Because the molecular function nodes have random indices when stored in PrimeKG, it is not apparent that molecular function that have the same parent function are close to each other, or their parent function in the molecular function profiles. Therefore, when performing truncated SVD and subsequently k-means clustering, the similar functions may not be clustered together if their indices are far apart. Further analysis could be done to reorder the molecular function nodes and then conduct hierarchical clustering, instead than k-means. These possible clusters may then be found in the final latent spaces for the two models.&lt;/p&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;In this post, we have modified and fine-tuned a Graph Neural Network, TxGNN originally designed for drug-repurposing prediction, for protein function prediction with a variety of initializations of the node embeddings. We observed that while much of the information in the initialization is forgotten during the training process, a small amount is retained, leading to slightly better performance on the test set in the final network. This provides a potential avenue for further study, investigating the overall effects of informed initialization techniques on GNN performance. Some of this investigation is discussed in Li et al. &lt;d-cite key=&quot;Li2023&quot;&gt;&lt;/d-cite&gt;, where they experiment with weight matrix initializations and propose a new paradigm for determining weight initializaiotns, but there is certainly more investigation to be done.&lt;/p&gt; </content> </entry> <entry> <title>Overparameterization of Neural Networks through Kernel Regression and Gaussian Processes</title> <link href="https://deep-learning-mit.github.io/blog/2023/overparameterization/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/overparameterization</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;!-- ideas for experiments: - visualization of functions learned by MLP of different widths/kernels/GP on a fixed dataset - figure on the connection between 3 methods - comparison test error as # of samples increase for a fixed dataset - table of r^2 for genomic (high-dimensional) data --&gt; &lt;p&gt;In this work, we will explore the successes of overparameterization of neural networks through evaluating the relationship between the Neural Tangent Kernel (NTK), MLPs, and Gaussian processes. Recent work has shown that overparameterized neural networks can perfectly fit the training data yet generalize well enough to test data. This was formalized as “the double descent curve” &lt;d-cite key=&quot;belkin2019reconciling&quot;&gt;&lt;/d-cite&gt;, which suggests that increasing model capacity results in improved performance.&lt;/p&gt; &lt;p&gt;To help elucidite our understanding of neural networks as the width increases, I wanted to understand the connections between neural networks, which are often regarded as “black boxes,” and other classes of statistical methods, such as kernels and NNGPs. My goal is to put neural networks in the greater contexts of statistical machine learning methods that are hopefully easier to reason with and interpret.&lt;/p&gt; &lt;h3 id=&quot;literature-review&quot;&gt;Literature Review&lt;/h3&gt; &lt;p&gt;There is already prior literature on the connections between these three classes of models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Kernel Regression $\iff$ MLPs: This connection was introduced in &lt;d-cite key=&quot;jacot2018neural&quot;&gt;&lt;/d-cite&gt;. In particular, they proved that the limit of a neural network as width approaches infinity is equivalent to kernel regression with the Neural Tangent Kernel (NTK).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;MLP $\iff$ Gaussian Processes: The connection for infinitely-wide one-layer neural networks was introduced in &lt;d-cite key=&quot;neal1996priors&quot;&gt;&lt;/d-cite&gt; and for deep networks in &lt;d-cite key=&quot;lee2017deep&quot;&gt;&lt;/d-cite&gt;. This comes from the observation that if the weights are sampled Gaussian i.i.d., then the Central Limit Theorem states that as the width approaches infinity, the output is also Gaussian. We also went over this briefly in class.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Gaussian Processes $\iff$ Kernel Regression: Other than the obvious fact that they both use kernels and the “kernel trick,” I could not really find a resource that established a clear connection between the two other than through the intermediary of MLPs. In this project, this is one link that I will try to explicitly establish.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Other relevant prior works I reviewed include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The formalization of the double descent curve in &lt;d-cite key=&quot;belkin2019reconciling&quot;&gt;, which uprooted our previous understanding of the bias-variance tradeoff and the notion that models should not overfit. This also motivates the use of infinite-wide neural networks (extreme overparameterization) for prediction tasks. Otherwise, conventional wisdom would say that these models overfit.&lt;/d-cite&gt;&lt;/li&gt; &lt;li&gt;Why is this problem even interesting? This paper &lt;d-cite key=&quot;radhakrishnan2022simple&quot;&gt; shows that kernels achieve competitive performance for important matrix completion tasks, so neural networks are not necessarily the only solution to many tasks of interest.&lt;/d-cite&gt;&lt;/li&gt; &lt;li&gt;The lecture notes from &lt;a href=&quot;https://web.mit.edu/modernml/course/&quot;&gt;this IAP class&lt;/a&gt;. I used some of the notation, definitions, and theorems from the lecture notes to write this post, but I also worked through some of the math on my own (e.g. the overparameterized linear regression proof for general $\eta$ and $w^{(0)}$, proving that $X^\dagger$ minimizes $\ell_2$ norm, etc.).&lt;/li&gt; &lt;li&gt;I also used &lt;a href=&quot;https://lilianweng.github.io/posts/2022-09-08-ntk/&quot;&gt;this blog&lt;/a&gt; to better understand the intuition behind NTKs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The gaps in prior knowledge I want to tackle include (1) the explicit connection between GPs and kernel regression and (2) how sparsity of kernel regression can help explain the generalization abilities of neural networks.&lt;/p&gt; &lt;h3 id=&quot;my-contributions&quot;&gt;My Contributions&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;The explicit connections between kernel regression, MLPs, and Gaussian Processes (GP), particularly kernel regression and GP.&lt;/li&gt; &lt;li&gt;How properties of overparameterized linear/kernel regression can help us understand overparameterization of neural networks, particularly the regularization of the weights.&lt;/li&gt; &lt;li&gt;Empirical demonstrations of the theory developed here.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To start, I work through the math to understand overparameterization in linear regression and connect the results to overparameterization in kernel regression.&lt;/p&gt; &lt;h3 id=&quot;overparameterization-in-linear-regression&quot;&gt;Overparameterization in Linear Regression&lt;/h3&gt; &lt;p&gt;Linear regression involves learning a predictor of the form $\hat{f}(x) = wx$, where $w \in \mathbb{R}^{1 \times d}, x \in \mathbb{R}^{d \times 1}$. Much like neural networks, we find $\hat{w}$ by minimizing the Mean Squared Error (MSE) of the prediction $\hat{f}$ from the target $y \in \mathbb{R}$ across all $n$ samples: \(\mathcal{L}(w) = \frac{1}{2}||y - \hat{f}(x)||_2^2\)&lt;/p&gt; &lt;p&gt;Without knowing much about the relationship between $n$ and $d$, it is not obvious that there is a closed form solution to this system of equations. Of course, if $n = d$ (and $X$ is full rank), then we can directly solve for $w$. Specifically, if $Y \in \mathbb{R}^{1 \times n}$, $X \in \mathbb{R}^{d \times n}$, $w \in \mathbb{R}^{1 \times d}$, then \(Y = wX \implies w = YX^{-1}.\)&lt;/p&gt; &lt;p&gt;What about when $n &amp;lt; d$ (underparameterized regime) or $n &amp;gt; d$ (overparameterized regime)? We need to turn to gradient descent then, \(w^{(t+1)} = w^{(t)} - \eta \nabla_w \mathcal{L}w^{(t)}.\) We can actually explicitly characterize the conditions for convergence and its limit for different values of the learning rate $\eta$ and initialization $w^{(0)}$. Namely, let us start with \(w^{(t+1)} = w^{(t)} - \eta \nabla_w \mathcal{L}(w^{(t)}) = w^{(t+1)} = w^{(t)} - \eta (-(y - w^{(t)}X))X^\top = w^{(t)} + \eta (y - w^{(t)}X)X^\top\) Using this equation, we can derive a closed form expression for $w^{(t)}$. \(\begin{align*} w^{(t+1)} &amp;amp;= w^{(t)} + \eta (y - w^{(t)}X)X^\top = w^{(t)} +\eta yX^\top - \eta w^{(t)} XX^\top = w^{(t)}(I - \eta X X^\top) + \eta y X^\top \\ w^{(1)} &amp;amp;= w^{(0)} (I - \eta XX^\top) + n y X^\top\\ w^{(2)} &amp;amp;= w^{(0)} (I - \eta XX^\top)^2 + n y X^\top(I - \eta XX^\top) + n y X^\top\\ w^{(3)} &amp;amp;= w^{(0)} (I - \eta XX^\top)^3 + n y X^\top(I - \eta XX^\top)^2 + n y X^\top(I - \eta XX^\top) + n y X^\top\\ &amp;amp;\dots\\ \end{align*}\) Let $A = (I - \eta XX^\top)$, $B = nyX^\top$, and $X = U\Sigma V^\top$ be the singular value decomposition of $X$ where $\sigma_1 \geq \dots \geq \sigma_r$ are the non-zero singular values. Then \(\begin{align*} w^{(t)} &amp;amp;= w^{(0)}A^\top + BA^{t-1} + BA^{t-2} + \dots + BA + B = w^{(0)}A^\top + B(A^{t-1} + A^{t-2} + \dots + A + I) = w^{(0)} A^t + (nyX^\top)(UU^\top + U(I - n\Sigma^2)U^\top + \dots + U(I - n\Sigma^2)^{t-1}U^\top) \\ &amp;amp;= w^{(0)} A^t + (nyX^\top)U(I + (I - n\Sigma^2) + \dots + (I - n\Sigma^2)^{t-1})U^\top = w^{(0)}(I - n XX^\top)^t + nyX^\top U\begin{bmatrix} \frac{1 - (1 - \eta\sigma_1^2)^t}{n\sigma_1^2} &amp;amp; &amp;amp; &amp;amp;\\ &amp;amp; \frac{1 - (1 - \eta\sigma_2^2)^t}{n\sigma_2^2} &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 0 \end{bmatrix}U^\top \end{align*}\\\) From this equation, we can derive many insights into the conditions for convergence. In particular, if we want the RHS to converge, we require $|1 - \eta \sigma_1^2| &amp;lt; 1 \implies -1 &amp;lt; 1 - \eta\sigma_1^2 &amp;lt; 1$. Thus, when $\eta &amp;lt; \frac{2}{\sigma_1^2}$ (which implies $\eta \leq \frac{2}{\sigma_2^2}, \eta \leq \frac{3}{\sigma_3^2}, \dots$), gradient descent for linear regression converges.&lt;/p&gt; &lt;p&gt;With this condition on $\eta$, we can further characterize $w^{(\infty)}$. \(\begin{align*} w^{(\infty)} &amp;amp;= \lim_{t \rightarrow \infty} w^{(0)}(I - \eta XX^\top)^t + n yX^\top U \begin{bmatrix} \frac{1}{n\sigma_1^2} &amp;amp; &amp;amp; &amp;amp;\\ &amp;amp; \frac{1}{n\sigma_2^2} &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 0 \end{bmatrix}U^\top = \lim_{t \rightarrow \infty} w^{(0)}(UU^\top - \eta U \Sigma^2 U^\top)^t + yV\Sigma^\top U^\top U \begin{bmatrix} \frac{1}{\sigma_1^2} &amp;amp; &amp;amp; &amp;amp;\\ &amp;amp; \frac{1}{\sigma_2^2} &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 0 \end{bmatrix}U^\top \\ &amp;amp;= \lim_{t \rightarrow \infty} w^{(0)}U(I - \eta \Sigma^2)^tU^\top + yV\Sigma^\top \begin{bmatrix} \frac{1}{\sigma_1^2} &amp;amp; &amp;amp; &amp;amp;\\ &amp;amp; \frac{1}{\sigma_2^2} &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 0 \end{bmatrix}U^\top = w^{(0)}U\begin{bmatrix} 0 &amp;amp; &amp;amp; &amp;amp;\\ &amp;amp; 1 &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 1 \end{bmatrix}U^\top + yV\begin{bmatrix} \frac{1}{\sigma_1} &amp;amp; &amp;amp; &amp;amp;\\ &amp;amp; \frac{1}{\sigma_2} &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 0 \end{bmatrix}U^\top =w^{(0)}U\begin{bmatrix} 0 &amp;amp; &amp;amp; &amp;amp;\\ &amp;amp; 1 &amp;amp; &amp;amp;\\ &amp;amp; &amp;amp; \ddots &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 1 \end{bmatrix}U^\top + yX^\dagger \\ \end{align*}\) Note the dependency on this result on $w^{(0)}$. If $w^{(0)} = 0$, then $w^{(\infty)} = yX^\dagger$. Furthermore, we can also prove that $w = yX^\dagger$ is the minimum $\ell_2$ solution. Suppose there exists another solution, $\tilde{w}$. If $Xw = X\tilde{w}$, then $\tilde{w} - w \perp w$ because \((\tilde{w} - w)w^\top = (\tilde{w} - w)w^\top = (\tilde{w} - w)(y(X^\top X)^{-1}X^\top)^\top = (\tilde{w}-w)X(X^\top X^{-1})^\top y^\top = 0\) Thus, \(\|\tilde{w}\|_2^2 = \|\tilde{w} - w + w\|_2^2 = \|\tilde{w} - w\|_2^2 + \|w\|_2^2 + 2(\tilde{w}-w)w^\top = \|\tilde{w} - w\|_2^2 + \|w\|_2^2 \geq \|w\|_2^2.\)&lt;/p&gt; &lt;p&gt;This characterization is consistent when $n = d$, $n &amp;lt; d$, and $n &amp;gt; d$. If $n = d$, then $X^\dagger = (X^\top X)^{-1} X^\top = X^{-1}(X^{\top})^{-1} X^\top = X^{-1}$. When $n &amp;gt; d$ and the rank of $X$ is $d$, then when $\nabla_w \mathcal{L}(w) = 0$, then $(y-wX)X^\top = 0 \implies w = yX^\top(XX^\top)^{-1}$. $XX^\top \in \mathbb{R}^{d \times d}$ is invertible since $X$ is full rank, so $w = yX^\top(XX^\top)^{-1} =y(X^\top X)^{-1}X^\top = yX^\dagger$.&lt;/p&gt; &lt;p&gt;We are particularly interested in the overparameterized regime, i.e. when $n &amp;gt; d$. The results above show that when $w^{(0)} = 0$, even though there are an infinite number of $w$ that satisfy $y = Xw$, gradient descent converges to the minimum $\ell_2$-norm solution, $w = yX^\dagger$. This sparsity may help prevent overparameterization even when there are enough parameters to fully memorize the input data.&lt;/p&gt; &lt;p&gt;Why is this analysis helpful? This characterization may help us understand the solution obtained by kernel regression, which can be viewed as just linear regression on a nonlinear, high-dimensional space.&lt;/p&gt; &lt;h3 id=&quot;overparameterization-in-kernel-regression&quot;&gt;Overparameterization in Kernel Regression&lt;/h3&gt; &lt;p&gt;We will start with a brief definition of kernel regression. Intuitively, kernel regression is running linear regression after applying a non-linear feature map, $\psi$, onto the datapoints $x \in \mathbb{R}^{d}$. Formally, we require that $\psi: \mathbb{R}^{d} \rightarrow \mathcal{H}$, $w \in \mathcal{H}$, and the predictor $\hat{f}: \mathbb{R}^{d} \rightarrow \mathbb{R}$ to take the form $\langle w, \psi(x)\rangle_{\mathcal{H}}$, where $\mathcal{H}$ is a Hilbert space. A Hilbert space is a complete metric space with an inner product. Intuitively, Hilbert spaces generalize finite-dimensional vector spaces to infinite-dimensional spaces, which is helpful for us because this allows for infinite-dimensional feature maps, an extreme example of overparameterization. All the finite-dimensional inner product spaces that are familiar to us, e.g. $\mathbb{R}^n$ with the usual dot product, are Hilbert spaces.&lt;/p&gt; &lt;p&gt;At first glance, it might seem impossible to even store the weights of infinite-dimensional feature maps. However, this problem is resolved by the observation that weights from solving linear regression will always a linear combination of the training samples. In particular, since $yX^\dagger$ has the same span as $X$, we can always rewrite the weights as $w = \sum_{i=1}^n \alpha_i x_i^\top$, where $x_i$ denotes the $i$ th sample. What’s really interesting is that this can be extended to kernels as well.&lt;/p&gt; &lt;p&gt;Specifically, for kernel regression, we seek a solution to the MSE problem: \(\mathcal{L}(w) = \|y-\hat{x}\|_2^2 = \|y-\langle w,\psi(x)\rangle\|_2^2.\)&lt;/p&gt; &lt;p&gt;We know that the weights must take the following form, \(w = \sum_{i=1}^n \alpha_i \psi(x_i).\)&lt;/p&gt; &lt;p&gt;Thus, expanding out the loss function, we have that \(\mathcal{L}(w) = \frac{1}{2}\|y-\langle w, \psi(x)\rangle\|_2^2 = \frac{1}{2}\|y-\langle \sum_{i=1}^n \alpha_i \psi(x_i), \psi(x_i)\rangle\|_2^2 = \frac{1}{2}\sum_{j=1}^n (y_j -\langle \sum_{i=1}^n \alpha_i \psi(x_i), \psi(x_j)\rangle)^2 = \frac{1}{2}\sum_{j=1}^n (y_j -\langle \alpha, \begin{bmatrix} \langle \psi(x_1), \psi(x_j) \rangle \\ \langle \psi(x_2), \psi(x_j) \rangle \\ \vdots \\ \langle \psi(x_n), \psi(x_j) \rangle \\ \end{bmatrix}\rangle)^2.\)&lt;/p&gt; &lt;p&gt;Thus, rather than storing the weights $w$ that act on the feature map directly, we just need to store $\alpha$, the weights acting on the samples. Moreover, another observation from this equation is that we don’t even need to define the feature map directly. We only need to store the inner product of each sample with every other sample. Formally, this inner product is called a kernel ($K: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$). With a slight abuse of notation, we will also use $K$ to denote the matrix of inner products, $K(X,X)$.&lt;/p&gt; &lt;p&gt;Much like our discussion in class on Gaussian Processes (GP), kernels can be thought of as a “distance” or “covariance” function on samples. Some well-known kernels include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gaussian kernel: $K(x,\tilde{x}) = \exp(|x - \tilde{x}|_2^2)$&lt;/li&gt; &lt;li&gt;Laplacian kernel: $K(x,\tilde{x}) = \exp(|x - \tilde{x}|_2)$&lt;/li&gt; &lt;li&gt;Neural Tangent kernel with ReLU activation: $K(x,\tilde{x}) = \frac{1}{\pi}(x^\top \tilde{x}(\pi - \arccos(x^\top \tilde{x})) + \sqrt{1 - (x^\top \tilde{x})^2}) + x^\top \tilde{x}\frac{1}{\pi}(\pi - \arccos(x^\top \tilde{x}))$&lt;/li&gt; &lt;li&gt;Linear kernel: $K(x,\tilde{x}) = x^\top \tilde{x}$&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The linear kernel is equivalent to linear regression, and (as we will explore later), the Neural Tangent kernel with ReLU activation approximates an infinitely wide neural network with $\phi(z) = \sqrt{2}\max(0,z)$ activation.&lt;/p&gt; &lt;p&gt;Note also that all of these kernels, however finite, represent infinite-dimensional feature maps. For example, the feature map for the Gaussian kernel is $\psi(x) = \Big(\sqrt{\frac{(2L)^m}{p_1!p_2!\dots p_d!}}x_1^{p_1}x_2^{p_2}\dots x_d^{p_d}\Big)_{p_1,p_2,\dots,p_d \in \mathbb{N} \cup {0}}.$ It is remarkable that kernel regression even does well in practice considering it works in an extremely over-parameterized regime.&lt;/p&gt; &lt;p&gt;However, our analysis using linear regression may shed some light on why. In particular, recall that our loss function is \(\mathcal{L}(w) = \frac{1}{2}\sum_{j=1}^n (y_j - \alpha K(X,X)).\)&lt;/p&gt; &lt;p&gt;Since this is just linear regression in $\mathcal{H}$, gradient descent converges to $\alpha = yK^\dagger$ if $\alpha^{(0)} = 0$. This means the predictor for kernel regression looks like \(\hat{f}(x) = \alpha K(X,x) = yK^{\dagger}K(X,x).\)&lt;/p&gt; &lt;p&gt;Since $K(X,X)$ is a square matrix, (technically, $n = d$ from the linear regression case), this equation can be solved directly. Moreover, $\alpha$ is the minimum $\mathcal{H}$-norm solution, just like how the weights from the linear regression model is the minimum $\ell_2$-norm solution.&lt;/p&gt; &lt;p&gt;The ability to be solved in closed form is an important property of kernel regression. In practice, $\alpha^{(0)}$ cannot be initialized to $0$ in gradient descent, so neural networks do not necessarily converge to the minimum-norm solution that kernels do. This may offer some explanation for the predictive ability of kernels on tabular data.&lt;/p&gt; &lt;p&gt;Now, let us formally define the Neural Tangent Kernel. The NTK for a neural network is defined as the outer product of the gradients of the network’s output with respect to its parameters, averaged over the parameter initialization distribution. Formally, if $f(x; w)$ is the output of the network for input $ x $ and parameters $ w $, the NTK is given by:&lt;/p&gt; \[K_{\text{NTK}}(x, \tilde{x}) = \mathbb{E}_{w}\left[\left\langle \frac{\partial f(x; w)}{\partial w}, \frac{\partial f(\tilde{x}; w)}{\partial w} \right\rangle\right].\] &lt;p&gt;The intuition for this comes from understanding how parameters change in neural networks during gradient descent.&lt;/p&gt; &lt;p&gt;In particular, note that \(\frac{df(x;w)}{dt} = \frac{df(x;w)}{dw} \frac{dw}{dt} \approx \frac{df(x;w)}{dw} (-\nabla_w \mathcal{L}(w)) = -\frac{1}{N}\sum_{i=1}^N \underbrace{\nabla_w f(x;w)^\top \nabla_w f(x_i;w)}_{NTK} \nabla_f\mathcal{L}(f,y_i).\)&lt;/p&gt; &lt;p&gt;From this equation, we see that during gradient descent, the network $f$ changes based on its effect on the loss function weighted by the “covariance”/”distance” of $x$ w.r.t. the other samples. The intuition for the NTK thus comes from the way that the neural network evolves during gradient descent.&lt;/p&gt; &lt;d-cite key=&quot;jacot2018neural&quot;&gt;&lt;/d-cite&gt; &lt;p&gt;established that training an infinite-width neural network $f(x;w)$ with gradient descent and MSE loss is equivalent to kernel regression where the kernel is the NTK.&lt;/p&gt; &lt;p&gt;To further understand the connections between the NTK and wide neural networks, I benchmarked the performance of wide neural networks and the NTK on the task of predicting the effects of a gene knockout on a cell.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 1. Experiment workflow.&lt;/p&gt; &lt;p&gt;All the datasets are publicly available on &lt;a href=&quot;https://depmap.org/portal/&quot;&gt;DepMap&lt;/a&gt; and I processed the data the same way as I did in &lt;d-cite key=&quot;cai2023synthetic&quot;&gt;&lt;/d-cite&gt; (but the experimental results I describe here are new). In short, I have 998 cells embedded with a 30,000-dimensional vector of the expression and mutation status of genes in the cell. The target task is to predict the effect of knocking out the gene KRAS in each cell. The “effect” is a scalar that indicates how alive the cell is, with negative values indicating that the cell is more dead and positive values indicating that the cell is more alive.&lt;/p&gt; &lt;p&gt;Biological datasets are well-suited for the analysis of overparameterized models because the embeddings are by default extremely high-dimensional, i.e. $d » n$. However, since I want to test the effects of increasing the width of neural networks and I do not want the shape of the weight matrix to be $\lim_{k \rightarrow \infty}\mathbb{R}^{30,000 \times k}$, I reduced the computational complexity of this problem by first running PCA on the cell embedding to reduce $d$ to $500$. Thus, $X \in \mathbb{R}^{998 \times 500}$ and $Y \in \mathbb{R}^{998 \times 1}$. I did a simple 80/20 training/test split on the data, so $X_{train} \in \mathbb{R}^{798 \times 500}$ and $X_{test} \in \mathbb{R}^{200 \times 500}$.&lt;/p&gt; &lt;p&gt;I then benchmarked a one hidden layer MLP, i.e. $A\phi(Bx)$ with ReLU activation, where $A \in \mathbb{R}^{k \times 1}, B \in \mathbb{R}^{500 \times k}$, as $k$ ranged from ${10,110,210,\dots,9,910}$. I also benchmarked the NTK on the same task. There are several interesting insights from this experiment.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The NTK always exactly fits the training data by construction because we directly solve the MSE problem.&lt;/li&gt; &lt;li&gt;The MSE of a neural network as $k$ increases approaches the MSE of the NTK, which aligns with the theory. However, I want to note that if I shrink $d$, i.e. if I take $d = 10$ or $d=100$, the second point does not always hold. In those cases, the MSE of the NTK is much larger than the MSE of the neural network. That was a bit counterintuitive, but one explanation could be that the NTK is a poor approximation for the neural network in those cases because the neural network cannot be linearized when it is changing so drastically based on the small set of features.&lt;/li&gt; &lt;li&gt;The MSE asymptotically decreases as $k \rightarrow \infty$. This aligns with the theory of the double-descent curve. It would be interesting to test if the weights learned by the MLP enforces some sort of sparsity, e.g. by plotting $\frac{|A|_2}{|x|_2}$, where $A,x \in \mathbb{R}^{k \times 1}$ and $x \sim \mathcal{N}(0,I_k)$ (unfortunately, the latter does not have a nice form).&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;gaussian-processes&quot;&gt;Gaussian Processes&lt;/h3&gt; &lt;p&gt;Compared to linear and kernel regression, a Gaussian Process (GP) is a much more general class of nonparametric functions. Formally, a Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP can be thought of as a distribution over functions and is fully specified by its mean function $\mu(x)$ and covariance function $K(x, \tilde{x})$, (similar to kernel regression, this is also known as the kernel of the GP).&lt;/p&gt; &lt;p&gt;Given a set of points $X = {x_1, x_2, \ldots, x_n}$, the function values at these points under a GP are distributed as:&lt;/p&gt; \[\mathbf{f}(X) \sim \mathcal{N}(\mathbf{\mu}(X), K(X, X)),\] &lt;p&gt;where $ \mathbf{\mu}(X) $ is the mean vector and $ K(X, X) $ is the covariance matrix constructed using the kernel function $K$.&lt;/p&gt; &lt;p&gt;Key to the concept of Gaussian Processes is the closure of multivariate Gaussians under conditioning and marginalization. Since all the function values are jointly Gaussian, the value of a new function value, given the existing ones, is also Gaussian, e.g. assuming $\mu(X) = 0$,&lt;/p&gt; &lt;p&gt;\(f(x_{test}) | f(x_1)\dots f(x_n) = \mathcal{N}(\mu_{test},\Sigma_{test})\) where $\mu_{test}$ = $K(x,X)K(X,X)^{-1}f(X)$ and $\Sigma_{test}$ = $K(x,x) - K(x,X)K(X,X)^{-1}K(x,X)$. (The math for this is a bit tedious, so I omit that here.)&lt;/p&gt; &lt;h3 id=&quot;connecting-gaussian-processes-kernel-regression-and-mlps&quot;&gt;Connecting Gaussian Processes, Kernel Regression, and MLPs&lt;/h3&gt; &lt;p&gt;It is interesting to note the similarities between this closed form for the predictor of a Gaussian process and the predictor for kernel regression. In fact, $\mu_{test}$ is exactly the same as $\hat{f}(x)&lt;em&gt;{kernel}$. This suggests GPs parameterize the class of functions drawn from a normal distribution with mean $\mu&lt;/em&gt;{test}$ while kernel regression converges to a deterministic function that is exactly $\mu_{test}$. In other words, I think that the function learned by kernel regression can be thought of as the maximum of the posterior distribution of the GP with the same kernel.&lt;/p&gt; &lt;p&gt;To test this insight, I ran an experiment to see how similar a Gaussian Process trained on a fixed dataset is to kernel regression with the same kernel.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 2. Results of Gaussian Process Regression and Kernel Ridge Regression on synthetic data with the same kernel function.&lt;/p&gt; &lt;p&gt;I sampled $X \sim \mathcal{N}(5,1)$ and $Y \sim \sin(X) + \mathcal{N}(0,0.2)$. I then trained a Gaussian Process and kernel ridge regression on the data with $K(x,\tilde{x}) = -\exp{\frac{|x-\tilde{x}|_2^2}{2}} + Id$. As expected, the function learned by kernel ridge regression closely matches the mean of the class of functions learned by the GP.&lt;/p&gt; &lt;p&gt;Another connection between kernel regression and GPs can be made through the introduction of a one hidden layer MLP. See below figure.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 3. Visualization of kernel regression, MLPs, and Gaussian Processes.&lt;/p&gt; &lt;p&gt;Starting with kernel regression, if we fix the “feature map,” $B $, then training gradient descent with $A^{(0)} = 0$ is equivalent to training kernel regression with $K(x,\tilde{x}) = \langle \phi(Bx), \phi(Bx) \rangle$. This is intuitive because again, we can just think of kernel regression as linear regression ($A$) after applying a nonlinear feature map, ($\phi \circ B$).&lt;/p&gt; &lt;p&gt;The connection between neural networks and Gaussian Processes is a bit more complicated. Suppose we are in the overparameterized regime and $A \in \mathbb{R}^{1 \times k}$ and $B \in \mathbb{R}^{k \times d}$. Forgoing the bias term out of simplity, the output of the network is \(f(x) = A\phi(Bx) = \sum_{i=1}^k A_i\phi(Bx)_i.\) If the weights of the network are sampled i.i.d. Gaussian, then $f(x)$ is a sum of i.i.d. Gaussians and so as $k \rightarrow \infty$, the Central Limit Theorem states that the output of the network will also be Gaussian with some fixed mean and covariance, i.e. in the limit, \(f(x) \sim \mathcal{N}(0,\Sigma)\) \(\begin{bmatrix} f(x_1) \\ f(x_2) \\ \vdots \\ f(x_n) \end{bmatrix} \sim \mathcal{N}(0,K)\)&lt;/p&gt; &lt;p&gt;Now, let us compute $K$: \(K(x,\tilde{x}) = \mathbb{E}[f(x)f(\tilde{x})] = \mathbb{E}[A\phi(Bx)A\phi(B\tilde{x})] = \mathbb{E}\Big[\lim_{k \rightarrow \infty}\Big(\sum_{i=1}^k A_i \phi(Bx)_i\Big)\Big(\sum_{i=1}^k A_i \phi(B\tilde{x})_i\Big)\Big]\) Suppose for simplicity that $A \sim \mathcal{N}(0,I)$. Then $\mathbb{E}[A_iA_j] = 0$ and $\mathbb{E}[A_iA_i] = 1$: \(= \mathbb{E}\Big[\lim_{k \rightarrow \infty}\sum_{i=1}^k A_i^2 \phi(Bx)_i\phi(B\tilde{x})_i\Big] = 1 \lim_{k \rightarrow \infty} \sum_{i=1}^k \phi(Bx)_i\phi(B\tilde{x})_i= \underbrace{\lim_{k \rightarrow \infty} \langle \phi(Bx),\phi(B\tilde{x}) \rangle}_{k \times NNGP}.\)&lt;/p&gt; &lt;p&gt;The latter is essentially the definition of the Neural Network Gaussian Process, which is the kernel of the Gaussian Process that neural networks converge to when its width goes to infinity. (The NNGP has an extra $\frac{1}{k}$ term to allow the Law of Large Numbers to be used again.)&lt;/p&gt; &lt;p&gt;Ultimately, what this shows is that a neural network of infinite width over i.i.d. parameters is the class of Gaussian functions parameterized by the Neural Network Gaussian Process. With gradient descent, neural networks and kernel regression converge to a deterministic function that can be thought of as a sample from a GP.&lt;/p&gt; &lt;p&gt;The below figure summarizes my findings on the connections between the three types of function classes:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-overparameterization/Fig2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 4. Comparison of kernel regression, MLPs, and Gaussian Processes.&lt;/p&gt; &lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt; &lt;p&gt;To summarize, these are the implications of the NN-Kernel Regression-GP Connection:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Predictive Distribution: In the infinite-width limit, the predictive distribution of a neural network for a new input $x_{test}$ can be described by a Gaussian distribution with mean and variance determined by the NNGP.&lt;/li&gt; &lt;li&gt;Regularization and Generalization: Kernels inherently regularize the function space explored by the network. This regularization is not in the form of an explicit penalty but may arise from the minimum $\mathcal{H}$-norm solution of kernel regression. This may explain the observed generalization capabilities of wide neural networks.&lt;/li&gt; &lt;li&gt;Analytical Insights: This correspondence provides a powerful analytical tool to study the learning dynamics of neural networks, which are often difficult to analyze due to their non-linear and high-dimensional nature.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt; &lt;p&gt;A major limitation of this current work is that I evaluated overparameterized neural networks only through the lens of kernels/GPs. It would be interesting to try to understand the successes of neural networks through other metrics, such as evaluating test risk as width increases. Furthemore, it would also be interesting to characterize what happens when depth, rather than just width, increases. Another interesting next step would be expanding this analysis to understanding overparameterization of other architectures, such as CNNs and transformers, and their connections to kernel regression and Gaussian Processes.&lt;/p&gt; &lt;p&gt;Understanding neural networks through the lens of the NTK and Gaussian processes deepens our appreciation of the foundational principles in machine learning. It unifies three seemingly disparate areas: the powerful yet often opaque world of deep learning, the straightforward approach of kernel regression, and the rigorous, probabilistic framework of Gaussian processes. This confluence not only enriches our theoretical understanding but also paves the way for novel methodologies and insights in the practical application of machine learning algorithms.&lt;/p&gt; </content> </entry> <entry> <title>Exploring Methods for Generating Music</title> <link href="https://deep-learning-mit.github.io/blog/2023/exploring-music-generation/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/exploring-music-generation</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;The problem of music generation has been widely explored for a long time. Music has very similar parallels to how speech and language is structured. Just like language, music is temporal and in the traditional western sense, has a defined set of standards/rules for how music should be structured. What makes music generation a more challenging problem than language is that music has an artistic/expressive component as well as both low and high level structure. For “good” music, it isn’t enough to simply generate a series of notes and harmonies that obey music theory conventions. At the low level, “good” music makes use of varying dynamics, note groupings, and articulation. At the high level, “good” music may feature overarching motifs and specific &lt;a href=&quot;https://en.wikipedia.org/wiki/Musical_form&quot;&gt;forms&lt;/a&gt; (round, sonata form, ABAB, etc). This level of complexity is analagous to the problem of generating poetry and generating speech that mimics a human reciting it. The poetry will have structures like rhyme, rhythm, motifs, metaphors, etc. and the speech reading it will have to replicate expressiveness to be convinving. This level of complexity is not yet achievable with high level of robusts by current speech generators, LLMs, and NLP methods.&lt;/p&gt; &lt;p&gt;It is this level of structural complexity required for generating “good” music that make machine learning methods, specifically deep learning, a compelling approach to tackling the problem of generating “good” music. Deep learning methods should be able to capture music’s low level music theory structure as well as the high level It is the hope that given enough data and the right architectures, music generation will be able to mimick a level akin to the best human composers. While music generation such as OpenAi’s jukebox &lt;d-cite key=&quot;dhariwal2020jukebox&quot;&gt;&lt;/d-cite&gt; as yielded very good results, it is trained on pure audio frequencies. I will focus on musical generation and training from a “written” / musical structural perspective rather than audio. (Think human speech vs. language/text), as I think this can provide greater insight into how these models learn and what about musical structure is being learned.&lt;/p&gt; &lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt; &lt;p&gt;There has been several studies/project done in the generation of music. OpenAi has done work with using audio samples to generate music. They took a representation learning and autoencoder approach leveraging VQ-VAEs. Other work &lt;d-cite key=&quot;doi:10.1080/25765299.2019.1649972&quot;&gt;&lt;/d-cite&gt; took approaches similar to me and tried to analyze the “written” structure of music and used a combination of LSTMs and a midi encoding scheme to . Work has been done to capture the expressitivity of music &lt;d-cite key=&quot;10124351&quot;&gt;&lt;/d-cite&gt;, where they leverage large transformer models and condition them on emotion to generate music. There has been success in generating expressitivity based on this conditional approach. My work here aims to analyze purely the syntactical structure of music and will not be leveraging conditioning.&lt;/p&gt; &lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt; &lt;p&gt;Before music can be learned and generated, it needs to first be converted to a format that can be input to a ML model. To achieve this I used a subset of a piano midi dataset &lt;d-cite key=&quot;ferreira_aiide_2020&quot;&gt;&lt;/d-cite&gt; and utilized a &lt;a href=&quot;https://pypi.org/project/py-midicsv/&quot;&gt;program&lt;/a&gt; to convert from MIDI to .csv. Using this .csv file I encoded each note in the midi to a 107 dimensional vector. Where the first 106 dimensions correspond to midi-notes &lt;a href=&quot;https://www.inspiredacoustics.com/en/MIDI_note_numbers_and_center_frequencies&quot;&gt;A0-G9&lt;/a&gt;, and the last dimension is encodes the duration of the midi-note divided by the midi-clock/quarter frequency to get a duration of the note in quarter notes. Since note A0 corresponds to midi-note 21, all of the midinote values are subtracted by this baseline value when being encoded into the vector. If a midi-note is played it is encoded as “ON” in the .csv and as such is represented with a 1 in it’s corresponding index in the note vector. For example, if a C4 and A4 note (MIDI note 60, and 69 respectively) are played at the same time in a song, it will be encoded as a 107 dimensional zero vector with indices 37, 47 (60 (midi value) -21 (baseline)-1 (0-index notation)) being 1 and index 106 being the duration of the chord.&lt;/p&gt; &lt;p&gt;I then tested 3 different models to see how they performed. The first model I tested was an RNN with hidden_size = 64, RNN_layers = 2, and sequences of 24, 48, 64, and 200. I next tested LSTM models with hidden_size = 64, RNN_layers = 2, and sequences of 24, 48, 64, and 200 and compared a birection vs. single directional model. The last model I analyzed was a transformer. In which I first took my note encodings and created an embedded representation of the notes and combined this with positional encoding in the sequence of music to get my final embedding to pass into my transformer architecture.&lt;/p&gt; &lt;h1 id=&quot;resultsexperiments&quot;&gt;Results/Experiments&lt;/h1&gt; &lt;p&gt;I found that the RNN architecture to be the worst performing model. It has a high ringing for some training and mostly unstructured and random. The results of a sample music generation can be found &lt;a href=&quot;https://drive.google.com/drive/folders/1FiuobbyVUnwpUZUx_PYBR57qOwj5jYXe?usp=sharing&quot;&gt;here&lt;/a&gt;. The LSTM model took longer to train but performed better with hidden size = 64, sequence_length=48, and 30 epochs. I found that it worked even better when using a bidirectional architecture. A sample generation can be found &lt;a href=&quot;https://drive.google.com/drive/folders/10CzuEbuVXKCyLsY5vwQZjSKJT1ABqXbA?usp=sharing&quot;&gt;here&lt;/a&gt; in which it was fed the starting 10 notes of Polonaise in A-flat major, Op. 53 and was asked to generate a long sequence from that. The transformer took the longest to train and its results can be found &lt;a href=&quot;https://drive.google.com/drive/folders/1fGe7xUZyFNlFGMbGB8aXnVfSEx067ZaA?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;h1 id=&quot;closing-thoughts-and-future-work&quot;&gt;Closing Thoughts and Future Work&lt;/h1&gt; &lt;p&gt;As expected the base RNN architecture failed to generate anything meaningful. It took a while to find hyperparameters that would make the LSTM generate something of note, but when it did successfully generate music I was surprised by some of the resemblences it had to music in the training data.&lt;/p&gt; &lt;p&gt;One noticeable flaw in my work is that I that my metric for success outside of training error is qualitative. It would have been useful for evaluation of my model implementations if I had a quanititative metric. I originally calculated the loss of my models based on how they replicated unseen music from a test set given sequences from the same music, however losses for every model failed to converge in a reasonable amount of time. It is certainly difficult to tell if poor performance is due to implementation or a small dataset and limited compute resources.&lt;/p&gt; &lt;p&gt;Continuing on the idea of lack of data. One of the challenges I faced was in the curation of my dataset. I originally was going to generate music tokens for my network based on a very descriptive musical format cally &lt;a href=&quot;https://lilypond.org/&quot;&gt;lilypond&lt;/a&gt;. However, there were inconsisencies between samples of music in how they were resprented in the lilypond text format, so creation of a program to transcribe the text to a good format for representing music was very difficult which is why I turned to the more standardized MIDI file format. It is unfortunate because a lot of the the complex expression in music is lost in midi format, making it harder if not impossible for models trained on midi input to learn these complex representations/behavior. I say impossible because if data for musical expression is completely absent from training, then this important component of music is simply out of distribution and impossible to learn. So a better way to encode/represent music is needed for better results.&lt;/p&gt; &lt;p&gt;Moving forward, it would be interesting to explore how representation learning can be used to enhance the generation of music. I wanted to explore the use of VAEs and some of the more advanced variations like the one in used in OpenAi’s jukebox, VQ-VAE. These methods maybe be able to capture both the high level structure and complex low level structure found in music. I also want to explore methods for encoding the dynamics, articulation, and expression found in music, something I was not able to do this time around. Lastly, exploring a better way to encode and learn the duration of notes would lead to better music generation.&lt;/p&gt; </content> </entry> <entry> <title>Can Constrastive Learning Recommend Me a Movie?</title> <link href="https://deep-learning-mit.github.io/blog/2023/rep-learning-for-rec-systems/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/rep-learning-for-rec-systems</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;With the vast amount of information and content available online, the need for intelligent recommendation systems has only become more necessary. Many of the apps we use, YouTube, TikTok, Instagram, Netflix, Spotify, etc. all incorporate recommender systems to provide personalized content. But how do these systems work? An important factor in delivering good recomendations is having a system that can find an expressive and useful representation of users and items (where items are the specific piece of content we want to recommend).&lt;/p&gt; &lt;p&gt;Traditional approaches for developing recommender systems include collaborative filtering, matrix factorization, and deep neural networks such as multi-layer perceptrons (MLPs) and graph neural networks (GNNs) &lt;d-cite key=&quot;history&quot;&gt;&lt;/d-cite&gt;. Moreover, a focus on using a hybridized approach of the previous models are also in active research, with aims of balancing their various benefits and tradeoffs.&lt;/p&gt; &lt;p&gt;This project aims to explore if contrastive learning can be used to recommend movies for a user based on a their prior movie ratings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More specifically, by choosing different strategies of defining positive/negative pairs, can we learn a user embedding that facilites the downstream task of movie recommendation?&lt;/strong&gt;&lt;/p&gt; &lt;h2 id=&quot;background-and-related-work&quot;&gt;Background And Related Work&lt;/h2&gt; &lt;h3 id=&quot;contrastive-learning&quot;&gt;Contrastive Learning&lt;/h3&gt; &lt;p&gt;Contrastive learning is a self-supervised machine learning technique for training a model (often called an encoder) to distinguish between similar and dissimilar pairs of data points. The goal is to map each data point from its original representation space to a smaller dimensional latent space. If the encoder is trained well and is able to learn a good representation, the newly encoded data points should act as a sort of “compressed” version of the original data point while still containing some useful semantic information.&lt;/p&gt; &lt;p&gt;Contrastive learning has tradionally been used in the domains of computer vision and natural language processing. However, more recent work has shown that contrastive learning, when combined with graph neural networks (GNNs), can learn impressive representations when applied to recommender systems &lt;d-cite key=&quot;gnn&quot;&gt;&lt;/d-cite&gt;. For the purposes of this project, instead of using a GNN as our encoder, a simpler MLP will be used.&lt;/p&gt; &lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;This project explores creating a movie recommender system based on the &lt;a href=&quot;https://grouplens.org/datasets/movielens/&quot;&gt;MovieLens dataset&lt;/a&gt;. The small version of this dataset contains 10,000 ratings of 9,000 movies by 600 users on a 0-5 star scale. Data was collected by users of the MovieLens website, last updated in September 2018. An example of the primary &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ratings.csv&lt;/code&gt; dataset is shown below:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;userId&lt;/th&gt; &lt;th&gt;movieId&lt;/th&gt; &lt;th&gt;rating&lt;/th&gt; &lt;th&gt;timestamp&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;964982703&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;964981247&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;…&lt;/td&gt; &lt;td&gt;…&lt;/td&gt; &lt;td&gt;…&lt;/td&gt; &lt;td&gt;…&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;318&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;1445714835&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;333&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;1445715029&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;…&lt;/td&gt; &lt;td&gt;…&lt;/td&gt; &lt;td&gt;…&lt;/td&gt; &lt;td&gt;…&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;600&lt;/td&gt; &lt;td&gt;170875&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;1493846415&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;h3 id=&quot;preprocessing-of-dataset&quot;&gt;Preprocessing of Dataset&lt;/h3&gt; &lt;p&gt;The MovieLens dataset of user-movie interactions (movie ratings) is split into a training and test dataset. For each user, 95% of their interactions were randomly sampled and allocated to the training dataset, while the remaining 5% of interactions were allocated to the test dataset.&lt;/p&gt; &lt;p&gt;Thresholds were chosen to quantify whether a user “liked” a movie (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LIKE_THRESHOLD&lt;/code&gt;) or “disliked” a movie (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DISLIKE_THRESHOLD&lt;/code&gt;) based on that user’s rating. The training dataset was then filtered to only include interactions involving movies that had a minimum number of users who “liked” it and a minimum number of users who “disliked” the movie. This was to ensure that each movie had enough user data to facilite the computations for selecting positive / negative pairs.&lt;/p&gt; &lt;h3 id=&quot;positive-and-negative-pair-strategies&quot;&gt;Positive and Negative Pair Strategies&lt;/h3&gt; &lt;p&gt;An important component of contrastive learning involves the definintion of positive pairs and negative pairs. For a given interaction (user &lt;em&gt;u&lt;/em&gt; rates movie &lt;em&gt;m&lt;/em&gt;), what should be considered a similar interaction and what should be considered a dissimilar interaction?&lt;/p&gt; &lt;p&gt;Given an interaction by user ${u}$, let $\text{pos}(u) = u^+$ and $\text{neg}(u) = u^-$ where $(u,u^+)$ is a positive pair and $(u,u^-)$ is a negative pair. The goal will be to find the pair of functions $\text{pos}(), \text{neg()}$ such that a good representation is learned.&lt;/p&gt; &lt;h3 id=&quot;encoder-architecture&quot;&gt;Encoder Architecture&lt;/h3&gt; &lt;p&gt;The proposed encoder architecture is shown below. The encoder recieves as input a batch of userIds, $u$ , integers in the range $0 \leq u \leq 599 $. The first layer of the encoder is an embedding layer, mapping userIds to a vector of dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input_dim&lt;/code&gt;. This layer is followed by a 2-layer MLP with relu activations, with a hidden dimension of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidden_dim&lt;/code&gt; and an output dimension of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;latent_dim&lt;/code&gt;. Additionally, the final output of the encoder normalized.&lt;/p&gt; &lt;div class=&quot;row mt-3 align-items-center&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/encoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/encoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/encoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/encoder.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Architecture for encoder, where &lt;strong&gt;input_dim&lt;/strong&gt; = 1024, &lt;strong&gt;hidden_dim&lt;/strong&gt; = 600, &lt;strong&gt;latent_dim&lt;/strong&gt; = 200. &lt;/div&gt; &lt;h3 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h3&gt; &lt;p&gt;In order to evaluate the quality of the learned user representations, there are a handful of metrics that will be used.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Top K Movie Recommendation&lt;/strong&gt;: Movie recommendation will serve as a downstream task that acts as a proxy for how well the learned user representations are. To recommend movies for a user, the encoder is used to get the user embeddings for all users in the dataset. We then use the cosine-similarity to compute the N=10 nearest neighbors to our target user. From these N neighbors, we retreive all of their “liked” movies and sort by their respective ratings. The top K movies are returned as the system’s recommendations.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;recall@k&lt;/strong&gt;: A popular metric used for evaluating recommender systems is recall@k &lt;d-cite key=&quot;rec&quot;&gt;&lt;/d-cite&gt;. It measures the proportion of relevant items that were successfully retrieved from the top-k movie recommendations. Relevant items are defined as items that a user “likes” from the test dataset. The proportion of these items found in top-k recommendations from our recommender system (based on the learned encoder) is the recall@k. The higher the recall, the greater the overlap between our recommender’s recommended movies and the user’s actual preferred movies.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visualization of User Embeddings&lt;/strong&gt;: By visualzing the learned user representation’s ability to be distinguished into separate clusters, we can better examine the potential user clusters for any distinguishing features. By utilizing t-distributed Stochastic Neighbor Embedding (TSNE) for dimensionality reduction of the user embedding vectors, we can project users representations to the 2D plane and use traditional clustering algorithms for visualization &lt;d-cite key=&quot;rec&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Top Movies Per User Cluster&lt;/strong&gt;: To provide more insight into the resulting user embedding clusters, the top movies of the users in each cluster is also reported.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;In addition to standard hyperparamter-tuning techniques to optimize training, different positive pairs and negative pairs strategies will be tested.&lt;/p&gt; &lt;p&gt;All encoders were trained with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_epochs&lt;/code&gt; = 20, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size&lt;/code&gt; = 512, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lr&lt;/code&gt; = 0.0001 (using Adam optimizer), and contrastive triplet loss.&lt;/p&gt; &lt;h3 id=&quot;strategy-1&quot;&gt;Strategy 1&lt;/h3&gt; &lt;p&gt;For a given user $u_i$ a similar user is determined by a random selection from a set of candidate users. These candidate users consist of the subset of users that have “liked” the same movies that $u_i$ “liked”, i.e. their ratings $\geq$ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LIKE_THRESHOLD&lt;/code&gt;. Likewise, dissimilar users for $u_i$ were randomly selected from a set of candidate users that “disliked” the same movies $u_i$ “disliked”, i.e. their ratings $ &amp;lt; $ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DISLIKE_THRESHOLD&lt;/code&gt;.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;LIKE_THRESHOLD&lt;/th&gt; &lt;th&gt;DISLIKE_THRESHOLD&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;With these definitions of positive and negative pairs, an encoder was trained with the resulting user embeddings shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3 align-items-center&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-clusters-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-clusters-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-clusters-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-clusters.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-top-movies-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-top-movies-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-top-movies-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s1-top-movies.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Learned user embedding clusters and top movies using Strategy 1. &lt;/div&gt; &lt;p&gt;By examining the user embedding clusters, we see four loosely-defined user clusters. The top 5 highest rated movies by each cluster’s members are also depicted. A key takeaway is that we see a repetition of the same movies across each cluster, movies like &lt;em&gt;The Nutty Professor&lt;/em&gt;, &lt;em&gt;Mission Impossible 2&lt;/em&gt;, &lt;em&gt;Ace Ventura: When Nature Calls&lt;/em&gt;, etc. These are all very popular and well-liked movies with a wide audience. The prevalence of highly-rated and popular movies such as these leads to a bias in our positive pairs. Since many users are fans of these movies, they are all considered similar users, i.e. our definition of similarity is too weak. The following strategies will try to address this.&lt;/p&gt; &lt;h3 id=&quot;strategy-2&quot;&gt;Strategy 2&lt;/h3&gt; &lt;p&gt;In order to decrease the influence of popular movies, one strategy is to filter out all movies that are “liked” by a certain number of users. We define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POPULARITY_THRESHOLD&lt;/code&gt; = 100, which removes all movies with over 100 “liked” users. As a result, the distribution of “liked” users per movie is relatively uniform. The definitions of positive and negative pairs remains the same as in Strategy 1.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;LIKE_THRESHOLD&lt;/th&gt; &lt;th&gt;DISLIKE_THRESHOLD&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3 align-items-center&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-clusters-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-clusters-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-clusters-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-clusters.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-top-movies-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-top-movies-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-top-movies-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s2-top-movies.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Learned user embedding clusters and top movies using Strategy 2. &lt;/div&gt; &lt;h3 id=&quot;strategy-3&quot;&gt;Strategy 3&lt;/h3&gt; &lt;p&gt;A different method for reducing the influence of popular movies was to normalize each users ratings. By subtracting a movie’s average rating across all users from any particular user’s rating, we are able to determine whether the user liked the movie more than others or disliked it more than others. Popular movies only have an impact if the user really liked (or disliked) it relative to everyone else.&lt;/p&gt; &lt;p&gt;Using this new strategy, for any user $u_i$, instead of randomly selecting a similar user from candidates that “liked” a movie in common, these candidate users are ranked such that the candidate that has the highest normalizes rating is selected (the opposite is true for choosing a disimilar user). Therefore, instead of having a positive pair of users who rated the same movie highly, the positive pair will consist of users who both gave the same movie a higher rating than the average user.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;LIKE_THRESHOLD&lt;/th&gt; &lt;th&gt;DISLIKE_THRESHOLD&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3 align-items-center&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-clusters-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-clusters-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-clusters-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-clusters.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-top-movies-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-top-movies-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-top-movies-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s3-top-movies.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Learned user embedding clusters and top movies using Strategy 3. &lt;/div&gt; &lt;h3 id=&quot;strategy-4&quot;&gt;Strategy 4&lt;/h3&gt; &lt;p&gt;Despite the previous strategies, there still seems to be a lack of cohesion among the resulting user embedding clusters. The final strategy tested was a hybrid approach. In this scenario, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LIKE_THRESHOLD&lt;/code&gt; has been raised and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DISLIKE_THRESHOLD&lt;/code&gt; lowered in an attempt to narrow the candidate pools to more extreme users. Moreover, Strategies 2 and 3 are combined. Highly popular movies are removed and normalized ratings are used.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;LIKE_THRESHOLD&lt;/th&gt; &lt;th&gt;DISLIKE_THRESHOLD&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3 align-items-center&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-clusters-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-clusters-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-clusters-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-clusters.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-top-movies-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-top-movies-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-top-movies-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-01-rep-learning-for-rec-systems/s4-top-movies.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Learned user embedding clusters and top movies using Strategy 4. &lt;/div&gt; &lt;h3 id=&quot;analysis&quot;&gt;Analysis&lt;/h3&gt; &lt;p&gt;For each strategy, the recall@k for various values of k are shown, along with the sizes of the train and test datasets after filtering.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;Strategy 1&lt;/th&gt; &lt;th&gt;Strategy 2&lt;/th&gt; &lt;th&gt;Strategy 3&lt;/th&gt; &lt;th&gt;Strategy 4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;recall@10 (%)&lt;/td&gt; &lt;td&gt;0.62&lt;/td&gt; &lt;td&gt;1.29&lt;/td&gt; &lt;td&gt;0.73&lt;/td&gt; &lt;td&gt;0.78&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;recall@20 (%)&lt;/td&gt; &lt;td&gt;1.97&lt;/td&gt; &lt;td&gt;2.16&lt;/td&gt; &lt;td&gt;2.18&lt;/td&gt; &lt;td&gt;3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;recall@50 (%)&lt;/td&gt; &lt;td&gt;3.84&lt;/td&gt; &lt;td&gt;6.03&lt;/td&gt; &lt;td&gt;4.36&lt;/td&gt; &lt;td&gt;11.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Size Train Set&lt;/td&gt; &lt;td&gt;51,576&lt;/td&gt; &lt;td&gt;32,609&lt;/td&gt; &lt;td&gt;51,576&lt;/td&gt; &lt;td&gt;10,826&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Size Test Set&lt;/td&gt; &lt;td&gt;1,361&lt;/td&gt; &lt;td&gt;984&lt;/td&gt; &lt;td&gt;1,361&lt;/td&gt; &lt;td&gt;232&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;It appears that both Strategy 2 and Strategy 3 alone seemed to make a noticable improvement in recall, with Strategy 2 (the removal of the most popular movies) making a larger impact than normalizing ratings. Furthermore, by using both strategies along with a few other changes, a representation the resulted in a better recomender system and more well-defined embedding clusters was learned.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;From the above experiments, it seems that contrastive learning (even when used with a simple MLP encoder) can eventually learn a user embedding resulting in clusters. However, it seems like either a more advanced architecture or positive/negative pair mining procedures are required to ensure that the learned representations have a useful semantic meaning. Weak positive pairs resulted from the presence of popular movies with diverse audiences. Previous work in applying contrastive learning to recommender systems highlight more complex formulations of ranked loss functions, assigning different weights depending on whether the pairs are hard or easy negative samples &lt;d-cite key=&quot;ranked&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;An interesting extension of this project could explore the use of GNNs as the basis of the encoder architecture, as these types of models more naturally preserve the structure of user-movie interactions.&lt;/p&gt; </content> </entry> <entry> <title>Improving CLIP Spatial Awareness Using Hard Negative Mining</title> <link href="https://deep-learning-mit.github.io/blog/2023/spacial-CLIP/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/spacial-CLIP</id> <content type="html">&lt;h1 id=&quot;introduction-clip-doesnt-know-its-left-and-rights&quot;&gt;Introduction: CLIP doesn’t know its left and rights&lt;/h1&gt; &lt;p&gt;Multimodal learning has come into prominence recently, with text-to-image synthesis models such as DALLE or Stable Diffusion, and image-text contrastive learning models such as CLIP. In particular, CLIP has proven to be extremely useful in learning zero-shot capabilities from paired image and text data.&lt;/p&gt; &lt;p&gt;However, recent work has highlighted a common limitation in multimodal models: the ability to capture spatial relationships. Spatial relationships can be defined as how objects in an image are positioned concerning other objects. For example, A is next to B or B is on top of A. Although Language models now demonstrate an understanding of word order and spatial awareness, multimodal models still struggle to capture this relationship in both the image and captions.&lt;/p&gt; &lt;h2 id=&quot;downstream-tasks&quot;&gt;Downstream tasks&lt;/h2&gt; &lt;p&gt;Improving captioning abilities is an important building block in overcoming this limitation in all multimodal models. Creating synthetic captions from images is an already popular method in developing training data for other models such as DALLE-3. However, limitations in captioning abilities carry over to downstream tasks, and therefore, models such as DALLE-3 often also struggle to generate images from prompts that include spatial relationships. We hope that demonstrating the ability to generate spatially-aware captions will also lead to improvements in other Vision-Language models in the future.&lt;/p&gt; &lt;h2 id=&quot;semantic-similarity&quot;&gt;Semantic similarity&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Taken from Yamada et al. &lt;/div&gt; &lt;p&gt;CLIP is trained to maximize the similarity between embeddings of images and text. This leads to CLIP matching semantically similar images and captions but not understanding finer-grained details. Concept Association is especially an issue when there are multiple objects in an image where CLIP struggles to reason about the object’s attributes (Yamada 2022). Additionally, because of the focus on semantic similarity, CLIP also struggles with spatial relationships between objects.&lt;/p&gt; &lt;h1 id=&quot;winoground&quot;&gt;Winoground&lt;/h1&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/winoground_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/winoground_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/winoground_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/winoground_example.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Taken from Thrush et al. &lt;/div&gt; &lt;p&gt;Spatial awareness has been explored explicitly throughout previous literature. Thrush et al. in Winoground created an evaluation dataset that targets compositional reasoning. Each data point contains two captions and two images, where the captions contain the same words only in different orders. The difference in word ordering drastically changes the meaning of the sentence and therefore the image associated with the alternative caption also is completely different. The task then becomes to match the images to the correct captions (Thrush 2022).&lt;/p&gt; &lt;h2 id=&quot;evaluation-specifics-and-results&quot;&gt;Evaluation Specifics and Results&lt;/h2&gt; &lt;p&gt;We are going to use the image-to-caption evaluation of Winoground which aims to match captions to each image in constrast to images to captions. Different models have differnt matching strategies; CLIP uses the higher dot product similarity score when deciding which caption fits each image. Since there are in total, 4 different possible matchings out of the 2 image/caption pairs, random chance would score 25%. However, many multimodal models fail to score much higher than random chace. CLIP (ViT-B/32) scores 30.75% while the best models only score 38%.&lt;/p&gt; &lt;h2 id=&quot;spatial-examples&quot;&gt;Spatial Examples&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; An example of spatial image/caption pairs. Taken from Thrush et al. &lt;/div&gt; &lt;p&gt;CLIP has shown to be an extremely difficult benchmark for multimodals - and there are multitude of reasons why. First, changing the word orders creates image/caption pairs that need fine-grained reasoning capabilities to differentiate. One of the many reasoning capabilities needed to do well is spatial reasoning. We filter out 101 examples of CLIP that contain image/captions that require spatial reasoning to create a more task-speciific benchmark. Our filtering is caption-based and targets key words that may indicate spatial relationships. We will refer to this filtered out evaluation benchmark as, Winoground-Spatial.&lt;/p&gt; &lt;h1 id=&quot;hard-negative-examples&quot;&gt;Hard Negative Examples&lt;/h1&gt; &lt;p&gt;Hard negative examples are negative examples that are close to our anchor pair. These are examples that are close in some way to our positive example, but still wrong. Oftentimes, these examples are hard to distinguish from one another, and therefore cause the model trouble.&lt;/p&gt; &lt;h2 id=&quot;clip-loss&quot;&gt;CLIP Loss&lt;/h2&gt; &lt;p&gt;As a refresher on how CLIP is trained, CLIP first calculates an N by N similarity matrix from the dot products of the two embeddings. The model the calculates a loss function as the average of two cross entropies. The task becomes a classification task where we classify the correct caption for each image and the correct image for each caption, thus leading to two cross entropy functions.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; CLIP similarity matrix. Radford et al. &lt;/div&gt; &lt;p&gt;We modify this training procedure to include additional hard negative captions. For each image/caption pair, we generate M additional negative captions. We then calculate an N by NM similarity matrix from the dot products. Then, we only modify the loss function for image classification cross entropy function to include negative captions alongisde the original N captions. We don’t modify the caption classification cross entropy function since the negative examples don’t have a corresponding “image”.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; CLIP similarity matrix with negative examples. &lt;/div&gt; &lt;h2 id=&quot;data-and-augmentation&quot;&gt;Data and Augmentation&lt;/h2&gt; &lt;p&gt;How do we generate negative examples? We first have to create a fine-tuning dataset that contains image/caption pairs that display spatial relationships. To do this, we utilize the dataset Flickr30k, a dataset that contains 31,000 images collected from Flickr along with 5 captions annotated by human annotators. We chose this dataset due to it’s caption quality alongside the fact that many of the image/caption pairs contain multiple objects.&lt;/p&gt; &lt;p&gt;We then filter out image/caption pairs based on the captions in a similar way we created our evalutation benchmark, Winoground-Spatial. We use 20 key words and phrases such as: “left”, “on top of”, “beneath”, etc. to create a training set of roughly 3,600 examples. Although there are most likely more spatial examples, we choose this method as it is cost-effective while still ensuring the quality of the traning set being only examples of spatial relationships.&lt;/p&gt; &lt;p&gt;Data augmentations have been a commonly used as a method to prevent overfitting in image classification tasks. Although it is common to perform image augmentations, Fan et al. introduce LaCLIP to perform text augmentations on captions to create additional image/caption pairs. This method can be thought of as generating additional “positive pairs”. In order to generate text-augmentations, they utilize language models such as llama7b and GPT-3 to ensure the sentences generated are still grammatically correct. They use in-context learning and prompts such as, “Rewrite this caption of an image vividly, and keep it less than thirty words:”.&lt;/p&gt; &lt;p&gt;We follow a similar procedure to generate our negative examples. For each image/caption pair, we prompt GPT-3.5-turbo-instruct to do different augmentations. Details of the prompts are provided in the later experiments.&lt;/p&gt; &lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt; &lt;p&gt;For all experiments, we use a base model of CLIP(ViT-B/32) pre-trained on OpenAI’s WIT provided by OpenClip. We then use OpenAI’s API to generate augmentations. In total, the cost of generating augmentations were under $50 in credits.&lt;/p&gt; &lt;h2 id=&quot;experiment-1-switching-word-order&quot;&gt;Experiment 1: Switching word order&lt;/h2&gt; &lt;p&gt;Our first experiment explores how switching the word order may serve as hard negative examples. This method is inspired by the benchmark we are using, where the captions share the same words but in a different order. For each caption, we generate a single hard negative caption. The prompt we use is displayed below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GPT-word-order-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GPT-word-order-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GPT-word-order-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-spacial-CLIP/GPT-word-order.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; In-context-learning prompt used to augment word order. &lt;/div&gt; &lt;p&gt;We discover adding a single hard-negative example to each example already leads to an impressive performance boost. The accuracy improves from 19.8% to a staggering 50.5% from fine-tuning.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;Pretrained CLIP&lt;/th&gt; &lt;th&gt;Word Order CLIP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Pairs matched correctly&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Accuracy&lt;/td&gt; &lt;td&gt;0.198&lt;/td&gt; &lt;td&gt;0.505&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We did some extra probing and noticed the majority of the improvement was from distinguishing left and right. From the additional 31 examples our fine-tuned model got correct, 18 of them were examples that the captions included the keyword of either left or right. This is consistent with our training set, where the most popular keyword of our examples is left/right.&lt;/p&gt; &lt;h2 id=&quot;experiment-2-replacing-key-spatial-words&quot;&gt;Experiment 2: Replacing key spatial words&lt;/h2&gt; &lt;p&gt;We then explore how a different augmentation workflow could impact the accuracy. In this experiment, we augment the captions to replace the keyword with another spatial keyword. For example, the keyword “on top of” could be replaced by “underneath” or “to the right of”. We again, utilize GPT to ensure the captions are still grammatically and logically correct. Because of the number of keywords avaialable, we explore how the number of negative examples during training time may affect the model’s accuracy.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;0 negative examples (Pretrained CLIP)&lt;/th&gt; &lt;th&gt;1 negative examples&lt;/th&gt; &lt;th&gt;5 negative examples&lt;/th&gt; &lt;th&gt;10 negative examples&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Pairs matched correctly&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;31&lt;/td&gt; &lt;td&gt;65&lt;/td&gt; &lt;td&gt;55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Accuracy&lt;/td&gt; &lt;td&gt;0.198&lt;/td&gt; &lt;td&gt;0.307&lt;/td&gt; &lt;td&gt;0.644&lt;/td&gt; &lt;td&gt;0.545&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We can notice that from 0-5 negative training examples, there is a distinctive increase in model accuracy. However, an interesting result is the dropoff in accuracy from 5 training examples to 10. We did some probing into why this may be the case in the training data. One hypothesis may be the training examples for hard negatives are incorrect, in that, by a human they could be interpreted as positive examples. For example, object A could be both next to and above object B, but we are training CLIP to recognize the keyword above to be false in this case. Another hypothesis is the difficulty in training examples stunting training and needing more data. This could be case when looking at the loss function, on whether it has fully converged or not.&lt;/p&gt; &lt;h1 id=&quot;conclusion-and-limitations&quot;&gt;Conclusion and Limitations&lt;/h1&gt; &lt;p&gt;Although we have not fully tackled the issue of spatial awareness, we have made signifigant progress from our base model of CLIP, with the highest accuracy being at 64.4% compared to 19.8%. This proof of concept work shows how hard-negative examples could boost improvements in specific reasoning tasks. The concept of using these hard-negative examples are not limited to spatial relationships: it could be interesting to examine how hard negative tasks may improve other Winoground examples that require reasoning capabilities such as counting. We also note that there is a possiblity that improving the training data may not be enough, and that the architecture may need a change to fully solve spatial relationships.&lt;/p&gt; &lt;h3 id=&quot;references&quot;&gt;References:&lt;/h3&gt; &lt;p&gt;1.Robinson, J. D.; Chuang, C.-Y.; Sra, S.; Jegelka, S. Contrastive Learning with Hard Negative Samples. In Proceedings of the International Conference on Learning Representations, 2021.&lt;/p&gt; &lt;p&gt;2.Thrush Tristan, Jiang Ryan, Bartolo Max, Singh Amanpreet, Williams Adina, Kiela Douwe, and Ross Candace. 2022. Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 5238–5248.&lt;/p&gt; &lt;p&gt;3.Fan, L., Krishnan, D., Isola, P., Katabi, D., and Tian, Y. (2023a). Improving clip training with language rewrites. arXiv preprint arXiv:2305.20088.&lt;/p&gt; </content> </entry> <entry> <title>Multimodal Commonsense</title> <link href="https://deep-learning-mit.github.io/blog/2023/multimodal-commonsense/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/multimodal-commonsense</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;In recent years, language models have been proven to be quite proficient in producing human-like text, computing somewhat semantically-meaningful and human-interpretable word and token embeddings, and generating realistic conversation. However, there is a vast distinction between mimicking human linguistics from data and forming an understanding of the world and its abstract connections from data. The latter describes the commonsense knowledge of a language model, or its ability to reason about simple relationships, interactions, and general logic of the world.&lt;/p&gt; &lt;p&gt;With the advent and growth of large language models in recent years (and months), understanding the world and developing deeper underlying representations of physical and abstract concepts through text alone has become much more feasible and tractable. Yet, there is only so much someone or something can understand by simply reading about it. When evaluating the performance of language models in this context, does the language model simply mimic this knowledge or does it inherently possess it? One paradigm through which to formalize this is through a deeper categorization of common sense.&lt;/p&gt; &lt;p&gt;In particular, physical common sense, or knowledge about the physical world and its properties, is fundamental knowledge for realizing the world and the interactions within it. Physical common sense is a naturally multimodal concept, though, that for humans requires a combination of several senses to perceive, as physical properties are manifested in multiple modalities. A lack of info in any modality may make an object visually ambiguous, or otherwise manifest some misunderstanding of an object. Can we expand the capabilities of language models by imbuing them with multifaceted input to expand its knowledge base beyond text alone?&lt;/p&gt; &lt;p&gt;In this work, I focus on evaluating the physical commonsense reasoning ability of unimodal and multimodal models from text-based tasks under multimodal input. I specifically compare the performance of a text-only language model with a multimodal vision-language model and investigate (a) whether the multiple modalities of input in pretraining the multimodal model can have comparable performance to a text-specialized model, and (b) whether the supplementation of relevant image data at inference time boosts the performance of the multimodal model, compared to a previously text-only input.&lt;/p&gt; &lt;p&gt;Intuitively, vision data should benefit the physical commonsense reasoning of a model by providing the inputs the additional feature of a physical manifestation. Here, I investigate whether image data truly gives deep learning models an additional dimension of representation to benefit its commonsense reasoning.&lt;/p&gt; &lt;h1 id=&quot;related-works&quot;&gt;Related Works&lt;/h1&gt; &lt;p&gt;Several previous works evaluate language models on unimodal text-based commonsense reasoning. A number of common sense benchmarks for LMs exist, evaluating a variety of common sense categories &lt;d-cite key=&quot;bisk2019&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;sap2019&quot;&gt;&lt;/d-cite&gt;, from which fine-tuning has shown to improve general commonsense capabilities of state-of-the-art models. Li et al., 2021 &lt;d-cite key=&quot;li2021&quot;&gt;&lt;/d-cite&gt; analyzes the performance of the Gopher language model in zero-shot and few-shot learning with varying model sizes, finding that their LM performed relatively well in physical common sense, but worse in other common sense categories. Zhao et al., 2023 &lt;d-cite key=&quot;zhao2023&quot;&gt;&lt;/d-cite&gt; investigates the downstream impact of LLMs’ “commonsense model” of the world on robot task generation; they find that using the LLM commonsense knowledge as a heuristic policy achieves better-reasoned decision-making, implying that LLM common sense can empirically benefit physical interaction with the world.&lt;/p&gt; &lt;p&gt;The intersection between text and vision in models has also been explored in several works, though not in the context of commonsense reasoning. For example, text-to-image models have shown significantly greater improvement in improving &amp;amp; expanding the text encoder as opposed to a similar increase in size of the image diffusion model &lt;d-cite key=&quot;saharia2022&quot;&gt;&lt;/d-cite&gt;, showing the effect of powerful text embeddings in image generation. Common sense benchmarks with multimodal inputs have also been created and demonstrate an increase in multimodal model performance after fine-tuning &lt;d-cite key=&quot;yu2022pacs&quot;&gt;&lt;/d-cite&gt;, but they don’t investigate the empirical effect or the representational differences between utilizing additional modalities of data versus not.&lt;/p&gt; &lt;p&gt;More generally, the effect of additional modalities of data on downstream performance is studied in Xue et al. 2022 &lt;d-cite key=&quot;xue2023modality&quot;&gt;&lt;/d-cite&gt; in the context of crossmodal knowledge distillation, where they conclude that multimodal input isn’t unconditionally beneficial; multimodal teacher models maybe perform better than unimodal teachers, but students of multimodal teachers tend to perform better than those of unimodal teachers.&lt;/p&gt; &lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt; &lt;h2 id=&quot;commonsense-benchmarks&quot;&gt;Commonsense Benchmarks&lt;/h2&gt; &lt;p&gt;It’s important to note that there are many distinguishing categories of commonsense knowledge. Physical common sense (e.g., a ball rolls down an incline instead of remaining still), social common sense (e.g., shouting at a person may incite fear), temporal common sense (e.g., pan-frying chicken takes longer than oven-roasting one), and numerical/logical common sense (e.g., basic arithmetic) are a few examples that all require different modalities of reasoning and may favor some models &amp;amp; architectures over others. Here I focus on physical common sense, since intuitively vision data may influence a model’s physical knowledge the most.&lt;/p&gt; &lt;p&gt;Commonsense benchmarks can be further categorized into (a) multiple-choice evaluation, where given a short background prompt, a model must select the most reasonable option or continuation from a set of given options, and (b) generative evaluation, where a model must generate an answer or continuation to the prompt. Here, I will focus on multiple-choice evaluation, as multiple-choice benchmarks provide a more concrete and reliable metric for determining similarity to “human” judgment. To evaluate the commonsense performance of both the unimodal and multimodal models, the HellaSwag benchmark is used.&lt;/p&gt; &lt;h2 id=&quot;hellaswag&quot;&gt;HellaSwag&lt;/h2&gt; &lt;p&gt;The HellaSwag benchmark &lt;d-cite key=&quot;zellers2019&quot;&gt;&lt;/d-cite&gt; is designed to evaluate physical, grounded, and temporal common sense. Given a few-sentence-long story or prompt, the model must choose the correct continuation from four choices. The prompts are generated from (a) ActivityNet &lt;d-cite key=&quot;caba2015activitynet&quot;&gt;&lt;/d-cite&gt;, a large-scale video benchmark for evaluating Human Activity Understanding containing annotations for segments of YouTube videos, and (b) WikiHow &lt;d-cite key=&quot;koupaee2018wikihow&quot;&gt;&lt;/d-cite&gt;, a large-scale text summarization dataset. Data splits are provided by the HellaSwag benchmark, but only the train and validation splits are used here, as the test set labels are not public.&lt;/p&gt; &lt;p&gt;Here, for evaluating the multimodal model, I use only the entries generated from ActivityNet, as each ActivityNet prompt has an associated source ID from which the original source video may be accessed. From the video, image data can be scraped to augment the multimodal model’s fine-tuning and inference. The image data generation process is described in more detail in a following section.&lt;/p&gt; &lt;p&gt;Due to resource and time constraints, only a subset of this data was used for training and evaluation. Given the large size of the original HellaSwag benchmark, the sampled subset of the original data contains 10% of the original data. Each datum within the sampled dataset is sampled randomly from the original train/validation set, and each prompt within the sampled dataset is verified to have a publicly available video associated with it, i.e., the associated YouTube video is not private or deleted. Implications of this limitation are discussed further in the Limitations section below.&lt;/p&gt; &lt;div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1. Example prompts from the HellaSwag benchmark. Image sourced from the original HellaSwag paper &lt;d-cite key=&quot;zellers2019&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;text-only-language-model&quot;&gt;Text-Only Language Model&lt;/h2&gt; &lt;h3 id=&quot;roberta&quot;&gt;RoBERTa&lt;/h3&gt; &lt;p&gt;RoBERTa &lt;d-cite key=&quot;liu2019roberta&quot;&gt;&lt;/d-cite&gt; is used for evaluation of text-only models for physical commonsense reasoning. I use RoBERTa for its established high performance in general natural language processing tasks while being relatively compact and fast to run. A dropout layer and a linear classification head is used with the RoBERTa model to generate the logits for label classification of each prompt.&lt;/p&gt; &lt;h2 id=&quot;vision-text-multimodal-model&quot;&gt;Vision-Text Multimodal Model&lt;/h2&gt; &lt;h3 id=&quot;clip&quot;&gt;CLIP&lt;/h3&gt; &lt;p&gt;The CLIP (Contrastive Language-Image Pre-Training) model is a multimodal vision and language model &lt;d-cite key=&quot;radford2021clip&quot;&gt;&lt;/d-cite&gt;. It was introduced as a novel, simplified pretraining strategy utilizing the large amounts of public available data from the Internet to form (image, text) pairs, demonstrating high performance in downstream tasks such as OCR, geo-localization, and action recognition. Since CLIP utilizes natural language processing methods for processing text and image captions, it also boasts an impressive language model, making the model useful for both unimodal and multimodal tasks.&lt;/p&gt; &lt;p&gt;In the experiments described below, the multimodal model is compared to the unimodal model via text sequence classification and text + vision sequence classification for determining the most likely ending to each HellaSwag prompt, so high baseline performance in both of these tasks is an essential starting point, which CLIP provides. Like for the RoBERTa model, a dropout layer and a linear classification head is used in conjunction with CLIP to perform the label classification for each prompt.&lt;/p&gt; &lt;h3 id=&quot;image-data-generation&quot;&gt;Image Data Generation&lt;/h3&gt; &lt;p&gt;To collect the supplementary vision data for fine-tuning and evaluating the multimodal model, an additional scraping script is used to collect the relevant image data for each HellaSwag prompt. As described before, each prompt in the HellaSwag benchmark is generated from an associated ActivityNet prompt. Each ActivityNet prompt contains a source ID for the corresponding YouTube video, as well as a time segment containing the start and end time (in seconds) for the relevant video annotation. Using this information, each text prompt can be supplemented with an additional image prompt via a frame from the corresponding YouTube video.&lt;/p&gt; &lt;p&gt;A custom script is used to access each prompt’s corresponding YouTube video and scrape image data. The script works as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;From a HellaSwag entry, obtain the source ID for the corresponding ActivityNet entry.&lt;/li&gt; &lt;li&gt;From the ActivityNet entry, obtain the YouTube video source ID (to be used directly in the YouTube URL) and the time segment indicating the start/end time of the annotated clip.&lt;/li&gt; &lt;li&gt;Download a low-resolution copy of the YouTube video via accessing the URL &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://www.youtube.com/watch?v={source_id}&lt;/code&gt;. Here, we download the 144p resolution copy of each video.&lt;/li&gt; &lt;li&gt;Capture a single selected frame from the video data. Note: the selected frame is determined by calculating the average between the video clip’s start and end time, then scraping the frame of the video at that timestamp. Implications of this frame selection are described in more detail in the Limitations section below.&lt;/li&gt; &lt;li&gt;Save the frame as image data for multimodal fine-tuning.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This pipeline is used on the (sampled) HellaSwag train, validation, and test sets so that image data is available for both fine-tuning of the multimodal model, as well as inference for evaluation.&lt;/p&gt; &lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt; &lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt; &lt;p&gt;For fine-tuning and evaluation of the unimodal and multimodal models, a subset of the HellaSwag dataset is used, as already described above. Further summary of the sampled dataset can be found in Table 1.&lt;/p&gt; &lt;p&gt;To prepare the data for Multiple Choice Classification, the data from each prompt must be preprocessed as follows. Each prompt in the HellaSwag dataset is broken into three components: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctx_a&lt;/code&gt;, which contains the first sentence(s) of the prompt, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctx_b&lt;/code&gt;, which contains the initial few words of the final sentence, and four &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ending&lt;/code&gt;s all stemming from the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctx_a&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctx_b&lt;/code&gt; but each with different conclusions. This particular formatting of the data is important for the RoBERTa tokenizer, where each sequence within an inputted text pair must be a complete sentence. Each prompt then generates four text pairs of the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(ctx_a, ctx_b + ending_i)&lt;/code&gt; for each of the four endings. This allows for the multiple choice classification head to compute the most likely of the four endings, given the same context &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctx_a&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctx_b&lt;/code&gt;.&lt;/p&gt; &lt;div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag_summary-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag_summary-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag_summary-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag_summary.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Table 1. Summary of sampled HellaSwag dataset. &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;p&gt;The architecture of neither the RoBERTa nor CLIP are designed for sequence or multiple choice classification, so a separate linear classification head follows each of the unimodal RoBERTa, unimodal CLIP, and multimodal CLIP models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Text-only fine-tuning&lt;/strong&gt;: The training and validation sets for fine-tuning are formatted and preprocessed as described above. To adjust the weights of the classifier and the core embedding model, each model is fine-tuned on the HellaSwag training data and evaluated during training on the validation data for 20 epochs. Since only the text prompt is inputted to CLIP here, only the CLIP text embedding is used for classification.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Text-image fine-tuning&lt;/strong&gt;: To fine-tune the multimodal CLIP model, the original training and validation datasets are augmented by adding each prompt’s relevant corresponding image data (from the process described in the Image Data Generation section). The multimodal model is then fine-tuned on both the text prompts as before and the relevant image data simultaneously. With both text and image input, CLIP outputs a combined text-image embedding that is used for the classification head, instead of the text-only embedding from before.&lt;/p&gt; &lt;p&gt;After fine-tuning, each model is evaluated on the withheld HellaSwag test dataset for classification accuracy. For both the text-only and text-image fine-tuning, I perform three total repetitions for each model and average the results in Figure 1.&lt;/p&gt; &lt;div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/accuracy-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/accuracy-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/accuracy-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/accuracy.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2. Accuracy results for each model, averaged across three runs. &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;As shown in the accuracy results, the RoBERTa model performs the best, while the unimodal CLIP model performs worse, and the multimodal CLIP model only slightly better than the unimodal CLIP but still marginally worse than RoBERTa. RoBERTa likekly performs so well because of its generally high performance in other text-based tasks, and its bidirectional contextual embeddings allow for evaluation of a prompt/ending holistically. In this setup, the supplementary image data did not provide any significant empirical improvement to the multimodal model, as shown by the insignificant improvement in downstream performance when comparing the text-only to text-image CLIP models.&lt;/p&gt; &lt;p&gt;However, I attempt to provide an explanation for this shortcoming through further investigation of the supplementary images. Below, I display the class activation map of the image data from a particular prompt to attempt to visualize why the additional modality of data had little effect on the classification distinguishability across the four endings of the prompt. Figure 2 shows the image (which is the same for all four endings) and the individual image attention masks generated from each ending corresponding to the following context: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A lady named linda, creator of paint along is demonstrating how to do an acrylic painting. She starts with a one inch flat brush and yellow and white acrylic paint. she ...&lt;/code&gt;&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint0-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint0-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint0-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint0.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_paint3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3. The class activation maps for the following prompt: &quot;A lady named linda, creator of paint along is demonstrating how to do an acrylic painting. She starts with a one inch flat brush and yellow and white acrylic paint. she ...&quot; and the following endings: (1) &quot;... then paints about six shades of purple color on the paper.&quot; (2) &quot;... makes x patterns across the canvas with the yellow color.&quot; (3) &quot;... puts on a cream colored coat and begins painting the white wooden fence without a paint shaker.&quot; (4) &quot;... powders the paint in an electric flat brush and applies it over the wall.&quot; The image caption that generated the shown attention masks is the full sentence pair, i.e., context + ending &lt;/div&gt; &lt;p&gt;Notice that across all four prompt/ending pairs, CLIP attends primarily to the same location on the image. While the image data might enrich the model’s representation of the prompt itself, the similarity across the generated attention masks demonstrates that the image doesn’t serve to distinguish the endings from each other and, therefore, has little effect in influencing the likelihood of any particular ending from being more likely. In this setup, the text embedding alone determines the classifier output, and the lack of image distinguishing power provides some explanation for the similarity in downstream performance between the unimodal and multimodal CLIP models.&lt;/p&gt; &lt;p&gt;However, it’s possible that the attention masks were only so similar because all endings were prepended by the same exact context. In the case of Figure 2, the context describes an interaction with the painting, so it may be natural for all attention masks to focus on the painting, regardless of the conclusion of the ending. What if we restrict the image caption to contain only the final sentence (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ctx_b&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ending&lt;/code&gt;)? Figure 3 displays the class activation maps for this setup (though, not from an additional CLIP model fine-tuned on this image caption setup).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end0-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end0-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end0-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end0.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multimodal-commonsense/linda_end3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4. The class activation maps for the endings above, without the prepended context. &lt;/div&gt; &lt;p&gt;We see that using the final sentence without the preceding context generates more varied attention masks, so does this unconditionally allow for more diversity in the image/common sense representation in the joint text/image embedding? I claim that the answer is no; having the entire context for analysis is fundamental for common sense reasoning, so removing a significant portion of the context promotes greater ambiguity in both the intent of the prompt/image caption and the benefit of the attention mask. Using only the final sentence may produce more varied results in the image attention mask, but this may potentially be more detrimental than beneficial by attending to an irrelevant portion of the image that may detract from the commonsense ground truth answer.&lt;/p&gt; &lt;p&gt;Further investigation into different formulations of the image caption with respect to the original prompt in this manner may result in truly richer representations and more meaningful results for downstream model performance.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;In this work, I compare the physical commonsense reasoning capbility of a text-only language model with a multimodal vision-language model and evaluate whether the multiple modalities of input in pretraining the multimodal model can have comparable performance to a text-specialized model, and whether the addition of relevant image data for inference boosts the performance of the multimodal model. I find that, within the proposed experimental setup, the effects of image data supplementation are insignificant, though I provide a potential explanation for this unintuitive result via class activation maps of the multimodal model’s image attention data; alternative formulations for this text-image data augmentation may provide better and more intuitive results. Overall, I provide an empirical experimental pipeline and analysis for potential factors toward further artifical intelligence models’ physical commonsense reasoning, and their internal representations of the world.&lt;/p&gt; &lt;h2 id=&quot;ethical-implications&quot;&gt;Ethical Implications&lt;/h2&gt; &lt;p&gt;It’s also important to note the ethical considerations of “improving” the commonsense reasoning capabilities of deep learning models. Converging on a universally-accepted definition of common sense is utopian, so the interpretation of common sense evaluation must be constantly scrutinized. The biases and malicious elements of a model’s knowledge base must be investigated to ensure that fine-tuning on common sense benchmarks are not further accumulated and embedded into the model. Physical common sense is relatively simple for finding a ground truth answer or natural continuation, but for social common sense, for instance, what a model “should” predict for a particular situation or prompt is much more ambiguous.&lt;/p&gt; &lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt; &lt;p&gt;The implementation and constraints of this work imply some limitations. One evident limitation is the size of both the benchmark dataset and the models used. Evaluating uni- and multimodal models on the full HellaSwag benchmark, including all of both ActivityNet and WikiHow entries, may conclude in slightly different results. Furthermore, newer and bigger models for both text and vision-text models exist; for example, if evaluation is extended to generative prompt evaluation, the recently released GPT4 model &lt;d-cite key=&quot;openai2023gpt4&quot;&gt;&lt;/d-cite&gt; can be evaluated in this framework given its original text-only functionality and its new image input capabilities.&lt;/p&gt; &lt;p&gt;On the topic of generative prompt evaluation, this work only uses multiple-choice prompts for the simplicity and clarity of its evaluation results. However, generative prompts may more closely reflect human-generated responses and may be more representative of multimodal capabilities. Finally, making progress toward a more general-purpose intelligent system means extending the common sense evaluation to more categories than physical. Designing a more comprehensive multimodal model for common sense requires evaluation on all modalities of common sense, and will likely also require additional modalities of input data (e.g., audio cues for better social common sense performance).&lt;/p&gt; </content> </entry> <entry> <title>Exploring Univariate Time Series Anomaly Detection using VAE's</title> <link href="https://deep-learning-mit.github.io/blog/2023/Exploring-Generative-Models-In-Time-Series/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Exploring-Generative-Models-In-Time-Series</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Anomaly detection in time series data is a extensively studied field in academia, holding significant importance due to its wide-ranging applications in real-world scenarios. Time series are present everywhere, and the ability to detect anomalies is vital for tasks such as identifying potential health issues, predicting system failures, or recognizing regime changes in business operations. There are a wide range of methods that have been developed over the years in order to tackle this crucial yet challenging problem. Classical methods approaches rooted in statistics have long been employed, but in recent years, researchers have began to experiment with adapting deep learning techniques to achieve performance improvements.&lt;/p&gt; &lt;p&gt;The deep methods can generally be classified into distinct approaches. The first approach is forecasting, where the method attempts to learn the generating process of the series, and then classifies a point in the series as anomalous if the ground truth value deviates significantly from the predicted value. The second type of approach reconstruction. The models attempt to learn the generating process of the series in a latent space. The model then reconstructs the series, and uses a well designed reconstruction score in order to classify the series points as normal or anomalous. DONUT is an example of a method that falls into this category.&lt;/p&gt; &lt;h2 id=&quot;problem-formulation-and-background&quot;&gt;Problem Formulation and Background&lt;/h2&gt; &lt;p&gt;Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; explore time series anomaly detection within the framework of key performance indicator time series. These time series show high levels of seasonality, a result of being a derivative of human action. They propose a method called DONUT, a highly adaptable anomaly detection method that can perform both supervised, semi-supervised, and unsupervised detection. Their main contributions include modifications to the traditional VAE formulation, the discovery that DONUT requires training on both normal and anomalous data (contrary to other methods), and a kernel density estimation interpretation of the latent space. Given a series \(x_1, x_2, ....x_T\), and potentially labels \(y_1, y_2, ...y_T\), where \(y_t\) is 1 when \(x_t\) is an anomaly and 0 otherwise, for any time \(t &amp;lt; T\) the method should be able to classify \(x_t\) as normal or anomalous.&lt;/p&gt; &lt;p&gt;We begin by defining what an anomaly means in the context of time series. Darban et al &lt;d-cite key=&quot;darban2022deep&quot;&gt;&lt;/d-cite&gt; classify temporal anomalies as follows. Global, contextual, seasonal, trend, and shapelet. Global anomalies are points with extreme values in comparison with the rest of the series. Contextual anomalies are deviations from the context or neighborhood of a point. Seasonal anomalies are deviations from the typical seasonal patterns in the series. Trend anomalies are transitions or deviations from a local or global trend in a series, and shapelet anomalies are subsequences that have different shapelets from the sequence. Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; make a further distinction by defining abnormal points as points that are either anomalous or missing. Conversely, points that are neither anomalous nor missing are classified as normal. These definitions give us a framework for analyzing what types of anomalies a method is able to detect.&lt;/p&gt; &lt;h2 id=&quot;overview-on-vaes&quot;&gt;Overview on VAE’s&lt;/h2&gt; &lt;p&gt;Generative modeling refers to the objective of generating data from compact, low-dimensional representations. Representation learning can be a byproduct of generative modeling, where the generative model attempts to learn lower dimension representations of data such that inputs with similar high level features will be close to each other in the low dimension representation. Conversely, inputs that have dissimilar features will be far away from each other in the lower dimension representation space. These properties imply that the representation learner learns a good representation of the data that captures as much distinguishing information as possible. VAE’s achieve this through a two step process. Given an input x, an encoder is learned that maps the the input to a latent space, and then a decoder takes the latent space representation and maps it back up to the original feature space. The key property of VAE’s is that they can attempt to enforce a specific distribution in the latent space, such that we can sample from it and generate real looking outputs. The goal is to learn a model \(P_{\theta}(x) = \int p_{\theta}(x | z) p_z(z)dz\), where x are the inputs and z is a random variable in our latent space. In DONUT, and in most other VAE methods, \(p_{\theta}(x | z)\) and \(p_z(z)\) are chosen to be gaussian. Given this model, we would like to find the parameters that maximize the log likelihood \(log P_{\theta}(x)\). This is often an intractable integral to solve or approximate, so a trick called importance sampling is used. We can rewrite the integral as&lt;/p&gt; \[P_{\theta}(x) = \int p_{\theta}(x | z) p_z(z) \frac{q_z(z)}{q_z(z)}dz\] &lt;p&gt;where \(q_z(z)\) is a distribution we know how to sample from. Now, we rewrite this expression as an Expectation&lt;/p&gt; \[E_{z \sim q_z}[p_{\theta}(x | z) \frac{p_z(z)}{q_z(z)}]\] &lt;p&gt;We can now use monte carlo integration to estimate this expectation. This estimation will be inefficient to estimate with the wrong choice of \(q_z\). It turns out that&lt;/p&gt; \[q_z(z) = p_{\theta}(z | x)\] &lt;p&gt;is the optimal choice for \(q_z(z)\), and because this distribution might be hard to sample from, we use the variational inference trick where we find an approximation to this distribution by minimizing the objective&lt;/p&gt; \[J_q = KL(q_{\psi}(z | x) || p_{\theta}(z | x))\] &lt;p&gt;Thus we can now define an objective to be minimized that is fully parametrized by \(\theta\) and \(\psi\).&lt;/p&gt; \[J_p = -log E_{z \sim q_{\psi}(z | x)}[p_{\theta}(x | z) \frac{p_z(z)}{q_{\psi}(z | x)}]\] &lt;p&gt;The monte carlo estimate of this expecation produces a baised estimation of \(\theta\), so instead of optimizing the objective directly, we optimize a lower bound of the negated objective. Using Jensen’s inequality and expanding out the log terms, we know that&lt;/p&gt; \[-J_p \geq E_{z \sim q_{\psi}(z | x)}[log p_{\theta}(x | z) + log p_z(z) - log q_{\psi}(z | x)] = E_{z \sim q_{\psi}(z | x)}[log p_{\theta}(x | z)] - KL(q_{\psi}(z | x) || p_z(z))\] &lt;p&gt;This expectation lower bound is known as the ELBO, and is the surrogate objective that VAE’s optimize in order to learn good encoders and decoders.&lt;/p&gt; &lt;h2 id=&quot;donut&quot;&gt;DONUT&lt;/h2&gt; &lt;p&gt;The key goal of DONUT is to take a series with normal data and potentially anomalous data, learn how to represent the normal features of the series, and then use these representations to compute a reconstruction probability score. Intuitively, if the method learns to represent normal inputs well, an anomalous input will have a low chance of being well reconstructed, and thus will have a low reconstruction probability. The challenge is that in order for the method to work really well, it is important that the method does not attempt to learn good representations for anomalous data. Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; achieve this by formulating a modified objective function called the modified ELBO (M-ELBO). The idea is to reduce the contribution of anomalous and missing points to the learned representation. M-ELBO is defined as such&lt;/p&gt; \[E_{z \sim q_{\psi}(z | x)}[\sum_{w = 1}^W \alpha_w log p_{\theta}(x | z)+ \beta log p_z(z) - log q_{\psi}(z | x)]\] &lt;p&gt;Where \(\alpha_w\) is 1 when \(x_w\) is not an abnormal point, and 0 when \(x_w\) is abnormal. \(\beta = (\sum_{w = 1}^W \alpha_w) / W\). We will take a deep dive into this modified elbo through empiricall experiments and by considering what role each term in the objective plays in both the learning of the latent space, and performance.&lt;/p&gt; &lt;p&gt;The authors also introduce two innovations that serve to improve performance, something we will reproduce in our experiments. The first innovation is markov chain monte carlo imputation of the missing points. The authors hypothesize that during testing, the presence of missing points in a given sample window might bias the reconstruction of the window, and thus affect the reconstruction probability, so they introduce iterative generation of normal points that can replace the missing points. Additionaly, the authors implement “missing point injection”. Before each training epoch, they inject missing points into the training samples by randomly selecting a subset of training sample points and removing the points (setting their values to zero). Note that the original samples will be recovered after the epoch is completed. They claim that missing point injection amplifies the effect of M-ELBO by forcing DONUT to learn the normal representation of data in abnormal windows. It certainly helps to improve performance, and we will perform a more thorough emperical analysis on both injection, and the \(\beta\) term in the M-ELBO.&lt;/p&gt; &lt;p&gt;The authors formulate the reconstruction probability as follows. They begin with the expression&lt;/p&gt; \[p_{\theta}(x) = E_{p_{\theta}(z)}[p_{\theta}(x | z)]\] &lt;p&gt;The authors claim that this does not work well emperically, and thus choose to use \(E_{q_{\phi}(z | x)}[log p_{\theta}(x | z)]\) as the reconstruction probability score. If the negation of these scores exceed a given threshold, the point will be classified as an anomaly.&lt;/p&gt; &lt;p&gt;We now describe the model structure of DONUT. The encoder \(q_{\phi}(z | x)\) is represented by a deep fully connected net that maps x to a lower dimension feature space. Then there are two readout heads that map the learned features from the net to a mean and variance, which we will denote \(\mu_z\) and \(\sigma_z\). We can then sample \(z\) from \(N(\mu_z, \sigma_z)\). The decoder \(p_{\theta}(x | z)\) is represented by a deep fully connected net that maps a latent variable \(z\) to a larger feature space. There are then two readout heads that map the learned features to a mean and variance, which we will denote \(\mu_x\) and \(\sigma_x\). We can then sample \(x\) from \(N(\mu_x, \sigma_x)\)&lt;/p&gt; &lt;h2 id=&quot;experimental-setting-and-evaluation&quot;&gt;Experimental Setting and Evaluation&lt;/h2&gt; &lt;p&gt;Before we lay out the experimental findings and their implications, we begin by briefly describing the datasets used and their characteristics, model architectures, training, and the metrics used for evaluation. We will use three datasets for experimentation, two of which come from the repository of the original paper. The first dataset is called “cpu” and is a series representing some cpu related kpi sampled every minute. The second dataset is called “g” and is also sampled every minute. The third dataset is air temperature time series from CIMIS station 44 in Riverside California, sampled at hourly intervals in the month of March from 2009 to 2019. The dataset did not come with time stamps. These series are all mostly normal, with few anomaly points. This makes the problem more challenging and interesting.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_plot-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_plot-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_plot-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_plot.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of the cpu series with anomaly points colored red &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_decomp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_decomp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_decomp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cpu_series_decomp.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of the cpu series seasonal decomposition &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_plot-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_plot-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_plot-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_plot.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of the g series with anomaly points colored red &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_seasonal-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_seasonal-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_seasonal-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/g_series_seasonal.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of the g series seasonal decomposition &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_plot-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_plot-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_plot-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_plot.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of the weather series with anomaly points colored red &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_decomp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_decomp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_decomp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/weather_series_decomp.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of the weather series seasonal decomposition &lt;/div&gt; &lt;p&gt;The cpu time series shows clear seasonality, and has an anomaly ratio of .015. The second series has much less clear seasonality, an anomaly ratio of .06, and is generally less smooth than the first series. This allows us to test the robustness of DONUT on a series that showcases less obvious seasonality, and draw some insights and comparisons on performance on series with relatively different smoothnesses. The weather series also displays clear seasonality and is smoother than the second series, but it differs from the other two series in that there are no anomalies in the training data. Thus, DONUT must learn to detect anomalies by training on purely normal data.&lt;/p&gt; &lt;p&gt;We create the training and testing data as follows. We begin by standardizing both the training and testing splits in order to represent all of the series on the same scale. We then set any missing values in the series to zero. Finally, we perform slide sampling in order to turn the series into windows of length \(W\). For each window, we will be predicting whether the last value in the window is an anomaly or not. We use a window size of 120 for the first two datasets which means our windows encapsulate two hours of information. For the weather dataset, we use a window size of 24, so each window encapsulates a day of information.&lt;/p&gt; &lt;p&gt;We will use the same metrics described by Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt;. Given the probability reconstruction scores, we can compute the precision and recall for a given threshold. The original paper and our experiments are not entirely concerned with the process of automatic threshold selection, so f scores and the ROC curve serve as valid evaluation metrics of the reconstruction probability scores. The main performance metric will be the best f score. Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; also allow for delayed detection. That is, in any contiguous sequence of anomaly points, if a point is classified as an anomaly, then the anomaly points that came before it will also be classified as an anomaly if they had not previously been before. The authors claim this is reasonable, as the delay, (the time between the first detected anomaly point and the first point in the contiguous sequence) is quite low. This also seems reasonable in a practical setting, as being able to detect an anomaly within a few hours of it happening can still give you enough time to act.&lt;/p&gt; &lt;p&gt;For our experiments. We will use fairly small and simple architectures. The baseline VAE in the paper is done using fully connected networks, and so we will use a fully connected network with depth two. We also experiment with CNN VAE’s, and in order to try and compare performance with the fully connected VAE encoders and decoders, we also use a CNN with two layers. We perform experiments on behavior when the latent dimension is increased, and needed to double the width and depth of the fully connected VAE in order to allow for training to converge.&lt;/p&gt; &lt;h2 id=&quot;reproducing-results-and-establishing-baselines&quot;&gt;Reproducing Results and Establishing Baselines&lt;/h2&gt; &lt;p&gt;Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; show that using both mcmc imputation and missing data injection along M-ELBO improves performance over just using M-ELBO. In our first experiment, we compare the performance of DONUT on the cpu dataset with both mcmc imputation and missing data injection, just mcmc impuation, just missing data injection, and neither of the methods. For each configuration, and for future experiments, we will run the full training and prediction loop 10 times and average results due to the randomness baked into training and prediction with VAE’s. We see that the averge best f score is highest when we use both mcmc imputation and missing data injection. We also plot density estimate of the f scores from the four configurations, and notice that the f scores of the configuration with both mcmc imputation and missing data injection show more right skewness and fatter tails that the other four configurations.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Configuration&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;avg best f score over 10 runs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;both&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.642&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;just inject&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.613&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;just mcmc&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.5737&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;neither&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.588&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/f_score_distribution-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/f_score_distribution-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/f_score_distribution-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/f_score_distribution.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of distribution of the f scores shaded by the configuration &lt;/div&gt; &lt;p&gt;Next, we run DONUT with the baseline configurations for each of our three datasets. We randomly sample a third of the training data and plot the selected samples mappings in 3-d z space. We also plot the predicted anomaly points by the model with the highest f score over the 10 runs for each of the three datasets.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;avg best f score over 10 runs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;cpu&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.642&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;g&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.881&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;weather&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;.996&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; noticed a phenomenon they labeled “time gradient” in latent space.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/original_paper_latent-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/original_paper_latent-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/original_paper_latent-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/original_paper_latent.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Plot of learned latent space from the original paper of one of the datasets in the paper. &lt;/div&gt; &lt;p&gt;They noticed that the latent space was spread out according to time of the day, as time of the day likely encoded a large amount of information about the shape of the series. We did not notice such a phenomenon in our experiments. This is likely the result of a difference in experimental setting, but could also be the result of the local variation within the seasonal data, and the fact that similar shapes occur all over the series irrespective of time. We see that on the second datset, DONUT learned to classify many troughs in the series as anomalous. It was able to detect both global anomalies, as well as contextual and seasonal anomalies, as seen by its ability to detect sudden flat shapes in the series and sudden spikes in the unsual places.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The baseline predictions on the g dataset. Ground truth anomalies are colored red and predicted anomalies are green &lt;/div&gt; &lt;p&gt;The performance on the third datset is quite suprising. Given no anomalous data in the train set, DONUT was able to achieve a .996 average best f score on the testing data. This highlights DONUT’s ability to really learn the normal features of a series. Thus during testing, it was not able to reconstruct the anomalous parts of the series as well as the normal parts. While this result does not contradict the claim that it is important to train on both normal and anomalous data, it certainly suggests that there is still value on learning purely normal qualities of the data. M-ELBO does not fully remove learning of anomalous qualities of the data in the latent space, which could lead to unexpectedly high reconstruction probability scores on anomalous testing data&lt;/p&gt; &lt;h2 id=&quot;understanding-the-latent-space&quot;&gt;Understanding The Latent Space&lt;/h2&gt; &lt;p&gt;It is important that we gain further insights on the latent space, as it is the bottle neck of any VAE method. We will perform a variety of experiments that aim to fully uncover how each term in ELBO controls the characteristics of the latent space. We begin by first explaining the findings and claims of the original paper.&lt;/p&gt; &lt;p&gt;The authors claim that the number of dimensions in the latent space plays a huge role. A small dimension latent space would not allow you to capture enough information, and too big a latent space would cause DONUT to perhaps capture too much information, including anomalous representations. They found that latent dimensions between 3 and 10 typically produced good results. They next discuss how they believe each term in the ELBO contributes to the time gradient phenomena they observe. We restate the M-ELBO objective&lt;/p&gt; \[E_{z \sim q_{\psi}(z | x)}[\sum_{w = 1}^W \alpha_w log p_{\theta}(x | z)+ \beta log p_z(z) - log q_{\psi}(z | x)]\] &lt;p&gt;We can rewrite this objective as&lt;/p&gt; \[E_{z \sim q_{\psi}(z | x)}[\sum_{w = 1}^W \alpha_w log p_{\theta}(x | z)+ \beta log p_z(z)] + H[z | x]\] &lt;p&gt;Where \(H[z | x]\) is entropy. The authors claim that the first term, \(log p_{\theta}(x | z)\) requires the latent space to be able to reconstruct normal x well, thus it pushes latent representations of dissimilar x further away from eachother. The second term, \(log p_z(z)\), serves to encourage the gaussian shape in the latent space and thus encourages the latent space to not expand too much. However, we shrink the contribution of this term by the ratio of normal points in our training data. The entropy term encourages expansion of the latent space, as it is largest when the latent space encodes as much information as possible. This should happen when the latent represenations are as distinguishing as possible.&lt;/p&gt; &lt;h2 id=&quot;effects-of-changing-the-latent-distribution&quot;&gt;Effects of Changing the latent distribution&lt;/h2&gt; &lt;p&gt;Most VAE methods traditionally represent the latent space as a mixture of gaussians, both for its simplicty, as well as its flexibility and ability to approximate many complicated distributions. What happens when we use other types of distributions? We will analyze what happens to performance and the shape of the latent space when we represent it as a mixture of Student-T distributions with 10 degrees of freedom. We hypthesize that replacing a mixture of gaussians with a mixture of any other symmetric distribution will not cause any profound differences in the shape of the latent space, at least in 3 dimensions, however, a symmetric latent space with fatter tails could lead to worse reconstruction performance. Consider \(P_{\theta}(x | z)\), where z is sampled from the latent space. With a fatter tailed distribution, we are more likely to sample a z that is further away from the mean of its distribution. This behavior can be beneficial for generative purposes but for reconstruction purposes, this behavior is likely detrimental and will lead to lower likelihoods that a given x came from the sampled z. We now analyze the empericall effects for all three datasets. For the cpu dataset, we notice that the latent space does not look drasticaly different, considering we only plot a random subset of it. We do however notice a performance dip.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Latent Distribution&lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gaussian&lt;/td&gt; &lt;td&gt;.642&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;t with 10 df&lt;/td&gt; &lt;td&gt;.593&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_tdist_cpu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_tdist_cpu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_tdist_cpu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_tdist_cpu.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset made by guassian mixture, and the image on the right is the latent space of the cpu dataset made by a t-distribution mixture &lt;/div&gt; &lt;p&gt;Similarly for the g dataset, we see a slight performance reduction, but a similarly shaped latent space.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Latent Distribution&lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gaussian&lt;/td&gt; &lt;td&gt;.8809&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;t with 10 df&lt;/td&gt; &lt;td&gt;.871&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/laten_space_tdist_g-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/laten_space_tdist_g-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/laten_space_tdist_g-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/laten_space_tdist_g.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the g dataset made by guassian mixture, and the image on the right is the latent space of the g dataset made by a t-distribution mixture &lt;/div&gt; &lt;p&gt;For the weather dataset, the performance reduction is negligible which suggests that the means of our learned latent space truly represent the normal patterns of the series. (Note that this dataset did not come with timestamps. Disregard any time colorations on latent space plots)&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Latent Distribution&lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gaussian&lt;/td&gt; &lt;td&gt;.996&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;t with 10 df&lt;/td&gt; &lt;td&gt;.995&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_tdist_weather-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_tdist_weather-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_tdist_weather-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_tdist_weather.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the weather dataset made by guassian mixture, and the image on the right is the latent space of the weather dataset made by a t-distribution mixture &lt;/div&gt; &lt;p&gt;This brief analysis suggests that the gaussian distribution is truly a good adaptable choice for our latent space. It allows for some variability when doing generative modeling, but also allows for a more robust estimator of reconstruction probability.&lt;/p&gt; &lt;h2 id=&quot;should-we-scale-the-entropy-term-in-m-elbo&quot;&gt;Should we Scale the Entropy term in M-ELBO?&lt;/h2&gt; &lt;p&gt;Xu et al &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; provide a weighting system for the first two terms of M-ELBO, but choose not to add any weighting to the entropy term. They hypothesize that because of the entropy term’s role in expanding the latent space, it is perhaps better to keep it untouched. We will perform an empircal analysis on the effects on weighting the entropy term.&lt;/p&gt; &lt;p&gt;In our first experiment, we choose a reasonable choice for the weight of the entropy term. We will use \(\beta\) to weight both \(logP_{z}(z)\) and \(logq_{\psi}(z | x)\). Thus M-ELBO becomes&lt;/p&gt; \[E_{z \sim q_{\psi}(z | x)}[\sum_{w = 1}^W \alpha_w log p_{\theta}(x | z)+ \beta log p_z(z) - \beta log q_{\psi}(z | x)]\] &lt;p&gt;We can reformulate the M-ELBO in terms of the KL divergence to hypothesize what effects scaling \(logq_{\psi}(z | x)\) by \(\beta\) might have.&lt;/p&gt; \[E_{z \sim q_{\psi}(z | x)}[log p_{\theta}(x | z)] - KL(q_{\psi}(z | x)^{\beta} || p_z(z)^{\beta})\] &lt;p&gt;Using the power rule of logarithms, we can rewrite this objective as&lt;/p&gt; \[E_{z \sim q_{\psi}(z | x)}[log p_{\theta}(x | z)] - \beta KL(q_{\psi}(z | x) || p_z(z))\] &lt;p&gt;Thus we have essentially applied shrinkage to the KL divergence between the prior and the posterior based on the amount of abnormal data in our training data. This would perhaps encourage the latent space to look more gaussian, such that the prior probability dominates the posterior probability in order to increase the M-ELBO lower bound. Thus we can hypothesize that our latent space will perhaps experience shrinkage. This would certainly be undesired behavior if our goal is to expand our latent space and allow for more distinguishing latent space represenations while keeping some form of structure.&lt;/p&gt; &lt;p&gt;We now analyze the emperical results. We first analyze the effects on the cpu dataset. There does seem to be signs of shrinkage in the latent space when it is weighted, however there is no clear absolute shrinkage or expansion. The shape is certainly different, and it seems like the latent space expanded in the negative direction in the second dimension of the latent space, and shrunk in the positive direction. We also observe a performance increase.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unweighted Entropy&lt;/td&gt; &lt;td&gt;.642&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Weighted Entropy&lt;/td&gt; &lt;td&gt;.665&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space%20cpu.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_cpu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_cpu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_cpu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_cpu.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset without weighting the entropy term, and the image on the right is the latent space of the cpu dataset with a weighted entropy term &lt;/div&gt; &lt;p&gt;On the g dataset, we can certainly see a differently shaped latent space. We notice that the third dimension of the latent space expanded, while the first and second dimensions showed some level or shrinkage compared to the baseline. We do see a slight reduction in performance compared to the baseline&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unweighted Entropy&lt;/td&gt; &lt;td&gt;.8809&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Weighted Entropy&lt;/td&gt; &lt;td&gt;.875&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_g.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_g-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_g-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_g-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_g.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the g dataset without weighting the entropy term, and the image on the right is the latent space of the g dataset with a weighted entropy term &lt;/div&gt; &lt;p&gt;Finally, for the weather dataset, we also see that weighting the entropy term did not lead to absolute expansion or shrinkage of our latent space. We observe shrinkage in the third dimension of the latent space, slight shrinkage in the first dimension, and slight expansion in the second dimension. We also observe a slight performance dip.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unweighted Entropy&lt;/td&gt; &lt;td&gt;.9967&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Weighted Entropy&lt;/td&gt; &lt;td&gt;.9928&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_weather.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_weather-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_weather-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_weather-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_q_weather.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the weather dataset without weighting the entropy term, and the image on the right is the latent space of the weather dataset with a weighted entropy term &lt;/div&gt; &lt;p&gt;These results suggest that weighting the entropy term can lead to shrinkage of the latent space. It certainly lead to different latent space shapes, where we observed expansion in some dimensions and shrinkage in others. There are also no conclusive results in its affects on performance, as we saw improved performance in one dataset and decreased performance in the other two.&lt;/p&gt; &lt;p&gt;We will now perform a more general experiment on the effects on weighting the entropy term with the cpu dataset. Instead of weighting the entropy term with \(\beta\), we will try different weights between 0 and 1 and observe the effects. We increased the capacity of our VAE network, so we rerun the experiments on weighting entropy with \(\beta\) and not weighting entropy in order to have a valid comparison of results.&lt;/p&gt; &lt;p&gt;When the entropy term is weighted by zero, we notice a very speherically shaped latent space which looks like a unit gaussian ball. This matches up with a quick evaluation of the elbo. There is no more reshaping of our latent space by the entropy term, and thus DONUT learns a latent space that matches up with the gaussian prior. With a weight of .2, we again see a circular latent space, however there is more deviation from a spherical shape. We continue to see this phenomenon of deviating from a spherical shape when the weights increase. We also notice that the points become more clustered as the weights increase. There seems to be a level of shrinkage as the weights increase, but for weights equal to .8 and .9, we see the latent space expand again. These results indicate that it is unlikely that weighting the entropy term has any effect on expanding the latent space. Results even suggest that non zero weights can lead to shrinkage. However, weighting the entropy term certainly affects the shape of our latent space, and the ability of the VAE to learn representations that look less guassian.&lt;/p&gt; &lt;p&gt;The performance results provide some interesting insights, and can serve to motivate future areas of exploration. We see that performance is maximal when the weights are very low, or around .6 and .7. When the weights are low, the latent space is very constrained, and thus DONUT will learn learn purely normal representations of the data. As the weights increase, and the ability of DONUT to learn latent representations that deviate from purely guassian increases, we generally see consistently good performance that is comparable to the zero weight case. With weights larger than .8, we begin to see a dip in performance. With large weights, the latent space deviates the most from being gaussian shaped and perhaps begins to learn anomalous representations. This suggests a benefit to enforcing more normality and constraint on the shape of our latent space for the purposes of anomaly detection. This could mean not weighting the prior term by \(\beta\), or adding some additional terms to M-ELBO that somehow enforce the latent space to look more guassian.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;weight&lt;/th&gt; &lt;th&gt;avg best f score over 5 runs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;.682&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.1&lt;/td&gt; &lt;td&gt;.673&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.2&lt;/td&gt; &lt;td&gt;.657&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.3&lt;/td&gt; &lt;td&gt;.602&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.4&lt;/td&gt; &lt;td&gt;.666&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.5&lt;/td&gt; &lt;td&gt;.634&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.6&lt;/td&gt; &lt;td&gt;.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.7&lt;/td&gt; &lt;td&gt;.688&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.8&lt;/td&gt; &lt;td&gt;.602&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.9&lt;/td&gt; &lt;td&gt;.625&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 or unweighted&lt;/td&gt; &lt;td&gt;.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Beta weighted&lt;/td&gt; &lt;td&gt;.6&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_cpu_expandednet-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_cpu_expandednet-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_cpu_expandednet-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_latent_space_cpu_expandednet.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q0-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q0-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q0-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q0.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset without weighting the entropy term, and the image on the right is the latent space of the cpu dataset with a weight of 0 on the entropy term &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q4.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset with a weight of .2 on the entropy term, and the image on the right is the latent space of the cpu dataset with a weight of .4 on the entropy term. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q6-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q6-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q6-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q6.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q8-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q8-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q8-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_q8.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset with a weight of .6 on the entropy term, and the image on the right is the latent space of the cpu dataset with a weight of .8 on the entropy term &lt;/div&gt; &lt;h2 id=&quot;empirical-exploration-of-the-effects-of-beta-and-the-missing-data-injection-rate&quot;&gt;Empirical Exploration of the Effects of Beta and the Missing Data Injection Rate&lt;/h2&gt; &lt;p&gt;We now perform analysis on exactly how \(\beta\) affects $p_z(z)$, both through experimenting with differing rates of missing data injection, as well as manually adjusting \(\beta\) and observing the results.&lt;/p&gt; &lt;p&gt;We restate M-ELBO in tems of the KL divergence.&lt;/p&gt; \[E_{z \sim q_{\psi}(z | x)}[log p_{\theta}(x | z)] - KL(q_{\psi}(z | x) || p_z(z)^{\beta})\] &lt;p&gt;As \(\beta\) decreases, the KL divergence increases. In order to decrease the divergence, the VAE should decrease the posterior probability, which could lead to a more spread out or non gaussian looking latent space, or rather one where we learn very distinguishing representations. As seen from our previous analysis, this might be undesired behavior for the purposes of anomaly detection. Performing automatic reduction of \(\beta\) by increasing the missing data injection rate could prevent DONUT from learning enough about the normal patterns in the training data, and thus performance will likely suffer if the injection rate gets too large.&lt;/p&gt; &lt;p&gt;We begin first by trying out \(\beta\) values between 0 and 1 in order observe the effects, and motivate adjusting the missing data injection rate.&lt;/p&gt; &lt;p&gt;When \(\beta\) is set to to 0, we see that the latent space looks fairly compact and non spherical. At \(\beta\) between .1 and .4, we can see that the latent space is quite spread out, and displays some spherical properties, especially for \(\beta\) = .3. For \(\beta\) between .4 and .9, we can see that the sampled latent space begins to look more and more compact, yet there is still a reasonable spread in the latent space. There does not seem to be a clear relationship between the spread and shape of the latent space and perfomance, however, we note that the \(\beta\) that resulted in the highest performance was \(\beta\) = .3, whose latent space looks the most spherical. This again supports the notion that when the latent space looks more gaussian, anomaly detection is improved.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Beta&lt;/th&gt; &lt;th&gt;avg best f score over 5 runs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;.648&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.1&lt;/td&gt; &lt;td&gt;.595&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.2&lt;/td&gt; &lt;td&gt;.591&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.3&lt;/td&gt; &lt;td&gt;.686&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.4&lt;/td&gt; &lt;td&gt;.633&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.5&lt;/td&gt; &lt;td&gt;.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.6&lt;/td&gt; &lt;td&gt;.623&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.7&lt;/td&gt; &lt;td&gt;.614&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.8&lt;/td&gt; &lt;td&gt;.669&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;.9&lt;/td&gt; &lt;td&gt;.646&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1 or unweighted&lt;/td&gt; &lt;td&gt;.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Beta weighted&lt;/td&gt; &lt;td&gt;.6&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_0-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_0-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_0-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_0.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset with a weight of zero on the prior term, and the image on the right is the latent space of the cpu dataset with a weight of .1 on the prior term &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_3.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_5.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset with a weight of .3 on the prior term, and the image on the right is the latent space of the cpu dataset with a weight of .5 on the prior term. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_7.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_9-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_9-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_9-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/latent_space_cpu_pbeta_9.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The image on the left is the latent space of the cpu dataset with a weight of .7 on the prior term, and the image on the right is the latent space of the cpu dataset with a weight of .9 on the prior term &lt;/div&gt; &lt;p&gt;In our experiments on adjusting the missing injection rate, we saw a significant decrease in performance as the rate increased, even reaching an average best f score of .06 when the rate was .8. It is unclear from our experiments whether this is the result of training not converging, as we do observe high loss values, or simply bad performance of DONUT when a vast majority of the data is missing, which would be expected behavior. This is something that would need to be explored further&lt;/p&gt; &lt;h2 id=&quot;improving-vae-architecture&quot;&gt;Improving VAE Architecture&lt;/h2&gt; &lt;p&gt;For the purposes of simplicity, DONUT utilizes fully connected layers for both the encoder and the decoder. While these choices certainly produce decent results, perhaps we can implement architectures that can better utilize the temportal information encoded within each window. We explore using a one dimensional CNN for the encoder in DONUT. Perhaps CNNs are better able to learn representations that encode more temporal information within a sample window. In order to make the CNN network as comparable as possible with the fully connected network, we will only use two convolution layers. We apply a kernel size of 3, and a stride of 1. We also use max pooling to downsample the data.&lt;/p&gt; &lt;p&gt;For the cpu dataset, we observe significant performance improvements with the CNN architecture. We notice the detection of contextual anomalies, which are non obvious local deviations. The latent space looks fairly spherical, however there does not seem to be any noticeable time gradient behavior in the latent space, despite the improved ability of the encoder to take advantage of temporal information.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Architecture&lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2 layer CNN&lt;/td&gt; &lt;td&gt;.714&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2 layer fc&lt;/td&gt; &lt;td&gt;.642&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/best_baseline_pred_cpu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/best_baseline_pred_cpu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/best_baseline_pred_cpu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/best_baseline_pred_cpu.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_cpu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_cpu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_cpu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_cpu.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; On the top are the ground truth and predicted anomaly points by the baseline DONUT with fully connected encoders on the cpu dataset. On the bottom are the ground truth and predicted anomaly points by DONUT with CNN encoders on the cpu dataset. Ground truth anomalies are colored in red, and predicted anomalies are colored in green &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_cpu-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_cpu-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_cpu-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_latent_space_cpu.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Latent space plot for DONUT with CNN encoders on the cpu dataset &lt;/div&gt; &lt;p&gt;We did not see this same performance improvement in the other two datasets. Additionally, we struggled to achieve stable training on the weather dataset, and so further work needs to be done to achieve convergence in order to perform evaluations on the efficiacy of CNNs with that dataset. For the g dataset, we noticed a significant performance reduction. The difference between the performance on the cpu dataset and the g dataset could suggest that CNN architectures could lead to overfitting on less smooth time series. Looking at the plot of predicted anomalies seems to suggest this, as DONUT with a CNN encoder seems to predict that a larger number of the troughs in the g series are anomaly points, an indicator of potential overfitting to the series pattern.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Architecture&lt;/td&gt; &lt;td&gt;avg best f score over 10 runs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2 layer CNN&lt;/td&gt; &lt;td&gt;.824&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2 layer fc&lt;/td&gt; &lt;td&gt;.881&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/baseline_pred_g.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_g-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_g-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_g-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Exploring-Generative-Models-In-Time-Series/cnn_pred_g.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; On the top are the ground truth and predicted anomaly points by the baseline DONUT with fully connected encoders on the g dataset. On the bottom are the ground truth and predicted anomaly points by DONUT with CNN encoders on the g dataset. Ground truth anomalies are colored in red, and predicted anomalies are colored in green &lt;/div&gt; &lt;p&gt;This is an interesting area of exploration for DONUT. There are a variety of architectures such as RNN’s and transformers that have shown superior performance on time series data, and those could be adapted to this method to improve performance over both CNN and fully connected architectures.&lt;/p&gt; &lt;h2 id=&quot;choosing-number-of-latent-space-dimensions&quot;&gt;Choosing Number of Latent Space Dimensions&lt;/h2&gt; &lt;p&gt;For the purposes of plotting the latent space in our experiments, we chose to use use a latent space with dimension three. However, intuitively, and as shown in the paper, choosing higher a higher dimension latent space can lead to performance improvements. &lt;d-cite key=&quot;xu2018unsupervised&quot;&gt;&lt;/d-cite&gt; explain that not only does increasing the size of the latent space increase the quality of the representations learned in the latent space, but it also serves to improve the stability of training. We will look to explore whether the properties of a series can give insight into good choices for the size of the latent space, and perhaps motivate automatic selection of the number of dimensions in the latent space.&lt;/p&gt; &lt;p&gt;We hypothesize that smoother series do not need as large a dimension in the latent space as series that display higher levels of roughness. Intuitively, in smoother series, the anomalies should be more “obvious”, while in less smooth series, rough behavior could be mistaken for an anomalous pattern.&lt;/p&gt; &lt;p&gt;We take a technique from smoothing splines, which are function estimates obtained from noisy observations of some data process. Smoothing splines enforce a roughness penalty on the function estimate, defined as such &lt;d-cite key=&quot;wang2011smoothing&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; \[\int \hat{f}(x) ^2 dx\] &lt;p&gt;We will use a finite difference estimate of this penalty on the standardized series to define a metric that can be used to describe the roughness/smoothness the series. Now that we have defined a metric describing the smoothness of a series, we can evaluate the best choice of number of latent dimension for series of differing levels of smoothness. In order to converge during training, we had to double the width of the fully connected VAE, and also double its depth.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;th&gt;Roughness Penalty&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;cpu&lt;/td&gt; &lt;td&gt;.061&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;g&lt;/td&gt; &lt;td&gt;.598&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;weather&lt;/td&gt; &lt;td&gt;.023&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We begin with the cpu dataset. We notice that performance significantly increases when the latent space is 6 dimensions, but performance begins to drop off as the number of dimensions increases, which suggests overfitting.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;number of dimensions&lt;/th&gt; &lt;th&gt;avg best f score over 5 iterations&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;. 637&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;.833&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;.826&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;.797&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;For the g dataset, performance peaks when the latent space has 9 dimensions. We also see slightly better performance with a latent space dimension of 12 compared to 6&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;number of dimensions&lt;/th&gt; &lt;th&gt;avg best f score over 5 iterations&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;. 889&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;.882&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;.894&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;.885&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;For the weather dataset, we notice a consistent performance improvement when the number of dimensions is increased.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;number of dimensions&lt;/th&gt; &lt;th&gt;avg best f score over 5 iterations&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;. 994&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;.997&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;.998&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;These results do not provide any clear picture on whether there is any relationship between the smoothness of a series and the best choice for the number of latent dimensions. For our smoothest series (weather), we observed consistent improvement as the number of dimensions increases. The roughest series (g) also seems to show this behavior. However, we see that increasing the number of dimensions for the cpu dataset decreases performance.&lt;/p&gt; &lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding Thoughts&lt;/h2&gt; &lt;p&gt;Generative models present an interesting approach to the problem of anomaly detection in time series. They present an extremely customizable class of hypotheses that allow us to design a fairly robust probabilistic anomaly detector. Through the experiments we ran, we gained further insights into DONUT, and VAE’s more generally as anomaly detectors. We explored what characteristics of the learned latent space can lead to improved anomaly detection performance, and how we can modify ELBO to achieve those goals. We also see that there is huge potential for exploring more complex encoder architectures for additional performance improvements. Perhaps VAE’s can become a robust tool for anomaly detection, and provide benefit to a large variety of peoples and industries.&lt;/p&gt; </content> </entry> <entry> <title>Graph Transformers</title> <link href="https://deep-learning-mit.github.io/blog/2023/graphs-transformers/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/graphs-transformers</id> <content type="html">&lt;h2 id=&quot;motivation--project-outline&quot;&gt;Motivation &amp;amp; Project outline&lt;/h2&gt; &lt;p&gt;Our project aims to advance the understanding of Transformers in graph theory, focusing on the Shortest Path Problem, a cornerstone of graph theory and Dynamic Programming (DP). We introduce a custom Graph Transformer architecture, designed to tackle this specific challenge. Our work begins with a theoretical demonstration that the shortest path problem is Probably Approximately Correct (PAC)-learnable by our Graph Transformer. We then empirically test its performance, comparing it against simpler models like Multilayer Perceptrons (MLPs) and sophisticated benchmarks like Graph Neural Networks (GNNs). This study seeks to validate the Graph Transformer as an effective tool for solving fundamental graph-based problems, and “simple” DP problems in particular.&lt;/p&gt; &lt;div class=&quot;row align-items-center mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/erdos_renyi.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/erdos_renyi.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/erdos_renyi.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/erdos_renyi.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer-architecture-diagram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer-architecture-diagram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer-architecture-diagram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer-architecture-diagram.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; Left: example of an Erdős–Rényi graph, right: original Transformer architecture &lt;/div&gt; &lt;h2 id=&quot;introduction--literature-review&quot;&gt;Introduction &amp;amp; Literature review&lt;/h2&gt; &lt;p&gt;Transformers have shown significant effectiveness in domains that require an understanding of long-range dependencies and contextual information. Originally prominent in natural language processing&lt;d-cite key=&quot;devlin2018bert&quot;&gt;&lt;/d-cite&gt;, their applications have expanded to include areas such as computer vision&lt;d-cite key=&quot;dosovitskiy2020image&quot;&gt;&lt;/d-cite&gt; and speech recognition&lt;d-cite key=&quot;wang2020transformer&quot;&gt;&lt;/d-cite&gt;. Recent explorations have also delved into Transformers’ abilities in mathematical tasks like arithmetic, GCD computations, and matrix operations&lt;d-cite key=&quot;DBLP:journals/corr/abs-2112-01898&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;charton2023transformers&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;lample2019deep&quot;&gt;&lt;/d-cite&gt;, shedding light on the learning mechanisms of these models.&lt;/p&gt; &lt;p&gt;A particular area of interest within these applications is graph problems. Recent research has assessed Transformers’ performance in this domain&lt;d-cite key=&quot;DBLP:journals/corr/abs-2106-05234&quot;&gt;&lt;/d-cite&gt; and explored adapting the Transformer architecture to fit the context of graph problems&lt;d-cite key=&quot;DBLP:journals/corr/abs-1905-12712&quot;&gt;&lt;/d-cite&gt;. However, much of the current research does not focus on Transformers’ comprehension of fundamental graph challenges, such as the shortest path problem. Notably, in the studies mentioned above, the shortest path is often directly input as a matrix, with each entry $i,j$ representing the shortest path distance between nodes $i$ and $j$. Our study will investigate Transformers’ performance on “raw” graph data, where only edge weights, the adjacency matrix, and positional encodings are provided. The Transformer will be trained to predict the shortest path from a designated node 0 to all other nodes, in the form of an $n\times1$ vector&lt;d-cite key=&quot;DBLP:journals/corr/abs-1905-13211&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;We will demonstrate that, by adapting the Transformer architecture for our purposes, the shortest path problem and other “simple” dynamic programming (DP) challenges are Probably Approximately Correct (PAC)-learnable by the model. Our approach is based on the framework developed for GNNs&lt;d-cite key=&quot;DBLP:journals/corr/abs-1905-13211&quot;&gt;&lt;/d-cite&gt; and adapted to our Graph Transformer.&lt;/p&gt; &lt;h2 id=&quot;graph-transformer-model-design&quot;&gt;Graph Transformer Model Design&lt;/h2&gt; &lt;p&gt;Let’s dive into our Graph Transformer model, drawing inspiration from the classical Transformer architecture.&lt;/p&gt; &lt;h3 id=&quot;vanilla-transformer&quot;&gt;Vanilla Transformer&lt;/h3&gt; &lt;p&gt;We first recall the vanilla architecture of Transformers, described in &lt;d-cite key=&quot;DBLP:journals/corr/VaswaniSPUJGKP17&quot;&gt;&lt;/d-cite&gt;, which is fundamentally built on two key ideas: tokenization and attention, both of which we adapt for graph data.&lt;/p&gt; &lt;p&gt;In our context, think of tokens like the attributes of nodes in Graph Neural Networks (GNNs). These tokens are packets of information, allowing transformers to handle diverse data types, including graphs. The process begins with a token net, which is a sequence of linear and non-linear layers. This is somewhat equivalent to the alternating aggregation and combination stages in a GNN, where each node processes and integrates information from its neighbors.&lt;/p&gt; &lt;p&gt;The real game-changer in transformers, however, is the attention mechanism, layered on top of the token net. This mechanism involves a set of matrices known as query, key, and value. These matrices enable tokens to use information from the nodes they’re paying attention to, in order to learn and update their own values.&lt;/p&gt; &lt;p&gt;Here’s a simple way to visualize it. Imagine each token in the transformer scanning the entire graph and deciding which nodes (or other tokens) to focus on. This process is driven by the query-key-value matrices. Each token creates a ‘query’, which is then matched against ‘keys’ from other tokens. The better the match, the more attention the token pays to the ‘value’ of that other token. Mathematically, this can be expressed as:&lt;/p&gt; \[Attention(Q, K, V) = softmax \left(\frac{QK^T}{\sqrt{d_k}} \right)V\] &lt;p&gt;In this formula, $ Q $, $ K $, and $ V $ represent the query, key, and value matrices, respectively. The term $ \sqrt{d_k} $ is a scaling factor based on the dimensionality of the keys.&lt;/p&gt; &lt;p&gt;While the process in Graph Neural Networks (GNNs) might seem similar, there’s an essential distinction to be made. In GNNs, the flow of information is local, with nodes exchanging information with their immediate neighbors. However, in our Graph Transformer model, we employ self-attention to potentially allow each node (or token) to consider information from the entire graph. This includes nodes that might be several steps away in the graph structure.&lt;/p&gt; &lt;p&gt;One axe of our research is then to explore the potential benefits - or drawbacks - of this global perspective, and seeing how leveraging global information compares to the traditional local feature aggregation used in GNNs, in the context of graph theory challenges like the Shortest Path Problem. By enabling each node to have a broader view of the entire graph, we’re exploring how this approach influences the prediction quality (Accuracy) and the efficiency of path computations, specifically focusing on the speed at which the network adapts and learns (Training Efficiency).&lt;/p&gt; &lt;p&gt;A full Transformer will be a sequence of self-attention layers and MLPs. We now turn to the specifics of how we implement it, starting with tokenization.&lt;/p&gt; &lt;div class=&quot;row justify-content-center align-items-center mt-3&quot;&gt; &lt;div class=&quot;col-md-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer_DL-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer_DL-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer_DL-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/transformer_DL.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; Vanilla Transformer architecture (Source: 6.S898 Lecture) &lt;/div&gt; &lt;h3 id=&quot;tokenization-approach-and-positional-encoding&quot;&gt;Tokenization Approach and Positional Encoding&lt;/h3&gt; &lt;p&gt;The first step in our model is converting graph information (including nodes, edges, and their weights) into a format suitable for transformers. We’ve developed a method to encode this graph data into tokens.&lt;/p&gt; &lt;p&gt;Each token in our system is a vector with a length of $2n$. Here, $n$ represents the number of nodes in the graph. Half of this vector contains binary values indicating whether a connection exists to other nodes (1 for a connection, 0 for no connection). The other half of the vector holds the weights of these edges.&lt;/p&gt; \[\text{Token} = [\text{Edge Connections (Binary Values)}, \text{Edge Weights}] = [\mathbf{a}, \mathbf{w}]\] &lt;p&gt;This structure seems sufficient to capture the essential structure of the graph. But, to further aid the transformer in identifying the shortest path, we can introduce additional local information into these tokens through positional encoding. Encoding positional information of the nodes has already be achieved in various ways, for example, using graph kernels &lt;d-cite key=&quot;DBLP:journals/corr/abs-2106-05667&quot;&gt;&lt;/d-cite&gt;. Here we choose a simpler onehot encoding method : we assign an arbitrary rank to each node and include an indicator vector within the token. This vector, also of size $n$, points to the node’s position. With this addition, each token becomes a vector of size $3n$:&lt;/p&gt; \[\text{Token} = [\text{Edge Connections}, \text{Edge Weights}, \text{Positional Encoding}] = [\mathbf{a}, \mathbf{w}, \mathbf{1}]\] &lt;p&gt;We plan to rigorously test both approaches as part of our diverse model lineup.&lt;/p&gt; &lt;h2 id=&quot;attention-in-graph-transformers---the-necessity-of-a-skip-connection&quot;&gt;Attention in Graph Transformers - the Necessity of a Skip-Connection&lt;/h2&gt; &lt;p&gt;The Query-Key-Value (QKV) Attention Mechanism is a pivotal aspect of how Graph Transformers can effectively learn the Shortest Path Problem. Building on the insights from Dudzik et al. &lt;d-cite key=&quot;dudzik2022graph&quot;&gt;&lt;/d-cite&gt;, who illustrated the capacity of GNNs to tackle Dynamic Programming challenges, including the Shortest Path Problem, we delve into how Transformers might achieve similar feats using attention.&lt;/p&gt; &lt;p&gt;Recall the Bellman-Ford algorithm’s key update step for the Shortest Path Problem, expressed as:&lt;/p&gt; \[d_i^{k+1} = \min_j d_j^k + w_{i, j}\] &lt;p&gt;In this context, our hypothesis is that Transformers could replicate this dynamic through the attention mechanism, which we prove mathematically in Appendix A. The key observation is that the softmax layer would be able to mimic the $ \min $ operator, as long as the query-key cross product is able to retrieve $d_j + w_{i,j}$ for all nodes $i,j$. Intuitively, this can be done if each query token $i$ picks up on the node’s positional encoding, and each key token $j$ on the node’s current shortest path value $d_j$ and edges values $w_j$. Taking the cross product of the onehot encoding $i$ with edges values $w_j$ would then return exactly $w_{i,j}$ for all $i,j$. To select only seighboring connections, we’ll use an appropriated attention mask.&lt;/p&gt; &lt;!-- Imagine queries being tailored to pinpoint the positional encoding of node $i$, while keys focus on the edge value $w_{i,j}$ between node $i$ and its neighbor $j$, as well as the connections $a_j$ which inform about the current shortest distance $d_j$. The attention would concentrate on neighbors, considering both the edge weights and the current shortest distances of these neighbors. The softmax step in the attention mechanism would then allow the token for node $i$ to zero in on the neighbor that minimizes a combination of edge weight and actual distance, paralleling the logic in the Bellman-Ford algorithm. --&gt; &lt;p&gt;However, there is a catch. The learning process might not fully grasp the Bellman-Ford update using the attention mechanism alone. After the attention picks up on the correct minimizer neighbour token $j$, it needs to update the the current node $i$’s values. The Bellman-Ford update isn’t a simple operation on the tokens like a sum. For instance, we only want $d_i^k$ to change, and we want to update it with the correct $w_{i,j}$. This is where the idea of incorporating a skip-connection mechanism comes into play. By concatenating tokens $i$ (the input) and $j$ (the attention’s output) before feeding them to the MLP layer following the self-attention layer, we could effectively emulate the Bellman-Ford update process.&lt;/p&gt; &lt;p&gt;Overall, combining attention and skip-connection could ensure our Graph Transformer can comprehensively learn and apply the Bellman-Ford logic to solve the Shortest Path Problem. We offer a mathematical proof of this concept in Appendix A, using a slightly different tokenization method.&lt;/p&gt; &lt;p&gt;Additionally, it’s worth considering that our Graph Transformer might be learning an entirely distinct logical process for solving the Shortest Path Problem. Still, proving that such a logic is within the model’s grasp underlines the model’s versatility in addressing some graph-related and/or dynamic programming challenges. We’ll tackle this notion in the next part about learnability and algorithmic alignment.&lt;/p&gt; &lt;div class=&quot;row justify-content-center align-items-center mt-3&quot;&gt; &lt;div class=&quot;col-md-6 mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/skip_connection-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/skip_connection-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/skip_connection-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/skip_connection.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; Graph Transformer - Skip connection &lt;/div&gt; &lt;h3 id=&quot;model-architecture-overview&quot;&gt;Model Architecture Overview&lt;/h3&gt; &lt;p&gt;In this section, we revisit the architecture of our Graph Transformer, which is an adaptation of the standard Transformer model. Our model is composed of a sequence of self-attention layers and MLPs, each augmented with a skip-connection. The tokens in our model encapsulate both edge connections and their corresponding weights, alongside positional encoding.&lt;/p&gt; &lt;p&gt;The most notable feature of our architecture is the introduction of the attention mask. This mask restricts the attention of each token to its immediate neighbors, aligning our approach more closely with the local message-passing process typical in GNNs. The inclusion or not of this feature and the resultant effect in our architecture marks the crucial difference between the global vs. local token aggregation methodologies that we discussed earlier.&lt;/p&gt; &lt;h2 id=&quot;a-measure-of-learnability&quot;&gt;A measure of learnability&lt;/h2&gt; &lt;p&gt;Our project falls into the wider research interest in the interaction between network structures and specific tasks. While basic and common structures such as MLPs are known to be universal approximators, their effectiveness varies based on the amount of data required for accurate approximations. Notably, their out-of-sample performance often lags behind task-specific architectures, such as Graph Neural Networks (GNNs) in graph-related problems, which highlights the issue of a network’s generalization capacity.&lt;/p&gt; &lt;p&gt;To evaluate theoretically the ability of transformers to effectively learn the Shortest Path Problem and similar challenges, we position our study within the framework of PAC (Probably Approximately Correct) Learning. This framework allows us to explore the concept of algorithmic alignment. Algorithmic alignment is here crucial as it pertains to a model’s capability to emulate a given algorithm with a minimal number of modules, each of relatively low complexity. Such approach has already been taken by Xu et. al &lt;d-cite key=&quot;DBLP:journals/corr/abs-1905-13211&quot;&gt;&lt;/d-cite&gt; to give a better understanding of the reasoning process of complex networks like GNNs, and it is instrumental in assessing the adaptability and efficiency of transformers in learning and solving complex graph-based tasks.&lt;/p&gt; &lt;h3 id=&quot;algorithmic-alignment&quot;&gt;Algorithmic Alignment&lt;/h3&gt; &lt;p&gt;In this section, we delve into a series of definitions to establish the mathematical groundwork of our investigation.&lt;/p&gt; &lt;p&gt;We first recall a definition of the PAC-Learnibility:&lt;/p&gt; &lt;h4 id=&quot;definition-pac-learning-and-sample-complexity&quot;&gt;Definition (PAC learning and sample complexity)&lt;/h4&gt; &lt;p&gt;Let \(\{x_i,y_i\}_{i=1}^M\) be i.i.d. samples from some distribution $ \mathcal{D} $, and suppose $ y_i = g(x_i) $ for some underlying function $ g $. Let \(f = \mathcal{A}(\{x_i, y_i\}_{i=1}^M)\) be the function generated by a learning algorithm $ \mathcal{A} $. Then $ g $ is $ (M, \epsilon, \delta) $-learnable with $ \mathcal{A} $ if&lt;/p&gt; \[\mathbb{P}_{x \sim \mathcal{D}} [\| f(x) - g(x) \| \leq \epsilon] \geq 1 - \delta\] &lt;p&gt;where $ \epsilon &amp;gt; 0 $ is the error parameter and $ \delta \in (0, 1) $ the failure probability.&lt;/p&gt; &lt;p&gt;We then define the &lt;em&gt;sample complexity&lt;/em&gt; as \(\mathcal{C_A}(g, \epsilon, \delta) = \min M\) for every $ M $ such that $ g $ is $ (M, \epsilon, \delta) $-learnable with $ \mathcal{A} $.&lt;/p&gt; &lt;p&gt;This is a crucial concept in computational learning theory that helps us understand the feasibility of learning a given function from a set of examples to a certain degree of approximation, with a certain level of confidence.&lt;/p&gt; &lt;p&gt;Next, we outline a definition that connects the concepts of function generation with the architecture of neural networks.&lt;/p&gt; &lt;h4 id=&quot;definition-generation&quot;&gt;Definition (Generation)&lt;/h4&gt; &lt;p&gt;Let $ f_1, \ldots, f_n $ be module functions, $ g $ a reasoning function and $ \mathcal{N} $ a neural network. We say that $ f_1, \ldots, f_n $ generate $ g $ for $ \mathcal{N} $, and we write \(f_1, \ldots, f_n \underset{\mathcal{N}}{\equiv} g\) if, by replacing $ \mathcal{N}_i $ with $ f_i $, the network $ \mathcal{N} $ simulates $ g $.&lt;/p&gt; &lt;p&gt;Using these ideas, we then introduce a key point for our project: algorithmic alignment, which we intend to validate for Transformers applied to the Shortest Path Problem.&lt;/p&gt; &lt;h4 id=&quot;definition-algorithmic-alignment&quot;&gt;Definition (Algorithmic alignment)&lt;/h4&gt; &lt;p&gt;Consider a neural network $ \mathcal{N} $ with $ n $ modules \(\mathcal{N}_i\) that tries to approximate a reasoning function $ g $. Suppose that there exists $ f_1, \ldots, f_n $ some module functions such that \(f_1, \ldots, f_n \underset{\mathcal{N}}{\equiv} g\). Then $ \mathcal{N} $ is $ (M, \epsilon, \delta) $-algorithmically aligned with $ g $ there are learning algorithms \(\mathcal{A}_i\) for the \(\mathcal{N}_i\)’s such that \(n \cdot \max_i \mathcal{C}_{\mathcal{A}_i} (f_i, \epsilon, \delta) \leq M\).&lt;/p&gt; &lt;p&gt;A small number of sample $ M $ would then imply good algorithmic alignment, i.e. that the algorithmic steps $f_i$ to simulate g are &lt;em&gt;easy to learn&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Finally, we state the following theorem, proven by Xu et al. &lt;d-cite key=&quot;DBLP:journals/corr/abs-1905-13211&quot;&gt;&lt;/d-cite&gt; It provides evidence that generalization benefits from algorithmic alignment.&lt;/p&gt; &lt;h4 id=&quot;theorem-1-algorithmic-alignment-improves-sample-complexity&quot;&gt;Theorem 1 (Algorithmic alignment improves sample complexity)&lt;/h4&gt; &lt;p&gt;Fix $\varepsilon$ and $\delta$. Suppose ${x_i, y_i} \sim D$, where $|x_i| &amp;lt; N$, and $y_i = g(S_i)$ for some $g$. Suppose $\mathcal{N}_1, \dots \mathcal{N}_n$ are $\mathcal{N}$’s MLP modules in sequential order. Suppose $\mathcal{N}$ and $g$ algorithmically align via functions $f_1, …, f_n$, as well as the following assumptions.&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;i. Algorithm stability&lt;/strong&gt;&lt;/em&gt;. Let $\mathcal{A}$ be the learning algorithm for the \(\mathcal{N}_i\)’s. Suppose \(f = \mathcal{A}(\{x_i, y_i\}^M_{i=1})\), \(\hat{f} = \mathcal{A}(\{\hat{x}_i, y_i\}^M_{i=1})\). For any x, \(\|f(x) - f(\hat{x})\| &amp;lt; L_0 \cdot \max_i\|x_i - \hat{x}_i\|\), for some \(L_0\).&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;ii. Sequential learning&lt;/strong&gt;&lt;/em&gt;. We train the \(\mathcal{N}_i\)’s sequentially. The inputs for $\mathcal{N}_j$ are the outputs from the previous modules \(\mathcal{N}_1, \dots, \mathcal{N}_{j-1}\), while labels are generated by the correct functions \(f_{1}, ..., f_{j-1}\).&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;iii. Lipschitzness&lt;/strong&gt;&lt;/em&gt;. The learned functions $f_j$ satisfy \(\|f_j(x) - f_j(z)\| \leq L_1\|x - z\|\), for some $L_1$.&lt;/p&gt; &lt;p&gt;Then g is learnable by N.&lt;/p&gt; &lt;h3 id=&quot;application-to-transformers&quot;&gt;Application to Transformers&lt;/h3&gt; &lt;p&gt;We now apply this theoretical framework to Transformers. The justifications of the results in this part will be a combination of sketch of mathematical proofs and empirical evidence. We first state a first result:&lt;/p&gt; &lt;h4 id=&quot;lemma-1-transformers-algorithmically-align-with-the-shortest-path-problem&quot;&gt;Lemma 1 (Transformers algorithmically align with the Shortest Path Problem)&lt;/h4&gt; &lt;p&gt;Let $ \mathcal{T} $ be a Transformer, let $ g $ be the reasoning function of the Shortest Path Problem applied to a graph with $n$ nodes. Then $ \mathcal{T} $ is algorithmically aligned with $ g $.&lt;/p&gt; &lt;p&gt;We can directly prove this lemma. Let $ f_1, \ldots, f_n $ be the Bellman-Ford update processes of the Shortest Path Problem: \(d_u^{k+1} = \min_{v \in \mathcal{N}(u)} d_v^{k} + c(u, v)\) where $\mathcal{N}(u)$ is the set of neighbors of node $u$. From Bellman-Ford algorithm, we have: \(f_1, \ldots, f_n \underset{\mathcal{T}}{\equiv} g\), with $g$ being the shortest path function.&lt;/p&gt; &lt;p&gt;Then, from our discussion on Transformers attention layers and proof in Appendix A, each attention-MLP sequence $\mathcal{N}_i$ has a learning algorithm $\mathcal{A}_i$ such that $f_i$ is learnable with $\mathcal{A}_i$. Each sample complexity is then bounded by M, which concludes the proof.&lt;/p&gt; &lt;p&gt;We can now state the following theorem:&lt;/p&gt; &lt;h4 id=&quot;theorem-2-transformers-can-learn-the-shortest-path-problem&quot;&gt;Theorem 2 (Transformers can learn the Shortest Path Problem)&lt;/h4&gt; &lt;p&gt;Let $ \mathcal{T} $ be a Transformer, let $ g $ be the shortest path function. Then, $g$ is learnable by $\mathcal{T}$.&lt;/p&gt; &lt;p&gt;We provide here a sketch of a proof of this theorem. From Lemma 1, $\mathcal{T}$ and $g$ algorithmically align via $f_1, \ldots, f_n$. We must now check the 3 assumptions of Theorem 1.&lt;/p&gt; &lt;p&gt;Sequential Learning &lt;strong&gt;(ii)&lt;/strong&gt; is clearly true, since transformers architectures incorporate sequence of MLPs (associated with attention layers). Li et al &lt;d-cite key=&quot;li2023transformers&quot;&gt;&lt;/d-cite&gt; have provided an empirical proof of the algorithm stability &lt;strong&gt;(i)&lt;/strong&gt; of transformers. Finally, considering a self-attention token network combined with a ReLU-MLP for each layer of $\mathcal{T}$, every function in the related learning algorithm $\mathcal{A}_i$ (softmax, ReLU, Linear) is Lipschitz-continuous, hence their combination is Lipschitz-continuous too, which validates Assumption &lt;strong&gt;(iii)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We can then apply Theorem 1 and conclude the proof.&lt;/p&gt; &lt;p&gt;Having laid the theoretical foundation for our problem, we now turn our attention to the practical application, where we employ our Graph Transformer to the concrete task of learning and solving the Shortest Path Problem.&lt;/p&gt; &lt;h2 id=&quot;methodology-for-training-and-evaluation&quot;&gt;Methodology for Training and Evaluation&lt;/h2&gt; &lt;h3 id=&quot;constructing-the-dataset&quot;&gt;Constructing the Dataset&lt;/h3&gt; &lt;p&gt;For training and evaluating our different models, we generate a comprehensive dataset comprising 50,000 samples, each representing a graph. These graphs were randomly created following the Erdős–Rényi model, specifically the $\mathcal{G}(n, p)$ variant, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; represents the number of nodes and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; is the probability of edge formation between any two nodes. In our dataset, each graph consists of 10 nodes (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n = 10&lt;/code&gt;), and the edge probability (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt;) is set at 0.5. This setting ensures a balanced mix of sparsely and densely connected graphs, providing a robust testing ground for the Graph Transformer’s ability to discern and compute shortest paths under varied connectivity scenarios .&lt;/p&gt; &lt;p&gt;Furthermore, we assign to the edges in these graphs some weights that are integral values ranging from 1 to 10. This range of weights introduces a second layer of complexity to the shortest path calculations, as the Graph Transformer must now navigate not only the structure of the graph but also weigh the cost-benefit of traversing various paths based on these weights. The inclusion of weighted edges makes the dataset more representative of real-world graph problems, where edges often have varying degrees of traversal difficulty or cost associated with them.&lt;/p&gt; &lt;p&gt;This dataset is designed to challenge and evaluate the Graph Transformer’s capability in accurately determining the shortest path in diverse graph structures under different weight conditions. The small number of nodes ensures a wide variability in the degree of connectivity in a sample graph. It also allows for an initial performance evaluation on smaller-scale problems, with the potential to extend these studies to larger-scale graphs in the future. Hence, the dataset’s structure supports a comprehensive assessment of the model’s performance and its adaptability to a wide range of graph-related scenarios.&lt;/p&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/shortest_path_counts.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/shortest_path_counts.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/shortest_path_counts.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/shortest_path_counts.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;50%&quot; height=&quot;50%&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; Shortest path distribution for our entire dataset (50,000 samples) &lt;/div&gt; &lt;h3 id=&quot;training-protocols&quot;&gt;Training Protocols&lt;/h3&gt; &lt;p&gt;In the fixed dataset approach we’ve employed, the dataset is pre-constructed with 50,000 graph samples and remains unchanged throughout the training process. This method, involving a consistent 60/20/20 split for training, validation, and testing, ensures that every model is assessed under the same conditions at each epoch. This consistency is crucial for our primary goal: to compare the performance of different models or architectures in a controlled and repeatable manner. To an on-the-fly approach, where data is dynamically generated during each training epoch, introduces more variability. This variability can be beneficial in a second step for thoroughly testing the robustness and adaptability of a single model, as it faces new and diverse scenarios in each epoch. However, for our first objective of directly comparing different models, the fixed dataset approach provides a more stable and reliable framework to begin with.&lt;/p&gt; &lt;p&gt;We use the Adam Optimizer because it’s good at handling different kinds of data and works efficiently. The learning rate is set at a standard value of 0.001, which serves as a common and reliable starting point, ensuring a consistent basis for comparing the learning performance across all models.&lt;/p&gt; &lt;p&gt;Our main tool for measuring success is the L1 loss function. This function is suited for our shortest path problem because it treats all mistakes the same, whether they’re big or small. It’s different from the L2 loss, which is harsher on bigger mistakes. This way, our model pays equal attention to finding shorter and longer paths correctly.&lt;/p&gt; &lt;h3 id=&quot;metrics-and-evaluation-criteria&quot;&gt;Metrics and Evaluation Criteria&lt;/h3&gt; &lt;p&gt;We use two main metrics to check how good our models perform: L1 Loss and Accuracy. L1 Loss adds up all the differences between the predicted and actual path costs across all nodes. It’s a direct way to see how well the model is doing.&lt;/p&gt; \[L1 \, Loss = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|\] &lt;p&gt;where $ N $ is the total number of nodes, $ y_i $ is the actual path cost for the $i$-th node, and $ \hat{y}_i $ is the predicted path cost for the $i$-th node.&lt;/p&gt; &lt;p&gt;Accuracy is the second measure. It shows what percentage of nodes the model got exactly right in predicting the shortest path. It’s a simple way to understand how precise our model is.&lt;/p&gt; \[Accuracy = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} \times 100\%\] &lt;p&gt;Here, a prediction is counted as “correct” if its rounded value is the true shortest path. I.e., if the model predicts 10.3 for a node, but the true sortest path is 11, this is marked as incorrect. If it predicts 10.7, it will be counted as correct.&lt;/p&gt; &lt;p&gt;Together, these two measures help us see how well our Graph Transformer is doing compared to other models like MLPs and GNNs, especially in solving shortest path problems in graphs.&lt;/p&gt; &lt;h2 id=&quot;results-and-comparative-analysis&quot;&gt;Results and Comparative Analysis&lt;/h2&gt; &lt;p&gt;In our analysis, we compared the performances of MLPs, Transformers, and GNNs using our generated dataset. Initially, we evaluated the performance of each architecture across different sizes by recording in-sample and out-of-sample losses at each epoch, along with out-of-sample accuracy. We compared three model sizes: “small,” “mid,” and “large,” which correspond to the depth of the model. For GNNs, this signifies the number of iterations; for Transformers and MLPs, it refers to the number of layers. Small models have 2 iterations/layers, mid models 5, and large models 10.&lt;/p&gt; &lt;p&gt;To maintain fair comparisons, the MLP and the Transformer were designed to have an equal total number of trainable parameters at each size. We excluded GNNs from this comparison, as they outperformed both models with significantly fewer parameters.&lt;/p&gt; &lt;h3 id=&quot;gnn-performance&quot;&gt;GNN performance&lt;/h3&gt; &lt;p&gt;Our GNNs demonstrated exceptional performance on the shortest path task. Tailoring the model’s architecture to this problem (using maximum aggregation and initializing node features appropriately) likely contributed to this success. However, several interesting observations emerged from our results. We compared GNNs of three different sizes: small (2 iterations, 13k parameters), medium (5 iterations, 32k parameters), and large (10 iterations, 64k parameters).&lt;/p&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_train_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_train_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_train_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_train_loss.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_loss.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_acc.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_acc.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_acc.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/gnn_val_acc.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; GNN Training loss, validation loss &amp;amp; validation accuracy for different sizes &lt;/div&gt; &lt;p&gt;We observed that both medium and large GNNs achieved over 99% out-of-sample accuracy after just a few epochs. The large model’s performance aligns with expectations, as it conducts 10 iterations in total—matching the maximum number of iterations required by standard shortest-path-finding algorithms like Bellman-Ford for n-node graphs.&lt;/p&gt; &lt;p&gt;Surprisingly, the medium-sized model, with only 5 iterations, also achieved similar accuracy. This initially seems counterintuitive since 5 iterations suggest that information can only propagate to nodes within 5 neighbors. However, as noted in &lt;d-cite key=&quot;DBLP:journals/corr/abs-1905-13211&quot;&gt;&lt;/d-cite&gt;, our GNN effectively learns an optimized version of the Bellman-Ford algorithm, requiring only half the number of iterations compared to the standard algorithm.&lt;/p&gt; &lt;p&gt;This explains why the medium GNN can converge efficiently, but the small model with just 2 iterations cannot. Even with an optimized Bellman-Ford algorithm, a 2-iteration GNN would only correctly solve paths shorter than or equal to 5 nodes, limiting its overall learning capacity.&lt;/p&gt; &lt;h3 id=&quot;mlp-performance&quot;&gt;MLP performance&lt;/h3&gt; &lt;p&gt;Although GNNs quickly converged to near-perfect predictions, their inherent suitability for the shortest path task was expected. To gauge the Transformers’ performance more accurately, we compared them with MLPs, which are not specifically designed for this task. As indicated in &lt;d-cite key=&quot;DBLP:journals/corr/abs-1905-13211&quot;&gt;&lt;/d-cite&gt;, MLPs struggle with iterative algorithms like Bellman-Ford due to difficulties in learning for-loop structures. We analyzed MLP performance across three sizes: small (2 layers, 44k parameters), medium (4 layers, 76k parameters), and large (8 layers, 142k parameters). It’s important to note that each GNN size had roughly half the parameters of the corresponding MLP size.&lt;/p&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_train_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_train_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_train_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_train_loss.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_loss.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_acc.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_acc.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_acc.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/mlp_val_acc.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; MLP Training loss, validation loss &amp;amp; validation accuracy for different sizes &lt;/div&gt; &lt;p&gt;The smaller MLP models converged faster, yet both small and medium models barely exceeded 50% accuracy, even after extensive training (16 epochs for GNNs and 64 for MLPs). This supports the hypothesis that MLPs face challenges in learning iterative algorithms.&lt;/p&gt; &lt;p&gt;Increasing model size or training duration did not significantly improve performance; the largest model struggled particularly with fitting the problem. While more hyperparameter tuning might enhance the “large” model’s performance, the “medium” model’s struggles suggest that MLPs have inherent difficulties with this task, regardless of parameter count.&lt;/p&gt; &lt;h3 id=&quot;transformer-performance&quot;&gt;Transformer performance&lt;/h3&gt; &lt;p&gt;Turning our attention to Transformers, we initially doubted their ability to match GNN performance levels. However, the question remained: could they outperform MLPs, and if yes by how much? We began by testing a basic Transformer version (no attention mask, positional encoding, or skip connection). To ensure fair comparisons, all model sizes maintained approximately the same number of parameters as the MLPs, with equivalent layers/iterations (small: 2 layers, 44k parameters; medium: 5 layers, 86k parameters; large: 10 layers, 172k parameters).&lt;/p&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; Transformer training loss, validation loss &amp;amp; validation accuracy for different sizes &lt;/div&gt; &lt;p&gt;A notable improvement in accuracy was observed, with the best-performing Transformer model reaching 70% accuracy. The training was stopped at 64 epochs to maintain consistency across all models. As it does not show signs of overfitting, extending training beyond 64 epochs might further enhance the Transformer’s performance. Interestingly, increasing the model size to over 150k parameters did not significantly boost performance under our hyperparameter settings. The small and medium architectures exhibited similar performance, with the medium model slightly outperforming after a few epochs.&lt;/p&gt; &lt;p&gt;Regarding sizes, similarly to the MLP, increasing the depth and parameter count of the transformer over 150k parameters doesn’t seem to help with the model’s performance, at least with our set of hyperparameters (as this big of a transformer is long to train, we haven’t been able to do much hyperparameter tuning). The small and medium architectures seem almost tied, but the medium one seems to perform better after a few epochs.&lt;/p&gt; &lt;p&gt;Our hypothesis in Part 1 suggested that Transformers, capable of performing $O(n^2)$ operations per attention head, should learn loop structures more effectively. However, their learning is constrained by the specific operations allowed in the attention mechanism. To test this, we proposed three enhancements to our Transformer: an attention mask, positional encoding, and a skip connection, as outlined in Part 1 and Appendix A. We hypothesized that these additions would enable the Transformer to better learn the Bellman-Ford iteration step.&lt;/p&gt; &lt;h3 id=&quot;transformer-with-attention-mask-positional-encoding--skip-connection&quot;&gt;Transformer with Attention Mask, Positional Encoding &amp;amp; Skip Connection&lt;/h3&gt; &lt;p&gt;As discussed in Part 1, we adapted our Transformer model to include these three components, expecting an improvement in performance. The attention mask, a fundamental feature of Transformers, enables the model to focus on specific token relationships. In our setup, each token (node) attends only to its neighbors, as dictated by the adjacency matrix. We incorporated the attention mask into the medium-sized Transformer for comparison.&lt;/p&gt; &lt;p&gt;Next, we added positional encoding. Based on our Part 1 discussion, positional encodings can inform the feedforward network (FFN) about the neighboring tokens selected by the attention layer. We used basic one-hot encodings, effectively adding an $n×n$ identity matrix or concatenating an $n×1$ one-hot vector to each token. Although more sophisticated encodings might be beneficial, we demonstrated the feasibility of using one-hot encodings for the Bellman-Ford update.&lt;/p&gt; &lt;p&gt;Finally, we implemented a custom skip connection. Instead of a standard sum skip connection, our model concatenates the input and output of the attention head before feeding it into the FFN. This approach potentially allows the attention head to select a neighbor, with the FFN combining its token with the receiving node’s token.&lt;/p&gt; &lt;p&gt;We added each augmentation stepwise, building upon the previous modifications (e.g., transformer_pos_enc includes positional encoding, attention mask, and is medium-sized). Here are the results:&lt;/p&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss_all.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss_all.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss_all.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_train_loss_all.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss_all.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss_all.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss_all.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_loss_all.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc_all.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc_all.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc_all.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-graphs-transformers/comparative_plots/transformer_val_acc_all.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption mt-3&quot;&gt; Transformer training loss, validation loss &amp;amp; validation accuracy for different sizes &lt;/div&gt; &lt;p&gt;Each augmentation step led to clear improvements. Over 64 epochs, our base model’s out-of-sample accuracy improved from 70% to over 90%. The positional encoding contributed the most significant enhancement, which was somewhat surprising given its simplicity. Overall, these results support our hypothesis regarding the Transformer’s capacity to learn the Bellman-Ford iteration step.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this project, we compared MLPs, Transformers, and GNNs in solving graph-related problems, with a focus on the shortest path in Erdos-Renyi graphs. Our findings indicate GNNs excel in such tasks due to their specialized architecture. However, the adaptability of Transformers, particularly with architectural modifications like attention masks, positional encodings, and skip connections, is a significant discovery. While these models showed promise, larger MLP and Transformer models faced convergence issues, highlighting the need for better hyperparameter optimization in future work.&lt;/p&gt; &lt;p&gt;Transformers’ theoretical success in approximating the Bellman-Ford algorithm, verified by empirical results, suggests potential in a subset of dynamic programming (DP) problems where DP updates are simple and manageable by attention heads. However, their capability is inherently limited compared to the theoretically more versatile GNNs, due to the softmax and linear combination constraints in attention mechanisms. Future work could delve into designing Transformer models with enhanced attention mechanisms, potentially broadening their applicability in complex DP problems. Investigating the synergy between Transformers and GNNs could also lead to innovative hybrid models.&lt;/p&gt; &lt;p&gt;Overall, our exploration sheds light on the potential of Transformers in graph-related tasks, suggesting they could offer valuable insights and solutions, alongside the more established GNNs. This finding could open up interesting possibilities for research and innovation in neural network applications, particularly in solving complex graph-related challenges.&lt;/p&gt; &lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt; &lt;h3 id=&quot;appendix-a&quot;&gt;Appendix A.&lt;/h3&gt; &lt;p&gt;We present here a mathematical proof of how the Graph Transformer Architecture can learn the Bellman-Ford update in the Shortest Path Problem.&lt;br /&gt; We consider a slightly different tokenization: for every node $i$, at layer $k$, we encode its information in a tensor of the form:&lt;/p&gt; \[t_i^k = [\mathbb{1}_i, w_i, d_i^k]\] &lt;p&gt;where $\mathbb{1}_i \in \mathbb{R}^n$ is the positional encoding, $w_i \in \mathbb{R}^n$ the edges weights and $d_i^k$ the current shortest distance computed at layer $k$.&lt;/p&gt; &lt;p&gt;Recall the formula of query-key-value attention:&lt;/p&gt; \[t_i = \frac{\sum_{j} \exp^{-q_i&apos; k_j / \sqrt{2n+1}}v_j}{\sum_{j} \exp^{-q_i&apos; k_j / \sqrt{2n+1}}}\] &lt;p&gt;Set up the weights matrices as:&lt;/p&gt; \[\begin{cases} W_Q = \begin{pmatrix} I_{n+1} &amp;amp; O_{n \times n+1} \\ 1_n &amp;amp; 0_{n+1} \end{pmatrix}\\ W_K = \begin{pmatrix} O_{n+1 \times n} &amp;amp; I_{n+1} \end{pmatrix}\\ W_V = I_{2n+1} \end{cases}\] &lt;p&gt;so that \(q_i&apos; k_j = w_{j,i} + d_j\) &lt;em&gt;i.e.&lt;/em&gt; attention is determined by the update values of the Bellman-Ford equation.&lt;/p&gt; &lt;p&gt;Hence taking the softmax - and if necessary augmenting the weights of the matrices by a common factor -, we have the ouput \(t_{j^\star}\) for the appropriate node \(j^\star = \text{argmin}_j \{w_{j,i} + d_j\}\).&lt;/p&gt; &lt;p&gt;Notice that in this configuration \(t_{j^\star}\) is not enough to retrieve the desired edge weight \(w_{i, j^\star}\) : we need the positional encoding from node $i$.&lt;/p&gt; &lt;p&gt;The skip-connection achieves this, by concatenating original input $t_i$ with attention output \(t_{j^\star}\). We can then retrieve the desired value \(w_{j^\star,i} + d_{j^\star}\) with the MLP of layer $k$, which concludes the proof&lt;/p&gt; </content> </entry> <entry> <title>Learning a Lifted Linearization for Switched Dynamical Systems</title> <link href="https://deep-learning-mit.github.io/blog/2023/croneillproposal/"/> <updated>2023-12-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/croneillproposal</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;blockquote&gt; All models are wrong, but some are useful. —George Box &lt;/blockquote&gt; &lt;p&gt;Deep neural networks are incredibly capable of generating models from data. Whether these are models that allow for the classification of images, the generation of text, or the prediction of a physical system’s dynamics, neural networks have proliferated as a favored way of extracting useful, predictive information from set of data &lt;d-cite key=&quot;rombach2021highresolution, Brown2020, Tsipras2020&quot;&gt;&lt;/d-cite&gt;. But while well-tuned and well-designed neural networks can demonstrate miraculous performance at a given task, raw accuracy is not the only measure of a model’s usefulness.&lt;/p&gt; &lt;p&gt;In robotics, the speed at which a model can be run and its explainability can be just as important as the accuracy of its predictions. Techniques such as model predictive control can enable remarkable performance even when they’re based on flawed predictive models &lt;d-cite key=&quot;Rawlings2022&quot;&gt;&lt;/d-cite&gt;. In practice, most of these models are linearizations of more accurate, nonlinear equations. Produced by considering low order truncations of the Taylor series, these linearizations can be run incredibly efficiently on modern computer hardware and are amenable to linear analysis techniques for explainability purposes.&lt;/p&gt; &lt;p&gt;Nevertheless, this kind of linearization has its own weaknesses. Chief among them is the inherently local nature of the approach: a Taylor series must be taken around a single point and becomes less valid further away from this location. As an alternative, lifting linearization approaches inspired by Koopman Operator theory have become more commonplace &lt;d-cite key=&quot;Koopmanism, brunton2021modern, AsadaDE, Lusch2018, Shi2022&quot;&gt;&lt;/d-cite&gt;. These techniques seek to linearize a system by lifting it to a higher dimensional representation where the dynamics can be made to evolve linearly over time. While such models can suffer from the curse of dimensionality when compared to their lower-order Taylor series brethren, they can offer greater accuracy while still providing most of the benefits of a linear model.&lt;/p&gt; &lt;p&gt;\(f(x)|_{x=a}\approx f(a)+\frac{f&apos;(a)}{1!}(x-a)\)&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; A truncated Taylor series makes use of the derivatives of a function around a point. &lt;/div&gt; &lt;p&gt;Deep neural networks have emerged as a useful way to produce these lifted linear models &lt;d-cite key=&quot;Lusch2018&quot;&gt;&lt;/d-cite&gt;. An encoder is used to transform a system’s state into a higher dimensional latent space of “observables”. These observables are then fed through a linear layer which evolves the system forward in time: a linear dynamical model. In the literature, this approach has come to be known as Deep Koopman Networks (DKNs). We can see how these networks can learn lifted linear models for physical systems by considering a simple pendulum.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/deepnet-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/deepnet-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/deepnet-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/deepnet.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; An example of a neural network architectured used to learn observables for a linear Koopman model, taken from &lt;d-cite key=&quot;lusch2018deep&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/DKN_simplepen-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/DKN_simplepen-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/DKN_simplepen-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/DKN_simplepen.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Performance of a DKN for predicting a damped, simple pendulum across a set of trajectories. On the left, the dotted lines represent the ground truth trajectories, while the lines connected by crosses are the predicted trajectories. On the right, the MSE of the trajectories for the full 30 time steps of data is presented. &lt;/div&gt; &lt;p&gt;While the potential of DKNs has already been explored in recent years, the field is still being actively studied. In this blog, I am interested in exploring how a DKN can be used to model a particular kind of a dynamical system: one with piecewise dynamics that vary discretely across state space. These systems are inherently challenging for traditional, point-wise linearization techniques. To explain this, we can consider an example inspired by our old friend, the simple pendulum.&lt;/p&gt; &lt;p&gt;Consider a pendulum as before, but with the addition of two springs located at $\theta=30\degree$ and $\theta=-30\degree$. If we to consider a point arbitrarily close to one of these springs, say at $\theta=29.99…\degree$, then a Taylor series about this point – even with infinite terms – would not be able to accurately represent the dynamics when the spring is engaged. In contrast, a lifted linearization may better model such a system thanks to its ability to incorporate information beyond a single point.&lt;/p&gt; &lt;p&gt;\(\begin{align} \ddot\theta =f(\theta,\dot\theta) =\begin{cases} -g\sin{\theta}-b\dot\theta, &amp;amp; \theta\in [-30^\circ,30^\circ]\\ -g\sin{\theta}-b\dot\theta-k(\theta+30), &amp;amp; \theta&amp;lt;-30^\circ\\ -g\sin{\theta}-b\dot\theta-k(\theta-30), &amp;amp; \theta&amp;gt;30^\circ \end{cases} \end{align}\)&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; The dynamics of a pendulum with a pair of springs can be expressed as a set of piecewise equations. $k=1000$ is the stiffness of the springs and $b=1$ is the damping constant. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/spring_diagram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/spring_diagram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/spring_diagram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/spring_diagram.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Diagram of the damped pendulum system with a pair of fixed springs, space at equal angles away from $\theta=0$. &lt;/div&gt; &lt;p&gt;Although that isn’t to say that a brute-force implementation of a DKN would necessarily be all too successful in this case either. Piecewise, switched, or hybrid systems (terminology depending on who you ask) are composed of particularly harsh nonlinearities due to their non-continuous derivatives. These can be difficult for lifted linearization approaches to model &lt;d-cite key=&quot;Bakker:KoopHybrid, Govindarajan:KoopHyPend, NgCable&quot;&gt;&lt;/d-cite&gt;, with some systems theoretically requiring an infinite number of observables to be accurately linearized. This project is motivated by the question of whether we could modify the standard DKN approach to be more amenable for piecewise systems, specifically by taking inspiration from the common practice of pre-training neural networks.&lt;/p&gt; &lt;p&gt;As a bit of a spoiler for the conclusion of this report, we don’t end up seeing any noticeable improvement from pre-training the DKN. Nevertheless, the process of experimenting with the proposed approaches was an insightful experience and I am happy to share the results below.&lt;/p&gt; &lt;h2 id=&quot;proposed-approaches&quot;&gt;Proposed Approaches&lt;/h2&gt; &lt;p&gt;I experimented with two approaches for pre-training our DKN, one inspired by curriculum learning &lt;d-cite key=&quot;Soviany2022&quot;&gt;&lt;/d-cite&gt; and another seeking to leverage an intuitive understanding of a lifted linearization’s observables. We then compared the results to an aggregate DKN model trained from scratch with 50 observables.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/aggregate_DKN-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/aggregate_DKN-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/aggregate_DKN-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/aggregate_DKN.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; A DKN with 50 observables trained from scratch for the pendulum with springs. On the left, the dotted lines again represent ground truth trajectories while the lines connected by crosses are predictions. &lt;/div&gt; &lt;p&gt;In the case of applying curriculum learning, we considered an approach with a data-based curriculum. In these cases, the difficulty of the training data is gradually increased over time. This has the potential benefit of allowing a model to more readily learn a challenging task, while also preventing a situation where a model is not sufficiently ‘challenged’ by new data during the training process. Our curriculum learning approach sought to take advantage of DKNs’ already good performance for the standard pendulum case. Intuitively, we identify the spring’s stiffness as the primary source of increased difficulty in our toy system. With this in mind, I created four data sets with different values for the spring constant, $k=0,10,100,1000$. A single model was then trained sequentially on these data sets. If our intuition is correct, we would expect to see the model gradually learn to account for the presence of the spring while maintaining the dynamics of a simple pendulum closer to the origin.&lt;/p&gt; &lt;p&gt;For the second approach tested in this project, it is necessary to consider what an observable is meant to represent in a lifted linearization. As an additional piece of terminology, the function which is used to generate a given observable is referred to as an observable function &lt;d-cite key=&quot;brunton2021modern&quot;&gt;&lt;/d-cite&gt;. While it may be possible to use different sets of observable functions to linearize a given system, it is possible to find a set of observable functions that are analogous to a linear system’s eigenvectors. The evolution of these observables in time, referred to as Koopman eigenfunctions, is defined by an associated complex eigenvalue. Much like their eigenvector cousins, these eigenfunctions can provide useful information on how the system might evolve over time, including information on how the time evolution may vary spatially.&lt;/p&gt; &lt;p&gt;Based on this understanding of Koopman eigenfunctions, we are motivated to see if a DKN could be coaxed into more readily learning spatially-relevant observables. If we consider our system of interest, the pendulum with springs, we posit that different regions of state space would be primarily influenced by different eigenfunctions. In particular, the larger central region where the pendulum’s dynamics are independent of the springs may be expected to be affected by a set of eigenfunctions with a lower spatial frequency and a global relevance. That is, eigenfunctions which better represent the dynamics of the system averaged throughout the state space and which may be valid everywhere – even when the springs are engaged, the natural dynamics of the pendulum are still in effect. In contrast, the dynamics when the springs are engaged (each spring is active in a comparatively smaller region of state space) may rely heavily on a set of eigenfunctions that are only locally relevant.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/pend_statespace-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/pend_statespace-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/pend_statespace-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/pend_statespace.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; On the left, a visualization of trajectories used to train the models for the pendulum with springs. Dotted vertical lines mark where the boundary between regions of state space where the springs are and are not engaged. On the right, we see the trajectories considered for the system when there are no springs. Note that the presence of the springs compress `squeeze&apos; the higher energy trajectories further away from the origin of the state space. &lt;/div&gt; &lt;p&gt;While I believe that this is an interesting thought, it is worth noting that this intuitive motivation is not necessarily backed up with a rigorous mathematical understanding. Nevertheless, we can empirically test whether the approach can lead to improved results.&lt;/p&gt; &lt;p&gt;In contrast to the curriculum learning approach, we have only a single set of data: that generated from a model of a pendulum with a spring stiffness of $k=1000$. Instead of the standard approach of DKN, where a larger number of observables is considered to (in general) allow for a system to be more easily linearized, we deliberately constrain the latent space dimension to be small. The intention is for this restriction to limit the number of observable functions that the model can represent, encouraging it to learn observables with a low spatial frequency and which are relevant across a larger region of state space. In our system of interest, this would be observable functions that represent the dynamics of the pendulum without the springs.&lt;/p&gt; &lt;p&gt;Once we have initially trained this smaller model, we use its encoder within a larger model. This initial encoder is kept fixed in future training processes so that it continues to represent the same set of observables. An additional encoder is then then in the larger model, with the goal being to learn additional observables capable of making up for the initial model’s deficiencies. If the initial model learned the low spatial frequency observables as hoped, then we would expect this additional encoder to learn observables that are more relevant in areas where the springs are exerting a force on the pendulum. In practice, we could see this as a particular form of curriculum learning where the complexity of the model is increased over time. A key difference here compared to traditional approaches is that instead of increasing the complexity of the model by adding layers depth-wise, we are effectively increasing the width of the model by giving it the ability to learn additional observables.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/model_arch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/model_arch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/model_arch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/model_arch.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The architecture used to train the combined model. A smaller, 10 observable model was first trained, before a larger model was then trained to learn an additional 40 observables. &lt;/div&gt; &lt;h2 id=&quot;the-model&quot;&gt;The Model&lt;/h2&gt; &lt;p&gt;To reduce the influence that other factors may have in the results of our experiments, I sought to minimize any changes to the overall structure of the DKNs being used, save for those being studied. Chief among these was the number of hidden layers in the network, the loss function being used, and the input. Other variables, such as the optimizer being used, the batch size, and the learning rate, were also kept as unchanged as feasible. The need to tune each of these other hyperparameters and the challenges in doing so are well-documented in the machine learning field, and as such I won’t spend any additional time describing the processes involved.&lt;/p&gt; &lt;p&gt;The general &lt;em&gt;encoder&lt;/em&gt; architecture of the networks being used was as follows, with $D_x$ being the number of states (2, in the case of the pendulum) and $D_e$ being the number of observables:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Layer&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Input Dimensions&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Output Dimensions&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Nonlinearity&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Linear&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;$D_x$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ReLU&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Linear&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ReLU&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Linear&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;$D_e$&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;None&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;In addition to the encoder network, a linear layer was present to determine the time evolution of the observables. For this linear layer, the input and output dimensions were both D_e + D_x since our final set of observables always had the system’s states concatenated onto those learned by the encoder.&lt;/p&gt; &lt;p&gt;The loss function that I used was composed of two main components: a loss related to the time evolution of the observables being output by the encoder, and a loss related to the time evolution of the state variables. In the literature, additional loss terms are often included to help regularize the network during training. These were not found to be significant in the testing done for this report, however and so were excluded. Tests were also done with different weights between the state loss and the observable loss, with an equal balance between the two found to provide reasonable outcomes. Another hyperapameter that we needed to tune is for how many time steps to enforce a loss on the values predicted by the model. In this report, we stuck to 30 time steps although significant experimentation was not done to explore how varying this parameter may have affected the results. We did briefly look into whether having a weight on any of the loss terms which decayed over time would improve training and did not see any immediate benefits.&lt;/p&gt; &lt;p&gt;\(\mathrm{loss}=\mathrm{multistep\_loss\_state}+\mathrm{multistep\_loss\_observables}\) \(\mathrm{multistep\_loss\_state}=\sum^{30}_{t=1}\lvert\lvert(\psi(\textbf{x}_t)-K^t\psi(\textbf{x}_0))[:2]\rvert\rvert_{\mathrm{MSE}}\) \(\mathrm{multistep\_loss\_observables}=\sum^{30}_{t=1}\lvert\lvert(\psi(\textbf{x}_t)-K^t\psi(\textbf{x}_0))[2:]\rvert\rvert_{\mathrm{MSE}}\)&lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; The loss function ultimately used for each of the models considers the prediction error for both the state and the observables. $\psi$ represents the act of using the model&apos;s encoder and then concatenating the state as an additional pair of observables. $K$ represents the linear layer in the architecture used to model the time evolution of the lifted state. &lt;/div&gt; &lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt; &lt;h3 id=&quot;curriculum-learning&quot;&gt;Curriculum Learning&lt;/h3&gt; &lt;p&gt;The initial model for stiffness $k=0$ was trained on the simple pendulum dynamics for 600 epochs, and served as the pre-trained model for this approach. Subsequent models were each trained for 200 epochs with the Adam optimizer and a decaying learning rate scheduler. When analyzing the performance of these models, we looked at how the error for a set of trajectories not in the training set evolved over time.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/curriculum_results-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/curriculum_results-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/curriculum_results-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/curriculum_results.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Performance of the model trained using curriculum learning after each of the learning stages. We observe that performance decreases over time, and that the original model trained when $k=0$ seems to perform the best. &lt;/div&gt; &lt;p&gt;By this metric, we observe the performance of the model gradually getting worse. While this on its own is not too surprising, the final model ends up performing significantly worse than a DKN with the equivalent number of observables trained from scratch. Interestingly, it looks like the final model is unstable, with the trajectories blowing up away from the origin. Looking into this, issues surrounding the stability of linearized models is not a new phenomenon in the field of Koopman linearizations. Prior works have proposed several methods to help alleviate this issue, such as by adding an addition term to the loss function which stabilizes the time-evolution matrix. While there was no time to implement this change for this report, it could be an interesting modification to attempt for future work.&lt;/p&gt; &lt;h3 id=&quot;learning-new-observables&quot;&gt;Learning New Observables&lt;/h3&gt; &lt;p&gt;While trying to gradually learn additional observables for the model, we started with a network that learned 10 observable functions and trained it for 600 epochs. Once this process was complete, an extended model learned an additional 40 observable functions for an additional 600 epochs. The end result was comparable in performance to a single aggregate model of 50 observables trained from scratch. The aggregate model did appear to specifically outperform our gradually trained model during the initial time steps, while slightly underperforming in comparison at the later time steps. This may be due to some differences in the stability of the two learned linear models, although further investigation would be needed to verify this. Part of the motivation for this method was the hope that the network would learn locally relevant observable functions. The learned observables were plotted on a grid to visualize them and see if this were the case, but not distinctive, qualitative features indicating that different observables were learned for different regions of state space.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/combined_results-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/combined_results-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/combined_results-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/combined_results.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The combined model doesn&apos;t see any noteworthy improvement in performance when compared to the standard DKN approach. While not shown here, the combined model was found to be sensitive to how many observables were learned by each of its constituents. For example, having 30 observables in the first encoder and 20 in the second led to worse results. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/obs_visualization-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/obs_visualization-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-croneillproposal/obs_visualization-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-croneillproposal/obs_visualization.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Visualization of a pair of observables from the combined model, arbitrarily selected as the first observable from both encoder 1 (left) and encoder 2 (right). While only these two observables are shown here, plots for all 50 were produced. We noticed that observables from encoder 1 (the fixed model) tended to show `kinks&apos; around $\theta=+-30\degree$. This may indicate that it was learning to account for the presence of the springs. In contrast, encoder 2 (the extended model) learned observable functions that were generally smoother across state space. &lt;/div&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this project, we sought to test two modifications to a DKN training scheme on an example of a piecewise dynamical system. By using a curriculum learning process or gradually increasing the number of observable functions, we hypothesized that the DKN would show better performance than an aggregate model trained from scratch. Ultimately, we found that neither of the proposed methods led to significant improvements.&lt;/p&gt; &lt;p&gt;One of the potential causes of underperformance is the learned linear models’ instability. While this is a known issue regarding lifted linearization techniques &lt;d-cite key=&quot;ng2022learned, Mamakoukas2023Stable&quot;&gt;&lt;/d-cite&gt;, attempting to resolve the issue would require further work and additional study into how best to do so for this use case. The example model of a pendulum with springs could also have been chosen poorly. I opted to experiment with this system since it was physically meaningful, and I believed that it would be a simple toy model that wouldn’t require large models with extensive compute requirements. But observing the dramatic change in performance that occurred in the linear models simply through the addition of the springs made me wonder whether this system truly was as simple as I had initially made it out to be. It is possible that larger and more elaborate models with more observables and resources for training are necessary to learn an appropriate linearization.&lt;/p&gt; &lt;p&gt;It is also worth considering the severe limitations of this study, imposed upon it by the need to tune a wide variety of hyperparameters. Even in the process of creating a linear model for the simple pendulum, I observed a wide range of performance based upon how the cost function or learning rate were varied. While some effort was taken to tune these and other hyperparameters for the models I explored, this process was far from exhaustive.&lt;/p&gt; &lt;p&gt;Moreover, the proposed changes to the typical DKN architecture only served to add additional hyperparameters into the mix. What spring stiffnesses should be used during curriculum learning? Should the learning rate be decreased between different curriculums, or should the number of epochs be varied? How about the ratio of observables between the two models used in the second approach, is a 10:40 split really optimal? Some variations of these hyperparameters were considered during this project, but again an exhaustive search for optimal values was impossible.&lt;/p&gt; &lt;p&gt;This means that there is a chance that I simply used the wrong selection of hyperparameters to see better performance from the tested approaches, it highlights the sensitivity that I observed in the performance of the DKNs. Even beyond the considerations described thus far, there are further considerations that can impact the structure and performance of learned linearizations. Some approaches augment the state variables with time-delayed measurements, for example. In other cases, the state variables are not included as observables and are instead extracted using a decoder network. This latter case is of particular interest, since recent work in the field has identified that certain types of nonlinear systems are impossible to linearize with a set of observables that include the states.&lt;/p&gt; &lt;p&gt;Ultimately, while the experiments in this project didn’t agree with my hypothesis (and resulted in some underwhelming predictive performance) I gained a newfound appreciation for the process of training these models along the way.&lt;/p&gt; </content> </entry> <entry> <title>Sparse Autoencoder Universality - Under What Conditions are Learned Features Consistent?</title> <link href="https://deep-learning-mit.github.io/blog/2023/universal-features/"/> <updated>2023-12-10T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/universal-features</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Neural networks are black boxes. We understand the process by which they are created, but just as understanding the principle of evolution yields little insight into the human brain, designing a model’s optimization process yields little insight into how that model reasons. The field of mechanistic interpretability attempts to understand how human-understandable concepts combine within a model to form its output. With sufficiently good interpretability tools, we could ensure reasoning transparency and easily find and remove harmful capabilities within models (such as hallucinations) &lt;d-cite key=&quot;marks2023geometry&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In 2022, Anthropic identified a core challenge in interpreting a model’s reasoning layer-by-layer: polysemanticity, a phenomenon in which a single neuron activates for many different concepts (e.g. academic citations, English dialogue, HTTP requests, and Korean text). This is a result of a high-dimensional space of concepts (‘features’) being compressed into the lower-dimension space of the neural network &lt;d-cite key=&quot;Elhage2022-wh&quot;&gt;&lt;/d-cite&gt;. Sparse autoencoders, a form of dictionary learning, help to linearly disentangle polysemantic neurons into interpretable features &lt;d-cite key=&quot;bricken2023monosemanticity&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Sparse autoencoders work by projecting a single layer of a neural network into a higher-dimension space (in our experiments, we train autoencoders ranging from a 1:1 projection to a 1:32 projection) and then back down to the size of the original layer. They are trained on a combination of reconstruction loss, their ability to reconstruct the original input layer, and a sparsity penalty, encouraging as many weights as possible to be 0 while retaining good performance &lt;d-cite key=&quot;cunningham2023sparse&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(2)-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(2)-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(2)-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(2).png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;(https://www.alignmentforum.org/posts/wqRqb7h6ZC48iDgfK/tentatively-found-600-monosemantic-features-in-a-small-lm)&lt;/p&gt; &lt;p&gt;The intuition behind sparse autoencoders is that if each neuron in the input layer learns n features, then projecting to n dimensional space while retaining all the information from the input layer should theoretically leave us with one feature represented in each encoded neuron. Then, these neurons should all be monosemantic, meaning they should each represent one interpretable concept. Because the columns of the decoder matrix tell us how these encoded neurons linearly combine to recreate the input layer, each column of the decoder matrix represents one feature of the network (in other words, what linear combination of neurons represents an individual concept). &lt;d-cite key=&quot;cunningham2023sparse&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;However, because sparse autoencoders were only popularized as an interpretability method earlier this year by Anthropic, the literature on them is, for lack of a better word, sparse. In particular, we were curious about whether the features learned by sparse autoencoders are universal. In other words, we’d like to know if the learned features are similar regardless of variables like autoencoder size, model size, autoencoder training set, and model training set. If they are, it shows both that sparse autoencoders consistently extract the correct features and that learned features are similar across different model sizes and training sets. If they aren’t, it would be evidence that sparse autoencoders don’t accurately capture the full scope of features a model represents and that we cannot easily transfer them across different models.&lt;/p&gt; &lt;p&gt;In our experiments, we train autoencoders of projection ratios ranging from 1:1 to 1:32 on five different Pythia models: 70m, 160m, 410m, 160m deduped, and 410m deduped. In some cases, we exclude data from Pythia 410m because running experiments on it was too computationally expensive. We train on the first four layers of each model to provide additional insight into how the efficacy of autoencoders changes as one moves deeper into the model. We also train autoencoders on two different datasets from the same distribution to test whether the learned features change in response to small perturbations in training order or distribution. Together, these models let us answer a few broad questions surrounding the consistency of learned features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do learned features consistently transfer between different model sizes and training datasets?&lt;/li&gt; &lt;li&gt;Are learned features consistent across different autoencoder sizes?&lt;/li&gt; &lt;li&gt;Do sparse autoencoders learn interpretable features less consistently in later layers where reasoning may become more abstract or hard to follow?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These meta-level questions build on Anthropic’s feature-extraction process outlined below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(3)-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(3)-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(3)-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/image%20(3).png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;(This image is from Cunningham et. al &lt;d-cite key=&quot;cunningham2023sparse&quot;&gt;&lt;/d-cite&gt;)&lt;/p&gt; &lt;p&gt;To answer these questions, we use the following three metrics in a variety of comparisons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mean cosine similarity (MCS) between decoder weights – since the columns of the decoder matrix represent the features, we can use them to measure the similarity of the learned features. To compare two decoders, we start by taking the mean cosine similarity between the first column in the first decoder and every column in the second decoder. Because the decoders might learn features in different orders, we take the maximum of these similarities. We repeat this process for every column in the first decoder, and then we take the average similarity across the columns.&lt;/li&gt; &lt;li&gt;Correlation between activation vectors of encoded layers – another way of inspecting the features learned by a sparse autoencoder is to examine when different neurons in the encoded layer activate on different types of token. So, to compare two autoencoders, we pass over 10,000 tokens of text through their respective models and save vectors representing each encoded neuron’s activations across those tokens. Then, as with mean cosine similarity, we took the maximum correlation between a neuron in the first encoder and any neuron in the second encoder, and then averaged these values across every neuron. If two encoders typically had the same neurons activating for the same tokens, this is strong evidence that the encoders learned similar features.&lt;/li&gt; &lt;li&gt;Feature frequency of an autoencoder – because neurons in encoded layers are intended to represent specific individual concepts, we expect them to activate much less than typical neurons in a neural network. We used this metric both as a way of verifying that our autoencoders are working as intended and as a way of evaluating how easily autoencoders are able to learn monosemantic features as we vary other parameters. To create feature frequency plots, we pass over four million tokens through the model and plot the frequency with which a feature activates (usually around once every 10-1000 tokens) against the number of features which had that frequency.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Specifically, we ran the following experiments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On the question of whether learned features consistently transfer between different model sizes and training datasets: we created feature frequency plots, tables of correlations, and MCS graphs to contrast different model sizes along with deduped and original models.&lt;/li&gt; &lt;li&gt;On the question of whether learned features are consistent across different autoencoder sizes: we created feature frequency plots, MCS tables, and graphs of pairwise activation correlations and MCS to contrast features learned by different autoencoder sizes.&lt;/li&gt; &lt;li&gt;On the question of whether sparse autoencoders learn interpretable features less consistently in later layers where reasoning may become more abstract or hard to follow: we create feature frequency plots contrasting learned feature frequencies at different layers throughout Pythia 70m and Pythia 160m.&lt;/li&gt; &lt;/ul&gt; &lt;h1 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h1&gt; &lt;p&gt;We ran baselines for both MCS and correlations by taking the corresponding measurement between autoencoders trained on two different layers as well as randomly initialized weights. For MCS, the baseline was around 0.15 and was always below 0.20 in our experiments. For correlations, random measured to be about .40.&lt;/p&gt; &lt;h2 id=&quot;training-and-evaluating-sparse-autoencoders&quot;&gt;Training and evaluating sparse autoencoders&lt;/h2&gt; &lt;p&gt;We trained a range of sparse autoencoders on the activations at the output of the MLP of various Pythia models. We used 100 million tokens of web text, from a HuggingFace dataset to train each autoencoder. As seen from the loss curve, this is likely over training. We spent some time fine-tuning the hyperparameters and conferred with other researchers who have trained similar autoencoders. You can see from our loss curve that we are likely over training. Since we are partially optimizing for reconstruction loss, we did not expect the quality of the model to decrease on test sets significantly. We ran our model with and without the sparse autoencoder or a small dataset and saw the perplexity go up from 25 to 31, which we were content with. However, there is a lot of room left for improvement to get better sparse autoencoders.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/W&amp;amp;B%20Chart%2012_12_2023,%2011_09_59%20PM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/W&amp;amp;B%20Chart%2012_12_2023,%2011_09_59%20PM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/W&amp;amp;B%20Chart%2012_12_2023,%2011_09_59%20PM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/W&amp;amp;B%20Chart%2012_12_2023,%2011_09_59%20PM.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;(total loss curve of an 1:8 autoencoder trained on Pythia-70m)&lt;/p&gt; &lt;h2 id=&quot;do-learned-features-consistently-transfer-between-different-model-sizes-and-training-datasets&quot;&gt;Do learned features consistently transfer between different model sizes and training datasets?&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plot-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plot-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plot-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plot.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Activation frequencies are distributed roughly symmetrical around 0.01, meaning that the modal encoded neuron activated around once every one hundred tokens. This is solid evidence that our sparse autoencoders were effectively learning sparse, monosemantic representations. If a neuron was only needed every one hundred tokens to reconstruct the input, it likely represents a very specific concept rather than many concepts all at once. We see no clear trend when varying model size, demonstrating that this does not have much effect on an autoencoder’s ability to extract monosemantic features.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The table above measures the correlations between 1:8 autoencoders trained on layer 0 of three different model sizes. You can see that autoencoders trained on models closer in size have a higher correlation factor of their features, suggesting that smaller autoencoders may not store some of the features that large autoencoders do.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Deduped%20vs%20non-deduped-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Deduped%20vs%20non-deduped-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Deduped%20vs%20non-deduped-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/Deduped%20vs%20non-deduped.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The above graph shows the MCS between autoencoders trained on deduped vs regular models. We anticipated the MCS of these models to be fairly high, but these were some of the lowest results we have seen, with autoencoders trained on layer 0 (of any of the three models we looked at) being around .4. Notably, all of our MCS were above .15 which was our baseline.&lt;/p&gt; &lt;h2 id=&quot;are-learned-features-consistent-across-different-autoencoder-sizes-and-training-datasets&quot;&gt;Are learned features consistent across different autoencoder sizes and training datasets?&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plots_by_layer-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plots_by_layer-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plots_by_layer-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/frequency_plots_by_layer.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Sparsity tends to increase when the projection ratio increases, which makes sense, as a larger layer needs to use each neuron less often. This is evidence that our autoencoders are not learning all possible features, and using even larger autoencoders would allow us to unpack more features.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Four%20AE%20sizes%20on%20Pythia-70m%20MCS-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Four%20AE%20sizes%20on%20Pythia-70m%20MCS-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Four%20AE%20sizes%20on%20Pythia-70m%20MCS-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/Four%20AE%20sizes%20on%20Pythia-70m%20MCS.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The above table looks at MCS loss of different sized autoencoders on Pythia 70m. Interestingly, we observed that MCS between autoencoders whose dimensions have the same ratio (e.g. 4:8 vs 8:16) are similar (e.g. both are .870.)&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_Graph_70m-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_Graph_70m-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_Graph_70m-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_Graph_70m.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m/7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m/7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m/7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m.png/7.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/Correlations%20between%201-8%20AEs%20trained%20on%203%20model%20sizes.png&quot; class=&quot;img-fluid z-depth-2&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m/7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m/7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m/7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-universal-features/Correlation_graph_160m.png/7.jpg&quot; class=&quot;img-fluid z-depth-2&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Activation correlations and MCS were very high for all autoencoder projection ratios, demonstrating that different size autoencoders learn very similar features. Note that all metrics were lower for the autoencoders with a 1:1 projection ratio, likely because they were penalized on sparsity while not having any additional space with which to represent concepts. This means the total information they could retain was likely much less than the other sizes. We see a slight upward trend as autoencoder projection ratio increases, which is small enough that it could probably be chalked up to the exact mean-max methodology used in the calculations. In the MCS graphs, the orange line represents mean-max MCS going from the smaller projection size to the larger projection size, where the blue line is the inverse. It is positive evidence that the blue line is much lower, because we should expect the most important features to correlate strongly with some of the features learned by the larger autoencoder, while the many features learned by the larger autoencoder should not all necessarily have a match in the smaller one.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;In this post, we explored the potential of sparse autoencoders as tools for interpreting neural networks, particularly focusing on their capability to disentangle polysemantic neurons into interpretable, monosemantic features. Our experiments, conducted on various configurations of Pythia models and sparse autoencoders, aimed to understand the consistency and universality of the features extracted by these autoencoders across different model sizes, training datasets, and autoencoder dimensions.&lt;/p&gt; &lt;p&gt;Our findings indicate that sparse autoencoders are indeed effective in learning sparse, monosemantic representations. This effectiveness is observed across different model sizes and is not significantly impacted by the size of the model, suggesting a level of universality in the features extracted. However, our results also reveal that the correlation between features tends to be higher in autoencoders trained on models closer in size, hinting at some limitations in the transferability of learned features across vastly different model scales.&lt;/p&gt; &lt;p&gt;Interestingly, we observed a tendency towards increased sparsity in the representations as we moved into the later layers of the network. This suggests that higher-level concepts in these layers might be more specialized and interpretable, aligning with intuitive expectations about neural networks.&lt;/p&gt; &lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt; &lt;p&gt;Limitations of sparse autoencoders include that they are extremely computationally intensive, especially if one wants to interpret multiple layers of a network, neural networks are not entirely human-interpretable to begin with, so their learned features will never quite represent human concepts, and all the metrics we use to analyze them rely on overall trends rather than individual features, so despite our ability to provide evidence to help answer broad questions, our analysis is still very imprecise.&lt;/p&gt; &lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt; &lt;p&gt;One future direction is focussing on training better sparse autoencoders, ones with lower reconstruction and sparsity loss. Given that we did not optimize our project for this and were limited by time and compute, it is very possible that better sparse autoencoders can improve our results.&lt;/p&gt; &lt;p&gt;It would also be interesting to train the same sparse autoencoder architectures on different datasets and see whether they are invariant to small perturbations in the dataset. If not, it’s evidence that the method may not work as well as we hope.&lt;/p&gt; &lt;p&gt;Finally, we could start to look at the features that the autoencoders are finding. We were able to measure similarity and correlations but did not have the time to look at the actual concepts that the representations were finding. This could give us additional insight into similarities between models that we currently are overlooking.&lt;/p&gt; &lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt; &lt;p&gt;Special thanks to Sam Marks for suggesting the initial experiment ideas and to &lt;a href=&quot;https://www.mitalignment.org/&quot;&gt;MIT AI Alignment&lt;/a&gt; for providing connections with mentorship and compute resources.&lt;/p&gt; </content> </entry> <entry> <title>Optimizations of Transformers for Small-scale Performance</title> <link href="https://deep-learning-mit.github.io/blog/2023/diaz-proposal/"/> <updated>2023-12-10T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/diaz-proposal</id> <content type="html">&lt;div class=&quot;text-center&quot;&gt; &lt;a name=&quot;figure-1&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://discuss.tensorflow.org/uploads/default/original/2X/4/44b54935a57a92b71902d81265e9bc3c6d99fb12.gif&quot; width=&quot;700&quot; height=&quot;250&quot; /&gt; &lt;p class=&quot;caption&quot;&gt; Figure 1: Attention Maps of a Vision Transformer (DINO). Source: &lt;a href=&quot;https://github.com/sayakpaul/probing-vits&quot;&gt;https://github.com/sayakpaul/probing-vits &lt;/a&gt;. &lt;/p&gt; &lt;/div&gt; &lt;h2 id=&quot;transformers-great-but-fall-short&quot;&gt;Transformers: Great but fall short&lt;/h2&gt; &lt;h3 id=&quot;basic-background&quot;&gt;Basic Background&lt;/h3&gt; &lt;p&gt;Transformers have well-earned their place in deep learning. Since the architecture’s introduction in&lt;d-cite key=&quot;AttentionIsAllYouNeed&quot;&gt;&lt;/d-cite&gt;, we have seen huge improvements in our model’s capabilities. The most notable of which being natural language processing (NLP) with large-language models such as GPT-4 stunning the world at-large.&lt;/p&gt; &lt;p&gt;Originally designed for NLP, the transformer architecture has been robust in other domains and tasks. For example, it has been translated, with success, to de-novo protein design&lt;d-cite key=&quot;Grechishnikova2021&quot;&gt;&lt;/d-cite&gt;, the medical field&lt;d-cite key=&quot;Hu2022&quot;&gt;&lt;/d-cite&gt;, and, of most relevance, computer vision&lt;d-cite key=&quot;Dosovitskiy2020&quot;&gt;&lt;/d-cite&gt;. This behaviour differs from architectures of the past like RNNs and CNNs which have been limited to one domain. The potent generalizability of the transformer lies within the self-attention mechanism. Without getting to much into detail, self-attention enables nodes within a neural network to probe the input sequence, determine what is most interesting, and attend towards the region of interest by dynamically updating its weights. Visualization of attention can be seen in &lt;a href=&quot;#figure-1&quot;&gt;Figure 1&lt;/a&gt;. By probing the data landscape, the architecture enables long-range dependencies to be modeled regardless of distance. From a Fourier perspective, the transformer caters towards the low-frequency information in the data and deciphers how each element of an input sequence all relate to each other&lt;d-cite key=&quot;Wang2022&quot;&gt;&lt;/d-cite&gt;. These connections help the transformer accurately model global information in the data perhaps indicating why they are so powerful. In this blog, we will specifically examine the transformer in vision, determine how it can be improved, and evaluate new strategies to increase its viability on small datasets.&lt;/p&gt; &lt;div class=&quot;col-sm text-center&quot;&gt; &lt;a name=&quot;figure-2&quot;&gt;&lt;/a&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/vit_workflow-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/vit_workflow-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/vit_workflow-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-diaz-proposal/vit_workflow.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: ViT workflow. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;vision-the-problem&quot;&gt;Vision: The Problem&lt;/h3&gt; &lt;p&gt;The Vision Transformer (ViT)&lt;d-cite key=&quot;Dosovitskiy2020&quot;&gt;&lt;/d-cite&gt; introduced the transformer to the computer vision world in late 2020. The ViT is simple: it funnels image patches into a tokenization scheme, adds positional encoding, and feeds these tokens into a transformer block. A graphical workflow of the ViT from the original paper can be seen in &lt;a href=&quot;#figure-2&quot;&gt;Figure 2&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Since its introduction, the ViT and associated variants have demonstrated remarkable benchmarks in image classification&lt;d-cite key=&quot;Liu2021&quot;&gt;&lt;/d-cite&gt;, image restoration&lt;d-cite key=&quot;Liang2021&quot;&gt;&lt;/d-cite&gt;, and object detection&lt;d-cite key=&quot;Li2022&quot;&gt;&lt;/d-cite&gt;. Much of these new methods can compete and even outperform long-established CNNs. However, ViTs are data-hungry requiring extensive amounts of training data to surpass CNNs. In small scale training, ViTs are burdensome to train and achieve sub-par performance compared to their CNNs counterparts&lt;d-cite key=&quot;Naimi2021&quot;&gt;&lt;/d-cite&gt;. In &lt;d-cite key=&quot;Zhu2023&quot;&gt;&lt;/d-cite&gt;, they investigate this discrepancy by comparing the feature and attention maps of small-scale CNNs and ViTs, respectively. The authors determine the ViT lacks the ability to learn local information and has ill-suited representation capacity in the lower layers. In contrast, the CNN demonstrate remarkable inductive bias due to weight sharing and locality properties which enable high-frequency modeling&lt;d-cite key=&quot;Park2022&quot;&gt;&lt;/d-cite&gt;. The ViT’s low-frequency and the CNNs high-frequency capacity has initiated a wave of new models aimed at combining the two for comprehensive modeling capability&lt;d-cite key=&quot;Si2022&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;ConvViT&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Despite the complementary nature of these architectures, they break the fidelity of the transformer and make for difficult analysis. Therefore, there exists a gap in the traditional transformer architecture to perform in small-data regimes, particularly in vision. Motivated by this shortcoming, we aim to investigate and improve the current ViT paradigm to narrow the gap between CNNs and ViTs on small-data. In particular, we examine novel initialization schemes, removal of component parts in our transformer block, and new-learnable parameters which can lead to better performance, image throughput, and stable training on small-scale datasets.&lt;/p&gt; &lt;div class=&quot;col-sm text-center&quot;&gt; &lt;a name=&quot;figure-3&quot;&gt;&lt;/a&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-diaz-proposal/transformer.svg&quot; class=&quot;img-fluid rounded z-depth-1&quot; style=&quot;width: 300px;&quot; /&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: Standard transformer encoder block. Encoder can be stacked for x amount of layers. &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;transformer-block&quot;&gt;Transformer Block&lt;/h3&gt; &lt;p&gt;To serve as a basis of comparison, we will examine the stanford transformer block seen in &lt;a href=&quot;#figure-3&quot;&gt;Figure 3&lt;/a&gt;. The block is identical to &lt;d-cite key=&quot;AttentionIsAllYouNeed&quot;&gt;&lt;/d-cite&gt; with the exception of using layer normalizations before the multi-headed attention (MHA) and multi-level perceptron (MLP) blocks as opposed to after. In practice, this placement has been shown to be more stable and increase performance&lt;d-cite key=&quot;Liu2020&quot;&gt;&lt;/d-cite&gt;. With the exception of this modification, the block has seen little improvements over the years testifying to its robustness. However, recent trends in theory hints towards ways we could break this notion – all while enjoying increased performance.&lt;/p&gt; &lt;p&gt;Before we delve into these advances and their implications, consider the following transformer block information flow:&lt;/p&gt; \[\displaylines{ \text{Attention} = \text{A}(X) = \text{Softmax}\Biggl(\frac{XW_{Q}W_{K}^{T}X^{T}}{\sqrt{k}}\Biggl) \\ \\ \text{A}(X) \in \mathbb{R}^{T\times T}}\] &lt;p&gt;which is shortly followed by:&lt;/p&gt; \[\displaylines{ \text{S}(X) = \text{A}(X)W_{V}W_{O} \\ \\ \text{S}(X) \in \mathbb{R}^{T\times d} }\] &lt;p&gt;and:&lt;/p&gt; \[\text{Output} = \text{MLP}(\text{S}(X))= \text{Linear}(\text{GELU}(\text{Linear}(\text{S}(X))))\] &lt;p&gt;where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Embedded input sequence: \(X \in \mathbb{R}^{T \times d}\)&lt;/li&gt; &lt;li&gt;Linear queury and key layers: \(W_{Q},W_{K} \in \mathbb{R}^{d \times k}\)&lt;/li&gt; &lt;li&gt;Linear value and projection layers: \(W_{V}, W_{O} \in \mathbb{R}^{d \times d}\)&lt;/li&gt; &lt;li&gt;MLP Linear layers: \(\text{Linear} \in \mathbb{R}^{d \times d}\)&lt;/li&gt; &lt;li&gt;\(T =\) # of tokens, \(d =\) embedding dimension, \(k = \frac{d}{H}\), \(H =\) # of attention heads&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The flow of information mirrors the transformer block in &lt;a href=&quot;#figure-3&quot;&gt;Figure 3&lt;/a&gt;. Readers unfamiliar with transformer intricacies such as MHA and MLPs are encouraged to read&lt;d-cite key=&quot;AttentionIsAllYouNeed&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Recently, there have been many proposals on how the transformer block can be further modified to increase data throughput and eliminate “redundant” or “useless” parts that do not have any significant contribute to the tranformer’s modeling capabilities. For example, &lt;d-cite key=&quot;2302.05442&quot;&gt;&lt;/d-cite&gt;, used a parallel MHA and MLP incorporated into a large-scale ViT for stable and efficient training. Throughout this blog, we will focus on the ideas overviewed and proposed by &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt; as they present intriguing results and a synthesis on the current state of this research topic. The interested reader is encouraged to study their paper for a more extensive understanding of the ideas.&lt;/p&gt; &lt;div class=&quot;col-sm text-center&quot;&gt; &lt;a name=&quot;figure-4&quot;&gt;&lt;/a&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/simplified_block-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/simplified_block-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/simplified_block-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-diaz-proposal/simplified_block.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4: Comparison between trasnformer architectures. &lt;em&gt;Left&lt;/em&gt;: Standard block as shown in Figure 3. &lt;em&gt;Bottom Right&lt;/em&gt;: Parallel block proposed in. &lt;em&gt;Top Right&lt;/em&gt;: Newly proposed encoder. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The overaching theme of &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt; was to take the standard trasnformer block and evaluate the necessity of each component. In doing so, they removed each component part and studied its effects on performance. Understandably, blindly removing components will lead to unstable training and ill-performance (i.e. if one were to remove the skip connnections, they would encounter vanishing gradients as seen &lt;a href=&quot;#figure-14&quot;&gt;Figure 14&lt;/a&gt;). However, &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt; took the approach of removal combined with recovery. For example, when the authors removed skip connections, they required a modification to the self-attention matrix of the form:&lt;/p&gt; \[\text{A}(X) \leftarrow (\alpha\text{I} + \beta \text{A}(X))\] &lt;p&gt;where \(\alpha\) and \(\beta\) are learnable scalars and intialized to \(1\) and \(0\), respectively, and \(\text{I} \in \mathbb{R}^{T \times T}\) is the identity matrix. This modification intiailizes the self-attention matrix providing a pathway towards training stability. They further entertained a more complicated scheme with a third parameter, but we only consider the two parameter version for simplicity. By this iterative removal and recovery process, the authors converged towards the final transformer block seen in &lt;a href=&quot;#figure-4&quot;&gt;Figure 4&lt;/a&gt;. The most shocking aspect of this proposed block is the removal of the \(W_{V}\) and \(W_O\) layers. They arrived to this justification by initialializing \(W_{V}\) and \(W_{O}\) to the identity with separate, learnable scalars and training a model. Over the course of training, the scalar ratios converged towards zero&lt;d-footnote&gt;This is a slight simplification. Look at Section 4.2 and Figures 4 and 20 in He et. al 2023 for a more detailed explanation.&lt;/d-footnote&gt;. Due to the heavy cost and speed these linear layers present, removal of them decreases parameters counts and enables more data throughput. A concise PyTorch interpretation of the new block can be seen below:&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ShapedAttention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Determining if hidden dimension of attention layer is divisible by number of heads &lt;/span&gt; &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Width and number of heads are not divisble.&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Setting vars &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Creating Linear Layers &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Learnable Scalars: alpha_init and beta_init are up to user &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Softmax &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Input: &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# x: shape (B x T x dim) &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Outputs: &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# attn_output: shape (B x T x width) &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Compute keys and queries &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;W_K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;W_Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Scaled dot-product &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Shaped attention &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_scores&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The performance of the final transformer block (referred to as SAS-P) demonstrated powerful results. In the &lt;a href=&quot;#figure-5&quot;&gt;Figure&lt;/a&gt;, the simplified transformer matches the standard block in cross-entropy loss even when taken through a long runtime. Additionally, Figure 6 in &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt; demonstrates the model scales better with depth which is of paramount importance in modern neural network.&lt;/p&gt; &lt;div class=&quot;col-sm text-center&quot;&gt; &lt;a name=&quot;figure-5&quot;&gt;&lt;/a&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/fig5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/fig5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/fig5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-diaz-proposal/fig5.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5: Training speed experiment. Figure 6. in Ref. 15. Pre-LN is the standard transformer block. SAS-P is the block. It is shown with and without an initial layer normalization. &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;translation-to-vision-experimentation-and-analysis&quot;&gt;Translation to Vision: Experimentation and Analysis&lt;/h2&gt; &lt;p&gt;The results shown in &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt; show promise the transformer can be improved. Motivated by vision applications, we seek to implement such infrastructure, with slight modifications, and determine if it improves performance in small datasets.&lt;/p&gt; &lt;h3 id=&quot;vanilla-vs-simplified-comparison&quot;&gt;Vanilla vs. Simplified Comparison&lt;/h3&gt; &lt;p&gt;For evaluation, we compare the simplified transformer to a vanilla ViT. The vanilla ViT’s transformer block is identical to the formulation presented earlier. We use Conv2D patch embedding with a random initial positional embedding. For the simplified setup, we initialize \(\alpha = \beta = 0.5\) and do not use a centering matrix – although it has been shown to improve ViT performance&lt;d-cite key=&quot;2306.01610&quot;&gt;&lt;/d-cite&gt;. We use one Layer Normalization just prior to the transformer encoder. \(\text{Width} = 96\) is kept uniform throughout the model. The dataset is CIFAR-10 with a \(\text{batch size} = 256\). Data augmentations were random horizontal and vertical flips with 15º random rotations. Optimizer is AdamW with \(\text{lr} = 0.003\) and \(\text{weight decay} = 0.01\). We employ a cosine learning rate scheduler to maintain consistency with ViT literature, although &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt; empirically showed a linear learning rate is slightly advantegeous&lt;d-footnote&gt;Figure 11 in He et. al 2023.&lt;/d-footnote&gt;. We ran our model for \(\text{epochs} = 50\) with \(\text{runs} = 3\) to evalute run-to-run stability. A condensed version of the experiment choices can be seen in Table 1. The results can be seen in &lt;a href=&quot;#figure-6&quot;&gt;Figure 6&lt;/a&gt; and Table 2&lt;d-footnote&gt;To avoid clutter, only the training accuracies vs epochs are shown. Loss curves showed similar relationships.&lt;/d-footnote&gt;.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Table 1. Experiment 1: ViT Model Settings&lt;/th&gt; &lt;th&gt; &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;# of channels&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Image size&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Patch size&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Width&lt;/td&gt; &lt;td&gt;96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;# of heads&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;# of layers&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 0px;&quot;&gt; &lt;a name=&quot;figure-6&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/exp1_w96.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;70%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 6. Comparison between vanilla and simplified transformers. Width = 96. Layers/Depth = 8. &lt;/div&gt; &lt;/div&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Table 2. Experiment 2: Results&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Vanilla&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;Simplified&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;\(\Delta\)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Parameters&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;358186&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;209210&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;-41.59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Avg. epoch time (s)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;12.954&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;11.305&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;-12.73%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Experiment 1 showed the training evaluation trajectory is nearly identicable between the two models although the simplified outperforms by small margin. Although the subtle difference, it is noteworthy to mention the simplified version achieved mirroring performance with less parameters and higher image throughput. The similarity of the curves hints the removal of the skip connections, layer normalizations, and value/projection layers were merited, begging the question whether these components held our modeling power back.&lt;/p&gt; &lt;p&gt;This experimentation shows the similar nature of each model, but does not translate well to wider modern neural networks. In Experiment 2, we expanded to \(\text{width} = 128\) to determine if there is any emergent behaviour as the network becomes wider. We replicate everything in Experiment 1 and solely modify the width. The settings are restated in Table 3. The results for Experiment 2 can be seen in &lt;a href=&quot;#figure-7&quot;&gt;Figure 7&lt;/a&gt; and Table 4 below.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Table 3&lt;/th&gt; &lt;th&gt;Experiment 2: ViT Model Settings&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;# of channels&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Image size&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Patch size&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Width&lt;/td&gt; &lt;td&gt;128&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;# of heads&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;# of layers&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 0px;&quot;&gt; &lt;a name=&quot;figure-7&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/exp2_w128.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;70%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 7. Comparison between vanilla and simplified transformers. Width = 128. Layers/Depth = 8. &lt;/div&gt; &lt;/div&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Table 4. Experiment 2: Results&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Vanilla&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;Simplified&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;\(\Delta\)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Parameters&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;629130&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;364954&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;-41.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Avg. epoch time (s)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;13.093&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;11.735&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;-10.37%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The narrative is different for Experiment 2. The simplified version outperforms the vanilla version by a considerable margin. An adequate explanation for this discrepancy in vision tasks merits further exploration. However, considering the proposed unnecessary nature of the value and projection matrices, we can hypothesize they interfere with the modeling capability as more parameters are introduced.&lt;/p&gt; &lt;p&gt;Due to the sheer difference in outcomes between the models, we question how the models are attending towards various inputs to gain a better understanding of what is happening under the hood. To probe this curiosity, we trained the models with identical setting in Experiment 2, but modified the \(\text{depth} = \text{layers} = 12\). This model setup will be covered in more detail in future paragraphs. We inputted CIFAR-10 to each model and visualized a side-by-side comparison of attention maps for five input images. An interactive figure is seen &lt;a href=&quot;#figure-8&quot;&gt;Figure 8&lt;/a&gt;.&lt;/p&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 0px;&quot;&gt; &lt;div style=&quot;display: flex; flex-direction: column; align-items: center;&quot;&gt; &lt;a name=&quot;figure-8&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/attention_maps.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;70%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 8. Comparison between vanilla and simplified attention maps. Width = 128. Layers/Depth = 12. Interpolation method: &quot;nearest&quot;. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;There is a noticeable contrast in the attention maps. For the simplified model, the attention maps seem to place weight in a deliberation manner, localizing the attention towards prominent features in the input image. On the other hand, the vanilla model is choatic in its attention allocation. It is noteworthy that the vanilla model does place attention towards areas of interest, but also attends towards irrelevant information perhaps compromising its judgement at the time of classification. It can thus be reasoned the simplified model can better decipher which features are relevant demonstrating, even in low data regimes, the representational quality is increased.&lt;/p&gt; &lt;p&gt;While we have so far investigated width, it will be informative to understand how depth impacts the performance of the simplified version. In &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt;, they employ signal propagation theory, which is most prominent in deeper networks. Therefore, we suspect as we increase the depth of our models, the simplified version will outperform the vanilla version by a larger margin. Here, we set \(\text{layers} = 12\) and maintain \(\text{width}=128\). The training accuracies and experiment results are seen in &lt;a href=&quot;#figure-9&quot;&gt;Figure 9&lt;/a&gt; and Table 5.&lt;/p&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 0px;&quot;&gt; &lt;a name=&quot;figure-9&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/exp3_w128_l12.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;70%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 9. Comparison between vanilla and simplified transformers. Width = 128. Layers/Depth = 12. &lt;/div&gt; &lt;/div&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Table 5. Experiment 3: Results&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;Vanilla&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;Simplified&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;\(\Delta\)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Parameters&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;927370&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;531106&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;-42.72%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Avg. epoch time (s)&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;17.527&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;15.723&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;-10.29%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Again, the simplified model outperforms the vanilla model by a large margin. Although we have focused on performance in the past, we discern an interesting trend when we scaled the depth: the simplified version seemed to be more consistent from run-to-run (recall \(\text{runs} = 5\)). This leads us to believe that as we continue to scale the depth, the simplified version will be more stable. Future experimentation will be necessary to corroborate this claim.&lt;/p&gt; &lt;h3 id=&quot;initialization-schemes&quot;&gt;Initialization Schemes&lt;/h3&gt; &lt;p&gt;We have seen the impact simplification can have on the performance of the transformer performance and self-attention. However, the used initializatons of \(\alpha\) and \(\beta\) in Experiments 1, 2, and 3, was based on equal weighting between the initial attention matrix and the identity matrix. In &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt;, they employ a full weighting of the identity matrix and zero’d out the attention matrix at initialization. Here, we aim to determine the effect of different initialization values. Recall \(\alpha = \beta = 0.5\) in Experiments 1, 2, 3. Now, we investigate two more initializaton schemes: \(\alpha = 1.0\) and \(\beta = 0.0\) and vice-versa. We replicate the protocol used in Experiment 2 and only modify these learnable scalar at initializaton and set \(\text{runs} = 1\). The results are shown in &lt;a href=&quot;#figure-10&quot;&gt;Figure 10&lt;/a&gt;. Interestingly, the initialization scheme proposed by &lt;d-cite key=&quot;He2023&quot;&gt;&lt;/d-cite&gt;, does &lt;em&gt;not&lt;/em&gt; outperform the equal weighting or inverse weighting scheme. Understandably, it does poorly at initialization, but never recovers. The equal weighting and inverse weighting approaches show nearly identical performance often trading off superior performance from epoch-to-epoch.&lt;/p&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 0px;&quot;&gt; &lt;a name=&quot;figure-10&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/exp4_init_new.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;80%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 10. Various Initialization Schemes. &lt;/div&gt; &lt;/div&gt; &lt;p&gt;This lead us to believe the initializaton scheme could be improved. There has been some work on initializing vanilla ViTs&lt;d-cite key=&quot;Trockman2023&quot;&gt;&lt;/d-cite&gt; to gain performance. In &lt;d-cite key=&quot;Trockman2023&quot;&gt;&lt;/d-cite&gt;, a prominent diagonal was observed for the \(W_{q}W_{k}^{T}\) layers in ViT’s pre-trained on large datasets, which have been shown to outperform CNNs. The figure shown in the paper can be seen in &lt;a href=&quot;#figure-10&quot;&gt;Figure 10&lt;/a&gt;. This motivated the authors to provide a novel initialization scheme where the \(W_{Q}\) and \(W_{K}\) matrices are initialized in a way to encourage diagonal prominence in the forward pass. However, our findings contradicted this scheme, as our diagonal-dominant initialization scheme \(\alpha = 1\) and \(\beta = 0\) did not out perform the inverse or the equal weighting. This is likely due to the fact we have learnable parameters and do not initialize our \(W_{Q}\) and \(W_{K}\)’s directly, but rather the attention matrix post-softmax. However, it is important to realize that the learnable parameters still encourage diagonal prominence regardless of intialization. Although&lt;d-cite key=&quot;Trockman2023&quot;&gt;&lt;/d-cite&gt; used this initialization scheme to increase performance in small ViT’s trained from scratch, which encourages tokens to attend toward to themselves through the depth of the network, they did not take into consideration how the diagnolization varys from layer-to-layer. Seen in &lt;a href=&quot;#figure-10&quot;&gt;Figure 10&lt;/a&gt;, we can see the prominence of the diagnoal elements fades as we go deeper into the network. Observing this behaviour, we hypothesize the reason the initialization scheme of \(\alpha = 1\) and \(\beta = 0\) underperformed was not due to the initialization itself, but how it was applied to each layer. In other words, when we initialized \(\alpha = 1\) and \(\beta = 0\), we encouraged this token self-attentive nature throughout the depth of the network, when we should be encouraging it in the opening layers and tapering it off as we approach the end of the model.&lt;/p&gt; &lt;p&gt;To give more evidence to this hypothesis, we experimented with the following dynamic initialization scheme:&lt;/p&gt; \[\displaylines{ \alpha_i = \frac{1}{i}, \beta_i = 1 - \frac{1}{i} \\ \text{ where } i \in [1, 2, ..., L] \text{ and } L = \text{# of layers} }\] &lt;p&gt;The results from this initialization scheme compared to the uniform initializations can be seen in &lt;a href=&quot;#figure-12&quot;&gt;Figure 12&lt;/a&gt; The results show that the dynamic scheme outperform the results perhaps indicating the representation quality is connected toward encouraging self-token connection in the lower layers, while allowing for token’s to intermingle in higher layers. We further experiment with the inverse dynamic where we switch the \(\alpha\) and \(\beta\) values. The results in &lt;a href=&quot;#figure-13&quot;&gt;Figure 13&lt;/a&gt; show the dynamic approach is stronger during training then the inverse dynamic approach.&lt;/p&gt; &lt;div class=&quot;col-sm text-center&quot;&gt; &lt;a name=&quot;figure-11&quot;&gt;&lt;/a&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/diagonal_vit_tiny-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/diagonal_vit_tiny-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-diaz-proposal/diagonal_vit_tiny-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-diaz-proposal/diagonal_vit_tiny.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 11: Diagonal prominence in a pre-trained ViT Tiny. Layers 1-11 (Left-to-Right). Heads 1-3 (Top-to-Bottom). Extracted from Figure 1 of &lt;a href=&quot;https://arxiv.org/abs/2305.09828&quot;&gt;Mimetic Initialization of Self-Attention Layers&lt;/a&gt;. &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 0px;&quot;&gt; &lt;a name=&quot;figure-12&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/exp5_init_dynamic.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;80%%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 12. Experiment 5: Dynamic vs. Uniform Initializations. &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 10px;&quot;&gt; &lt;a name=&quot;figure-13&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/exp6_init_inverse.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;80%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 13. Experiment 6: Dynamic vs. Inverse Dynamic Initializations. &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;conclusion-and-limitations&quot;&gt;Conclusion and Limitations&lt;/h2&gt; &lt;p&gt;Through this blog post we have overviewed the simplification of our known transformer block and novel initialization schemes. We took the problem of small-scale training of ViT’s and looked to address it leveraging such ideas. Through a series of experiments and thoughtful schemes, we generated an informed and sophisticated approach to tackle such a problem. In the end, we generated a method that outperformed a tradtional ViT in small scales. We explored ways of scaling the ViT in width and depth and probed how the new model distributed attention. Our comparisons were intentionally simple and effective in addressing the underlying task and illustrating the models potential. Although the results presented showed promise, extensive validation needs to be performed in the future. It will be interesting to see how this new transformer block and intialization scheme can be further utilized in computer vision. For example, a logical next route to entertain is to compare convergence rates in larger scale ViT on datasets such as ImageNet-21k to see if the modeling advantage persists.&lt;/p&gt; &lt;p&gt;There are a few limitations in this study. For one, only one dataset was used. Using other datasets such as CIFAR-100 or SVHN would provide more insight into this methodology. Secondly, there is a need for more comprehensive evaluation and ablation studies to determine the true nature of the simplified transformer and initialization schemes. Third, a comparison to a smaller scale CNNs is needed to gauge where this method comparatively sits in modeling power.&lt;/p&gt; &lt;div class=&quot;l-page&quot; style=&quot;display: flex; flex-direction: column; align-items: center; margin-bottom: 10px;&quot;&gt; &lt;a name=&quot;figure-14&quot;&gt;&lt;/a&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-diaz-proposal/exp0.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;500px&quot; width=&quot;80%&quot;&gt;&lt;/iframe&gt; &lt;div class=&quot;caption&quot; style=&quot;margin-top: 10px; text-align: center;&quot;&gt; Figure 14. Experiment 0: Removal of skip connections in traditional ViT. &lt;/div&gt; &lt;/div&gt; </content> </entry> <entry> <title>Guided Transfer Learning and Learning How to Learn: When Is It Useful?</title> <link href="https://deep-learning-mit.github.io/blog/2023/guided-transfer-learning/"/> <updated>2023-12-10T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/guided-transfer-learning</id> <content type="html">&lt;h1 id=&quot;introductionmotivation-never-enough-data&quot;&gt;Introduction/Motivation: Never Enough Data&lt;/h1&gt; &lt;p&gt;If we take a step back and reflect upon the current state of AI, especially in domains like computer vision and NLP, it appears that the gap between machine and human intelligence is rapidly narrowing. In fact, if we only consider aspects such as the predictive accuracy of discriminatory models and the sensibility of outputs by generative models, it may seem that this gap is almost trivial or even nonexistent for many tasks. However, every time we submit a training script and leave for the next few hours (or few weeks), it becomes abundantly clear that AI is still nowhere near human intelligence because of one critical kryptonite: the amount of data needed to effectively train AI models, especially deep neural networks.&lt;/p&gt; &lt;p&gt;While we have tons of training data in domains such as general computer vision (e.g. ImageNet) and NLP (e.g. the entirety of the internet), other domains may not have this luxury. For example, bulk RNA-sequencing data in biomedical research is notoriously cursed with high dimensionality and extremely low sample size. Training AI models on bulk RNA-sequencing datasets often leads to severe overfitting. In order to successfully utilize AI in domains like biomedicine, the highest priority challenge that must be addressed is that of overcoming the necessity of exuberant amounts of training data.&lt;/p&gt; &lt;h2 id=&quot;machine-vs-human-intelligence&quot;&gt;Machine vs Human Intelligence&lt;/h2&gt; &lt;p&gt;It often feels like the requirement of having abundant training samples has been accepted as an inevitable, undeniable truth in the AI community. But one visit to a preschool classroom is all that it takes to make you question why AI models need so much data. A human baby can learn the difference between a cat and a dog after being shown one or two examples of each, and will generally be able to identify those animals in various orientations, colors, contexts, etc. for the rest of its life. Imagine how much more preschool teachers would have to be paid if you needed to show toddlers thousands of examples (in various orientations and augmentations) just for them to learn what a giraffe is.&lt;/p&gt; &lt;p&gt;Fortunately, humans are very proficient at few-shot learning– being able to learn from few samples. Why isn’t AI at this level yet? Well, as intelligence researchers have discussed &lt;d-cite key=&quot;Nikoli2017&quot;&gt;&lt;/d-cite&gt;, biological brains are not born as empty slates of neurons with random initial connections. Millions of years of evolution have resulted in us being born with brains that are already predisposed to learn certain domains of tasks very quickly, such as image recognition and language acquisition tasks. In these domains, learning a specific task like differntiating between a cat and a dog or between letters of the English alphabet doesn’t require exposure to many samples. Additionally, as we gain more experiences throughout life, we acquire general knowledge that can help us learn new tasks more efficiently if they’re similar to something we’ve learned before. Thus, naturally, the first step toward bridging the gap between natural and machine intelligence is somehow finding a way to predispose an AI to be able to learn any &lt;em&gt;specific&lt;/em&gt; task within a certain domain with very few samples. The advent of traditional transfer learning has attempted to approach this predisposition task from the “general knowledge” perspective.&lt;/p&gt; &lt;h2 id=&quot;traditional-transfer-learning-learning-general-knowledge&quot;&gt;Traditional Transfer Learning: Learning General Knowledge&lt;/h2&gt; &lt;p&gt;Transfer learning has been invaluable to almost all endeavors in modern deep learning. One of the most common solutions for tasks that have too little training data is to first pre-train the model on a large general dataset in the same domain, and then finetune the pre-trained model to the more specific downstream task. For example, if we need to train a neural network to determine whether or not a patient has a rare type of cancer based on an X-ray image, we likely will not have enough data to effectively train such a model from scratch without severe overfitting. We can, however, start with a model pre-trained on a large image dataset that’s not specific to cancer (e.g. ImageNet), and if we start training from those &lt;em&gt;pre-trained&lt;/em&gt; weights, the downstream cancer diagnostic task becomes much easier for the neural network to learn despite the small dataset size.&lt;/p&gt; &lt;p&gt;One way to intuitvely understand why this is the case is through the lens of “general knowledge”. &lt;d-cite key=&quot;Nikoli2017&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;gtl&quot;&gt;&lt;/d-cite&gt; When the model is pre-trained on ImageNet data, it learns a lot of knowledge about image data &lt;em&gt;in general&lt;/em&gt;; for example, the earlier layers of the model will learn low-level features detectors (e.g. edge detectors, simple shape detectors, etc.) that will likely be useful for &lt;em&gt;any&lt;/em&gt; computer vision task. This can be viewed as the model learning “general knowledge” about the domain of image data broadly. When we then fine-tune this model on a cancer dataset, the model doesn’t have to relearn the ability to detect these general, low-level features. This general knowledge encoded in the pre-trained weights regularizes the model and mitigates overfitting, as it &lt;em&gt;predisposes&lt;/em&gt; the model to learn relationships/feature detectors that are generalizable and sensible within the context of image data.&lt;/p&gt; &lt;p&gt;However, if transfer learning could solve all our problems, this blog post wouldn’t exist. When our downstream dataset is in the extremeties of the high dimensional, low sample size characterization (e.g. in fields like space biology research, more on this later), learning general knowledge in the form of pre-trained weights isn’t enough. &lt;d-cite key=&quot;hldsshard&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;hldsshard2&quot;&gt;&lt;/d-cite&gt; How, then, can we predispose models such that they can do extreme few-shot learning, or even &lt;em&gt;one-shot&lt;/em&gt; learning? Enter guided transfer learning.&lt;/p&gt; &lt;hr /&gt; &lt;h1 id=&quot;guided-transfer-learning-and-meta-learning-learning-inductive-biases&quot;&gt;Guided Transfer Learning and Meta-learning: Learning &lt;em&gt;Inductive Biases&lt;/em&gt;&lt;/h1&gt; &lt;p&gt;Guided transfer learning (GTL) &lt;d-cite key=&quot;gtl&quot;&gt;&lt;/d-cite&gt; is a meta-learning paradigm proposed by the group &lt;a href=&quot;https://robotsgomental.com&quot;&gt;Robots Go Mental&lt;/a&gt;. The main idea for guided transfer learning is that, instead of just having the AI model learn general knowledge, we also want the AI to learn &lt;em&gt;how&lt;/em&gt; to learn. Specifically, we want it to learn how to pick up new knowledge &lt;em&gt;most efficiently&lt;/em&gt; for a particular domain/modality of data. This means during pre-training, the model, in addition to learning good initial weights, will also learn &lt;strong&gt;&lt;em&gt;inductive biases&lt;/em&gt;&lt;/strong&gt; that affect the future, downstream training &lt;em&gt;process&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;GTL is a very novel method; its preprint was just released in the past few months! Hence, beyond the experiements in the original preprint, there has not been much exploration of some of its behavioral nuances and various application scenarios. So in this blog, I will be doing a few experiments that attempt to gain more insight into some of my questions that were left unanswered by the original GTL paper.&lt;/p&gt; &lt;p&gt;But before we get to that, let’s first get a rundown on how GTL works! The two most important concepts in GTL are &lt;strong&gt;scouting&lt;/strong&gt; and &lt;strong&gt;guide values&lt;/strong&gt;.&lt;/p&gt; &lt;h2 id=&quot;scouting&quot;&gt;Scouting&lt;/h2&gt; &lt;p&gt;Inductive biases, which affect what kind of functions a model can learn, are usually &lt;em&gt;built into&lt;/em&gt; the choice of deep learning architecture, or decided by other hyperparameters we humans choose. With guided transfer learning, they can now be &lt;em&gt;learned&lt;/em&gt; automatically during pre-training. It’s almost like the model is figuring out some of its own optimal hyperparameters for learning in a particular domain.&lt;/p&gt; &lt;p&gt;Sounds like magic, right? How does GTL allow a model to &lt;em&gt;learn&lt;/em&gt; inductive biases? Well, the core behind the GTL approach is a process known as &lt;strong&gt;scouting&lt;/strong&gt;, which is an alternative to traditional pre-training. The high-level idea is that it trains copies of the model, called scouts, on easier subproblems. These subproblems should be similar to the target downstream tasks, but easier so that the scouts are more likely to succesfully converge to a generalizable model. (If the scouts themselves overfit, then how can the inductive biases they learn help our downstream few-shot training not overfit?)&lt;/p&gt; &lt;p&gt;In the process of converging, the scouts keep track of which parameters in the model are important to keep flexible for efficient convergence and which ones aren’t. They’re basically logging their learning process.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/scouting-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/scouting-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/scouting-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/scouting.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;For example, if weight &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; increases drastically during training, it’s probably an important weight to change and we should keep it flexible. On the other hand, if weight &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; doesn’t change much at all or fluctuates in a very noisy manner (i.e. doesn’t change &lt;em&gt;meaningfully&lt;/em&gt;), it is probably not as important to change.&lt;/p&gt; &lt;p&gt;After the scouts are finished training, the collective feedback from all the scouts is used to decide what inductive biases to impose on the &lt;em&gt;main model&lt;/em&gt;, such that the main model can learn most efficiently for the particular domain of data and avoid &lt;em&gt;wasting effort&lt;/em&gt; and being &lt;em&gt;distracted/misguided&lt;/em&gt; by changing parameters that don’t really help in that domain.&lt;/p&gt; &lt;h2 id=&quot;guide-values&quot;&gt;Guide Values&lt;/h2&gt; &lt;p&gt;So what do these “inductive biases” actually look like, and how do they affect future training? The inductive biases in the context of GTL come in the form of &lt;strong&gt;guide values&lt;/strong&gt;. So after scouting, each parameter will not only have its usual weight value, but it will also have a guide value. During gradient decent, the normal update for a particular weight is then multiplied by its corresponding guide value. Thus, the larger the guide value, the more that parameter is allowed to change during downstream training.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/guide_values_2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; In this very simple neural network with two weights, we can see here that weight `A` has a guide value of 0.56, while weight `B` has a guide value of merely 0.01. Thus, weight `A` is more flexible, in other words allowed to change more, than weight `B` during downstream training. In fact, weight `B` is pretty much frozen, as its guide value of 0.01 makes it so that weight B can barely change throughout training. &lt;/div&gt; &lt;p&gt;Thus, the goal of scouting is to &lt;strong&gt;find these optimal guide values&lt;/strong&gt;, which will ultimately make the training &lt;em&gt;process&lt;/em&gt; more sparse (i.e. so that only the weights that are useful to change get changed). Note that this is different from making the &lt;em&gt;neural network model itself&lt;/em&gt; more sparse (i.e. setting weights/connections that are useless to zero).&lt;/p&gt; &lt;h2 id=&quot;calculating-guide-values&quot;&gt;Calculating Guide Values&lt;/h2&gt; &lt;p&gt;So how do we actually get the guide values after training the scouts? Well, as mentioned above, we keep track of how parameters change during the scout training processes. Specifically, during the training of each scout, we log the initial value and final value (i.e. value after convergence) of each parameter in the model. Then, we calculate how much each parameter changes throughout the process of convergence via some distance metric between its initial and final value. The default used in the GTL paper was the squared distance: \((w_b - w_f)^2\), where \(w_b\) is the baseline (initial) value of the parameter \(w\), and \(w_f\) is its final value.&lt;/p&gt; &lt;p&gt;Now, each scout will converge differently, since they are trained on slightly different subproblems (more on this later). To have a robust estimator of how much some parameter \(w\) changes during convergence, we take the mean squared change of the parameter across &lt;em&gt;all&lt;/em&gt; the scouts. Let’s call this value \(m_w\).&lt;/p&gt; &lt;p&gt;Assuming we have \(N\) scouts, this would be: \(m_w = \frac{1}{N}\sum_{i=1}^{N}(w_{b,i} - w_{f,i})^2\), where \(w_{b,i}\) and \(w_{f,i}\) are the initial and final values (respectively) of parameter \(w\) in scout \(i\).&lt;/p&gt; &lt;p&gt;Add on a 0-1 normalization across the \(m_w\)s of &lt;em&gt;all&lt;/em&gt; the parameters in the model, and we have our guide values (all of which are between 0 and 1)!&lt;/p&gt; &lt;p&gt;Intuitively, we can see that parameters that changed a lot throughout the convergence process in the scout models are deemed “important to change during training” and are thus given higher guide values (i.e. closer to 1), allowing them to be more flexible for downstream fine-tuning.&lt;/p&gt; &lt;p&gt;It’s really quite an elegant and simple approach, which is the beauty of it! It’s comparably lightweight in terms of both memory and computation compared to many other popular meta-learning/few-shot learning methods. &lt;d-cite key=&quot;NIPS2016_90e13578&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;woodward2017active&quot;&gt;&lt;/d-cite&gt; The devil is in the details on how to design the subproblems for scouts, which will be one of the main aspects we will be exploring in the experiments later in this blogpost.&lt;/p&gt; &lt;h2 id=&quot;experiment-and-results-from-the-gtl-paper&quot;&gt;Experiment and Results from the GTL Paper&lt;/h2&gt; &lt;p&gt;Before we get started with some of our own experiments to explore more nuances of GTL behavior and benefits, it might be nice to establish that– &lt;strong&gt;Yes, it does work!&lt;/strong&gt; Or, it at least provides very impressive benefits.&lt;/p&gt; &lt;p&gt;In the original GTL paper, Nikolić et al. tested how much benefit GTL would provide for few-shot learning tasks specifically in the domain of computer vision. Specifically, they tested one-shot learning capability on the Omniglot dataset. &lt;d-cite key=&quot;omniglot&quot;&gt;&lt;/d-cite&gt; The Omniglot dataset is a popular few-shot learning dataset containing characters from 50 different alphabets, with only 20 handwritten examples of each (I will also be using this dataset for a couple of my experiments below). The conventional problem set up with Omniglot is to train/fine-tune your model on just &lt;em&gt;one&lt;/em&gt; example of each character, and use the rest of the examples for validation/testing.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/omniglot2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; To test one-shot learning, a model is trained/fine-tuned on one example of a character and tested on its ability to identify that character in validation samples. Figure from Nikolić et al. &lt;d-cite key=&quot;gtl&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;To prepare a one-shot learner for this task, Nikolić et al. pre-trained a very basic CNN using the following GTL pipeline:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Pre-train the model &lt;em&gt;traditionally&lt;/em&gt;&lt;/strong&gt; on MNIST (lots of data there!). The goal here is to have the model acquire general knowledge in the form of pre-trained weights. No inductive biases yet.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scouting.&lt;/strong&gt; The meat of GTL, where &lt;em&gt;inductive biases&lt;/em&gt; are learned!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Downstream fine-tuning and evaluation&lt;/strong&gt; on Omniglot using the one-shot scheme described above.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The most interesting part is the second step: scouting! Remember, we have the following criteria for the scout problems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There needs to be &lt;em&gt;multiple&lt;/em&gt; different scouting problems (so the we can have an ensemble of &lt;em&gt;different&lt;/em&gt; scouts contributing to the guide value calculations, making the guide values more robust)&lt;/li&gt; &lt;li&gt;The scout problems need to be &lt;em&gt;easy&lt;/em&gt; enough so that the scouts can actually successfully learn generalizable models! Again, if the scouts themselves overfit, the guide values derived form them won’t be very helpful for downstream one-shot learning :)&lt;/li&gt; &lt;li&gt;The scout problems need to be &lt;em&gt;similar&lt;/em&gt; to the downstream task, i.e. in the same domain (in this case, computer vision) and of the same kind of problem (e.g. in this case, classification). If the scout problems are too different, why would the inductive biases be transferable?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given these criteria, Nikolić et al. used the following scheme for generating scouting tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create &lt;em&gt;subdatasets&lt;/em&gt; of MNIST (termed “cousin” problems in the paper), where each subdataset/cousin contains data for only &lt;em&gt;three&lt;/em&gt; of the digits in MNIST (120 of these cousin datasets were created in the paper).&lt;/li&gt; &lt;li&gt;Train a scout on each of the cousin problems (120 scouts total).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This scheme satisfies all three criteria above. We now have multiple different scouting problems. These scouting problems are also comparatively way easier than the downstream task (there’s &lt;em&gt;way&lt;/em&gt; more training data than Omniglot, and it’s only a 3-category classification problem). BUT, despite being easier, they’re still similar enough to the downstream task such that we can expect transferability (it’s still a handwritten character image classification task, after all).&lt;/p&gt; &lt;p&gt;And this worked quite spectacularly! Here are the results from their paper:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/paperresults2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Left plot shows validation accuracy curves for a mdoel that was traditionally pre-trained and a model that was pre-trained with the addition of GTL. Right plot shows the distribution of guide values in the model that was pre-trained with GTL. Figures from Nikolić et al. &lt;d-cite key=&quot;gtl&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;The plot on the left shows the validation curves for the downstream one-shot Omniglot task for 1) a model that was pre-trained traditionally (line in blue) and 2) the model that was pre-trained traditionally &lt;em&gt;and&lt;/em&gt; underwent GTL scouting (line in orange). Although the GTL model was still only to get around 25% validation accuracy, that’s quite impressive for only getting one example of each character, and is a signficant improvement over the model that only experienced traditional pre-training.&lt;/p&gt; &lt;p&gt;Interestingly, the plot on the right plots the distribution of guide values. We see a heavy right skew, indicating that most of the guide values are very close to 0! This means downstream fine-tuning has been made &lt;em&gt;very&lt;/em&gt; sparse (very few parameters were allowed to change drastically), providing very strong inductive biases that heaviliy influenced &lt;em&gt;how&lt;/em&gt; the model was allowed to learn. These inductive biases, as the results suggest, seem to be correct for the task at hand. But that shouldn’t be surprising because they were, in a way, &lt;em&gt;learned&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;And that is the beauty of GTL. We no longer have to “guess” what inductive biases (often in the form of architectural choices) might be appropriate for a certain domain; instead, we have these biases be “learned”!&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h1 id=&quot;answering-unanswered-questions-exploring-the-nuances&quot;&gt;Answering Unanswered Questions: Exploring the Nuances&lt;/h1&gt; &lt;p&gt;Now that we see GTL does provide noticeable benefit for one-shot learning tasks based on the experiemental results from Nikolić et al., I would like to run some additional experiments of my own to explore some of the nuances of when GTL can be helpful, how we can optimize the benefit we get from using it, and how we should go about designing scout problems. These questions had not been explored in the original GTL paper, and since no other piece of literature has yet to even mention GTL, I thought I’d take the lead and try to gain some initial insight into some of these open topics :)&lt;/p&gt; &lt;h2 id=&quot;experiment-1-can-gtl-compensate-for-lack-of-pre-training-data-not-just-lack-of-fine-tuning-data&quot;&gt;Experiment 1: Can GTL compensate for lack of &lt;em&gt;pre-training&lt;/em&gt; data (not just lack of &lt;em&gt;fine-tuning&lt;/em&gt; data)?&lt;/h2&gt; &lt;p&gt;So we’ve established that GTL can aid in learning &lt;em&gt;downstream&lt;/em&gt; tasks with few training samples, but it still requires a large amount of pre-training data (e.g. MNIST), much like traditional transfer learning. What I want to know now is: what if we don’t have &lt;em&gt;that&lt;/em&gt; much pre-training data? In such &lt;em&gt;low pre-training-data contexts&lt;/em&gt;, performance on downstream tasks usually suffers as a results when using traditional transfer learning. Can the addition of scouting/GTL &lt;em&gt;compensate&lt;/em&gt; for this lack of pre-training data? That is, can a model pre-trained with a small pre-training dataset + GTL do as well as a model that’s just traditionally pre-trained on a large pre-training dataset?&lt;/p&gt; &lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt; &lt;p&gt;To do test this, I pre-train a small CNN with a very similar GTL pipeline as the one used by Nikolić et al., but using only a mere &lt;em&gt;1000&lt;/em&gt; of the full 60,000 samples from the MNIST dataset during pre-training/scouting. A significantly smaller pre-training dataset! I’ll sometimes refer to this subset of MNIST as “small MNIST”. I then evaluate the performance of this model on an Omniglot one-shot task and compare it to 1) a model that is only traditionally pre-trained on small MNIST (no GTL) and 2) a model that is traditionally pre-trained on the full 60,000-sample MNIST (also no GTL).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_diagram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_diagram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_diagram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_diagram.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Experiment setup &lt;/div&gt; &lt;h3 id=&quot;downstream-task-specification&quot;&gt;Downstream Task Specification&lt;/h3&gt; &lt;p&gt;Note that the exact setup for the downstream Omniglot one-shot task used in the original GTL paper was not revealed. There are a few variations of one-shot learning setups, but the one I will be using is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Take a 100-cateogry subset of the full Omniglot dataset (that is, 100 unique characters)&lt;/li&gt; &lt;li&gt;Train the model on &lt;strong&gt;one example&lt;/strong&gt; of each unique character (i.e. 100 training samples total), and use the rest as a validation set (i.e. 1900 validation samples total)&lt;/li&gt; &lt;li&gt;The task is thus a 100-way classification problem (given a handwritten image, predict which of the 100 characters it is)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Since the specification above is likely not the exact Omniglot problem setup used by Nikolić et al., and the hyperparameters they used are also not specified in the original paper, some of the baseline results I’m using do not quite match to the corresponding results in the original paper.&lt;/p&gt; &lt;h3 id=&quot;results-and-analysis&quot;&gt;Results and Analysis&lt;/h3&gt; &lt;p&gt;With that said, here are the resulting &lt;em&gt;validation&lt;/em&gt; accuracy and loss curves for the downstream Omniglot one-shot task described above:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp1_val_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Validation accuracy and loss curves for 1) model traditionally pre-trained with large MNIST, 2) model traditionally pre-trained with small MNIST, 3) model pre-trained with GTL and small MNIST. Note that an automatic early stopping mechanism was implemented to cut off when training when validation accuracy stopped increasing. &lt;/div&gt; &lt;p&gt;As we can see, when GTL is not used, pre-training on a 1000 sample subset of MNIST results in notably worse performance on the one-shot downtream task compared to pre-training on the full 60,000 MNIST (16% vs 21% max validation accuracy). This is as expected.&lt;/p&gt; &lt;p&gt;&lt;em&gt;However&lt;/em&gt;, if we use small MNIST &lt;em&gt;and&lt;/em&gt; add scouting/GTL (using the same scout problem set up in the original GTL paper), we see that the resulting model ends up being able to reach &lt;em&gt;almost&lt;/em&gt; the same max validation accuracy as the model traditionally pre-trained on the full MNIST dataset (20% vs 21%).&lt;/p&gt; &lt;p&gt;What this suggests is that the &lt;em&gt;inductive biases&lt;/em&gt; learned by GTL can compensate for any decrease in “general knowledge” (encoded in the form of pre-trained weights) that comes from having a smaller pre-training dataset. &lt;strong&gt;So not only is GTL helpful when you don’t have enough downstream data, it can also be helpful when you don’t have enough pre-training data!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Additionally, if we inspect the validation &lt;em&gt;losses&lt;/em&gt;, we see that, depsite an apparent drop in validation accuracy, overfitting is still occuring in the shadows for all the models, as all the validation loss curves start rising after a certain point. However, the model that is pre-trained with GTL achieves the lowest validation loss of the three models before overfitting, and also starts overfitting the latest. So even though there’s no huge difference in the maximum validation accuracy achieved by the model that was pre-trained with GTL on small MNIST and the model that was traditionally pre-trained on full MNIST, the former is able to be &lt;em&gt;optimized further&lt;/em&gt; before overfitting, suggesting that &lt;strong&gt;GTL with a small pre-training dataset provides a stronger “regularizing” effect than traditional transfer learning with a large pre-training dataset!&lt;/strong&gt; This is certainly an interesting observation that could potentially have more obvious practical implications in certain scenarios, though we will not go into that further in this blog. The takeaway, however, is that GTL is, at the end of the day, really just a strong “regularizer”. If we look at how the orange and red curves look in both the accuracy and loss plots, we see the performance benefit that comes form adding GTL really just comes from the &lt;em&gt;delay of overfitting&lt;/em&gt;. This regularization-based mechanism of performance improvement by GTL makes sense, as strong inductive biases hold the model back from learning “just anything” that fits the downstream training data.&lt;/p&gt; &lt;h2 id=&quot;experiment-2-how-does-the-design-of-the-scouting-task-affect-downstream-performance&quot;&gt;Experiment 2: How does the design of the scouting task affect downstream performance?&lt;/h2&gt; &lt;p&gt;Okay, it seems so far that the scouting pipeline used in the original GTL paper seems to be pretty helpful for various scenarios. But how did the authors arrive at that specific scouting task formulation? What if we used different scouting tasks than the ones they did? How does that affect GTL performance, and what might such differences (if any) imply? After all, when we leave the context of MNIST and Omniglot, we’ll have to be designing these scouting tasks on our own…&lt;/p&gt; &lt;h3 id=&quot;setup-1&quot;&gt;Setup&lt;/h3&gt; &lt;p&gt;For the sake of experimental control, however, I will stick with MNIST and Omniglot for now (don’t worry, I deviate from these datasets in the next experiment). Here, I begin by testing the effects of changing &lt;em&gt;how many categoriess&lt;/em&gt; are included the cousin subdatasets that the scouts are trained on. The original paper used 3 categories per scout dataset (i.e. a 3-way classification task). What if used 2? Or 4? And if that makes a difference, why?&lt;/p&gt; &lt;p&gt;In my eyes, this experiment explores how &lt;em&gt;similarity&lt;/em&gt; between the scout tasks and the downstream task affects &lt;em&gt;transferability&lt;/em&gt;. Specifically, because the downstream Omniglot task is a 100-way classification problem, one might expect that scout tasks that include more classification categories (and are thus more similar to the donwstream task) would result in better transferability.&lt;/p&gt; &lt;p&gt;To test this, I use a 5000-sample subset of MNIST for pre-training/scouting (to save computation and time). For scouting, I create 120 cousin problems, as done in the paper. But instead of sticking to 3-category cousin problems, I also try 2-category, 4-category, and 5-category problems.&lt;/p&gt; &lt;h3 id=&quot;results-and-analysis-1&quot;&gt;Results and Analysis&lt;/h3&gt; &lt;p&gt;Here are the results:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_val_acc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_val_acc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_val_acc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_val_acc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/exp2_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;As we can see, apparently the number of categories doesn’t make too big of a difference in maximum validation accuracy! They all provide seemingly equal accuracy improvement from a baseline model pre-trained traditionally on the same 5000-sample MNIST subset. This isn’t too surprising. Compared to the 1000-way downstream classification, the difference between 2-way and 5-way classification tasks would intuitively seeem pretty negligible.&lt;/p&gt; &lt;p&gt;The validation loss plot tells a slightly different story, however. We see &lt;em&gt;most&lt;/em&gt; of the models pre-trained with GTL have similar loss curves, consisting of a lower minimal loss and more resilience to overfitting compared to the baseline model. However, the model based on scouts trained on &lt;em&gt;5-category&lt;/em&gt; cousin problems seems to achieve the &lt;em&gt;worst&lt;/em&gt; (highest) minimum validation loss! This seems… a bit hard to explain. Perhaps this is just due to stochasticity; after all, we see that overfitting still occurs &lt;em&gt;later&lt;/em&gt; relative to the baseline model, suggesting there still is some resilience to overfitting.&lt;/p&gt; &lt;p&gt;But a perhaps more interesting explanation (that admittedly could be &lt;em&gt;completely&lt;/em&gt; wrong) is that 5-category problems may have been too &lt;em&gt;difficult&lt;/em&gt; of a scouting task given the smaller subset of MNIST used (since lots of categories + few training samples is a often recipe for overfitting). That is, perhaps many of the &lt;em&gt;scouts&lt;/em&gt; themselves would have started overfitting while being trained on these subproblems, so the guide values derived from such scouts don’t end up providing robust enough inductive biases.&lt;/p&gt; &lt;p&gt;Again, this is just a speculation, but if it were true, this could suggest an interesting tradeoff between the &lt;strong&gt;easiness&lt;/strong&gt; of the scouting tasks and their &lt;strong&gt;similarity&lt;/strong&gt; to the target downstream task. Make a scouting task too easy, and it’s too different from the target downstream task, and transferability suffers as a result. Make a task too similar to the target downstream task, and it might be too difficult, causing the scouts themselves to overfit and the resulting guide values to be less useful. An intersting balance to think about and explore further.&lt;/p&gt; &lt;p&gt;The overarching takeaway from this experiment, however, seems to be that the exact number of categories for the scouting problems at this specific scale does not drastically affect downstream one-shot performance. Sure, I could have tried to keep increasing the number of categories, but keep in mind there’s also a bit of a tradeoff between number of categories and number of possible scouts past a certain point. For example, we would only be able to have one cousin problem with 10 categories (and it would be the whole MNIST dataset)!&lt;/p&gt; &lt;h2 id=&quot;experiment-3-what-about-unsupervisedself-supervised-settings&quot;&gt;Experiment 3: What about unsupervised/self-supervised settings?&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This particular experiment builds off of some previous work I have done outside of this class.&lt;/p&gt; &lt;p&gt;For the final experiment, I would like to provide a bit of my research background for context. I’m primarily intereted in applying/developing AI methodologies for biomedical research. Specifically, I work a lot with “omics” data (e.g. transcriptomics data like RNA-seq, proteomic data, etc.), which is a domain notoriously cursed with datsets characterized by high dimensionality and low sample size. This means that we are almost always forced to utilize pre-training and transfer learning in order to make any deep learning model work for specific downtream tasks. Sounds like the perfect context to apply GTL to!&lt;/p&gt; &lt;p&gt;However, there’s one very important caveat. Pre-training in the omics domain is usually &lt;strong&gt;self-supervised&lt;/strong&gt;, since large pre-training datasets are often aggregates of hundreds of smaller datasets from separate studies that don’t share the same labeling/metadata catogories. So far, whether it’s the original GTL paper or our own experiments above, we have only explored GTL in the context of &lt;em&gt;supervised&lt;/em&gt; pre-training, scouting, and fine-tuning. How can we adapt GTL when the pre-training (and perhaps the scouting) involve unlabeled data?&lt;/p&gt; &lt;p&gt;To explore this, I will build off of one of my previous research projects, conducted while I was an intern at NASA Ame’s Space Biology Division. The project involved pre-training (traditionally) a large RNA-seq BERT-like model (called scBERT &lt;d-cite key=&quot;scbert&quot;&gt;&lt;/d-cite&gt;) on a large &lt;em&gt;unlabeled&lt;/em&gt; collection of RNA-seq data (recount3 &lt;d-cite key=&quot;recount3&quot;&gt;&lt;/d-cite&gt;) in a self-supervised manner (via input masking, like the original BERT). I had evaluated this pre-trained model on a downstream classification task that involved predicting whether or not a mouse had been to space based on its RNA-seq profile. The downstream task involved fine-tuning on &lt;a href=&quot;https://osdr.nasa.gov/bio/&quot;&gt;NASA’s OSDR datasets&lt;/a&gt;, which contain RNA-seq data from ground control mice and &lt;em&gt;actual mice that have been to space&lt;/em&gt; during the NASA Rodent Research missions. The problem was, of course, that very few mice have been to space, so these datasets were tiny (i.e. 12 mice per dataset). It was quite an extreme exacerbation of the high dimensional, low sample size characterization I normally had to deal with. In that project, I showed that traditionally pre-training provided signficant benefits over training from scratch (as expected).&lt;/p&gt; &lt;h3 id=&quot;gtl-pipeline-for-scbert-scouting-problem&quot;&gt;GTL pipeline for scBERT: Scouting Problem&lt;/h3&gt; &lt;p&gt;Today, however, I would like to see if GTL can provide any additional benefits to that project. The most obvious challenge, as mentioned earlier, is creating scout problems out of an unlabeled pre-training dataset (recount3).&lt;/p&gt; &lt;p&gt;Sure, we could use self-supervised masked input prediction for scouting, which is how scBERT is pre-trained traditionally. However, it’s not immediately clear, at least to me, how exactly we would create &lt;em&gt;multiple different&lt;/em&gt; scout problems using this scheme (perhaps different masking patterns?). Additionally, we would ideally want the scout tasks to be more similar to the downstream task (which is a binary classification task, i.e. predicting whether or not a mouse sample is ground control or spaceflown) and share mostly the same architecture (i.e. more parameters with transferable guide values). Finally, as mentioned before, we would like to make the scouting tasks sufficiently easy so that the scouts can be successfully trained without overfitting. Given these criteria, I propose the following scouting problem:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Reduce the dimensionality of recount3 dataset using UMAP, keeping only the top 30 UMAP dimensions (to make the next step computationally tractable)&lt;/li&gt; &lt;li&gt;Cluster using K-means clustering. K=30 seems to provide visually logical clusters, so that’s the one we will go with.&lt;/li&gt; &lt;li&gt;To create subdatasets (“cousin” problems), we choose random &lt;em&gt;pairs&lt;/em&gt; of K-means clusters. Thus, each subdataset includes recount3 data from a random pair of clusters.&lt;/li&gt; &lt;li&gt;For each subdatset created, train a scout to classify the &lt;em&gt;cluster identity&lt;/em&gt; of the samples (a binary classification task). Thus, the scouting task is very similar to the downstream task (which is also binary classification). This &lt;em&gt;also&lt;/em&gt; means we can use the same exact model architecture for both the scouting tasks and the downstream task (maximal transferability!).&lt;/li&gt; &lt;/ol&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/clusteringrecount3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/clusteringrecount3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/clusteringrecount3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/clusteringrecount3.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Pipeline for creating scout problems. &lt;/div&gt; &lt;p&gt;Now, this might seem like a trivial task for the classifier. After all, we are clustering the data based on geometric proximity, then train a model to find decision boundaries between the clusters, so it would seem that the model could find a perfectly clean decision boundary pretty easily. However, keep in mind that the clustering is done in UMAP space, with only the top 30 UMAP components, while the classification is done in the original feature space. UMAP is a nonlinear transformation, so clusters that are easily perfectly separable in top 30 UMAP space may not be in the original space. However, it is definitely still a pretty easy task, but we &lt;em&gt;want&lt;/em&gt; the scouting tasks to be doable enough so that the scouts can easily converge to a generalizable relationship. So theoretically, it seems reasonable that this could work! (((Admittedly, it took a lot of playing around before deciding on the above scouting formulation; it just ended up being the one that worked the best. I can’t tell you exactly why, but my reasoning above is the best “intuitve” reasoning I could come up with.)))&lt;/p&gt; &lt;h3 id=&quot;gtl-pipeline-for-scbert-downstream-task&quot;&gt;GTL pipeline for scBERT: Downstream Task&lt;/h3&gt; &lt;p&gt;What about the downstream few-shot task? Here, I will use the same task that I had previously used to evaluate my traditionally pre-trained scBERT model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We train the model on a single NASA OSD dataset, OSD 105 &lt;d-cite key=&quot;OSD-105&quot;&gt;&lt;/d-cite&gt;, containing bulk RNA-seq data from 6 spaceflown and 6 ground control mice, and have it predict whether a mouse was spaceflown or ground control. A simple binary classification task, like the scouting problem, but much harder given the incredibly low sample size.&lt;/li&gt; &lt;li&gt;We then validate using another similar NASA OSD dataset, OSD 104 &lt;d-cite key=&quot;OSD-104&quot;&gt;&lt;/d-cite&gt;, also containing 6 spaceflown and 6 ground control mice.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It’s important to note that these two datasets, OSD 105 and 104, contain RNA-seq data from different muscle locations. OSD 105 contains tibilalis anterior data, while OSD 104 contains soleus data. However, since these datasets all contain data from some sort of mouse skeletal muscle tissue, we expect that cross-dataset generalizability would be reasonable for a strong generalizable model, and I actually intentionally chose datasets from different muscle tissues to test this difficult problem of cross-tissue generalizability.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/downstream-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/downstream-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/downstream-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/downstream.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Downstream few-shot binrary classification task on NASA OSDR datasets (predicting whether mouse is a ground control or spaceflown sample based on its RNA-seq profile). &lt;/div&gt; &lt;h3 id=&quot;gtl-pipeline-for-scbert-whole-pipeline&quot;&gt;GTL pipeline for scBERT: Whole Pipeline&lt;/h3&gt; &lt;p&gt;After deciding on the scouting problem formulation, the rest of the pipeline is pretty straightforward. Here’s the full pipeline:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Pre-train scBERT traditionally&lt;/strong&gt; on recount3 (self-supervised masked input prediction). This involves the encoder portion of the architecture, which embeds the input, and a reconstructor portion, which uses that embedding to reconstruct the masked input values. The goal here, as always, is to learn &lt;em&gt;general knowledge&lt;/em&gt; about the domain (RNA-seq) in the form of good &lt;em&gt;pre-trained weights&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scouting&lt;/strong&gt; on recount3, using the scouting formulation described above. Here, we replace the reconstructor portion of the scBERT architecture with a classification layer. The goal here is, of course, to learn &lt;em&gt;inductive biases&lt;/em&gt; in the form of &lt;em&gt;guide values&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Downstream few-shot fine-tuning&lt;/strong&gt; on NASA OSDR datasets, using the few-shot formulation described above. Here, we use the &lt;em&gt;same&lt;/em&gt; architecture as the scouts. &lt;em&gt;All guide values transfer over!&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/pipeline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/pipeline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/pipeline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/pipeline.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Full pipeline for applying GTL on scBERT. &lt;/div&gt; &lt;h3 id=&quot;results-and-analysis-2&quot;&gt;Results and Analysis&lt;/h3&gt; &lt;p&gt;And… here are the results for the downstream task! To compare, I’ve also included results for an scBERT copy that didn’t undergo any pre-training and an scBERT copy that was only traditionally pre-trained on recount3.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/results-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/results-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/results-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-02-guided-transfer-learning/results.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Downstream training and validation accuracy/loss curves for (from left to right): scBERT without any pre-training, scBERT traditionally pre-trained, scBERT pre-trained with scouting (GTL). A cosine annealing learning rate scheduler was used, ala the original scBERT paper, hence the jumps and fluctuations. &lt;/div&gt; &lt;p&gt;With no pre-training, we can see that severe overfitting to the training set happens almost immediately, with validation loss going up while training loss goes down. This makes sense given the extremely small size of the training set, and the fact that the training and validation sets are from different muscles. With traditional pre-training, however, we see that overfitting also does eventually happen, but right before it happens, at around 200 epochs, we get this sweet spot where validation loss is at a low and validation accuracy is at a high of around 90% (highlighted by the purple box). So it seems that general knowledge about RNA-seq data obtained from traditional pre=training already provides a regularizing effect that reigns in the model from overfitting immediately to the small dowsntream training dataset. These results are from my previous work and are nothing new.&lt;/p&gt; &lt;p&gt;Now, when we add &lt;em&gt;scouting&lt;/em&gt;, the max validation accuracy becomes 100%, which is an improvement from the traditionally pre-trained model, though this by itself may not be that notable given the already good validation accuracy after traditional pre-training. What’s potentially more interesting, however, is that this maximum validation performance is maintained over three times as many epochs compared to the traditionally pre-trained model, suggesting that the maximal performance achieved by the GTL model is more robust. However, it is also worth noting that the validation accuracy is a lot noisier and jumps around a lot more for this model compared to the others (keep in mind cosine annealing learning rate scheduler is being used for all these models). But overall, it seems that guided transfer learning provides a more robust regularization effect, giving it a longer period of time with peak validation performance before overfitting occurs.&lt;/p&gt; &lt;p&gt;This is quite exciting, as it shows that, given the right scouting problem setup, &lt;strong&gt;we can adapt GTL in settings where our pre-training data is unlabeled, as well!&lt;/strong&gt; The flexiblity of GTL that allows it to be adapted to such a large variety of scenarios is what, in my eyes, makes this method truly innovative!&lt;/p&gt; &lt;h1 id=&quot;closing-thoughts&quot;&gt;Closing Thoughts&lt;/h1&gt; &lt;h2 id=&quot;experiment-limitations-and-next-steps&quot;&gt;Experiment Limitations and Next Steps&lt;/h2&gt; &lt;p&gt;These experiements are merely to serve as a &lt;em&gt;preliminary&lt;/em&gt; exploration of the nuances of GTL beyond what was presented in the original paper, in hopes that more questions will be explored by the community as GTL gains further publicity and traction. Thus, there is clearly plenty of room for imporvement and next steps regarding these experiments.&lt;/p&gt; &lt;p&gt;For experiement 1, I think it would be cool to establish a more rigorous characterization of the amount of pre-training data (or rather lack thereof) that the addition of GTL can compensate for in terms of downstream performance. This might involve using arious &lt;em&gt;even smaller&lt;/em&gt; subsets MNIST and finding the boundary where a pre-training dataset is too small that even GTL cannot compensate for it.&lt;/p&gt; &lt;p&gt;The results of experiment 2 obviously leaves a lot of to be desired, as I only explored single-digit values for the number of categories use in the scout problems. These values are all over an order magnitude off from the number of categories in the downstream task, so none of them gave very useful insight into how “similar” scouting tasks need to be to the downstream task. This was, of course, limited by the MNIST dataset itself, which only had 10 categories. Perhaps using a pre-training dataset with more categories could allow a more comprehensive experiment of this type.&lt;/p&gt; &lt;p&gt;And for experiment 3, I wish I had more time to curate a more robust validation scheme for the downstream few-shot task. A validation set with only 12 samples was really not granular enough to precisely capture the potential benefits of adding GTL on top of traditional transfer learning. When the traditionally pre-trained model is already getting 11/12 prediction correct at its best, is 12/12 really that meaningful of an improvement?&lt;/p&gt; &lt;h2 id=&quot;how-exciting-is-gtl&quot;&gt;How Exciting is GTL?&lt;/h2&gt; &lt;p&gt;As promising as all these results are, GTL is, of course, not the perfect end-all be-all solution to few-shot learning. As was discussed in the original GTL paper and shown in the experiments above, GTL can only provide so much improvement before hitting a wall (e.g. the one-shot learning ability on Omniglot never surpassed 25% validation accuracy). It does not yet quite result in models that match the few-shot learning ability of human intelligence, and still requires a considerable amount of pre-training data. However, the lightweight nature, simplicity, elegance, and adaptibility of the model makes it so that it’s a (relatively) quick and easy solution to get a downstream performance boost on any AI pipelines that already utilize traditional transfer learning!&lt;/p&gt; </content> </entry> <entry> <title>Alive Scene</title> <link href="https://deep-learning-mit.github.io/blog/2023/Visualization-of-CLIP's-Learning-and-Perceiving-Dynamics/"/> <updated>2023-12-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Visualization of CLIP's Learning and Perceiving Dynamics</id> <content type="html">&lt;h2 id=&quot;enchanting-images-with-semantic-embedding&quot;&gt;Enchanting Images with Semantic Embedding&lt;/h2&gt; &lt;p&gt;“Alive Scene” is an advanced AI-driven project that revolutionizes the concept of scene capture, drawing inspiration from the enchanting, ever-changing portraits in the Harry Potter series. This innovative pipeline goes beyond traditional methods of capturing scenes as static images. Instead, it delves deep into the semantic understanding of each scene, enabling it to not only recreate these scenes with high fidelity but also to imbue them with the ability to act, evolve, and respond autonomously.&lt;/p&gt; &lt;p&gt;The following GIF image on the right is the output from the Alive Scene Pipeline. Notice that these scenes start from the same status.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/7cFU.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/7cFU.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/7cFU.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/7cFU.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Hogwarts Portraits &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Alive Scene captures cats&apos; napping behaviors &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The core of this project lies in its sophisticated AI algorithms that analyze and interpret the nuances of each scene, from the physical elements to the underlying emotions and narratives. This enables the system to generate dynamic, lifelike representations that are far from static images. These AI-crafted scenes possess the unique ability to change organically over time, reflecting the natural progression and evolution one would expect in real life.&lt;/p&gt; &lt;p&gt;Through “Alive Scene,” portraits and scenes are no longer mere representations; they become entities with a semblance of life, capable of exhibiting behaviors and changes that mirror the fluidity and spontaneity of living beings. There are three elements in this project, the first is using CLIP model as encoder to compress image into clip embeddings. Second, train a generator to reconstruct the original image from the CLIP embedding. then train a behavior model to lean the behavior of clip embeddings in the clip feature space; the behavior will use to drive the generator; making the scene representation alive. The following is the diagrams of the pipeline.&lt;/p&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/pipeline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/pipeline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/pipeline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/pipeline.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Alive Scene Pipeline &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt; &lt;p&gt;The CLIP (Contrastive Language–Image Pre-training) model&lt;d-cite key=&quot;radford2021learning&quot;&gt;&lt;/d-cite&gt;, represents a groundbreaking approach in integrating visual and textual data within the realm of artificial intelligence. In this project, it plays and important role to comprehend the scenario and characters’ behaviors in the scene. Detailed investigations&lt;d-cite key=&quot;wang2020understanding&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;shi2023understanding&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;zhao2017exact&quot;&gt;&lt;/d-cite&gt; offers insightful understanding of the model’s operations, showing the potential that CLIP embeddings could make a machine comprehend and compress complex information of images.&lt;/p&gt; &lt;p&gt;The study&lt;d-cite key=&quot;author2021cinn&quot;&gt;&lt;/d-cite&gt; explores using conditional Invertible Neural Networks (cINNs) for transforming still images into videos, highlighting cINNs’ prowess in handling static to dynamic content transitions. Although proficient in capturing motion, the model’s grasp on object/event types may benefit from CLIP embeddings enhancement. My project, unlike this work, aims to animate static scene representations with self-driven behaviors, not just manipulate videos.&lt;/p&gt; &lt;p&gt;Another significant work, “Make-A-Video”&lt;d-cite key=&quot;singer2022makeavideo&quot;&gt;&lt;/d-cite&gt;, introduces a text-to-video generation method utilizing text-to-image models. This approach circumvents the need for text-video paired data, learning from text-image data and unsupervised videos. It employs a spatiotemporal diffusion model and super-resolution techniques for high-quality video creation from text. My project differs, focusing on bringing life to existing videos or image sequences, rather than generating new content from text.&lt;/p&gt; &lt;p&gt;Despite the static background, the cats’ movements are so subtle that they pose a challenge for human observers to distinguish differences between frames. To visualize the clip embeddings of the frames from the video, I employ both UMAP and t-SNE&lt;d-cite key=&quot;maaten2008tsne&quot;&gt;&lt;/d-cite&gt; techniques for gaining more insights.&lt;/p&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_umap.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_umap.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_umap.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_umap.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; UMAP Visualization &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_tsne.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_tsne.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_tsne.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/latent_tsne.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; t-SNE Visualization &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The behavior over time resembles a ‘spaghetti’ pattern, indicating that certain scenarios or behaviors may recur (as seen in the crossings or interactions within the spaghetti diagram). Some intersecting points demonstrate similar tendencies, while others are more unpredictable, highlighting the complexity of the video.&lt;/p&gt; &lt;p&gt;Both visualizations provide a promising sign: the end and start frames are positioned close to those in the middle. This proximity allows the Alive Scene to operate seamlessly and endlessly. For example, when the Alive Scene approaches a point near the end, it can smoothly transition to a frame somewhere in the middle. Similarly, when it encounters a region where different frames cluster together, it has a variety of options to choose from for its next move. This flexibility is key to making the Alive Scene function effectively.&lt;/p&gt; &lt;h3 id=&quot;generator&quot;&gt;Generator&lt;/h3&gt; &lt;p&gt;The Generator (decoder) is a SIREN model, which employs CLIP semantic embeddings and positional embeddings of pixel coordinates to generate RGB colors&lt;d-cite key=&quot;sitzmann2019siren&quot;&gt;&lt;/d-cite&gt;. SIRENs, or Sinusoidal Representation Networks, diverge from traditional neural networks by utilizing sinusoidal activation functions instead of common ones like ReLU. These networks are adept at implicitly representing intricate data patterns, making them particularly advantageous for tasks that involve complex spatial structures or continuous data. The incorporation of periodic activation functions in SIRENs can significantly enhance deep learning capabilities, especially in fields such as computer vision and generative models.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/SIREN_DECODER-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/SIREN_DECODER-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/SIREN_DECODER-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/SIREN_DECODER.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; a SIREN model as the generator &lt;/div&gt; &lt;/figure&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/tp_siren.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/tp_siren.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/tp_siren.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/tp_siren.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; the training progression &lt;/div&gt; &lt;/figure&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/generated_.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/generated_.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/generated_.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/generated_.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Original Video vs Generated Video &lt;/div&gt; &lt;/figure&gt; &lt;p&gt;The code of the generator model (SIREN)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SineLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SineLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Siren&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;560&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Siren&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SineLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SineLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SineLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SineLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Init weights &lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uniform_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uniform_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uniform_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uniform_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uniform_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MLP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MLP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psnr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pixel_coordinates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pixel_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psnr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psnr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;behavior-model&quot;&gt;Behavior model&lt;/h3&gt; &lt;p&gt;This project introduces a customized asymmetrical Variational Autoencoder (VAE)&lt;d-cite key=&quot;kingma2014autoencoding&quot;&gt;&lt;/d-cite&gt; as the probabilistic model to predict motion within the CLIP embedding space. A VAE-like model may prove beneficial for this task for two primary reasons. Firstly, they are adept at learning a continuous, smooth latent space, facilitating efficient interpolation and manipulation of data representations. Given that the training data derives from a video, it is inherently sequential and should be represented in a continuous fashion. Secondly, VAEs utilize amortized inference, where the encoder is trained to generalize the mapping of inputs to the latent space across the dataset, as opposed to conducting inference anew for each input. For this project, the objective is to devise a method that allows for a smooth navigation within the observed embedding space.&lt;/p&gt; &lt;p&gt;The code of the behavior model (VAE)&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# BehaviorModel(inspired by VAE) &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BehaviorModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VAE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Encoder &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc21&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Mean &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc22&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Log variance &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Decoder &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reparameterize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bn3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bn4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fc5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reparameterize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Loss function &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BCE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;binary_cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KLD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BCE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KLD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Use Mean Squared Error for the reconstruction loss &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MSE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# KLD is unchanged &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KLD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MSE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KLD&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The process begins with a CLIP embedding as the input, which is then transformed by the model to output a motion vector. This vector retains the same dimensions as the CLIP embedding and is utilized to alter the original embedding, facilitating the generation of the subsequent frame based on this modified embedding.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/PrbabilisticModel-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/PrbabilisticModel-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/PrbabilisticModel-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/PrbabilisticModel.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; the Asymmetrical VAE &lt;/div&gt; &lt;/figure&gt; &lt;p&gt;In this case, I generate 200 frames for training; the number is quite small. To enhance the model’s learning efficacy, new data points are generated through linear interpolation between existing data points (frames). By doing this, I generated 1000 clip embeddings and frames. These newly created samples undergo normalization to conform to the geometric constraints of the CLIP embedding space, often characterized as a hypersphere. This normalization process ensures that the interpolated data points adhere to the distribution pattern of the original embeddings. As depicted in the diagram, this technique leads to a densified clustering of data points in close proximity to the original embeddings, which is advantageous. It implies a higher confidence in the authenticity of these new points due to their closeness to the authentic, or ground truth, data.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Interpolation.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Gaining more data points by Interpolation &lt;/div&gt; &lt;/figure&gt; &lt;p&gt;When operating the process that animates the Alive Scene, it occasionally generates artifacts. This may be caused by certain movements that deviate significantly from the observed reality. Please refer to the following GIF for an example.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/broken.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/broken.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/broken.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/broken.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Artifacts &lt;/div&gt; &lt;/figure&gt; &lt;p&gt;To resolve the issue, I have developed a post-processing technique that stabilizes the outcomes. The process begins by re-normalizing the resulting embedding onto the hypersphere. Following this, a weighted parameter is introduced to draw the vector incrementally toward the domain of previously observed CLIP embeddings. For example, if the weighting parameter is set to 0.1 for the observed embedding, it would be scaled by 0.1, while the predicted embedding is scaled by 0.9. These two are then summed to produce a final embedding that, while primarily influenced by the prediction, retains a subtle alignment with the observed data. This weighted approach aims to mitigate artifacts by anchoring the predictions within the realm of observed realities.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Post-curing-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Post-curing-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Post-curing-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/Post-curing.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Post-curing method &lt;/div&gt; &lt;/figure&gt; &lt;p&gt;By applying this method, the Alive Scene has started to yield more stable results. Interestingly, the outcomes are varied, exhibiting behaviors akin to a living creature — somewhat unpredictable yet within a framework of predictability.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/allt5.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; 4 different versions &lt;/div&gt; &lt;/figure&gt; &lt;h3 id=&quot;manipulation&quot;&gt;Manipulation&lt;/h3&gt; &lt;p&gt;The Alive Scene operates autonomously, and to explore the modulation of its behavior, I have introduced the concept of ‘temperature.’ This concept acts as a coefficient that scales the movement vector, thereby allowing the scene to exhibit behaviors that are either more expansive and varied, or more constrained and subtle, depending on the temperature setting.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/TEMPERATURE-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/TEMPERATURE-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/TEMPERATURE-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/TEMPERATURE.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; 4 different versions &lt;/div&gt; &lt;/figure&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/vt.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/vt.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/vt.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/vt.gif&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; different temperature settings &lt;/div&gt; &lt;/figure&gt; &lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt; &lt;p&gt;The “Alive Scene” project signifies a profound achievement in the domain of Deep Learning for scene representation. It leverages CLIP semantic embeddings to decode and imbue scenes with lifelike attributes, while also seamlessly integrating the potent SIREN model as a generator, capable of breathing vitality into the processed embeddings by producing authentic images.&lt;/p&gt; &lt;p&gt;Furthermore, the project implements an asymmetric Variational Autoencoder (VAE) to predict and model motion within the CLIP embedding space, thereby enhancing the dynamism and fluidity of the scenes.&lt;/p&gt; &lt;p&gt;However, the significance of this undertaking extends well beyond its technical accomplishments. By giving birth to scenes that autonomously and organically evolve, the project ushers in a transformative era of possibilities in digital storytelling and interactive media, fundamentally reshaping the landscape of creative expression in the digital realm.&lt;/p&gt; &lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt; &lt;p&gt;In this project, a SIREN model is trained to create a 2D scene representation. This model can be extended to generate a 3D scene by simply adding an additional output node to adopt the Neural Radiance Field (NeRF)&lt;d-cite key=&quot;mildenhall2020nerf&quot;&gt;&lt;/d-cite&gt; architecture. Such an enhancement allows the 3D Alive Scene to offer a more immersive and complex scene representation. Looking ahead, it’s conceivable that a non-player character (NPC) could be manipulated in this manner, especially if the model, when trained on a vast dataset, can learn more sophisticated behaviors. This approach has the potential to encapsulate all necessary information within a highly compact model, offering an extremely lightweight solution for dynamic scene generation.&lt;/p&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/2dvs3d-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/2dvs3d-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/2dvs3d-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-09-Alive%20Scene%20Enchanting%20images%20with%20Semantic%20Embedding/2dvs3d.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; extend the model for a 3D Scene &lt;/div&gt; &lt;/figure&gt; &lt;h3 id=&quot;potential-usages-and-contributions&quot;&gt;Potential Usages and Contributions:&lt;/h3&gt; &lt;p&gt;Digital Art and Entertainment: This project can revolutionize digital art and entertainment by offering dynamic, evolving scenes that enhance animations and virtual experiences.&lt;/p&gt; &lt;p&gt;Film and Animation: It can automate the generation of realistic backgrounds, streamlining the production process for films and animated content.&lt;/p&gt; &lt;p&gt;Advertising and Marketing: The project offers the capability to create interactive, dynamic advertising content, thereby engaging audiences more effectively.&lt;/p&gt; &lt;p&gt;Behavioral Studies: It provides a tool for in-depth analysis of human and animal behaviors, supporting research in fields such as psychology, ethology, and anthropology.&lt;/p&gt; &lt;p&gt;Cultural Preservation: This technology can enliven historical scenes or artworks in museums, offering visitors more immersive and engaging experiences.&lt;/p&gt; &lt;p&gt;Data Visualization: It introduces innovative methods for interacting with and interpreting complex data, useful in sectors like finance and healthcare.&lt;/p&gt; &lt;p&gt;Gaming: The project enables the creation of NPCs with realistic behaviors, significantly enhancing the gaming experience.&lt;/p&gt; &lt;p&gt;Architecture and Engineering: It can be applied for dynamic visualizations in architectural and engineering projects, aiding in design and planning.&lt;/p&gt; &lt;p&gt;Conservation: This technology can contribute to wildlife conservation by facilitating the study of animal behaviors in natural settings.&lt;/p&gt; </content> </entry> <entry> <title>Projected fast feedforward networks</title> <link href="https://deep-learning-mit.github.io/blog/2023/projected-fff-networks/"/> <updated>2023-12-05T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/projected-fff-networks</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Compression of neural networks is a crucial task in Machine Learning. There are three important performance metrics that we should take into account when deploying models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Size of the model. Having a smaller number of parameters that describe the model makes transferring it over network faster. In addition, being able to concisely represent the differences between original and finetuned model would enable storing and distributing a lot of possible finetunings, such as in Stable Diffusion LORA &lt;d-cite key=&quot;luo2023lcmlora&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;GPU memory needed to perform the inference. If the metric is lower, the model inference can be run on less expensive GPUs with less available memory. Some models could even be ran on smartphones or IoT devices&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Inference time. We also can take into account how does the time scales with the size of the batch&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Balancing these characteristics is a non-trivial task, since improvements in one of them could lead to a decline in other metrics. The optimal tradeoff depends on the environment in which the model is ran.&lt;/p&gt; &lt;p&gt;We will explore a way to significantly reduce the model size and the memory needed for inference, keeping the inference time reasonable. We achieve the size reduction by utilizing a common property of having small intrinsic dimension of objetive landscape that many models have.&lt;/p&gt; &lt;h2 id=&quot;related-works&quot;&gt;Related works&lt;/h2&gt; &lt;p&gt;There are several ways how the size of the model can be reduced. One of the popular techniques is model quantization. Quantization of a machine learning model involves decreasing the precision of weights for the sake of reduction of the total memory needed to store them. Quantized models can utilize 16, 8, or even 4-bit floats, with carefully selected summation and multiplication tables. There are different ways of dealing with the inevitable degradation of accuracy due to lack of precision, one possible way is described in paper &lt;d-cite key=&quot;nagel2021white&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;Another direction of model size optimization utilizes the notion of matrix low-rank approximation. The layers of neural networks are commonly represented as matrices, the simpliest example being the parameters of feedforward linear layer. Each matrix \(A\) has a Singular Value Decomposition \(A = U\Sigma V^*\), and, using this decomposition, it’s possible to get close low-rank approximation of \(A\). We note that a matrix of size \(n \times m\) of rank \(k\) can be stored in \(O((n+m)k)\) memory if we express it as a sum of outer products of \(k\) pairs of vectors, so if \(k\) is small, this representation uses much less memory than \(O(nm)\) — the memory used by the dense representation. One of the papers that compresses models with low-rank approximation is &lt;d-cite key=&quot;jaderberg2014speeding&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;However, we are going to explore another compression method, which utilizes small dimensionality of optimization landscape, which is common for many model-task pairs. When training a neural network, we have some loss \(\mathcal{L}\), and a parameter space \(\mathbb{R}^{p}\). Then, we are trying to find \(v \in \mathbb{R}^{p}\) such that \(\mathcal{L}(v)\) is minimized. Instead of searching over the whole space, we generate a linear operator \(\phi\colon \; \mathbb{R}^{d} \to \mathbb{R}^{p}\), where \(d &amp;lt; p\), and parametrize \(v\) as \(v = \phi u\), where \(u \in \mathbb{R}^{d}\). Li et al. &lt;d-cite key=&quot;li2018measuring&quot;&gt;&lt;/d-cite&gt; found that if the the matrix of $\phi$ has normalized columns, for many tasks it’s possible to find a network, parametrized the way above, where \(d\) is significantly smaller than \(p\), such that the network has at least 90% of metric value of the original network. Then, if our way to generate random projection matrix is seeded deterministically, we only need to store \(d\) floats, which are the coefficients of \(u\). During inference, we re-create \(\phi\), and restore the parameters of original architecture \(v = \phi u\). The compression ratio of this method is \(\frac{p}{d}\). In this blog, we will explore its potential modifications, and evaluate them by running experiments.&lt;/p&gt; &lt;h2 id=&quot;basic-experiment&quot;&gt;Basic experiment&lt;/h2&gt; &lt;p&gt;First, we test the method without any modifications. We use dataset MNIST &lt;d-cite key=&quot;deng2012mnist&quot;&gt;&lt;/d-cite&gt;, containing 30000 \(28 \times 28\) monochrome images of digits. Each image belongs to one of 10 classes, depending on the digit, and the task of the model is to classify these digits.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/mnist-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/mnist-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/mnist-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/mnist.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;For each of the experiment, we use a neural network with one hidden layer with 128 units and ReLU activations. We optimize the parameters with Adam and learning rate \(10^{-4}\). The training is ran for \(100\) epochs, our batch size is \(128\).&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;d&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;final val acc&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;17.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;16&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;20.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;64&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;50.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;256&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;71.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;512&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;61.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1024&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;61.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;original&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;95.65&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/random-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/random-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/random-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/random.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;better-initialization&quot;&gt;Better initialization&lt;/h2&gt; &lt;p&gt;We’ve noticed that the optimization of the compressed model does not converge fast. To initialize better, we can use pre-trained weights of non-compressed model \(v\).&lt;/p&gt; &lt;p&gt;Let \(A\) be the projection matrix that we used in the compression. Then, to convert compressed parameters of a model to the original ones, we need to multiply by \(A\) on the left. The idea is to start from the compressed parameters, such that after going to uncompressed space, they would be as close to \(v\) as possible by Eucledian distance. Then, we can use the formula for projection onto a linear subspace:&lt;/p&gt; \[u^{*} = \mathop{argmin}_u ||Au - v||^2 \Rightarrow u^{*} = (A^TA)^{-1}A^Tv\] &lt;p&gt;By initializing \(u\) this way, we achieve a faster convergence of the optimizer, because after projecting to subspace and returning to original coordinates, we get a parameter vector that is close to the optimal one, so it should be near the optimum in the coordinates of projection.&lt;/p&gt; &lt;p&gt;In our experiments, we compare how fast does the model train with random initializtion and with projection initialization.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;d&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;final val acc&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;17.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;16&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;28.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;64&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;51.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;256&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;71.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;512&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;83.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1024&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;90.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;original&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;95.65&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_random-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_random-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_random-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_random.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;distillation&quot;&gt;Distillation&lt;/h2&gt; &lt;p&gt;The concept of model distillation was introduced by &lt;d-cite key=&quot;hinton2015distilling&quot;&gt; &lt;/d-cite&gt;. The main idea is to train a compact model (“student”) to emulate a larger, pre-trained model (“teacher”). In our case, the compact model would be a reparametrized model with \(d\) dimensions, while the “teacher” model has all the original parameters. When training, we regress the mean square difference between the logits of the original and compact model.&lt;/p&gt; &lt;p&gt;We initialize the compressed model with the projection of the original model as in the previous section. In our experiments, we’ve noticed that this training procedure has comparable convergence speed, however, its validation accuracy reaches a plateau on a lower value than in regular training procedure.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/distil-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/distil-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/distil-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/distil.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non-distil-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non-distil-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non-distil-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non-distil.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; CIFAR-10 experiment. Network is logistic regression. Projections for d=700, on the left: with distilation, on the right: without distilation &lt;/div&gt; &lt;h2 id=&quot;independent-projections-for-layers&quot;&gt;Independent projections for layers&lt;/h2&gt; &lt;p&gt;In many cases, the model we are compressing contains several independent layers. Therefore, we can try to split the coordinates in the space to which we are projecting parameters so that each coordinate corresponds to exactly one layer. This constraint corresponds to the matrix of \(\phi\) being block-diagonal.&lt;/p&gt; &lt;p&gt;These changes improve the accuracy, and decrease the inference time (because for each layer we only need to use some part of the compressed coordinates), while keeping \(d\) constant.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_block-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_block-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_block-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/non_block.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/block-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/block-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/block-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/block.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; CIFAR-10 experiment. Network has one hidden layer with 32 units. On the left: one projection for d=2500. On the right: separate projections with d=500 and d=100 &lt;/div&gt; &lt;h2 id=&quot;gpu-memory-utilization&quot;&gt;GPU memory utilization&lt;/h2&gt; &lt;p&gt;Let we want to make inference with minimal possible usage of RAM. Let’s assume that the architecture of model that we are evaluating is an MLP. Then, using the compressed representation, we can use no more than \(O(\max(d, L))\), where \(d\) is the dimension to which we compressed the model, and \(L\) is the maximum size of the layer.&lt;/p&gt; &lt;p&gt;We describe the inference prodcedure consuming this little memory. We need to sequentially apply each of the feedforward networks in our MLP. For each layer, we have to transform the input vector \(x\) to the output \(y\). We fill in the output vector with zeros, and for each index \((i, j)\) in the weight matrix we need to make an update \(y_i \leftarrow y_i + A_{ij}x_j\). However, we don’t store any of the parameters in memory except for \(d\) compressed parameters. So, in order to get the value of \(A_{ij}\), we need to take the dot product of a row in the projection matrix and a vector of compressed parameters.&lt;/p&gt; &lt;p&gt;It is not obvious how to random access a row in a random matrix, where all columns should be normalized, and the outcomes during train and inference are consistent. We note that the true randomness of the projection matrix is not important for us. So, instead we can generate the \(i\)-th row by seeding the random to \(i\) and generating a row. During train, we generate the whole matrix this way, and compute the normalization coefficients of columns, which are included into the model’s representation in memory. During inference, to get the \(i\)-th row, we just need to sample a row and divide it by normalization coefficients pointwise. We have checked that this way of generating the projection matrix has no negative effects on the performance of the compressed model, compared to the truly random option.&lt;/p&gt; &lt;h2 id=&quot;diffusion-models&quot;&gt;Diffusion models&lt;/h2&gt; &lt;p&gt;We have also attempted to apply model compression to a different domains besides image classification. One of the problems we considered is generating 2D points from a certain distribution using a diffusion model. In this setup, we have a neural network that predicts the noise for a pair \((x, t)\) — point in space and time.&lt;/p&gt; &lt;p&gt;We use continuous time on \([0, 1]\), linear noise schedule with \(\beta_{min} = 0.3\), \(\beta_{max} = 30\), various-preserving SDE, batch size \(64\), sampling timesteps \(100\), ODE sampler. The distribution that we are trying to learn is a mixture of \(6\) gaussians. We use an MLP score net with \(2\)-dimensional input and \(32\)-dimensional Gaussian Fourier Projection time embeddings.&lt;/p&gt; &lt;p&gt;However, even setting the compression dimension \(1000\) or \(5000\) did not enable us to see good sampling results.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/gauss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/gauss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/gauss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/gauss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100_bad-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100_bad-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100_bad-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-projected-fff-networks/la_100_bad.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; On the left: sampling result with original model. On the right: sampling with compressed model, d=1000 &lt;/div&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;We have discussed a way to compress models, decreasing its size by several orders of magnitude. We identified ways to improve the validation accuracy of compressed models, such as doing the initializtion with projection and having independent projections for layers. This technique leads to surprising consequences, such as being able to do machine learning model inference with very small amount of RAM.&lt;/p&gt; </content> </entry> <entry> <title>Understanding Linear Mode Connectivity</title> <link href="https://deep-learning-mit.github.io/blog/2023/LinearMode/"/> <updated>2023-12-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/LinearMode</id> <content type="html">&lt;h2 id=&quot;instability-analysis-and-linear-mode-connectivity&quot;&gt;Instability Analysis and Linear Mode Connectivity&lt;/h2&gt; &lt;p&gt;The advent of transformer models stands as a pivotal advancement within the domain of machine learning, fundamentally reshaping the landscape of artificial intelligence. First introduced in 2017 through the seminal work “Attention is All You Need” by Vaswani et al., transformers have since exploded in both uses and applications, such as language and vision tasks. In fact, ChatGPT, which was the fastest-growing application in history (until Threads in 2023), is built using a transformer architecture. Although transformers can achieve state-of-the-art performance in many tasks, they are often limited by their size, which can create issues for memory and energy both during training and deployment. For example, GPT-3 has 175 billion parameters, and GPT-4, which was released earlier in 2023, has 1.76 trillion parameters! Compression techniques such as knowledge distillation and pruning can be used to deal with these issues, reducing the size of the network while retaining most of its capabilities. Several methods already exist for shrinking transformers such as weight pruning (Zhang et al. 2022), as well as post-training compression (Kwon et al. 2022). However, there is little research on the conditions under which a transformer can be effectively compressed or at what point during training a transformer compression should begin.&lt;/p&gt; &lt;p&gt;Frankle et al. (2020) suggest that &lt;em&gt;instability analysis&lt;/em&gt;—analyzing the stability of training with respect to stochastic gradient descent (SGD) noise—could be a way of identifying conditions under which pruning can be useful. To determine whether the outcome of training is stable w.r.t SGD noise, we create two copies of a network with the same initialization, and optimize those networks using different samples of SGD noise. We can then evaluate how similar or dissimilar the resulting networks are. For this purpose, Frankle et al. propose &lt;em&gt;linear interpolation instability&lt;/em&gt;, defined to be the maximum increase in error along the linear path in parameter space connecting the two resulting networks. When error is nonincreasing along this path, the networks are said to have &lt;em&gt;linear mode connectivity&lt;/em&gt;. In their paper, they propose that this instability analysis is related to lottery ticket networks, which are subnetworks from randomly-initialized dense neural networks that can achieve comparable test accuracy to the original network after training. They found that pruned networks that were capable of achieving near full test accuracy were stable to SGD noise, and showed linear mode connectivity.&lt;/p&gt; &lt;p&gt;Frankle et al. study linear mode connectivity in neural networks, which is a stricter version of mode connectivity. They train two networks with the same initialization on SGD noise (randomly augmented datasets) and calculate the maximum loss along the linear path between the two resulting network to quantitatively analyze the instability of the original network to noise.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/LMC.PNG-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/LMC.PNG-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/LMC.PNG-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-LinearMode/LMC.PNG&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; $W_i$ represents the weights at initialization, and the red lines represent two different paths through parameter space corresponding to different SGD noise. The figure on the left shows linear mode connectivity, but the figure on the right shows increasing loss along the blue interpolated path between local minima, which shows instability to SGD noise during training. &lt;/div&gt; &lt;p&gt;In our project, we plan to expand on the research from Frankle et al. and apply it to transformers. In doing so, we hope to study the conditions under which transformers can be effectively compressed as well as the optimization landscape of training transformers. We seek to evaluate linear mode connectivity in transformer architectures and whether it is an effective indicator for how effectively a transformer can be compressed.&lt;/p&gt; &lt;h2 id=&quot;transformers-and-related-work&quot;&gt;Transformers and Related Work&lt;/h2&gt; &lt;p&gt;We restricted our analysis of transformer architectures to the Vision Transformer (ViT) model proposed by Dosovitskiy (2021). ViT works by splitting an image into patches, then computing embeddings of those patches via linear transformation. After adding positional embeddings, the resulting embeddings are fed into a standard Transformer encoder. Due to runtime issues, we were unable to fully train transformers from scratch. We ended up working with and fine-tuning pretrained transformers, which were imported from the HuggingFace transformers package.&lt;/p&gt; &lt;p&gt;Shen et al. (2023) investigated a more general form of the lottery ticket hypothesis with ViTs, proposing ways to select a subset of the input image patches on which the ViT can be trained to similar accuracy as with the full data. However, they write “the conventional winning ticket [i.e. subnetwork] is hard to find at the weight level of ViTs by existing methods.”&lt;/p&gt; &lt;p&gt;Chen et al. (2020) investigated the lottery ticket hypothesis for pre-trained BERT networks, and did indeed find subnetworks at varying levels of sparsity capable of matching the full accuracy. Our work hoped to find similar results for vision transformers.&lt;/p&gt; &lt;p&gt;Linear mode connectivity is also deeply connected to the nature of the optimization landscape. This has important applications with regards to federated learning, and combining the results of independent models. For example, Adilova et al. (2023) showed that many deep networks have &lt;em&gt;layer-wise&lt;/em&gt; linearly connected minima in the optimization landscape, which they explain as being the result of the layer-wise optimization landscape being convex, even if the whole optimization landscape is not. They found similar behavior in vision networks trained on CIFAR-10.&lt;/p&gt; &lt;p&gt;In our project, we seek to evaluate the connection between linear mode connectivity and the existence of winning subnetworks. We expand on the work from Shen et al. and Chen et al. by incorporating the linear mode connectivity analysis proposed by Frankle et al. as well as search for conventional winning subnetworks in transformers for vision tasks. Our goal is to find conditions and methods for which transformers can be compressed while retaining high performance.&lt;/p&gt; &lt;h2 id=&quot;experiments-with-linear-mode-connectivity&quot;&gt;Experiments with Linear Mode Connectivity&lt;/h2&gt; &lt;p&gt;We decided to work with the pretrained ViT model from HuggingFace transformers, and to fine tune this model on CIFAR-10. We also augmented the data set of 32x32 images with a random 24x24 crop followed by resizing, followed by a random horizontal flip and color jitter (randomly changing brightness, contrast, saturation and hue). To evaluate linear mode connectivity, we train a pair of models with the same initialization on different randomly shuffled and augmented datasets.&lt;/p&gt; &lt;p&gt;In order to assess the instability of the original network to the dataset augmentations, we use the procedure described by Frankle et al. and evaluate the test loss and accuracy of the linearly interpolated models. The weights of the interpolated models are directly calculated from the weights of the trained models using evenly spaced values of \(\alpha\). The test dataset did not receive the augmentations that the training dataset did.&lt;/p&gt; &lt;p&gt;All models trained for the linear interpolation instability analysis were trained using the AdamW optimizer for 8 epochs with a learning rate of 2e-4. We use the default ViTImageProcessor imported from HuggingFace to convert the images into input tensors.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/interpolation_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/interpolation_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/interpolation_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-LinearMode/interpolation_loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The above plot shows the result of linear interpolation after fine tuning two copies of the pretrained model. The evaluation loss is non-increasing, and in fact decreases, possibly as an artifact of the fact that the test set did not recieve augmentations. Otherwise, it seems that there is linear mode connectivity, at least in the local optimization landscape when starting from a pretrained model.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/instability-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/instability-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/instability-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-LinearMode/instability.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;However, we failed to observe linear mode connectivity in randomly initialized transformers, noting an increase in test loss as well as a decrease in test accuracy around \(\alpha = 0.5\). The maximum observed test loss of the interpolated models is more than double the mean of the test losses of the original pair of trained models, which is much more than the threshold of a \(2\%\) increase used by the original authors.&lt;/p&gt; &lt;p&gt;The resulting networks seem to end up in disconnected local optima, implying that these networks are not invariant to the dataset augmentations. This is consistent with the analysis done by Frankle et al., who find that the stability of networks increases over the course of training.&lt;/p&gt; &lt;p&gt;Our results combined with the original analysis by Frankle et al. seems to suggest that linear mode connectivity emerges at some point during training, but we have yet to observe the point at which it emerges due to computation restraints and the size of the ImageNet dataset used to pretrain the ViT models.&lt;/p&gt; &lt;h2 id=&quot;pruning&quot;&gt;Pruning&lt;/h2&gt; &lt;p&gt;We used the PLATON compression algorithm (Zhang et al. 2022) during training to prune networks to different levels of sparsity. PLATON uses several “scores” to prune parameters. One score is parameter magnitude; smaller magnitude parameters tend to be pruned. However, in a complex network, small magnitude weights can still have a large impact; to measure this, PLATON uses the gradient-weight product \(\theta^T \nabla \mathcal{L}(\theta)\) as a first order Taylor approximation of the impact of the removal of a weight on the loss. PLATON also maintains uncertainties for all the weights, preferring not to prune weights with uncertain scores.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity20percent-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity20percent-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity20percent-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity20percent.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity5percent-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity5percent-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity5percent-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-LinearMode/sparsity5percent.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Pruning and retraining the pretrained model to 20% of its original size over 4 epochs results in a test accuracy of 95.3%, compared to 98% accuracy of the full model, and pruning to 5% resulted in 93.7% test accuracy. So although the compressed models cannot reach the accuracy of the original model, they are able to still maintain a relatively high test accuracy, and the PLATON algorithm does a good job of selecting weights. We also used the pruned weights at 20% sparsity to generate a mask, and applied this mask to the original model.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/pruned_weights_as_mask-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/pruned_weights_as_mask-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-LinearMode/pruned_weights_as_mask-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-LinearMode/pruned_weights_as_mask.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Blue and orange traces represent training loss and an exponential moving average resp. &lt;/div&gt; &lt;p&gt;When training the original model, but applying a mask (effectively setting the corresponding weights and gradients to zero), we were able to train the model to 93.6% test accuracy. This supports the lottery ticket hypothesis, since the PLATON algorithm can be used to identify a relatively small subset of weights from the pretrained network that can be trained high accuracy in isolation.&lt;/p&gt; &lt;h2 id=&quot;analysis-and-conclusions&quot;&gt;Analysis and Conclusions&lt;/h2&gt; &lt;p&gt;Our results with linear mode connectivity suggest that at some point during the training process, optimization ends up in a linearly connected local minimum, and further optimization will be stable to SGD noise. This is because we were indeed able to observe linear mode connectivity when fine tuning a pretrained mode. Additionally, with random initialization, we found the absence of linear mode connectivity. Unfortunately, we were not able to determine exactly where in the training process linear mode connectivity emerges.&lt;/p&gt; &lt;p&gt;It is notable that over the course of training, the loss does not seem to go down steadily, rather rapidly oscillating between high and low loss. The exponential moving average smooths it out, but it is still quite chaotic. During pruning, it seems plausible that the oscillations could correspond to weights being pruned, but the model approaches the target ratio of nonzero weights by the end of the third epoch of training, leaving the behavior in the final epoch unexplained. Furthermore, the training loss displays similar behavior while training the masked models. Further work could be done to investigate this phenomena and potentially make pruning/training more stable.&lt;/p&gt; &lt;p&gt;Our results with pruning show that a standard compression algorithm, PLATON, is able to sucessfully prune the pretrained ViT model to high levels of sparsity while maintaining relatively high accuracy. Our results with masking weights also suggest the existence of lottery ticket networks in the pretrained model, since we were able to train the corresponding subnetwork to a high level of accuracy. Unfortunately, the connection between linear mode connectivity and lottery ticket transforms remains very ambiguous, since we were unable to perform pruning experiments on models that did not demonstrate linear mode connectivity.&lt;/p&gt; &lt;p&gt;Further work could be done to investigate linear mode connectivity from different levels of pretraining as initialization, which would shed light on when the optimization of transformers settles into a connected minimum (or when it doesn’t). Further work on when linear mode connectivity arises, as well as experiments pruning the corresponding networks, would help determine if there is a connection between connectivity and the presence of lottery transformers. This would also be important for determining whether linear mode connectivity is a good indicator that transformers can be compressed more definitively. Additionally, as mentioned earlier, the existence of lottery networks in language models has already been investigated, and it would be interesting to see if this is related to linear mode connectivity as well.&lt;/p&gt; &lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt; &lt;p&gt;Adilova, L., Andriushchenko, M., Kamp, M., Fischer, A., &amp;amp; Jaggi, M. (2023). &lt;i&gt;Layer-wise Linear Mode Connectivity&lt;/i&gt;.&lt;/p&gt; &lt;p&gt;Frankle, J., Dziugaite, G. K., Roy, D. M., &amp;amp; Carbin, M. (2020). &lt;i&gt;Linear Mode Connectivity and the Lottery Ticket Hypothesis&lt;/i&gt;.&lt;/p&gt; &lt;p&gt;Zhang, Q., Zuo, S., Liang, C., Bukharin, A., He, P., Chen, W., &amp;amp; Zhao, T. (2022). PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, &amp;amp; S. Sabato (Eds.), &lt;i&gt;Proceedings of the 39th International Conference on Machine Learning&lt;/i&gt; (Vol. 162, pp. 26809–26823). PMLR. https://proceedings.mlr.press/v162/zhang22ao.html&lt;/p&gt; &lt;p&gt;Kwon, W., Kim, S., Mahoney, M. W., Hassoun, J., Keutzer, K., &amp;amp; Gholami, A. (2022). A fast post-training pruning framework for transformers. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, &lt;i&gt;35&lt;/i&gt;, 24101–24116.&lt;/p&gt; &lt;p&gt;Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp;amp; Houlsby, N. (2021). &lt;i&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/i&gt;.&lt;/p&gt; &lt;p&gt;Shen, X., Kong, Z., Qin, M., Dong, P., Yuan, G., Meng, X., Tang, H., Ma, X., &amp;amp; Wang, Y. (2023). &lt;i&gt;Data Level Lottery Ticket Hypothesis for Vision Transformers&lt;/i&gt;.&lt;/p&gt; &lt;p&gt;Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., &amp;amp; Carbin, M. (2020). &lt;i&gt;The Lottery Ticket Hypothesis for Pre-trained BERT Networks&lt;/i&gt;.&lt;/p&gt; </content> </entry> <entry> <title>Transformers vs. RNNs: How do findings from real-world datasets relate to the theory?</title> <link href="https://deep-learning-mit.github.io/blog/2023/TransformersAndRNNs/"/> <updated>2023-12-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/TransformersAndRNNs</id> <content type="html">&lt;h2 id=&quot;introduction--motivation&quot;&gt;Introduction &amp;amp; Motivation&lt;/h2&gt; &lt;p&gt;Since their invention, transformers have quickly surpassed RNNs in popularity due to their efficiency via parallel computing &lt;d-cite key=&quot;qin2023hierarchically&quot;&gt;&lt;/d-cite&gt;. They do this without sacrificing, and often improving, model accuracy. Liu et al has developed a theorhetical explanation for this by mathematically proving that transformers learn shortcuts to automata that RNNs are unable to take &lt;d-cite key=&quot;liu2023transformers&quot;&gt;&lt;/d-cite&gt;. However, the results of this proof were only tested on synthetic dataset, and the question of how Transformers perform better than RNNs on memory-based tasks without keeping track of recurrence is still relevant, particularly for developers &lt;d-cite key=&quot;liu2023transformers&quot;&gt;&lt;/d-cite&gt;. In this project, I analyzed and compared the performance of transformer and RNN based models on both a financial stock and medical ECG dataset. By practically testing the applications of RNNs and Transformers in two different settings, I aim to aid developers by suggesting considerations for them to have while choosing an architecture to work with.&lt;/p&gt; &lt;h2 id=&quot;background--prior-work&quot;&gt;Background &amp;amp; Prior Work&lt;/h2&gt; &lt;p&gt;Recurrent neural networks (RNN) are a type of neural network that were previously considered state-of-the-art for generating predictions on sequential data including speech, financial data, and video &lt;d-cite key=&quot;Kanagachidambaresan2021&quot;&gt;&lt;/d-cite&gt;. RNNs are distinct from other types of neural networks beecause they had an internal “memory” &lt;d-cite key=&quot;mhaskar2016learning&quot;&gt;&lt;/d-cite&gt;. This memory was based on the netowrk not only taking in the current input into its function but also information learned from all of the previous inputs &lt;d-cite key=&quot;Kanagachidambaresan2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;However, since Transformers were invented in 2017, they have rapidly made the use RNNs obsolete &lt;d-cite key=&quot;qin2023hierarchically&quot;&gt;&lt;/d-cite&gt;. Transformers were proposed as a simple network architecture based only on an attention mechanism, without the need for recurrence or convolutions &lt;d-cite key=&quot;NIPS2017_3f5ee243&quot;&gt;&lt;/d-cite&gt;. While they are quadratic in time complexity &lt;d-cite key=&quot;keles2022computational&quot;&gt;&lt;/d-cite&gt;, as opposed to RNNs’ linear complexity, their ability to parallelize makes them significantly faster to train &lt;d-cite key=&quot;NIPS2017_3f5ee243&quot;&gt;&lt;/d-cite&gt; than RNNs. Their popularity is broad, and Transformers are considered to be the gold standard in many fields including natural language processing, computer vision, and signal processing &lt;d-cite key=&quot;islam2023comprehensive&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;While Transformers were accepted to perform better, the question remained – why? Transformers do not keep track of recurrence but are somehow able to successfully complete memory-based tasks. Liu et al aimed to answer this question by exploring how transformers learn shortcuts to automata &lt;d-cite key=&quot;liu2023transformers&quot;&gt;&lt;/d-cite&gt;. They did this both by mathematical proof and also through experimentation on synthetic data sets. Their primary conclusion is that transformers are able to universally approximate these complex functions in few layers by building simple parallel circuits. This leads to improvement in computational efficiency and also performance improvements &lt;d-cite key=&quot;liu2023transformers&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Current research in the RNN space is largely focused on trying to leverage their inherently linear complexity to its advantage &lt;d-cite key=&quot;peng2023rwkv&quot;&gt;&lt;/d-cite&gt;, but I wondered if there were current applications where RNNs might already perform similarly or better. There are few papers in this space, and the ones that exist largely focus on a domain-specific performance mechanism &lt;d-cite key=&quot;10.3389/fnbot.2023.1157957&quot;&gt;&lt;/d-cite&gt;. With that, I wanted to characterize performance of RNNs versus Transformers across a range of metrics: including accuracy, training time, memory, and parameters. This project acts as an extension of Liu et al by looking at real-world datasets from different applications and seeing if the conclusions change in the real-world. This aims to fill a gap between practical applications and mathematical explanations by attempting to provide an intuitive understanding of real-world dataset performance between transformers and RNNs. This would provide a holsitic summary of the tradeoffs between RNNs and Transformers while aiming to provide insight as to why Transformers remain so dominant in the space.&lt;/p&gt; &lt;h2 id=&quot;methods--results&quot;&gt;Methods &amp;amp; Results&lt;/h2&gt; &lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt; &lt;p&gt;The first dataset I used was Yahoo Finance’s stock dataset, accessible through the yfinance API. I specifically looked at the closing price data from the S&amp;amp;P500 stock group which represents the stocks from the 500 largest companies. The second dataset I used was from Kaggle (available &lt;a href=&quot;https://www.kaggle.com/datasets/shayanfazeli/heartbeat/&quot;&gt;here&lt;/a&gt;). This dataset captures ECG data. I specifically used the abnormal and normal sub datasets that contained single-heart beat single-lead ECG data.&lt;/p&gt; &lt;h3 id=&quot;software&quot;&gt;Software&lt;/h3&gt; &lt;p&gt;I ran all of the code for this project using Python 3.10 in Google Colab. The APIs numpy, scipy, matplotlib, seaborn, keras, tensorflow, and yfinance were all used. The notebook used for the stock experiements is available &lt;a href=&quot;https://colab.research.google.com/drive/1NNZKU18bm7QsZaWOjX8EYD7NFoJK0jkQ#scrollTo=YQfvmYtVU3Yg&quot;&gt;here&lt;/a&gt; and the ECG experiments &lt;a href=&quot;https://colab.research.google.com/drive/1yBfvgPEaUau6ttJV9ufdkgVm6x9y-AI9#scrollTo=MsfkYwwCS6WC&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&quot;stock-model-comparisons&quot;&gt;Stock Model Comparisons&lt;/h3&gt; &lt;p&gt;I began my experiments by loading and visualizing the data. I wanted to compare the transformer and RNN models on a time-series prediction so I decided to use 11 months of data to predict the next 1 month behavior. To do this, I loaded data from July 1st, 2022 to July 31st 2022. Of note, the stock market is closed during weekends and holidays, so there were 251 days in my dataframe, and I trained on the first 231 days to predict the last 20. I then used an 80/20 train and test split.&lt;/p&gt; &lt;p&gt;I also visualized several iterations of ten random samples to better understand the dataset and ensure that I was preprocessing correctly.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_pre_norm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_pre_norm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_pre_norm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_pre_norm.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The closing price of 10 random stocks from the S&amp;amp;P 500 over a year timeframe. &lt;/div&gt; &lt;p&gt;Once I had the data set up, I began to build each model. In addition to a simple RNN architecture and a Transformer model, I also built an LSTM model which is a specialized subset of RNNs that aim to solve a vanishing gradient problem in traditional RNNs &lt;d-cite key=&quot;Sherstinsky_2020&quot;&gt;&lt;/d-cite&gt;. In addition to providing another reference model, I decided to include LSTM because I could easily test identical architectures between LSTM and RNNs &lt;d-cite key=&quot;lstmseries&quot;&gt;&lt;/d-cite&gt;. Additionally, LSTMs are notoriously slower to train and this provides a sanity check for my later timing results &lt;d-cite key=&quot;lstmseries&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In building the models, I tried to keep them all as simple and equivalent as possible for a fair comparison. This was simple for the LSTM and RNN, I just used two LSTM (or RNN) layers followed by a linear layer and then an output linear layer. Because of the different architecture of transformers, it didn’t seem possible to create a completely equivalent architecture. However, I tried to approximate this by having just a singular attention layer that didn’t have a feed foward network component and only had a standard layer normalization and then a multiheaded attention wiht 2 heads (the same number of layers for RNN/LSTM with the head size equivalent to the RNN/LSTM layer size). I followed this with a pooling layer, a linear layer (with the same size as the RNN/LSTM linear layer) and a linear output layer. I trained all models with a batch size of 25 and 30 epochs.&lt;/p&gt; &lt;p&gt;For each model, I measured RMSE for the predictions (used for accuracy), time used to train the model, memory used to train the model, number of parameters, and storage used for parameters. The results are shown in the following table.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;RMSE&lt;/th&gt; &lt;th&gt;Memory in Training (KB)&lt;/th&gt; &lt;th&gt;Time to Train (s)&lt;/th&gt; &lt;th&gt;Parameters (#)&lt;/th&gt; &lt;th&gt;Memory for Parameters (KB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;LSTM&lt;/td&gt; &lt;td&gt;155.61&lt;/td&gt; &lt;td&gt;16575097&lt;/td&gt; &lt;td&gt;151.76&lt;/td&gt; &lt;td&gt;54190&lt;/td&gt; &lt;td&gt;211.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RNN&lt;/td&gt; &lt;td&gt;149. 07&lt;/td&gt; &lt;td&gt;4856823&lt;/td&gt; &lt;td&gt;67.25&lt;/td&gt; &lt;td&gt;16750&lt;/td&gt; &lt;td&gt;65.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Transformers&lt;/td&gt; &lt;td&gt;36.46&lt;/td&gt; &lt;td&gt;3165225&lt;/td&gt; &lt;td&gt;87.00&lt;/td&gt; &lt;td&gt;2019&lt;/td&gt; &lt;td&gt;7.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;As expected, the LSTM model runs much slower with higher memory usage which is consistent with literature models &lt;d-cite key=&quot;lstmseries&quot;&gt;&lt;/d-cite&gt;. When comparing the models, on first look, it seems like the Transformer model greatly outperforms the other models, both in terms of accuracy and memory with comprable training times to the RNN. However, I noticed something odd when I visualized my results: while transformers performed better in RMSE, it didn’t look like they performed better.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_performance_yesLSTM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_performance_yesLSTM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_performance_yesLSTM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_performance_yesLSTM.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Performance of initial model experiments. Before the black line, is the training data of the closing price, after the line, the testing data as well as the predictions for all three models are shown. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_forcast_yesLSTM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_forcast_yesLSTM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_forcast_yesLSTM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/no_norm_forcast_yesLSTM.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The testing data compared to the predictions for all three models. &lt;/div&gt; &lt;p&gt;Besides noting that the models all did fairly well given their simplicity, this was puzzling. Addditionally, when I reran the models, I noted that the RMSE values for the LSTM/RNN models varied wildly with results between 50-550 whereas transformer’s performance was consistently around 35. To investigate, I printed out the RMSE for each prediction and analyzed them. I found that most errors were fairly small but there were a couple very large errors that ended up skewing the overall reported average. In visualizing that outlier and performance between the models, I saw that the prices for the outliers were much higher than most stocks, making the LSTM/RNN models predict a much lower price.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Outlier_performance_yesLSTM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Outlier_performance_yesLSTM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Outlier_performance_yesLSTM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Outlier_performance_yesLSTM.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Outlier visualization of one stock with all three model&apos;s performance. &lt;/div&gt; &lt;p&gt;Transformers still do okay here, likely do to the first normalization layer I used. Thus, to make the problem more equal, I decided to normalize all of the data at the onset.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_norm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_norm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_norm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/10SP500_norm.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Sample of data once normalized. &lt;/div&gt; &lt;p&gt;When rerunning the training, the tabular results match the visualizations. Surprisingly, Transformers perform worse than RNNs/LSTMs, with less memory used but no real difference in training time. Even with adding complexity to the Transformer model via increasing the feed-forward network complexity through increasing the size of the embedded feed forward network and increasing the number of attention layers, no performance difference was seen – the time to train just substantially increased.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;RMSE&lt;/th&gt; &lt;th&gt;Memory in Training (KB)&lt;/th&gt; &lt;th&gt;Time to Train (s)&lt;/th&gt; &lt;th&gt;Parameters (#)&lt;/th&gt; &lt;th&gt;Memory for Parameters (KB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;LSTM&lt;/td&gt; &lt;td&gt;0.125&lt;/td&gt; &lt;td&gt;8233179&lt;/td&gt; &lt;td&gt;128.25&lt;/td&gt; &lt;td&gt;54190&lt;/td&gt; &lt;td&gt;211.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RNN&lt;/td&gt; &lt;td&gt;0.121&lt;/td&gt; &lt;td&gt;4147757&lt;/td&gt; &lt;td&gt;87.58&lt;/td&gt; &lt;td&gt;16750&lt;/td&gt; &lt;td&gt;65.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Transformers&lt;/td&gt; &lt;td&gt;0.281&lt;/td&gt; &lt;td&gt;3148379&lt;/td&gt; &lt;td&gt;87.38&lt;/td&gt; &lt;td&gt;2019&lt;/td&gt; &lt;td&gt;7.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complicated Transformers&lt;/td&gt; &lt;td&gt;0.282&lt;/td&gt; &lt;td&gt;40052260&lt;/td&gt; &lt;td&gt;1243.01&lt;/td&gt; &lt;td&gt;16248&lt;/td&gt; &lt;td&gt;63.47&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Testing_yesLSTM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Testing_yesLSTM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Testing_yesLSTM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Testing_yesLSTM.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Performance of model experiments with normalized data. Before the black line, is the training data of the closing price, after the line, the testing data as well as the predictions for all three models are shown. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Forcast_yesLSTM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Forcast_yesLSTM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Forcast_yesLSTM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Forcast_yesLSTM.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; The testing data compared to the predictions for all three models with normalized data. &lt;/div&gt; &lt;p&gt;This seems to go against prior results which almost universally found Transformers faster without sacrificing efficiency &lt;d-cite key=&quot;qin2023hierarchically&quot;&gt;&lt;/d-cite&gt;. I hypothesized that this could be because Transformers generally have poor length generalization and perhaps this is an inoptimal time window for this particular model &lt;d-cite key=&quot;anil2022exploring&quot;&gt;&lt;/d-cite&gt;. This could also explain the lack of a time improvement as a transformer’s time complexity is quadtratic whereas RNN’s is linear «d-cite key=”peng2023rwkv”&amp;gt;&amp;lt;/d-cite&amp;gt;, &lt;d-cite key=&quot;keles2022computational&quot;&gt;&lt;/d-cite&gt;. Presumably, the quadratic performance will slow down the Transformer when parallelization becomes impossible &lt;d-cite key=&quot;peng2023rwkv&quot;&gt;&lt;/d-cite&gt;. To test this hypothesis, I decided to try to predict my results with a smaller time window.&lt;/p&gt; &lt;p&gt;I did this by predicting the closing price of just one day of data using a week of prior data. I normalized all data and retrained my models. I reverted back to the simple transformer model in an effort to test relatively equivalent model complexities.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;RMSE&lt;/th&gt; &lt;th&gt;Memory in Training (KB)&lt;/th&gt; &lt;th&gt;Time to Train (s)&lt;/th&gt; &lt;th&gt;Parameters (#)&lt;/th&gt; &lt;th&gt;Memory for Parameters (KB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;LSTM&lt;/td&gt; &lt;td&gt;0.386&lt;/td&gt; &lt;td&gt;9588885&lt;/td&gt; &lt;td&gt;19.00&lt;/td&gt; &lt;td&gt;53221&lt;/td&gt; &lt;td&gt;207.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RNN&lt;/td&gt; &lt;td&gt;0.381&lt;/td&gt; &lt;td&gt;4197690&lt;/td&gt; &lt;td&gt;13.45&lt;/td&gt; &lt;td&gt;15781&lt;/td&gt; &lt;td&gt;61.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Transformers&lt;/td&gt; &lt;td&gt;0.384&lt;/td&gt; &lt;td&gt;2707340&lt;/td&gt; &lt;td&gt;11.45&lt;/td&gt; &lt;td&gt;1050&lt;/td&gt; &lt;td&gt;4.1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;As the results show, my hypothesis was correct. The transformer performed much faster without a reduction in accuracy. However, it is also very possible that I didn’t see a time difference because I am using small models with a short training time. These timing differences could become larger with more computationally intensive models.&lt;/p&gt; &lt;h2 id=&quot;ecg-model-comparisons&quot;&gt;ECG Model Comparisons&lt;/h2&gt; &lt;p&gt;While the results from the stock dataset were interesting, I also wanted to test these models with a different type of input that perhaps would capture different underlying strengths and weaknesses of the models. I decided to use an ECG to predict the presence of an abnormality in the heart beat. This represents a difference in the stock dataset in three key ways:&lt;/p&gt; &lt;p&gt;1) The output is binary instead of discrete. 2) There is a better source of ground truth for this data. If there was a definitive way to predict the behavior of a stock, everyone would be rich, but that’s not the case – there’s inherently uncertainty and an expected level of innaccuracy. For health data, the person will have the condition or not and an experienced cardiologist would be able to definitively diagnose the patient. 3) The input has an expected, structured shape. All ECGs are supposed to look roughly the same and should have a similar visibility in the dataset. This has effects on the causality window used in models that I was interested in analyzing.&lt;/p&gt; &lt;p&gt;I first visualized my data for both the abnormal and normal heart beats. The overall sample size was around 9000 patients, and I artificially created a 50/50 split between abnormal and normal to prevent class imbalance. I once again used an 80/20 train/test split for my models.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ECG-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ECG-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ECG-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ECG.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Data visualization for ECG. &lt;/div&gt; &lt;p&gt;I immediately ran into difficulties once I began training with the performance of all models really being a coin toss between the two. I then focused my time on trying to build more complex models. For the RNN, I added more layers with varying dimensions and played around with adding dropout and linear layers. For the Transformer, I built up the feedforward network part of the algorithm by increasing the size of the embedded feed forward network and adding multiple attention layers. For both, I tuned hyperparameters such as the optimizer, batch size, and number of epochs. Despite this results still remined poor.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/Loss.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Loss curve for both models on training and validation data. &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ROC-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ROC-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ROC-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-TransformersAndRNNs/ROC.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; ROC curves for both models with displayed AUC values. &lt;/div&gt; &lt;p&gt;There is virutally no reduction on validation loss for the RNN graph, no matter what structure I chose. While there is a normal looking curve for transformer, the scale of loss reduction is very small when you consider the y-axis. Additionally, the RNN network never performed better than randomly, whereas the Transformer network was only slightly improved.&lt;/p&gt; &lt;p&gt;One interpretation of these results could be that the Transformer model performed better. However, because neither of these architectures perfomred overly sucessfully, I don’t think that is a sound conclusion. It is unclear to me if this is a shortcoming of my code or a difficulty with the problem and dataset. This would be an area where future work is required.&lt;/p&gt; &lt;p&gt;My main takeaway from this process of working with the ECG data was how much easier it was to tune and problemsolve with the Transformer than the RNN. For the Transformer, I was able to adjust the number of heads or the sizes of heads, or the feed foward network, etc, whereas, in the RNN, I really could only play with the layers of the RNN itself. While both of these architectures have black-box components, I found the Transformer a lot easier to work and play around with as a developer, and I could develop some intuition on what things I should change and why. This perhaps represents another difference from the transformer vs RNN debate but from a usability standpoint.&lt;/p&gt; &lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt; &lt;p&gt;There are several limitations of this project. First, I only analyzed a couple of different datasets. This is not representative of all of the different applications of GNNs and transformers, meaning the conclusions are limited to the datasets chosen and are not necessarily representative of the full field. Additionally, my implementation of the models may not be the most efficient ones. While I tried to test a wide range of parameters, due to limited resources available (ie time and being one person) there are undoubtably more optimal structures or hyperparameters that I did not test. This ability to not only test a limited number of parameters, but also architectures remains an overall limitation and challenge of the deep learning field &lt;d-cite key=&quot;limitation&quot;&gt;&lt;/d-cite&gt;. Noteably, I also worked with small datasets and models. While this was useful in running a lot of experiments quickly, the differences in architectures, especialy in regards to training time/memory, may be more important and more pronounced in larger-scale tasks.&lt;/p&gt; &lt;p&gt;Additionally, I did not test every metric of success. While I focused on number of trainable parameters, training time, memory, and accuracy – these are not the only things that matter in machine learning. For instance, in some applications, senstivity might matter a lot more than specificity and overall accuracy. In others, explainability of the model may be essential, such as time sensitive healthcare settings &lt;d-cite key=&quot;limitationexplain&quot;&gt;&lt;/d-cite&gt;. Thus, in making a decision in chosing one model over another, it is important to consider the specific application and benefits of each approach. These methods of success also might not be “fair” to each model. For instance, a much larger number of trainable parameters may be fine if overall time is not lost. If one has the capacity to run their machine over night but with limited memory, they might reevaluate these metrics of sucess. Given these limitations, it is important to do a holistic evaluation of architectures when deciding which to use for a deep learning project and to use this project as a guidelines of preliminary experiments to run in making that decision. This project does not serve as a ground truth as to why to choose one model over another.&lt;/p&gt; &lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt; &lt;p&gt;Transformers seem to be easier to work with when there are still questions surrounding the data. For instance, with the stock dataset, there may be circumstances where you would prefer a model that can perform well prior without normalizing the dataset if for instance, you care about the magnitude of closing prices between stocks. Similarly, for the ECG model, they were easier to tune with different hyper paramters and felt more intuitive in comparison to working with the RNN. Transformers also consistently used less memory with much fewer parameters across the board, which is important when working in resource-limited systems.&lt;/p&gt; &lt;p&gt;However, this project found that transformers are not always faster or more accurate than alternatives. While Liu et al found that typical transformers can find shortcuts to learn automata &lt;d-cite key=&quot;liu2023transformers&quot;&gt;&lt;/d-cite&gt;, this might not be the case for all datasets across all applications. Previous studies have found that length generalization is a deficiency of transformers &lt;d-cite key=&quot;anil2022exploring&quot;&gt;&lt;/d-cite&gt;, and this is supported by this project which found that for longer-term predictions, RNNs were the faster and more accurate approach post-normalization.&lt;/p&gt; &lt;p&gt;These findings underscore the importance of taking the time to test different architectures in your resarch and not assuming that just because Transformers are more popular, it doesn’t mean they are necessarily the best fit for your problem. In deep learning research, we often get bogged down in tuning a model and it’s important to take a step back and consider your assumptions about the task – which may include the broader model consideration.&lt;/p&gt; </content> </entry> <entry> <title>Exploring the latent space of text-to-image diffusion models</title> <link href="https://deep-learning-mit.github.io/blog/2023/latent-interpolation/"/> <updated>2023-12-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/latent-interpolation</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Diffusion models &lt;d-cite key=&quot;ho2020denoising&quot;&gt;&lt;/d-cite&gt; are a class of deep generative models that have shown promising results in many different tasks, including photorealistic image generation &lt;d-cite key=&quot;saharia2022photorealistic&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;ramesh2022hierarchical&quot;&gt;&lt;/d-cite&gt; and protein design &lt;d-cite key=&quot;watson2023novo&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;lee2023score&quot;&gt;&lt;/d-cite&gt;. Diffusion models work by gradually destroying structure of an object with $T$ steps of a fixed noising process, and then learning to reverse this process to recover the original object. This allows the model to learn the underlying structure of the data, and to generate new objects that are both realistic and diverse. The forward process $q( x_t | x_{t-1} )$ defines how noise is added to an original image $x_0$, and the reverse process $q( x_{t-1} | x_{t} )$ that we want to learn, can recover a less noisy version of an image.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/chicken_forward_reverse-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/chicken_forward_reverse-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/chicken_forward_reverse-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/chicken_forward_reverse.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Stable Diffusion (SD) &lt;d-cite key=&quot;rombach2022high&quot;&gt;&lt;/d-cite&gt; is an open-source latent text-to-image diffusion model which is able to realize images with fine grained details, when prompted with a textual input describing the desired characteristics of the output image. SD is reasonably fast compared to other diffusion models, since it performs the diffusion steps in a low dimensional latent space. The strategy consists of using an image encoder $\mathcal{E}: \mathcal{X} \rightarrow \mathcal{Z}^0$ which maps an image $x_0 \in \mathcal{X}$ to a lower dimensional image latent code $z_0 \in \mathcal{Z}^0$, and a latent decoder $\mathcal{D}: \mathcal{Z}^0 \rightarrow \mathcal{X}$ which recovers an image $\mathcal{D}(z_0)$ from the image latent code $z_0$. Using these two models it is possible to learn to denoise $z_T$, instead of $x_T$, which is also normally distributed, saving a lot in computing since the latent codes dimensionality are usually chosen to be much smaller than the original images dimensionality. During inference time, for a given input textual prompt $y$, we encode the prompt into a vector $s = \tau_\phi(y)$ using CLIP &lt;d-cite key=&quot;radford2021learning&quot;&gt;&lt;/d-cite&gt;, sample $z_T \sim \mathcal{N}(0, I)$, and provide these two tensors to the diffusion model $f_\theta: \mathcal{Z}^T \times \mathcal{S} \rightarrow \mathcal{Z}^0$, which generates $z_0 = f_\theta(z_T, s)$. We can then map this vector into an image using the decoder: $x_0 = \mathcal{D}(z_0)$ which hopefully is in the data distribution.&lt;/p&gt; &lt;h2 id=&quot;background-and-related-work&quot;&gt;Background and related work&lt;/h2&gt; &lt;p&gt;In order to be able to learn the complex interaction between textual descriptions and images coming from a very large multimodal dataset, SD has to organize its image latent space $\mathcal{Z}^T$ coherently. If the learned representations are smooth for instance, we could expect that $\mathcal{D}(f_\theta(z_T, s))$ and $\mathcal{D}(f_\theta(z_T + \epsilon, s))$, where $\epsilon$ is a tensor of same dimensionality as $z_T$ with values very close to 0, will be very similar images. A common technique to explore and interpret the latent space of generative models for images is to perform latent interpolation between two initial latent codes, and generate the $N$ images corresponding to each of the interpolated tensors. If we sample $z_\text{start}, z_\text{end} \sim \mathcal{N}(0, I)$, fix a textual prompt such that $s = \tau_\phi({y})$ and use SD to generate images conditioned on the textual information we could explore different techniques for generating interpolated vectors. A very common approach is linear interpolation, where for $\gamma \in [0, 1]$ we can compute:&lt;/p&gt; \[z_\text{linear}^{(\gamma)} = (1-\gamma)z_\text{start} + \gamma z_\text{end}\] &lt;p&gt;Mimicking these exact steps for three different pairs sampled latent codes for $(z_\text{start}, z_\text{end})$, and for each of them fixing a text prompt we get:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_interpolation.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;As we can see from the image, when we move away from both $z_\text{start}$ and $z_\text{end}$ we get blurred images after decoding the interpolated image latent codes, which have only high level features of what the image should depict, but no fine grained details, for $\gamma = 0.5$ for instance, we get:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/extreme_case-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/extreme_case-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/extreme_case-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/extreme_case.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In contrast, if we perform interpolation in the text space by sampling $z_T \sim \mathcal{N}(0, I)$, which is kept fixed afterwards, and interpolating between two text latent codes $s_\text{start} = \tau_\phi(y_\text{start})$ and $s_\text{end} = \tau_\phi(y_\text{end})$, we get something more coherent:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/text_interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/text_interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/text_interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/text_interpolation.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Latent interpolation is a very common technique in Machine Learning, particularly in generative models, &lt;d-cite key=&quot;gomez2018automatic&quot;&gt;&lt;/d-cite&gt; used interpolation in the latent space of a Variational Autoencoder (VAE) &lt;d-cite key=&quot;kingma2013auto&quot;&gt;&lt;/d-cite&gt; to generated molecules between two initial ones by encoding them in the VAE latent space, interpolating between them and using the decoder to obtain the molecules from the latents, &lt;d-cite key=&quot;upchurch2017deep&quot;&gt;&lt;/d-cite&gt; showed how interpolation can be used to perform semantic transformations on images, by changing features of a CNN. More broadly interpolation has also been studied in a probabilistic point of view &lt;d-cite key=&quot;lesniak2018distribution&quot;&gt;&lt;/d-cite&gt;, evaluating how different techniques might generate out of distribution samples, which we explore later in this blog post.&lt;/p&gt; &lt;p&gt;In this project we explore geometric properties of the image latent space of Stable Diffusion, gaining insights of how the model organizes information and providing strategies to navigate this very complex latent space. One of our focuses here is to investigate how to better interpolate the latents such that the sequence of decoded images is coherence and smooth. Depending on the context, the insights here could transferred to other domains as well if the sampling process is similar to the one used in SD. The experiments are performed using python and heavily relying on the PyTorch &lt;d-cite key=&quot;paszke2019pytorch&quot;&gt;&lt;/d-cite&gt;, Transformers &lt;d-cite key=&quot;wolf-etal-2020-transformers&quot;&gt;&lt;/d-cite&gt; and Diffusers &lt;d-cite key=&quot;von-platen-etal-2022-diffusers&quot;&gt;&lt;/d-cite&gt; libraries.&lt;/p&gt; &lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt; &lt;p&gt;In this section we compare several interpolation techniques. For reproducibility reasons we ran the experiments with the same prompt and sample latent vectors across different. We use Stable Diffusion version 1.4 from CompVis with the large CLIP vision transformer, the DPMSolverMultistepScheduler &lt;d-cite key=&quot;lu2022dpm&quot;&gt;&lt;/d-cite&gt;, 30 inference steps and a guidance scale of 7.5 &lt;d-cite key=&quot;dhariwal2021diffusion&quot;&gt;&lt;/d-cite&gt;. We use the prompt “An high resolution photo of a cat” and seed = 1 to generate both $z_\text{start}$ and $z_\text{end}$. The corresponding generated pictures are shown below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/endpoint_images-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/endpoint_images-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/endpoint_images-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/endpoint_images.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;linear-interpolation&quot;&gt;Linear Interpolation&lt;/h3&gt; &lt;p&gt;Although linear interpolation is still a very commonly used interpolation technique, it is known that is generates points which are not from the same distribution than the original data points &lt;d-cite key=&quot;agustsson2018optimal&quot;&gt;&lt;/d-cite&gt; depending on the original distribution of the points being interpolated. Particularly, for $z_{\text{start}}, z_{\text{end}} \sim \mathcal{N}(0, I)$ and $\gamma \in [0,1]$, we have:&lt;/p&gt; \[z_\text{linear}^{(\gamma)} = (1-\gamma)z_\text{start} + \gamma z_\text{end}\] &lt;p&gt;Hence:&lt;/p&gt; \[\begin{eqnarray} \mathbb{E}\left[z_\text{linear}^{(\gamma)}\right] &amp;amp;=&amp;amp; \mathbb{E}\left[(1-\gamma)z_\text{start} + \gamma z_\text{end}\right] \nonumber \\ &amp;amp;=&amp;amp; \mathbb{E}[(1-\gamma)z_\text{start}] + \mathbb{E}[\gamma z_\text{end}] \nonumber \\ &amp;amp;=&amp;amp; (1-\gamma)\mathbb{E}[z_\text{start}] + \gamma \mathbb{E}[z_\text{end}] \nonumber \\ &amp;amp;=&amp;amp; 0 \nonumber \end{eqnarray}\] &lt;p&gt;Therefore, the mean stays unchanged, but the variance is smaller than 1 for $\gamma \in (0,1)$:&lt;/p&gt; \[\begin{eqnarray} \text{Var}[z_\text{linear}^{(\gamma)}] &amp;amp;=&amp;amp; \text{Var}[(1-\gamma)z_\text{start} + \gamma z_\text{end}] \nonumber \\ &amp;amp;=&amp;amp; \text{Var}[\gamma z_\text{start}] + \text{Var}[(1-\gamma)z_\text{end}] \nonumber \\ &amp;amp;=&amp;amp; \gamma^2\text{Var}[z_\text{start}] + (1-\gamma)^2\text{Var}[z_\text{end}] \nonumber \\ &amp;amp;=&amp;amp; \gamma(2\gamma - 2)I + I \nonumber \\ &amp;amp;=&amp;amp; (\gamma(2\gamma - 2) + 1)I \nonumber \end{eqnarray}\] &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/linear_interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/linear_interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/linear_interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/linear_interpolation.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Given that the sum of two independent Gaussian distributed random variables results in a Gaussian distributed random variable, $z_\text{linear}^{(\gamma)} \sim \mathcal{N}(0, (\gamma(2\gamma - 2) + 1)I)$. This shows how the distribution of the interpolated latent codes change. To further understand the effect of this shift, we can use the interactive figure below. Where for $\text{std} \in [0.5, 1.5]$ we generate an image using the embedding $\text{std} \, z_\text{start}$:&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-01-latent-interpolation/variance.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;h3 id=&quot;normalized-linear-interpolation&quot;&gt;Normalized linear interpolation&lt;/h3&gt; &lt;p&gt;As shown before, linear interpolation is not a good technique for interpolation random variables which are normally distributed, given the change in the distribution of the interpolated latent vectors. To correct this distribution shift, we can perform a simply normalization of the random variable. We will refer this this as normalized linear interpolation. For $\gamma \in [0,1]$ we define $z_\text{normalized}^{(\gamma)}$ as:&lt;/p&gt; \[z_\text{normalized}^{(\gamma)} = \dfrac{z_\text{linear}^{(\gamma)}}{\sqrt{(\gamma(2\gamma - 2) + 1)}} \implies z_\text{normalized}^{(\gamma)} \sim \mathcal{N}(0, I)\] &lt;p&gt;Now, as we move further way from the endpoints $z_\text{start}$ and $z_\text{end}$, we still get coherent output images:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/normalized_interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/normalized_interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/normalized_interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/normalized_interpolation.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;slerp&quot;&gt;SLERP&lt;/h3&gt; &lt;p&gt;Spherical Linear Interpolation (Slerp) &lt;d-cite key=&quot;shoemake1985animating&quot;&gt;&lt;/d-cite&gt;, is a technique used in computer graphics and animation to smoothly transition between two orientations, especially rotations. If we let $\phi = \text{angle}(z_\text{start}, z_\text{start})$, then for $\gamma \in [0,1]$, the interpolated latent is defined by:&lt;/p&gt; \[\text{slerp}(z_\text{start}, z_\text{end}; t) = \dfrac{\sin((1-\gamma)\phi)}{\sin(\phi)}z_\text{start} + \dfrac{\sin(\gamma\phi)}{\sin(\phi)}z_\text{end}\] &lt;p&gt;where $\phi$ is the angle between $z_\text{start}$ and $z_\text{end}$. The intuition is that Slerp interpolates two vectors along the shortest arc. We use an implementation of Slerp based on Andrej Karpathy &lt;d-cite key=&quot;Karpathy2022&quot;&gt;&lt;/d-cite&gt;. As we can see from the images below, slerp generates very good quality interpolated vectors.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/slerp_interpolation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/slerp_interpolation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/slerp_interpolation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/slerp_interpolation.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;If we compare the obtained results with normalized linear interpolation we see that the generated images are very similar, but as opposed to normalized linear interpolation, we cannot easily theoretically analyze the distribution of generated latents. To have some intuition behind how these different techniques interpolate between two vectors and can sample and fix two vectors sampled from a 2-dimensional normal distribution. We can visualize how these trajectories compare with each other:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/interpolations_comparison-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/interpolations_comparison-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/interpolations_comparison-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/interpolations_comparison.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;translation&quot;&gt;Translation&lt;/h3&gt; &lt;p&gt;To further investigate some properties of the latent space we also perform the following experiment. Let $z_\text{concat} \in \mathbb{R}^{4 \times 64 \times 128}$ be the concatenation of $z_\text{start}$ and $z_\text{end}$ over the third dimension. We will denote by $z_\text{concat}[i, j, k] \in \mathbb{R}$ as a specific element of the latent code and $:$ as the operator that selects all the elements of that dimension and $m:n$ the operator that selects from elements $m$ to element $n$ of a specific dimension. We can create a sliding window over the concatenated latent and generated the corresponding images. We define the translation operator $\mathcal{T}$ such that $\mathcal{T}(z_\text{concat}; t) = z_\text{concat}[:, :, t:64+t]$, which is defined for $t = {0, \cdots, 64}$. The sequence of generated images can be visualized below using our interactive tool:&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-01-latent-interpolation/translation.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;p&gt;Surprisingly, we note that applying $\mathcal{T}$ to our concatenated latent code is materialized into a translation in image space as well. But not only the object translates, we also see changes in the images style, which is justified by changing some of the latent dimensions.&lt;/p&gt; &lt;p&gt;We can correct this behavior by mixing the two latent codes only in a single slice of the latent code. Let $\mathcal{C}(z_\text{start}, z_\text{end}; t)$ represent the concatenation of $z_\text{start}[:, :, 64:64+t]$ and $z_\text{end}[:, :, t:64]$ along the third dimension. With this transformation we obtain the following:&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-01-latent-interpolation/corrected_translation.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;p&gt;Hence, translation is also a valid interpolation technique and could be further expanded to generate an arbitrary size of latent vectors.&lt;/p&gt; &lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt; &lt;p&gt;In order to evaluate the quality of the generated interpolations we use CLIP, a powerful technique for jointly learning representations of images and text. It relies on contrastive learning, by training a model to distinguish between similar and dissimilar pairs of images in a embedding space using a text and an image encoder. If a (text, image) pair is such that the textual description matches the image, the similarity between the CLIP embeddings of this pair should be high:&lt;/p&gt; \[\text{CLIPScore(text,image)} = \max \left(100 \times \dfrac{z_{\text{text}} \cdot z_{\text{image}}}{ \lVert z_{\text{text}} \rVert \lVert z_{\text{image}} \rVert}, 0 \right)\] &lt;p&gt;For each interpolation strategy $f \in \{\text{linear}, \text{normalized}, \text{slerp}\}$ presented, we fix the prompt $\text{text} = $ “A high resolution image of a cat” and generate $n = 300$ interpolated latents $f(z_\text{start}, z_\text{end}, \gamma) = z_f^{(\gamma)}$ with $\gamma = \{0, \frac{1}{n-1}, \frac{1}{n-2}, \cdots, 1\}$. We then generate the images $x_f^{(\gamma)}$ from the interpolated latents, finally we use the CLIP encoder $\mathcal{E}_\text{CLIP}$ on the generated images to create image embeddings that can be compared with the text embedding the we define Interpolation Score $\text{InterpScore}(f, \text{text}, n)$ as:&lt;/p&gt; \[\text{InterpScore}(f, \text{text}, n) = \dfrac{1}{n} \sum_{\gamma \in \{0, \frac{1}{n-1}, \frac{1}{n-2}, \cdots, 1\}} \max \left(100 \times \dfrac{z_{\text{text}} \cdot \mathcal{E}_\text{CLIP}(x_\text{f}^{(\gamma)})}{ \lVert z_{\text{text}} \rVert \lVert \mathcal{E}_\text{CLIP}(x_\text{f}^{(\gamma)}) \rVert}, 0 \right)\] &lt;p&gt;Applying these steps we obtained the following results:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/clip_scores-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/clip_scores-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/clip_scores-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/clip_scores.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Surprisingly, linear interpolation performed better than normalized linear and slerp, this could indicate that CLIP scores might not be a good metric for image and text similarity in this context. Given that in this class project the main goal was to gain insights, as future work we could run a large scale experiment to check whether this behavior would be repeated. We can also visually inspect the quality of the interpolation by generating a video for each interpolation. From left to right we have images generated from latents from linear, normalized and slerp interpolations respectively:&lt;/p&gt; &lt;iframe width=&quot;720&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/6dEGSbam11o&quot;&gt; &lt;/iframe&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This work shows the importance of choosing an interpolation technique when generating latent vectors for generative models. It also provides insights of the organization of the latent space of Stable Diffusion, we showed how translations of the latent code corresponds to translations on image space as well (but also changes in the image content). Further investigation of the organization of the latent space could be done, where we could try for instance, to understand how different dimensions of the latent code influence the output image. As an example, if we fix a image latent and use four different prompts, which are specified in the image below, we get:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_dim-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_dim-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_dim-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-latent-interpolation/latent_dim.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;As we can see all the generated images have some common characteristics, all the backgrounds, body positions and outfits (both in color and style) of the generated images are very similar. This indicates that even without explicitly specifying those characteristics on the textual prompt, they are present in some dimensions of the image latent code. Hence, the images share those similarities. Understanding how we can modify the latent code such that we change the shirt color in all the images from blue to red would be something interesting. Additionally, we showed some indication that CLIP scores might not be a good proxy for evaluating quality images generated from an interpolation technique.&lt;/p&gt; </content> </entry> <entry> <title>Accelerating large model inference with speculative decoding - 6.s898</title> <link href="https://deep-learning-mit.github.io/blog/2023/speculative-decoding/"/> <updated>2023-11-16T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/speculative-decoding</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;h3 id=&quot;inference-in-autoregressive-models&quot;&gt;Inference in autoregressive models&lt;/h3&gt; &lt;p&gt;Autoregressive models, particularly transformers and RNNs, play a crucial role in tasks involving sequential data processing, such as natural language processing and time series analysis. However, a significant limitation of these models is their slow inference speed. The primary bottleneck in these models is associated with memory reads and writes, rather than arithmetic computations. This is especially problematic in larger models with vast parameter spaces, where efficient memory management is critical to performance. Further, these models generate outputs sequentially, one token at a time, with each new token depending on all previously generated tokens. This inherent sequential dependency limits the model’s ability to parallelize the token generation process, leading to inference latency much greater than that of models capable of processing data in parallel. The challenge is to overcome this sequential bottleneck without compromising the model’s ability to accurately capture dependencies in the data.&lt;/p&gt; &lt;p&gt;The central question this project addresses is whether it’s possible to introduce parallelism into the inference process of autoregressive models. A more specific aspect of this problem is whether probabilities for multiple tokens can be computed simultaneously, rather than processing each token individually. This project aims to enhance methods that have been proposed for parallelizing the decoding process, focusing on solutions that draw inspiration from speculative execution in processors and other systems design strategies.&lt;/p&gt; &lt;h3 id=&quot;speculative-execution-in-processors&quot;&gt;Speculative execution in processors&lt;/h3&gt; &lt;p&gt;Speculative execution is a technique used in CPU architecture to improve processing speed. Instead of waiting for sequential execution of instructions, processors predict which instructions are likely to be executed next and start processing them in advance. If the prediction is correct, this leads to a significant reduction in latency, as the processor has preemptively executed necessary instructions. If the prediction is incorrect, the processor discards the speculative results and reverts to the correct execution path. This method effectively utilizes CPU resources that would otherwise remain idle during the waiting period, thus optimizing the overall processing speed and reducing latency.&lt;/p&gt; &lt;h3 id=&quot;applying-speculative-execution-to-model-inference&quot;&gt;Applying speculative execution to model inference&lt;/h3&gt; &lt;p&gt;Inspired by speculative execution in processors, this project explores how similar principles can be applied to accelerate inference in large autoregressive models. The concept involves generating multiple potential outputs in parallel, using a smaller or draft model, and then evaluating these outputs with the larger target model. This mimics the speculative execution process where multiple paths are explored simultaneously, with the most promising path being selected as the final output. This approach, referred to as “speculative sampling” or “speculative decoding,” aims to introduce a level of parallelism in the inference process, enabling faster generation of outputs without compromising the quality or accuracy of the model’s predictions.&lt;/p&gt; &lt;h3 id=&quot;hierarchical-speculative-decoding&quot;&gt;Hierarchical speculative decoding&lt;/h3&gt; &lt;p&gt;In addition to implementing already proposed speculative decoding techniques, this project investigates a strategy that has the potential further speed up inference: hierarchical speculative decoding. This method aims to accelerate the smaller approximation model with an even smaller, faster model. While I experiment with two-layer (traditional) and three-layer hierarchies in this project, one could theoretically extend this idea to create an &lt;em&gt;n&lt;/em&gt; layer hierarchy, assuming sufficient memory. Although researchers developing speculative decoding algorithms and sampling methods have mentioned the potential viability of hierarchical speculative decoding, none have tried to implement it. Thus, this project aims to find an efficient implementation of the approach and determine if it actually further speeds up inference.&lt;/p&gt; &lt;h2 id=&quot;current-work&quot;&gt;Current Work&lt;/h2&gt; &lt;p&gt;Multiple papers have presented novel speculative decoding algorithms, with the nuance typically in the way that sampling is performed. The two most-referenced papers in this space are DeepMind’s Accelerating Large Language Model Decoding with Speculative Sampling (Chen et al.) &lt;a href=&quot;https://arxiv.org/pdf/2302.01318.pdf&quot;&gt;(paper)&lt;/a&gt; and Google Research’s Fast Inference from Transformers via Speculative Decoding (Leviathan et al.) &lt;a href=&quot;https://arxiv.org/pdf/2211.17192.pdf&quot;&gt;(paper)&lt;/a&gt;. This project draws its architecture from the latter, so we will more explore its approach in-depth and describe how its shortcomings motivated the experiments in this project.&lt;/p&gt; &lt;h3 id=&quot;general-setup&quot;&gt;General setup&lt;/h3&gt; &lt;p&gt;The approach presented in Fast Inference from Transformers via Speculative Decoding (Leviathan et al.) aims to accelerate inference from a target transformer-like model $M_p$. We present a distilled version of the speculative decoding set-up, algorithm, and evaluation here.&lt;/p&gt; &lt;p&gt;We start with two models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;$M_p$ (the target model)&lt;/li&gt; &lt;li&gt;$M_q$ (a smaller approximation model)&lt;/li&gt; &lt;/ol&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;$p(x_{t}&lt;/td&gt; &lt;td&gt;x_{&amp;lt;t})$ describes the sampling of token $x_t$ given pretext $x_{&amp;lt;t}$, and we will refer to this as just $p(x)$. The shorthand applies for $q(x)$.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Our goal is to generate $\gamma \in \mathbb{Z}^{+}$ completions quickly with the approximation model, check that the probability of those generations are identical to the target model’s (in parallel), and then reject and resample starting from the first “wrong” generation.&lt;/p&gt; &lt;h3 id=&quot;sampling-px&quot;&gt;Sampling $p(x)$&lt;/h3&gt; &lt;p&gt;In order to sample $p(x)$, we will sample $x \sim q(x)$ instead.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If $q(x)\leq p(x)$, we keep $x$&lt;/li&gt; &lt;li&gt;Otherwise, we reject $x$ with a $1-\frac{p(x)}{q(x)}$ probability. &lt;ul&gt; &lt;li&gt;If we end up rejecting $x$, we resample $x\sim\text{norm}(\max(0, p(x)-q(x)))$.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Basically, we want $x\sim p(x)$ to be &lt;em&gt;at least&lt;/em&gt; as likely as $x \sim q(x)$. Following the steps above is equivalent to just sampling $x \sim q(x)$, and the paper provides a comprehensive proof of this in its appendix.&lt;/p&gt; &lt;h3 id=&quot;the-algorithm&quot;&gt;The Algorithm&lt;/h3&gt; &lt;p&gt;We use an implementation of the following algorithm from Leviathan et al. We start with some conditioning $prefix$ (our starting tokens) and generate between $1$ and $\gamma+1$ tokens at once.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt; &lt;p&gt;To evaluate the effectiveness of this approach, we need to calculate the total wall time improvement of speculative decoding versus normal inference on the target model.&lt;/p&gt; &lt;p&gt;To make this evaluation more simple, assume we can run $\gamma + 1$ concurrent evaluations of $M_p$ in parallel. Now, we just need to get the cost of running $M_q$ (the approximation model).&lt;/p&gt; &lt;p&gt;Let $c$ = the cost coefficient, which is the ratio between the time for a single run of $M_q$ and a single run of $M_p$. $c$ will depend only on our hardware and software implementation details.&lt;/p&gt; &lt;p&gt;Now, we need some measure of how well $M_q$ approximates $M_p$.&lt;/p&gt; &lt;p&gt;Let $\beta$ be the &lt;em&gt;acceptance rate&lt;/em&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;$\beta_{x&amp;lt;t}$ is the probability of accepting $x_{t}\sim q(x_{t}&lt;/td&gt; &lt;td&gt;x_{&amp;lt;t})$ by speculative sampling.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/li&gt; &lt;li&gt;Assume that the $\beta$s are i.i.d.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let $\alpha=E(\beta)$. This gives us the average acceptance rate across many samples, which is a good measure of how well $M_q$ approximates $M_p$.&lt;/p&gt; &lt;p&gt;The expectation of the number of generated tokens is now a bounded geometric function of $\alpha$ (bounded by $\gamma$) :\(E(\text{# of generated tokens}) = \frac{1-\alpha^{\gamma + 1}}{1-\alpha}\)Given this relationship, we can derive the expected improvement factor for the total wall time (assuming longer generations):\(\frac{1-\alpha^{\gamma+1}}{(1-\alpha)(\gamma c+1)}\) For the sake of conciseness, we leave the full proof to the paper, but the general sketch relies on the fact that each run of Algorithm 1 costs $Tc\gamma + T$ (where $T$ is the cost of running one step of $M_p$). We run $M_q$ $\gamma$ times and $M_p$ once, and each run of Algorithm 1 produces $\frac{1-\alpha^{\gamma + 1}}{1-\alpha}$ tokens. Since the cost of producing a single token with a standard algorithm is $T$, we get the above improvement.&lt;/p&gt; &lt;h2 id=&quot;hierarchical-speculative-decoding-1&quot;&gt;Hierarchical Speculative Decoding&lt;/h2&gt; &lt;p&gt;How much faster can we make model inference by accelerating the approximation model with an even smaller, faster model? Let’s look at the case where we have three models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;$M_p$:&lt;/strong&gt; The target model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;$M_q$:&lt;/strong&gt; The first-level approximation model, used to approximate $M_p$.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;$M_r$:&lt;/strong&gt; The second-level, even smaller approximation model, used to approximate $M_q$.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With the introduction of $M_r$, we now need to consider additional parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;$\gamma_r$:&lt;/strong&gt; The number of concurrent evaluations that can be run using $M_r$.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;$\beta_r$:&lt;/strong&gt; The acceptance rate for $M_r$, analogous to $\beta$ for $M_q$.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;$\alpha_r = E(\beta_r)$:&lt;/strong&gt; The average acceptance rate for $M_r$, representing how well $M_r$ approximates $M_q$.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, $\beta$ for $M_q$ becomes a function of $\beta_r$, reflecting the hierarchical nature of this setup. The acceptance rate $\beta$ for $M_q$ now depends on how effectively $M_r$ approximates $M_q$, which in turn approximates $M_p$.&lt;/p&gt; &lt;p&gt;We can hypothesize that the effectiveness of $M_q$ in approximating $M_p$ might now be influenced by the performance of $M_r$. This could mean that $\beta$, and consequently $\alpha$, might be a function of $\alpha_r$.&lt;/p&gt; &lt;p&gt;The expectation of the number of generated tokens would now need to consider the hierarchical relationship. A new formula would be required to calculate this expectation, taking into account the performances of both $M_q$ and $M_r$.&lt;/p&gt; &lt;p&gt;Finally, the expected improvement factor for the total wall time would also need to be recalculated to reflect this hierarchical structure. This would involve integrating the costs and efficiencies of $M_r$ into our existing model, which so far only considered $M_q$ and $M_p$.&lt;/p&gt; &lt;p&gt;Whether or not this approach will actually speed up the model in practice is left to be determined experimentally.&lt;/p&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;I experimented on multiple transformer model families, most notably &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-125m&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-1.3b&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-13b&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The primary research questions I investigated include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How many orders of magnitude larger should $M_p$ be than $M_q$ to achieve the maximal improvement?&lt;/li&gt; &lt;li&gt;To what extent does hierarchical speculative decoding further speed up inference?&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;general-set-up-for-experiments&quot;&gt;General set-up for experiments&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;For the standard (non-hierarchical) speculative decoding, I implemented the algorithm exactly as described above. &lt;ul&gt; &lt;li&gt;I used a gamma value of 4&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;I used both top-k sampling and nucleus sampling, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k=20&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p=0.9&lt;/code&gt; constant throughout all experiments.&lt;/li&gt; &lt;li&gt;I typically prompted the models with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input_text = &quot;Once upon a&quot;&lt;/code&gt; and generated 20 tokens.&lt;/li&gt; &lt;li&gt;I used consistent sets of seeds (such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.manual_seed(898)&lt;/code&gt;) when running the same experiment across multiple model combinations for the sake of reproducibility and so that I could more easily compare results across models on shorter generation lengths.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;how-many-orders-of-magnitude-larger-should-m_p-be-than-m_q&quot;&gt;How many orders of magnitude larger should $M_p$ be than $M_q$?&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;To investigate this, I calculated inference time (tokens per second) on each of the following (approximator, target) model pairs: &lt;ul&gt; &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-125m&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-1.3b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-125m&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-13b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-1.3b&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-13b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;set-up-for-hierarchical-speculative-decoding&quot;&gt;Set-up for hierarchical speculative decoding&lt;/h3&gt; &lt;p&gt;I experimented with a three-level hierarchical approach using&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Small approximation model $M_r$: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-125m&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Approximation model $M_q$: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-1.3b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Target model $M_p$: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;facebook/opt-13b&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To add hierarchical decoding to the algorithm, I replaced the sampling of $M_q$, where we typically sample $x \sim q(x)$ with a sampling process that mirrors the sampling from the target model. So we sample from $x\sim r(x)$ instead, keep if it’s at least as likely in $q(x)$, and reject proportional to the likelihood of the sample under either model, adjusting the distribution as before if we need to sample again. This made the theoretical implementation rather simple, as we could re-use a lot of the code. The implementation in practice was slightly more difficult than expected, however, as my implementation of the two-layer speculative decoding didn’t permit direct functional composition, and I had to restructure the implementation a bit.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;calculating-c-for-each-model-pair&quot;&gt;Calculating $c$ for each model pair&lt;/h3&gt; &lt;p&gt;(The larger model is used as the target model $M_p$)&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;opt-125m&lt;/th&gt; &lt;th&gt;opt-1.3b&lt;/th&gt; &lt;th&gt;opt-13b&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;opt-125m&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-1.3b&lt;/td&gt; &lt;td&gt;0.015&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;0.022&lt;/td&gt; &lt;td&gt;0.015&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;This gives insight into the relative efficiencies of the models when performing assisted inference.&lt;/p&gt; &lt;h3 id=&quot;the-general-effect-of-speculative-decoding&quot;&gt;The general effect of speculative decoding&lt;/h3&gt; &lt;p&gt;Wall time improvements from speculative decoding have already been documented, so these results are not novel, but I include them here for further proof that the algorithm works and for comparison with other results.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Target Model&lt;/th&gt; &lt;th&gt;Approximation Model&lt;/th&gt; &lt;th&gt;Tokens/Second&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;0.047&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;opt-1.3b&lt;/td&gt; &lt;td&gt;0.087&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;opt-125m&lt;/td&gt; &lt;td&gt;0.057&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-1.3b&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;0.336&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-1.3b&lt;/td&gt; &lt;td&gt;opt-125m&lt;/td&gt; &lt;td&gt;1.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;In all cases, including an approximation model increases the model’s token per second inference rate.&lt;/p&gt; &lt;h3 id=&quot;acceptance-rates-and-wall-time-given-m_p-and-m_q&quot;&gt;Acceptance rates and wall time given $M_p$ and $M_q$&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Target Model&lt;/th&gt; &lt;th&gt;Approximator Model&lt;/th&gt; &lt;th&gt;Tokens/Second&lt;/th&gt; &lt;th&gt;Acceptance Rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;opt-1.3b&lt;/td&gt; &lt;td&gt;opt-125m&lt;/td&gt; &lt;td&gt;1.05&lt;/td&gt; &lt;td&gt;38%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;opt-125m&lt;/td&gt; &lt;td&gt;0.057&lt;/td&gt; &lt;td&gt;15%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;opt-1.3b&lt;/td&gt; &lt;td&gt;0.087&lt;/td&gt; &lt;td&gt;19%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;These results help us answer the question: &lt;em&gt;How many orders of magnitude larger should $M_p$ be than $M_q$?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;One order of magnitude seems to yield higher acceptance rates, and the smaller models were obviously faster.&lt;/p&gt; &lt;h3 id=&quot;results-of-hierarchical-speculative-decoding&quot;&gt;Results of hierarchical speculative decoding&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Target Model&lt;/th&gt; &lt;th&gt;Approximation Model&lt;/th&gt; &lt;th&gt;Tokens/Second&lt;/th&gt; &lt;th&gt;Acceptance Rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;0.047&lt;/td&gt; &lt;td&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;opt-1.3b&lt;/td&gt; &lt;td&gt;0.087&lt;/td&gt; &lt;td&gt;19%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;opt-125m&lt;/td&gt; &lt;td&gt;0.057&lt;/td&gt; &lt;td&gt;15%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;opt-13b&lt;/td&gt; &lt;td&gt;opt-1.3b, opt-125m&lt;/td&gt; &lt;td&gt;0.030&lt;/td&gt; &lt;td&gt;17%, 33%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;I found that running the three-layer hierarchical speculative decoding &lt;em&gt;did not&lt;/em&gt; speed up model inference, but I hypothesize that this is because of compute limitations. Running all three models on my computer given the parallelization requirements of the algorithm forced the program to map data to devices in a less-efficient way. I wasn’t able to find smaller pre-trained models with which I could test this on my local machine, so a future experiment should either train custom smaller models for the sake of inference in this setting or use a device with greater memory capacity.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This project explored the potential of speculative decoding, a technique inspired by speculative execution in processors, to accelerate inference in autoregressive models like transformers. Our exploration focused on implementing and extending existing methods of speculative decoding, particularly the ones proposed in the seminal works by Chen et al. and Leviathan et al., while also introducing early experiments with concept of hierarchical speculative decoding, which is to be further investigated.&lt;/p&gt; </content> </entry> <entry> <title>Unraveling Social Reasoning in LLMs: A Deep Dive into the Social IQA Benchmark</title> <link href="https://deep-learning-mit.github.io/blog/2023/unraveling-social-reasoning-in-llms/"/> <updated>2023-11-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/unraveling-social-reasoning-in-llms</id> <content type="html">&lt;h1 id=&quot;unraveling-social-reasoning-in-llms-a-decision-tree-framework-for-error-categorization&quot;&gt;Unraveling Social Reasoning in LLMs: A Decision Tree Framework for Error Categorization&lt;/h1&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Social commonsense reasoning is a skill most people acquire within the first few years of life, often without formal education. Consider this example of a social commonsense reasoning question:&lt;/p&gt; &lt;p&gt;Q: Kai was frantically running to a gate at the airport. Why was Kai running?&lt;/p&gt; &lt;p&gt;A) They were trying to catch a flight that departs soon&lt;/p&gt; &lt;p&gt;B) They were training for a marathon&lt;/p&gt; &lt;p&gt;C) They were testing out their new running shoe&lt;/p&gt; &lt;p&gt;Most would likely infer that Kai was rushing to catch a flight that would depart soon and choose A, the correct answer. Social commonsense reasoning, at its core, entails reasoning about the past, current, and future states of others.&lt;/p&gt; &lt;p&gt;Despite advancements in Large Language Models (LLMs), prompting models to achieve near-human levels of performance in different tasks across various domains, they have traditionally struggled with social commonsense reasoning tasks, often underperforming humans. Though, this isn’t surprising to most observers &lt;d-cite key=&quot;sap_neural_2023&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;To better understand why, previous studies have created benchmarks for social commonsense reasoning &lt;d-cite key=&quot;huang_towards_2023&quot;&gt;&lt;/d-cite&gt;; benchmarked how different models perform on various social commonsense reasoning benchmarks; and/or provided classifications for different question types testing social commonsense reasoning &lt;d-cite key=&quot;huang_towards_2023&quot;&gt;&lt;/d-cite&gt;. Certain question categories, like those directly related to social norms, are documented to exhibit higher error rates, suggesting that some aspects of social reasoning are more challenging for models to learn &lt;d-cite key=&quot;li_systematic_2022&quot;&gt;&lt;/d-cite&gt;. However, these works used older models that were not specifically designed for reasoning tasks.&lt;/p&gt; &lt;p&gt;Specifically, our blog investigates the question, &lt;strong&gt;What are underlying themes in social errors that large language models make?&lt;/strong&gt; From both a qualitative and quantitative perspective. The goal of our findings is to help discover if there are methods that could potentially address these errors.&lt;/p&gt; &lt;p&gt;To answer this question, we ran Flan-T5 on the Social IQA benchmark, which was introduced in 2019 and features 38,000 multiple-choice questions (MCQs) designed to gauge “emotional and social intelligence in everyday situations” &lt;d-cite key=&quot;sap_socialiqa_2019&quot;&gt;&lt;/d-cite&gt;. After qualitatively labeling 350 model errors, we made a striking discovery: over half of the questions Flan-T5 got wrong were due to problematic questions in the Social IQA dataset.&lt;/p&gt; &lt;p&gt;Upon making this curious realization, we pivoted our project from designing a decision tree abstraction for providing detailed categorization of social commonsense questions to analyzing and addressing the two types of errors:&lt;/p&gt; &lt;p&gt;Type 1: Errors stemming from the flawed construction of the Social IQA dataset&lt;/p&gt; &lt;p&gt;Type 2: Errors where Flan-T5’s choices don’t align with social commonsense.&lt;/p&gt; &lt;p&gt;In the first error group, even reasonable humans, including this blog post’s authors, disagreed with Social IQA’s “correct” answers. Questions in this first group have nonsensical contexts/questions, lack a single reasonable answer, or have many reasonable answers.&lt;/p&gt; &lt;p&gt;When examining questions in the second error group, we noticed that Flan-T5 often over-infers underlying reasons when a more straightforward answer exists. To address this group of errors, we visualized T5’s attention mechanisms when processing such questions.&lt;/p&gt; &lt;h2 id=&quot;background-and-related-works&quot;&gt;&lt;strong&gt;Background and Related Works&lt;/strong&gt;&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;LLMs and Reasoning&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Language models like GPT-4 have captured widespread media attention, given their question-answering capabilities.&lt;/p&gt; &lt;p&gt;Throughout the development and testing of LLMs, various tasks have been developed to empirically assess these models’ abilities and limitations. In literature, these tasks are typically categorized into two main groups: natural language understanding (NLU) and natural language generation (NLG). NLU tasks evaluate a language model’s ability to understand natural language. This includes tasks like Natural Language Inference, Reading Comprehension, and various reasoning tasks, including social commonsense reasoning &lt;d-cite key=&quot;naveed_comprehensive_2023&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;comprehensive-overview-of-social-commonsense-reasoning-benchmarks&quot;&gt;&lt;strong&gt;Comprehensive Overview of Social Commonsense Reasoning Benchmarks&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Over 100 large-scale benchmarks have been proposed to assess and compare models’ social commonsense reasoning abilities and to serve as resources for transfer learning &lt;d-cite key=&quot;davis_benchmarks_2023&quot;&gt;&lt;/d-cite&gt;. In general, these benchmarks aim to evaluate models’ abilities to infer the mental states, beliefs, and intentions of others. Their development was inspired by the Theory of Mind (ToM), a concept originating from childhood development psychology—some benchmark questions are, in fact, explicitly based on ToM tasks used to assess children’s social reasoning &lt;d-cite key=&quot;nematzadeh_evaluating_2018&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Notable benchmarks frequently mentioned in the literature include multiple-choice Question Answering (QA) benchmarks like the 2019 Social IQA &lt;d-cite key=&quot;sap_socialiqa_2019&quot;&gt;&lt;/d-cite&gt; and 2019 HellaSwag &lt;d-cite key=&quot;zellers_hellaswag_2019&quot;&gt;&lt;/d-cite&gt;, along with generative evaluation benchmarks like 2020 ProtoQA &lt;d-cite key=&quot;boratko_protoqa_2020&quot;&gt;&lt;/d-cite&gt;. Many of these benchmarks are based on structured knowledge banks about everyday commonsense reasoning like ATOMIC &lt;d-cite key=&quot;sap_atomic_2019&quot;&gt;&lt;/d-cite&gt; and COMET &lt;d-cite key=&quot;bosselut_comet_2019&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Similar to that of other studies evaluating LLMs’ commonsense knowledge, we use an MCQ benchmark and not a generative one because they are more simple and reliable for evaluation &lt;d-cite key=&quot;li_systematic_2022&quot;&gt;&lt;/d-cite&gt;; under tight time and resource constraints, we err on the side of a simple and reliable evaluation method, though, in future works, we would like to generalize our findings to more datasets.&lt;/p&gt; &lt;p&gt;However, despite their widespread use, benchmarking datasets like Social IQA are not without flaws. Previous studies have shown that many aspects of common sense are still untested by these benchmarks, indicating an ongoing need for reliable methods to evaluate social commonsense reasoning &lt;d-cite key=&quot;davis_commonsense_2015&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;h3 id=&quot;problems-with-social-iqa&quot;&gt;&lt;strong&gt;Problems With Social IQA&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Social IQA focuses on evaluating models’ abilities to reason about others’ mental states, aligning with Theory of Mind concepts &lt;d-cite key=&quot;gandhi_understanding_2023&quot;&gt;&lt;/d-cite&gt;. Each question in Social IQA comprises a context, a question, and three answer choices; for each of the 37,588 multiple-choice questions, the context, question, correct answer choice, and two incorrect answer choices were gathered through three phases of crowdsourcing on Amazon Mechanical Turk (MTurk) &lt;d-cite key=&quot;sap_socialiqa_2019&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In the first two phase, MTurk crowdsource workers sourced context sentences and questions using the ATOMIC knowledge base &lt;d-cite key=&quot;sap_atomic_2019&quot;&gt;&lt;/d-cite&gt;. In the third phase, MTurk workers generated correct answers for the given context-question pairs. Incorrect choices for each question were derived from correct answers to related questions &lt;d-cite key=&quot;sap_socialiqa_2019&quot;&gt;&lt;/d-cite&gt;. In our Discussion section, we will discuss issues with this methodology for sourcing negative answers.&lt;/p&gt; &lt;p&gt;Many critiques have been raised about the reliance on crowdsourcing for benchmarks, specifically, about the challenges in obtaining high-quality material &lt;d-cite key=&quot;davis_commonsense_2015&quot;&gt;&lt;/d-cite&gt;. Given the low pay on MTurk, workers often prioritize quantity over quality, leading to errors. There have even been reported instances of crowdsourcing workers using bots to help them complete more tasks.&lt;/p&gt; &lt;h3 id=&quot;prior-error-analysis-work-using-social-iqa-dataset&quot;&gt;&lt;strong&gt;Prior Error Analysis Work Using Social IQA Dataset&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;The authors of Social IQA conducted a preliminary error analysis of their dataset, finding that language models found questions about context pre-conditions, such as motivations and prior actions, to be much easier than those about stative attributes or predicting future actions. Interpreting these results, the authors hypothesized that models might be learning lexical associations rather than true meaning &lt;d-cite key=&quot;sap_socialiqa_2019&quot;&gt;&lt;/d-cite&gt;. This aligns with findings presented in other works &lt;d-cite key=&quot;zellers_hellaswag_2019&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Other research, such as Wang et al.’s &lt;d-cite key=&quot;wang_semantic_2021&quot;&gt;&lt;/d-cite&gt;, categorize Social IQA questions into four types: 1) Feelings and Characteristics, 2) Interaction, 3) Daily Events, and 4) Knowledge, Norm, and Rules. They found the final category to be the most challenging for models.&lt;/p&gt; &lt;h3 id=&quot;general-methodology-for-conducting-systematic-error-analysis-for-qa&quot;&gt;&lt;strong&gt;General Methodology for Conducting Systematic Error Analysis for QA&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Our research, aimed at identifying themes in social errors made by LLMs, draws inspiration from conventional methodologies for system error analysis in QA tasks. Moldovan et al.’s data-driven approach to QA error analysis, focusing on answer accuracy based on question stems, reveals that certain question types are more challenging for LLMs &lt;d-cite key=&quot;moldovan_performance_2003&quot;&gt;&lt;/d-cite&gt;. Rondeau et al. used feature extraction for systematic error analysis in the Stanford Question Answering Dataset (SQuAD), employing feature extractors and classifiers &lt;d-cite key=&quot;rondeau_systematic_2018&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;existing-approaches-to-improve-social-commonsense-reasoning&quot;&gt;&lt;strong&gt;Existing Approaches to Improve Social Commonsense Reasoning&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Our research also explores existing literature offering solutions for mitigating errors in social commonsense reasoning. Some of these works suggest incorporating external structured data, such as knowledge graphs, into models. For example, Chang et al. showed that integrating knowledge graphs like ConceptNet improves performance on Social IQA &lt;d-cite key=&quot;chang_incorporating_2020&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;However, despite confirming the effectiveness of this approach, studies like Mitra et al. also noted instances where models, even with access to relevant information that can directly lead to the correct answer, predicted incorrect answers based on irrelevant knowledge &lt;d-cite key=&quot;mitra_how_2020&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h2&gt; &lt;h3 id=&quot;step-1-applying-flan-t5-to-social-iqa&quot;&gt;&lt;strong&gt;Step 1: Applying Flan-T5 to Social IQA&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;We first prompted Flan-T5, known for its promising reasoning task performance &lt;d-cite key=&quot;chung_scaling_2022&quot;&gt;&lt;/d-cite&gt;, to answer 33,411 MCQs from the Social IQA dataset. The prompt format, emulating the examples &lt;d-cite key=&quot;bosma_introducing_2021&quot;&gt;&lt;/d-cite&gt; was as follows:&lt;/p&gt; &lt;p&gt;[Context].&lt;/p&gt; &lt;p&gt;Based on the context above, choose the best answer to the question:&lt;/p&gt; &lt;p&gt;[Question]&lt;/p&gt; &lt;p&gt;OPTIONS:&lt;/p&gt; &lt;p&gt;(A) [Answer A]&lt;/p&gt; &lt;p&gt;(B) [Answer B]&lt;/p&gt; &lt;p&gt;(C) [Answer C]&lt;/p&gt; &lt;p&gt;For your answer, return exactly one character, either A, B, or C.&lt;/p&gt; &lt;h3 id=&quot;step-2-qualitative-coding-of-350-errors&quot;&gt;&lt;strong&gt;Step 2: Qualitative Coding of 350 Errors&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Next, we used the following procedure, based on standard iterative qualitative coding methods, to categorize instances where Flan-T5’s response differed from the Social IQA dataset’s correct answer.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Initial Annotation: initially, for a subset of 100 rows, two independent coders annotated each row, noting the reasons for the discrepancy in the correct answer choice between the dataset and Flan-T5.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Theme Identification: the coders reviewed each other’s annotations and engaged in discussions to identify major themes in inconsistencies. Based on these discussions, they developed a formal set of tags to apply to the rows.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Tagging: finally, they applied these tags to a total of 350 rows&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;step-3-quantitative-error-analysis&quot;&gt;&lt;strong&gt;Step 3: Quantitative Error Analysis&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;We then analyzed the data to determine the frequency of each error type within our tagged dataset (n=350). We explored potential features, such as specific words, that contributed to the difficulty of the questions.&lt;/p&gt; &lt;h3 id=&quot;step-4-addressing-type-1-errors---developing-a-pruning-tool&quot;&gt;&lt;strong&gt;Step 4: Addressing Type 1 Errors - Developing a Pruning Tool&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Our objective here was to develop a tool that could use our tagged question set to accurately identify problematic questions. Unfortunately, this approach did not yield the desired results and needs future work.&lt;/p&gt; &lt;h3 id=&quot;step-5-addressing-type-2-errors---analyzing-through-attention-mechanism-visualization&quot;&gt;&lt;strong&gt;Step 5: Addressing Type 2 Errors - Analyzing through Attention Mechanism Visualization&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Finally, we shifted our focus to examining errors by visualizing the attention mechanisms of the model. This approach aimed to provide deeper insights into how the model processes and responds to various types of questions, particularly those categorized as Type 2 errors.&lt;/p&gt; &lt;h2 id=&quot;analysis-and-evaluations&quot;&gt;&lt;strong&gt;Analysis and Evaluations&lt;/strong&gt;&lt;/h2&gt; &lt;h3 id=&quot;general-accuracy-of-flan-t5-on-social-iqa&quot;&gt;&lt;strong&gt;General Accuracy of Flan-T5 on Social IQA&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Overall, Flan-T5 exhibits a high accuracy of 90% when presented with MCQs from Social IQA, which could be because it was fine-tuned “on a large set of varied instructions,” similar to the questions we present it &lt;d-cite key=&quot;bosma_introducing_2021&quot;&gt;&lt;/d-cite&gt;. This accuracy is much higher than BERT, which had a 64.5% accuracy &lt;d-cite key=&quot;sap_socialiqa_2019&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;set-of-formal-tags-derived-from-qualitative-coding&quot;&gt;&lt;strong&gt;Set of Formal Tags Derived from Qualitative Coding&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;In the initial annotation phase of qualitative coding, both coders were surprised to find many questions marked “incorrect” because of issues inherent in the Social IQA questions themselves (see below for an example). Therefore, we wanted to characterize why the Social IQA multiple choice questions were problematic: was it a lack of context comprehension, the unreasonableness of all answer options, or the presence of multiple equally reasonable answers?&lt;/p&gt; &lt;p&gt;During the theme identification phase, the coders established two groups of tags:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Errors arising from the flawed construction of the Social IQA dataset&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Errors due to Flan-T5’s responses not aligning with social commonsense&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;type-1-errors&quot;&gt;&lt;strong&gt;Type 1 Errors&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;For Type 1 errors, six labels were created:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Nonsensical Context: When the context sentence is incomprehensible to a reasonable human.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Nonsensical or Ambiguous Question: When the question is either nonsensical or too ambiguous.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Major Typo: Refers to incomprehensible parts of the Context, Question, or answer choices due to typos.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Multiple Reasonable Answers: When several answers appear equally reasonable, either due to similar meanings or general reasonableness.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;No Reasonable Answer: When no answer options seem appropriate or reasonable.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Incorrectly Labeled “Correct” Answer: When an alternative answer seems more reasonable than the one marked “correct.”&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Examples of Type 1 Errors&lt;/strong&gt;&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/errors-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/errors-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/errors-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/errors.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Examples of Questions and Answers &lt;/div&gt; &lt;h3 id=&quot;type-2-errors&quot;&gt;&lt;strong&gt;Type 2 Errors&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;For Type 2 errors: we devise the following set of three labels:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Overinfers: This tag is for questions where Flan-T5 seems to make a large leap in logic, resulting in it picking an answer choice that makes spurious assumptions when a much more direct and clear answer is available&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Associated but Incorrect: This is for questions where Flan-T5 picks an answer choice that is associated with the context and question, but is not what the question is specifically asking about. This differs from over-inferring in that this usually entails picking irrelevant answer choices.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Flan-T5 Incorrect (unspecified): all other mistakes attributable to Flan-T5.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;distribution-of-tags&quot;&gt;&lt;strong&gt;Distribution of Tags&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Looking at our annotated questions, we see that 65% of errors stemmed from the flawed construction of the Social IQA dataset. Meanwhile, 38% of errors were errors stemming from Social IQA not picking the right answer. Observe that it is possible for a question to be tagged with both a Type 1 tag and a Type 2 tag.&lt;/p&gt; &lt;p&gt;For Type 1 errors, we see that having multiple reasonable answers is by far the most common reason why a question is problematic. This was followed by having no reasonable answer, and the answer labeled “correct” not being the best available answer. Indeed, the top three reasons why a question is considered problematic all stem from questionable answer choices. This highlights how the construction of the answer choices, and thus Social IQA as a benchmark set, is problematic.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/socialiqaerrors-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/socialiqaerrors-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/socialiqaerrors-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/socialiqaerrors.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Histogram of the Types of Errors in SocialIQA &lt;/div&gt; &lt;p&gt;Next, we examine the distribution of Type 2 error tags. We see that the most common reason is Flan-T5 over-inferring.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/flant5errors-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/flant5errors-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/flant5errors-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/flant5errors.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Histogram of the Types of Errors FlanT5 Makes &lt;/div&gt; &lt;h3 id=&quot;analysis-of-question-types&quot;&gt;&lt;strong&gt;Analysis of Question Types&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;In our quantitative analysis, we identified key features contributing to lower accuracy in certain questions. Notably, questions containing the word ‘others’ scored lower in accuracy, with an average of 0.880, compared to the general accuracy score of 0.990. Furthermore, questions featuring repeated answer choices also exhibited a lower accuracy score of 0.818.&lt;/p&gt; &lt;h3 id=&quot;attempt-to-prune-social-iqa&quot;&gt;&lt;strong&gt;Attempt to Prune Social IQA&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Assessing models on social commonsense reasoning questions requires clear comprehension and consensus on the appropriateness of the questions and their answer choices. Our goal was to create a tool to classify the sensibility of these questions and answers. To achieve this, we experimented with various models, including Flan-T5 and GPT-4, asking them to evaluate the coherence of the questions. Unfortunately, the results were inconsistent, often varying with each regeneration of the response. Despite these challenges, we maintain that addressing this issue remains crucial.&lt;/p&gt; &lt;h3 id=&quot;visualization-of-attention-mechanism&quot;&gt;&lt;strong&gt;Visualization of Attention Mechanism&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;In our analysis of Type 2 errors, we focused on how the errors happen because Flan-T5 overinfers the underlying reasons not explicitly stated in the question instead of picking the more straightforward and correct answer, or picks some answer associated with the words in the context that isn’t directly related to the question.&lt;/p&gt; &lt;p&gt;In addition to providing qualitative analysis, we set out to provide some quantitative analysis to better understand why this was happening. Consider these linked notebooks, which visualize the cross attention and the encoder attention for &lt;a href=&quot;https://colab.research.google.com/drive/1M4XhFORT5KJgFaaVnZM3g5QS8JPV7Apq#scrollTo=JHkd_dwR2AiZ&quot;&gt;one correctly labeled example&lt;/a&gt; and &lt;a href=&quot;https://colab.research.google.com/drive/1VtU2kvJA5EKL4AeuaW-N0RwiBimb96MB#scrollTo=JHkd_dwR2AiZ&quot;&gt;one incorrectly labeled example&lt;/a&gt;, where Flan-T5 chooses an associated but incorrect answer. (Note that the specific images were chosen for brightness in the heatmaps, since the attention was normalized. Please reference the notebook.).&lt;/p&gt; &lt;p&gt;To visualize cross-attention, we looked at the cross-attention between the answer Flan-T5 generates and the encodings, across each layer and attention head in Flan-T5, grouping in both orders. To visualize the encoder attention, we looked at the average attention for each layer in the input encoding, and for the layer that saw the most drastic change (layer 2, starting from 0 index), we visualized the attention for each attention head.&lt;/p&gt; &lt;p&gt;Now, consider the context and question:&lt;/p&gt; &lt;p&gt;Cameron had a big paper due later in the week, so Cameron put pen to paper. What will Cameron want to do next?&lt;/p&gt; &lt;p&gt;A) research his topic&lt;/p&gt; &lt;p&gt;B) write an outline&lt;/p&gt; &lt;p&gt;C) redo his topic&lt;/p&gt; &lt;p&gt;Flan-T5 answers A), while the correct answer is “write an outline.” Notably, Flan-T5 doesn’t choose the third answer, “redo his topic.”&lt;/p&gt; &lt;p&gt;Therefore, we can see that Flan-T5’s is associated with the topic, but isn’t the correct answer, which is given by the phrase “put pen to paper.” Visualizing the average encoder attention and cross attention, we see that the contextualized embeddings and generation primarily focus on the words “big paper” and the question, but don’t pay much attention to the word “pen.”&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Cross Attention for Associated But Incorrect Answer &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated_encoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated_encoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated_encoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/associated_encoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Encoder Attention for Associated But Incorrect Answer &lt;/div&gt; &lt;p&gt;Generalizing our results a bit, we find that FLAN only pays reasonable attention (normalized attention &amp;gt; 0.05) to the keywords for 14 out of 26 examples tagged under “associated,” even for simpler questions such as&lt;/p&gt; &lt;p&gt;On the other hand, consider the question,&lt;/p&gt; &lt;p&gt;Sydney played basketball with her friends after school on a sunny afternoon. What does Sydney need to do before this?&lt;/p&gt; &lt;p&gt;A) take a nap before this&lt;/p&gt; &lt;p&gt;B) have a basketball before this&lt;/p&gt; &lt;p&gt;C) go home before this&lt;/p&gt; &lt;p&gt;Flan-T5 correctly answers “have a basketball before this,” not choosing “take a nap before this” or “go home before this.”&lt;/p&gt; &lt;p&gt;Indeed, we see the four vertical lines in the encoder and cross attentions that correspond to key phrases in the sentence. For the questions that Flan-T5 gets correct, it pays attention to the right keywords 9 out of 10 times. Lastly, note that for questions labeled “overinfer,” Flan-T5 pays attention to the right keywords 8 out of 10 times.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Cross Attention for Correct Answer &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct_encoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct_encoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct_encoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-unraveling-social-reasoning-in-llms/correct_encoder.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Encoder Attention for Correct Answer &lt;/div&gt; &lt;p&gt;Therefore, for more straightforward questions, namely, questions that have one straightforward answer, Flan-T5 can find the right keywords that lead it to the answer (i.e. the correct questions). On the other hand, for more challenging questions that require paying attention to specific keywords and reasoning from the perspective of a character (recall the Sally-Anne Test), Flan-T5 struggles more, with more variance between what it pays attention to and doesn’t (e.g. paper but not pen).&lt;/p&gt; &lt;p&gt;In addition, since Flan-T5 pays attention to the right keywords most of the time for the questions it overinfers on, this suggests that there’s some aspect of reasoning that’s not being captured via our attention visualizations, and that this reasoning isn’t performing that well.&lt;/p&gt; &lt;p&gt;Notably, something interesting to note is that for all of the examples, by the third encoder layer, on average, Flan-T5 doesn’t change its encodings, and for the cross attention, the attention remains consistent across all layers and (most) attention heads. Therefore, it seems like most of the “reasoning” is being performed in the encoding stage.&lt;/p&gt; &lt;p&gt;Therefore, some of our next steps are understanding how removing attention heads in a smaller affects the model’s ability to reason, given the large number of heads and layers (24 x 32) in Flan-T5-xxl . We visualized each encoder head for one layer, but this doesn’t immediately lend itself to an intuitive interpretation.&lt;/p&gt; &lt;h2 id=&quot;discussion&quot;&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/h2&gt; &lt;p&gt;Our work concentrated on analyzing two categories of errors and proposing solutions to address them. The two error types are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Errors originating from the flawed construction of the Social IQA dataset.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Errors where Flan-T5’s responses do not align with social commonsense.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;problems-with-social-iqa-1&quot;&gt;&lt;strong&gt;Problems with Social IQA&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Our analysis of Type 1 errors in the Social IQA dataset revealed significant issues. In examining n=350 incorrectly answered questions, we found that 65% had problems with their context, question, or answer choices. Additionally, 54.4% of these errors had multiple reasonable answers, 23.7% lacked any reasonable answer, and 14.0% seemed to have mislabeled correct answers. This indicates a substantial number of misleading answer choices in the Social IQA questions.&lt;/p&gt; &lt;p&gt;This issue partly stems from the dataset’s construction, which involved assigning crowdsourced workers tasks of writing positive answers for each question and sourcing negative answers from “different but related” questions. This approach likely contributed to the high error rate.&lt;/p&gt; &lt;p&gt;Since Social IQA is so frequently used in evaluating model performances and transfer learning tasks, the challenge is to identify and remove these flawed questions. Although our attempt to do this was unsuccessful due to time and budget constraints, we believe it is feasible. Many evaluations of large language models (LLMs) use crowdsourced multiple-choice questions, so a pruning tool to ensure benchmark reliability would be highly beneficial beyond the task of social commonsense reasoning.&lt;/p&gt; &lt;p&gt;Pruning the Social IQA dataset to eliminate most erroneous questions would also provide an opportunity to reassess older models.&lt;/p&gt; &lt;p&gt;Overall, our analysis of Type 1 errors underscores the need for caution in crowdsourcing benchmark questions. While crowdsourcing likely still remains the best solution for creating large benchmark sets, a pruning tool is essential to maintain the reliability of such datasets.&lt;/p&gt; &lt;p&gt;On the other hand, our analysis of Type 2 errors suggests that LLMs still might not match the social reasoning skills of humans for more complex scenarios. For simpler questions, they can often find a single keyword that informs their answer, while for more complex questions, they often miss important phrases and can’t necessarily think from another person’s perspective. For instance, recall how questions containing the keyword “other” result in Flan-T5 having considerably lower accuracy.&lt;/p&gt; &lt;h2 id=&quot;main-limitations&quot;&gt;&lt;strong&gt;Main Limitations&lt;/strong&gt;&lt;/h2&gt; &lt;p&gt;The primary limitations of our study are rooted in its scope and methodology. Firstly, we focused exclusively on a single model, Flan-T5, which may limit the generalizability of our findings. Additionally, our analysis was based on a relatively small sample size of n=350, and it involved only two coders. For a more robust and comprehensive evaluation, increasing the number of coders would be beneficial, particularly to assess intercoder reliability. Furthermore, implementing measures to mitigate recognition bias during the tagging process would enhance the validity of our results.&lt;/p&gt; </content> </entry> <entry> <title>Comparing data augmentation using VAEs and denoising-VAEs for limited noisy datasets</title> <link href="https://deep-learning-mit.github.io/blog/2023/denoisingVAE/"/> <updated>2023-11-11T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/denoisingVAE</id> <content type="html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;One of the significant challenges in this evolving landscape of machine learning is the prevalance of limited and noisy datasets. Traditional models and downstream tasks such as classification often struggle with such datasets, leading to suboptimal performance and a lack of generalizability.&lt;/p&gt; &lt;p&gt;Could this be tackled using auto-encoders, specifically, Denoising Autoencoders (DAE) and Variational Autoencoders (VAE)? Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer (KL divergence) that encourages this noise injection. But what if we could combine these strengths?&lt;/p&gt; &lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;p&gt;Denoising autoencoders (DAE)&lt;d-cite key=&quot;vincent2008extracting&quot;&gt;&lt;/d-cite&gt;, are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE)&lt;d-cite key=&quot;kingma2014autoencoding&quot;&gt;&lt;/d-cite&gt; are trained with noise injected in their stochastic hidden layer, with a regularizer (KL divergence) that encourages this noise injection. Denoising Variational Autoencoders (DVAEs) are an extension of the traditional variational autoencoder (VAE). The motivation for delving into the realm of DVAEs stems from a critical need - the ability to effectively interpret and utilize limited, noisy data. They merge the robustness of DAEs in handling noisy inputs with the generative prowess of VAEs. As highlighted in the research paper “Denoising Criterion for Variational Auto-Encoding Framework”&lt;d-cite key=&quot;denoisingVAE&quot;&gt;&lt;/d-cite&gt;, integrating a denoising criterion into the VAE framework refines the robustness of learned representations, thereby enhancing the model’s generalization ability over various tasks.&lt;/p&gt; &lt;p&gt;VAEs, known for their generative capabilities, introduce noise at the hidden layer level, potentially offering a means to augment limited datasets&lt;d-cite key=&quot;saldanha2022data&quot;&gt;&lt;/d-cite&gt;. On the other hand, DVAEs, an innovative extension of VAEs, introduce perturbation to input data, promising a more robust feature extraction and create additional, realistic augmentations of the data. Our aim here is to comprehensively analyze and contrast the efficacy of VAEs and DVAEs in augmenting such datasets. We hypothesize that while VAEs can offer some level of data enhancement, DVAEs, with their inherent denoising capability, might prove superior in extracting more reliable and robust features from noisy datasets.&lt;/p&gt; &lt;h2 id=&quot;research-problem-statement&quot;&gt;Research Problem Statement&lt;/h2&gt; &lt;p&gt;The first aspect of this research is to explore the dual functionality of DVAEs — their ability to denoise input data while concurrently learning a generative model of the data distribution. The next aspect is to to compare the performance of DVAEs against traditional VAEs in i) learning robust latent representations, and ii) in downstream classification tasks with richer varied datasets by utilising data augmentation aspect of these generative models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning Robust representation and Generating Synthetic data using DVAEs:&lt;/strong&gt; Can DVAEs dual capability of denoising input data and learning a generative model of the data distribution simultaneously be exploited to effectively learn robust representations from limited and noisy datasets and utilized to generate additional synthetic data (augmented dataset)? How does it compare to using traditional VAEs?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance Enhancement for downstream tasks:&lt;/strong&gt; How does the DVAE-generated synthetic data impact the performance metrics of downstream classification tasks? Compare performance metrics with traditonal VAE for different noise levels in test datasets.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;h3 id=&quot;dataset-selection-and-preparation&quot;&gt;Dataset Selection and Preparation&lt;/h3&gt; &lt;p&gt;The Fashion-MNIST dataset, which includes 60,000 training images, is selected for the experiments mentioned above. To simulate a limited data environment, a subset of 5,000 images is randomly selected from the dataset. We also create a noisy version of the training dataset to understand the efficacy in scenarios when clean input data is not available.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/fashionMNISTSamples-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/fashionMNISTSamples-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/fashionMNISTSamples-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/fashionMNISTSamples.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 1. Sample Fashion-MNIST images&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/noisyFashionMNISTSamples-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/noisyFashionMNISTSamples-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/noisyFashionMNISTSamples-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/noisyFashionMNISTSamples.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 2. Artificially Corrupted(Noised) Fashion-MNIST images&lt;/p&gt; &lt;h3 id=&quot;vae-and-dvae---architecture-and-hyperparameters&quot;&gt;VAE and DVAE - Architecture and Hyperparameters&lt;/h3&gt; &lt;p&gt;The VAE and DVAE architecture is similar and differ only in the sense that DVAE adds noise to input images before passing it to encoder.&lt;/p&gt; &lt;p&gt;The encoder comprises two hidden layers, each with 128 neurons. The input size is flattened to 28 * 28 dimensions. Each hidden layer in the encoder is followed by a ReLU activation function. The encoder’s output is connected to two separate layers: one for generating the mean (µ) and the other for the logarithm of the variance (log-variance), both projecting to a 4-dimensional latent space (z_dims).&lt;/p&gt; &lt;p&gt;On the decoding side, the architecture starts with the latent space and expands through a similar structure of two hidden layers, each with 128 neurons and ReLU activation functions. The final output layer reconstructs the original input size of 28 * 28 dimensions and applies a Sigmoid activation function.&lt;/p&gt; &lt;p&gt;This VAE/DVAE employs a reconstruction loss using the binary cross-entropy between the input and its reconstruction, and a regularization term(KL-Divergence) derived from the latent space to enforce a probabilistic distribution. Each model is trained for 60 epochs with batch size 128.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt; &lt;p&gt;For augmenting the dataset, we generate 2 newer samples or each input image. First, the image is passed through the encoder part of VAE/DVAE and then sample a latent representation vector around the obtained latent representaion - mean and std.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_data_augmentation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_data_augmentation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_data_augmentation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_data_augmentation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 3. Example: VAE Data Augmentation&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_data_augmentation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_data_augmentation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_data_augmentation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_data_augmentation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 4. Example: DVAE Data Augmentation&lt;/p&gt; &lt;h3 id=&quot;classification-networkcnn-architecture&quot;&gt;Classification Network(CNN) Architecture&lt;/h3&gt; &lt;p&gt;The Classification Network(CNN) architecture is comprised of a series of convolutional, activation, pooling, and fully connected layers. Initially, it features a convolutional layer with 1 input channel and 32 output channels, using 3x3 kernels, stride of 1, and padding of 1 with ‘reflect’ mode, followed by an ReLU activation function. This is succeeded by another convolutional layer that increases the depth to 64 filters, maintaining the same kernel size, stride, and padding, accompanied by the same activation function. Subsequently, a max pooling layer with a 2x2 kernel reduces the spatial dimensions of the feature maps, highlighting significant features. The data is then flattened, resulting in a feature vector with a length of 64 * 14 * 14, which feeds into a series of three linear layers, each with 128 units, interspersed with the activation function. This sequence of fully connected layers is designed to capture complex relationships in the data. Finally, the architecture has an output linear layer that maps to the number of outputs (num_outputs=10).&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;image_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;act_cls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;reflect&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;act_cls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;reflect&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;act_cls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;act_cls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;act_cls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;act_cls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;Here, we utilize the standard Fashion-MNIST dataset for our analysis. Initially, we train the VAE/DVAE network on a subset of 5,000 samples from the training dataset over 60 epochs. Following this, we employ the VAE/DVAE networks to generate synthetic data, leveraging the learned latent space representation for data augmentation purposes. The performance of the augmented datasets is then evaluated using the previously described CNN architecture for classification tasks.&lt;/p&gt; &lt;h3 id=&quot;vae-dvae-performance&quot;&gt;VAE-DVAE Performance&lt;/h3&gt; &lt;p&gt;DVAE’s training loss closely tracks the VAE’s loss throughout training. This is interesting because the DVAE is dealing with additional artificial noise, yet it performs on par with the standard VAE. The fact that the DVAE does not exhibit a significantly higher loss than the VAE might suggest that it is effectively denoising the data and learning a robust representation, despite the additional noise.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/TrainingLossOriginal-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/TrainingLossOriginal-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/TrainingLossOriginal-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/TrainingLossOriginal.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 5. Training Loss: VAE v/s DVAE&lt;/p&gt; &lt;h3 id=&quot;latent-space-visualization&quot;&gt;Latent Space Visualization&lt;/h3&gt; &lt;p&gt;Here, we are visualizing the latent space of VAE and DVAE, a high-dimensional space where each dimension represents certain features learned by the model from the data. For this, we plot a 10x10 grid of images where each image in the grid is generated by the model by varying the values in two chosen latent dimensions (i and j), while keeping the other dimensions set to zero. This helps in understanding the effect of each latent dimension on the generated output.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_LatentSpace-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_LatentSpace-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_LatentSpace-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAE_LatentSpace.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 6. VAE Latent Space Visualization&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_LatentSpace-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_LatentSpace-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_LatentSpace-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAE_LatentSpace.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 7. DVAE Latent Space Visualization&lt;/p&gt; &lt;p&gt;The lack of visible differences in the latent space structures of both VAE and DVAE indicates that the two models are learning similar representations. To delve into these nuances, we assess the effectiveness of augmented data (created using these learned latent spaces) in a subsequent classification task.&lt;/p&gt; &lt;h3 id=&quot;classification-performance&quot;&gt;Classification Performance&lt;/h3&gt; &lt;p&gt;To delve into the efficacy of VAE and DVAE in augmenting datasets for downstream image classification tasks, we trained a CNN on a limited subset of the Fashion-MNIST dataset to establish a baseline. Subsequently, we generated synthetic data using both VAE and DVAE, aiming to enrich the training dataset and observe the resultant impact on the CNN’s performance. This is crucial considering the initial constraint of limited training data to start with. We used Fashion-MNIST test dataset, which includes 10,000 test images, for evaluating the performance of learned CNN network.&lt;/p&gt; &lt;p&gt;We also tested robustness of these augmented datasets against varying levels of noise (artifically added to test dataset), simulating real-world conditions where test data often includes such imperfections, arising because of the limitations in measurement tools.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/LimitedDatasetLC-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/LimitedDatasetLC-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/LimitedDatasetLC-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/LimitedDatasetLC.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 8. CNN Learning Curve for Limited Dataset&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAEAugmentedLC-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAEAugmentedLC-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAEAugmentedLC-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/VAEAugmentedLC.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 9. CNN Learning Curve for VAE Augmented Dataset&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAEAugmentedLC-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAEAugmentedLC-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAEAugmentedLC-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-11-denoisingVAE/DVAEAugmentedLC.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 10. CNN Learning Curve for DVAE Augmented Dataset&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset Type \ Noise Level&lt;/th&gt; &lt;th&gt;No Noise&lt;/th&gt; &lt;th&gt;2.5% Noise&lt;/th&gt; &lt;th&gt;5% Noise&lt;/th&gt; &lt;th&gt;7.5% Noise&lt;/th&gt; &lt;th&gt;10% Noise&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Limited Dataset&lt;/td&gt; &lt;td&gt;83.56%&lt;/td&gt; &lt;td&gt;83.39%&lt;/td&gt; &lt;td&gt;83.11%&lt;/td&gt; &lt;td&gt;82.33%&lt;/td&gt; &lt;td&gt;81.75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;VAE Augmented Dataset&lt;/td&gt; &lt;td&gt;84.18%&lt;/td&gt; &lt;td&gt;84.03%&lt;/td&gt; &lt;td&gt;83.57%&lt;/td&gt; &lt;td&gt;82.68%&lt;/td&gt; &lt;td&gt;81.43%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DVAE Augmented Dataset&lt;/td&gt; &lt;td&gt;85.32%&lt;/td&gt; &lt;td&gt;84.98%&lt;/td&gt; &lt;td&gt;84.67%&lt;/td&gt; &lt;td&gt;83.98%&lt;/td&gt; &lt;td&gt;82.59%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h4 id=&quot;artificially-corrupted-fashion-mnist&quot;&gt;Artificially corrupted Fashion-MNIST&lt;/h4&gt; &lt;p&gt;Here, we deliberately introduced artifical noise to the standard Fashion-MNIST dataset to effectively simulate the real-world scenario where training data is not cleaned and is often noisy and imperfect. Such conditions often pose significant challenges in learning effective representations, making our approach highly relevant for understanding the adaptability and efficiency of VAE and DVAE models in handling noisy data. This way we expose the model and train it on a variety of noise patterns while forcing it to reconstruct the original noised image. The model will learn to effectively separate noise from the signal and will be less likely to overfit to the ‘clean’ aspects of the training data and can thus perform better on unseen, noisy data. This improves the generalization capabilities of the model making it more suitable for practical applications.&lt;/p&gt; &lt;p&gt;Here, we generated synthetic data using both VAE and DVAE which are trained on artifically corrupted Fashion-MNIST dataset. We then compare the performance of CNN network for three datasets - Limited Noisy Dataset with no augmentation, VAE Augmented dataset and DVAE Augmented Dataset, where representations are learned using the noisy training set. Consistent with our earlier methodology, we further evaluated the robustness of CNNs trained with these datasets by testing them against varying levels of noise in the test dataset.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset Type \ Noise Level&lt;/th&gt; &lt;th&gt;No Noise&lt;/th&gt; &lt;th&gt;2.5% Noise&lt;/th&gt; &lt;th&gt;5% Noise&lt;/th&gt; &lt;th&gt;7.5% Noise&lt;/th&gt; &lt;th&gt;10% Noise&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Limited Noisy Dataset&lt;/td&gt; &lt;td&gt;83.77%&lt;/td&gt; &lt;td&gt;83.79%&lt;/td&gt; &lt;td&gt;83.61%&lt;/td&gt; &lt;td&gt;83.36%&lt;/td&gt; &lt;td&gt;82.98%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;VAE Augmented Dataset&lt;/td&gt; &lt;td&gt;85.24%&lt;/td&gt; &lt;td&gt;84.99%&lt;/td&gt; &lt;td&gt;84.62%&lt;/td&gt; &lt;td&gt;84.04%&lt;/td&gt; &lt;td&gt;83.20%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DVAE Augmented Dataset&lt;/td&gt; &lt;td&gt;85.48%&lt;/td&gt; &lt;td&gt;85.38%&lt;/td&gt; &lt;td&gt;85.10%&lt;/td&gt; &lt;td&gt;84.89%&lt;/td&gt; &lt;td&gt;84.58%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt; &lt;p&gt;Here are the key findings from our research:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Learning from Augmented Data:&lt;/strong&gt; We observed that the CNN trained with data augmented by both VAE and DVAE demonstrated improved accuracy and generalization capabilities, especially when compared to the CNN trained on a limited dataset. This underscores the effectiveness of generative models in enriching training datasets, leading to more robust learning.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Superiority of DVAE in Handling Noise:&lt;/strong&gt; The CNN trained with DVAE augmented data consistently outperformed the one trained with traditional VAE augmented data in tests involving noisy conditions. This aligns perfectly with our research hypothesis about the dual functionality of DVAEs — not only do they learn a generative model of the data distribution but also excel in denoising input data.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Robustness to Varied Noise Levels:&lt;/strong&gt; A crucial aspect of our research was evaluating the performance of augmented datasets under various noise levels. The augmented datasets, especially those generated by DVAEs, maintained consistent performance across different noise conditions. This suggests that the models have not only learned the essential features of the data but are also adept at filtering out noise.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In downstream classification tasks, DVAE-generated synthetic data improved performance metrics, surpassing those achieved with traditional VAE-generated data, particularly in tests with varied noise levels. This validates our hypothesis and highlights the potential of DVAEs in real-world applications where data is limited and data quality is a critical factor.&lt;/p&gt; &lt;p&gt;The next steps for this research could be to focus on expanding the types of noise tested in our experiments to evaluate the adaptability and robustness of DVAEs in a broader range of real-world scenarios. We could conduct more comprehensive data augmentation experiments to delve deeper into the capabilities of DVAEs in enhancing neural network learning and generalization.&lt;/p&gt; </content> </entry> <entry> <title>Emoji3Vec</title> <link href="https://deep-learning-mit.github.io/blog/2023/transformer-elo-prediction/"/> <updated>2023-11-10T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/transformer-elo-prediction</id> <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;In machine learning, models often create or learn internal representations for the inputs they are given. For instance, an image might become a vector containing the RGB data for every pixel. These internal representations are then processed and transformed until the model finally translates its representation into the desired output form (via softmax over all output possibilities, for example).&lt;/p&gt; &lt;p&gt;The lower dimensional internal representations, known as embeddings, can often carry semantic meaning which can help us understand the data better. Inspired by word2vec, a project for learning embeddings for words, we attempt to learn embeddings for emojis that are semantically interpretable. Learning accurate representations is important for downstream tasks, for example: sentiment analysis and other kinds of classification run better with useful embeddings.&lt;/p&gt; &lt;h1 id=&quot;background-and-related-work&quot;&gt;Background and Related Work&lt;/h1&gt; &lt;p&gt;Although similar ideas have been explored in the past, we felt that there was still a gap in prior research: specifically, we wanted to create a lightweight model that still learned emoji embeddings directly from data and context.&lt;/p&gt; &lt;p&gt;First, it is important to mention the influential and well known &lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;, commonly referred to as word2vec &lt;d-cite key=&quot;mikolov2013word&quot;&gt;&lt;/d-cite&gt;. Word2vec was trained on a massive dataset of around 6 billion words, and was able to produce some very accurate embeddings that were proven to be useful in downstream tasks. For instance, doing the following arithmetic on the embeddings associated with each word produced: King - Man + Woman = Queen. This was an incredible result and inspired much work in the NLP domain in the following years.&lt;/p&gt; &lt;p&gt;In 2016, a paper called &lt;a href=&quot;https://arxiv.org/pdf/1609.08359.pdf&quot;&gt;emoji2vec: Learning Emoji Representations from their Description&lt;/a&gt; &lt;d-cite key=&quot;eisner2016emoji&quot;&gt;&lt;/d-cite&gt; was published. As the name suggests, this paper sought to produce embeddings for emojis to be used in the same vector space as the word2vec embeddings, and attempted to do it by using emoji descriptions. The researchers trained their model with baseline embeddings taken directly from summing the word embeddings for each word in every emoji’s description. For instance, the embedding for “😍” began as the sum of the word embeddings (taken from word2vec) of: “smiling” + “face” + “with” + “heart” + “eyes”. The main benefit of this approach was a strong baseline that could be established without any training data. Recently, in 2021, another paper called &lt;a href=&quot;https://bashthebuilder.github.io/files/Emojional.pdf&quot;&gt;Emojional: Emoji Embeddings&lt;/a&gt; &lt;d-cite key=&quot;barry2021emojional&quot;&gt;&lt;/d-cite&gt; was published that extended this approach, adding in additional words (that are related, as judged by Google News) to each baseline embedding. For instance, “✊” was set to be the result of: “raised fist” + “resistance” + “black lives matter” + …&lt;/p&gt; &lt;p&gt;After considering the above papers, we decided to create a model that would train similarly to word2vec (using naturally sourced data, and from context as opposed to a description) that also was able to produce useful embeddings on smaller amounts of data/memory/training time. Specifically, we felt that the descriptions would err when emojis began to mean different things than they are described as. For instance, the skull emoji is perhaps more often used to indicate embarassment or disagreement than actual death or skulls. This is addressed somewhat in the 2021 Emojional paper, but that one is very limited by the exact words it puts into each emoji’s embedding, and is less adaptable to new meanings. Further, we felt that there was value in creating a more lightweight model that was still able to produce meaningful representations, both to simply be easier to train and run and also to perhaps find optimizations that wouldn’t have been found if we had the option of just training on a larger set of data/training for a longer time.&lt;/p&gt; &lt;h1 id=&quot;methods-and-results&quot;&gt;Methods and Results&lt;/h1&gt; &lt;p&gt;We trained two sets of emoji embeddings to map emojis to the same 300-dimensional space as the one FastText uses for its word embeddings. The first was trained on a set of emoji descriptions, with the intention to learn emoji embeddings that reflect the literal appearances of each emoji. We closely follow the methodology as described in the emoji2vec paper to use as a baseline. The second was trained on a set of emoji-containing tweets, with the intention to learn emoji embeddings that reflect how they’re used online.&lt;/p&gt; &lt;h2 id=&quot;training-emoji-embeddings-with-descriptions&quot;&gt;Training Emoji Embeddings with Descriptions&lt;/h2&gt; &lt;h3 id=&quot;data-cleaning&quot;&gt;Data Cleaning&lt;/h3&gt; &lt;p&gt;We started with a &lt;a href=&quot;https://github.com/pwiercinski/emoji2vec_pytorch/blob/master/data/raw_training_data/emoji_joined.txt&quot;&gt;dataset&lt;/a&gt; of emoji descriptions from the Unicode emoji list. After cleaning, we were left with about 6000 descriptive phrases for 1661 emojis within a Python dictionary mapping emojis to various corresponding descriptions. Examples of entries include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;‘🐏’: {‘horn’, ‘horns’, ‘male’, ‘ram’, ‘sheep’}&lt;/li&gt; &lt;li&gt;‘🆘’: {‘distress signal’, ‘emergency’, ‘sos’, ‘squared sos’}&lt;/li&gt; &lt;li&gt;‘👷’: {‘builder’, ‘construction worker’, ‘face with hat’, ‘safety helmet’}&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One detail is that we had to generate a bijective mapping between emojis and integers for model training. We encourage those attempting similar projects to save this mapping (in a pickle file, for example) for later use. Leon was very sad when he lost this mapping and couldn’t make sense of his first trained model’s outputted embeddings.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/project-vis-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/project-vis-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/project-vis-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/project-vis.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;a visualization of how we cleaned our data, from an example of a tweet&lt;/em&gt;&lt;/p&gt; &lt;h3 id=&quot;generating-training-and-test-data&quot;&gt;Generating Training and Test Data&lt;/h3&gt; &lt;p&gt;With a representation learning framework in mind, we randomly generated positive and negative descriptions for each emoji. We defined an emoji’s positive samples as descriptions that truly corresponded to the emoji, and we defined its negative samples as other descriptions in the dataset that weren’t used to describe the emoji. Guided by the emoji2vec paper, we generated positive and negative samples in a 1:1 ratio.&lt;/p&gt; &lt;h3 id=&quot;model-training&quot;&gt;Model Training&lt;/h3&gt; &lt;p&gt;After generating positive and negative samples, we used a pretrained FastText model to calculate the average of the embeddings of each word in each description. Put mathematically, if we let the sequence of words in a description be \(w_1, w_2, \dots, w_k\), the set of all strings be \(\mathcal{W}\), and the FastText model be expressed as a mapping \(f: \mathcal{W} \mapsto \mathbb{R}^{300}\), we calculated our description embeddings as&lt;/p&gt; \[\frac{1}{k}\sum_{i=1}^kf(w_i).\] &lt;p&gt;This is a notable deviation from the methodology as described in the emoji2vec paper. Instead of using word2vec embeddings, we chose FastText because it uses sub-word tokenization and thus supports out-of-vocabulary strings as input. We also averaged the description embeddings instead of simply taking a summation to normalize for description length.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#creates a dictionary mapping descriptions to avg. word embeddings descr_to_embedding = dict() for descr in all_descriptions: word_lst = descr.split(&apos; &apos;) #split description into list of words embed_lst = [] for i in range(len(word_lst)): #repl. words by their embeddings embed_lst.append(torch.tensor(ft[word_lst[i]])) avg_embedding = torch.mean(torch.stack(embed_lst, dim=0), dim=0) #take mean over embeddings descr_to_embedding[descr] = avg_embedding &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;We again followed the emoji2vec training methodology. For every emoji embedding \(x_i\) and description embedding \(v_i\), the authors model \(\sigma(x_i^T v_j)\) as the probability of the description matching with the emoji, where \(\sigma\) is the sigmoid function. Then our model minimizes the binary cross-entropy loss function&lt;/p&gt; \[\mathcal{L}(x_i,v_j,y_{ij}) = -\log(\sigma(y_{ij}x_i^T v_j + (1-v_{ij})x_i^T v_j))\] &lt;p&gt;where \(y_{ij}\) is 1 when \(v_j\) is a positive sample and 1 otherwise.&lt;/p&gt; &lt;p&gt;The authors don’t describe the exact model architecture used to learn the emoji embeddings, so we likely also deviate in methodology here. Our model is very simple: on some input emoji \(x_i\), we pass it through an nn.Embedding() module, compute \(\sigma(x_i^T v_j)\), and pass it to nn.BCELoss(). This way, the only learnable parameters in the model are in nn.Embedding(), and model training is as efficient as possible.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# the main model class # follows the Emoji2Vec training class EmojiDict(nn.Module): def __init__(self, n_emojis): # n_emojis: the number of emojis we&apos;re learning representations of super().__init__() self.embedding = nn.Embedding( num_embeddings = n_emojis, embedding_dim = 300 # size of word2vec embedding ) self.sigmoid = nn.Sigmoid() def forward(self, x, sample): # x: a batch of emoji indices, shape (B, ) # sample: a batch of avg&apos;d embeddings, shape (B, 300) x = self.embedding(x) # performing a batched dot product x = torch.unsqueeze(x, dim=1) #(B x 1 x 300) sample = torch.unsqueeze(sample, dim=2) #(B x 300 x 1) result = torch.bmm(x, sample) #(B x 1 x 1) result = torch.flatten(result) #(B, ) result = self.sigmoid(result) #should output probabilities return result #should be shape (B, ) &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;t-sne-on-learned-embeddings&quot;&gt;t-SNE on Learned Embeddings&lt;/h3&gt; &lt;p&gt;We trained the model for 60 epochs over a 80-20 train-test split of 250 positive and 250 negative samples for each emoji. We used an Adam optimizer with the default parameters, and model training took roughly an hour. The model achieved 0.19 logloss and 0.98 accuracy on a validation set.&lt;/p&gt; &lt;p&gt;After the model was trained, we took emoji embedding weights from the model’s nn.Embedding() module and projected them down to two dimensions using t-SNE.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojidict-triplefit-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojidict-triplefit-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojidict-triplefit-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojidict-triplefit.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We can see that the model is excellent at grouping emojis that have similar appearances. Nearly all the faces are in the top-left, the zodiac symbols are in the bottom-left, the flags are at the bottom, the foods are on the right, the modes of transportation are in the top-right… the list can keep going. While there are some random emojis scattered about, similar emojis generally are similar in embedding space as well.&lt;/p&gt; &lt;h3 id=&quot;emoji-emoji-similarities&quot;&gt;Emoji-Emoji Similarities&lt;/h3&gt; &lt;p&gt;To confirm this idea quantitatively, we can fix individual emojis and look at its nearest neighbors in embedding space with cosine distance.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Emoji&lt;/th&gt; &lt;th&gt;1-NN&lt;/th&gt; &lt;th&gt;2-NN&lt;/th&gt; &lt;th&gt;3-NN&lt;/th&gt; &lt;th&gt;4-NN&lt;/th&gt; &lt;th&gt;5-NN&lt;/th&gt; &lt;th&gt;6-NN&lt;/th&gt; &lt;th&gt;7-NN&lt;/th&gt; &lt;th&gt;8-NN&lt;/th&gt; &lt;th&gt;9-NN&lt;/th&gt; &lt;th&gt;10-NN&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;😍&lt;/td&gt; &lt;td&gt;💖&lt;/td&gt; &lt;td&gt;😻&lt;/td&gt; &lt;td&gt;😄&lt;/td&gt; &lt;td&gt;😀&lt;/td&gt; &lt;td&gt;😚&lt;/td&gt; &lt;td&gt;💟&lt;/td&gt; &lt;td&gt;😘&lt;/td&gt; &lt;td&gt;😊&lt;/td&gt; &lt;td&gt;😽&lt;/td&gt; &lt;td&gt;💑&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;😀&lt;/td&gt; &lt;td&gt;😄&lt;/td&gt; &lt;td&gt;😊&lt;/td&gt; &lt;td&gt;😃&lt;/td&gt; &lt;td&gt;🙂&lt;/td&gt; &lt;td&gt;😑&lt;/td&gt; &lt;td&gt;😁&lt;/td&gt; &lt;td&gt;😸&lt;/td&gt; &lt;td&gt;🤗&lt;/td&gt; &lt;td&gt;😆&lt;/td&gt; &lt;td&gt;🤧&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;💀&lt;/td&gt; &lt;td&gt;☠&lt;/td&gt; &lt;td&gt;🆎&lt;/td&gt; &lt;td&gt;🌫&lt;/td&gt; &lt;td&gt;🐁&lt;/td&gt; &lt;td&gt;⛓&lt;/td&gt; &lt;td&gt;⛸&lt;/td&gt; &lt;td&gt;🌮&lt;/td&gt; &lt;td&gt;🦅&lt;/td&gt; &lt;td&gt;⚖&lt;/td&gt; &lt;td&gt;🐙&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;🚀&lt;/td&gt; &lt;td&gt;🛰&lt;/td&gt; &lt;td&gt;👽&lt;/td&gt; &lt;td&gt;🚡&lt;/td&gt; &lt;td&gt;🛳&lt;/td&gt; &lt;td&gt;📡&lt;/td&gt; &lt;td&gt;🚢&lt;/td&gt; &lt;td&gt;📋&lt;/td&gt; &lt;td&gt;🚎&lt;/td&gt; &lt;td&gt;🆚&lt;/td&gt; &lt;td&gt;🛥&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We see here that the nearest neighbors also generally make sense. 😍’s nearest neighbors all involve love or positive emotions, and 🚀’s neighbors are generally about space or modes of transport. Interestingly, only 💀’s first neighbor seems remotely similar to it. We believe that this is just because death is a mostly unrepresented theme in emojis.&lt;/p&gt; &lt;h3 id=&quot;word-emoji-similarities&quot;&gt;Word-Emoji Similarities&lt;/h3&gt; &lt;p&gt;Since we trained emoji embeddings into the same space as the FastText word embeddings, we can also look at the nearest emoji neighbors to any English word!&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Word&lt;/th&gt; &lt;th&gt;1-NN&lt;/th&gt; &lt;th&gt;2-NN&lt;/th&gt; &lt;th&gt;3-NN&lt;/th&gt; &lt;th&gt;4-NN&lt;/th&gt; &lt;th&gt;5-NN&lt;/th&gt; &lt;th&gt;6-NN&lt;/th&gt; &lt;th&gt;7-NN&lt;/th&gt; &lt;th&gt;8-NN&lt;/th&gt; &lt;th&gt;9-NN&lt;/th&gt; &lt;th&gt;10-NN&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;happy&lt;/td&gt; &lt;td&gt;😃&lt;/td&gt; &lt;td&gt;😺&lt;/td&gt; &lt;td&gt;😌&lt;/td&gt; &lt;td&gt;😹&lt;/td&gt; &lt;td&gt;🏩&lt;/td&gt; &lt;td&gt;😊&lt;/td&gt; &lt;td&gt;💛&lt;/td&gt; &lt;td&gt;😂&lt;/td&gt; &lt;td&gt;😞&lt;/td&gt; &lt;td&gt;😁&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;sad&lt;/td&gt; &lt;td&gt;😔&lt;/td&gt; &lt;td&gt;😭&lt;/td&gt; &lt;td&gt;😒&lt;/td&gt; &lt;td&gt;🙁&lt;/td&gt; &lt;td&gt;😟&lt;/td&gt; &lt;td&gt;😞&lt;/td&gt; &lt;td&gt;🙍&lt;/td&gt; &lt;td&gt;😢&lt;/td&gt; &lt;td&gt;😁&lt;/td&gt; &lt;td&gt;😯&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;lit&lt;/td&gt; &lt;td&gt;🚨&lt;/td&gt; &lt;td&gt;🕎&lt;/td&gt; &lt;td&gt;🌆&lt;/td&gt; &lt;td&gt;🔦&lt;/td&gt; &lt;td&gt;📭&lt;/td&gt; &lt;td&gt;🎇&lt;/td&gt; &lt;td&gt;🕯&lt;/td&gt; &lt;td&gt;💫&lt;/td&gt; &lt;td&gt;🏥&lt;/td&gt; &lt;td&gt;💡&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;bitcoin&lt;/td&gt; &lt;td&gt;💛&lt;/td&gt; &lt;td&gt;🤑&lt;/td&gt; &lt;td&gt;🎮&lt;/td&gt; &lt;td&gt;💙&lt;/td&gt; &lt;td&gt;🌈&lt;/td&gt; &lt;td&gt;🤓&lt;/td&gt; &lt;td&gt;📱&lt;/td&gt; &lt;td&gt;📅&lt;/td&gt; &lt;td&gt;🐰&lt;/td&gt; &lt;td&gt;🍆&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Again, the nearest neighboring emojis generally make sense. Bitcoin’s nearest neighbors are considerably less accurate than the others. Since our nearest neighbors are much more accurate for other English words like “cryptocurrency”, we attribute this inaccuracy to FastText having poor embeddings for “Bitcoin”, which was much less popular word when FastText was trained (in 2015).&lt;/p&gt; &lt;p&gt;One thing to note from these nearest-neighbor tables is that embeddings trained with the emoji2vec method take words very literally. “🚀” is related to space and transportation, and “lit” is related to things that literally light up. As such, these embeddings won’t adjust to semantic changes in emojis as slang develops and people become increasingly clever in their emoji use.&lt;/p&gt; &lt;h2 id=&quot;training-emoji-embeddings-with-twitter-data&quot;&gt;Training Emoji Embeddings with Twitter Data&lt;/h2&gt; &lt;h3 id=&quot;data-cleaning-1&quot;&gt;Data Cleaning&lt;/h3&gt; &lt;p&gt;We started with a &lt;a href=&quot;https://www.kaggle.com/datasets/rexhaif/emojifydata-en?select=test.txt&quot;&gt;dataset&lt;/a&gt; of emoji-containing tweets. Motivated by the data cleaning done in the emojiSpace paper, we remove duplicate tweets, numbers, hashtags, links, emails, and mentions. Then, we extract the “context” words and emojis around each emoji with a window size of 4 in both directions and tokenize it. We cleaned only a subsample of the tweets due to constraints on memory and compute. Even so, after cleaning, we were left with about 272,000 contexts for 1251 emojis. Examples of contexts for the emoji 🤑 include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;(‘the’, ‘promotion’, ‘code’, ‘works’, ‘we’, ‘will’, ‘be’, ‘giving’)&lt;/li&gt; &lt;li&gt;(‘my’, ‘grind’, ‘all’, ‘week’, ‘i’, ‘ain’t’, ‘been’, ‘getting’)&lt;/li&gt; &lt;li&gt;(‘cash’, ‘in’, ‘on’, ‘sunday’, ‘thank’, ‘you’, ‘so’, ‘much’)&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;generating-training-and-test-data-1&quot;&gt;Generating Training and Test Data&lt;/h3&gt; &lt;p&gt;With a representation learning framework in mind, we randomly generated positive and negative descriptions for each emoji. We defined an emoji’s positive samples as descriptions that truly corresponded to the emoji, and we defined its negative samples as other descriptions in the dataset that weren’t used to describe the emoji. Guided by the emoji2vec paper, we generated positive and negative samples in a 1:1 ratio.&lt;/p&gt; &lt;p&gt;As in the earlier model, we randomly generated positive and negative contexts for each emoji. We defined an emoji’s positive samples equivalently as before, but this time we used the set of all contexts across all emojis as the set of negative examples. Doing this is obviously not ideal, but it provided a huge performance boost when generating data. Additionally, with such a large dataset, drawing a positive sample as a negative one happens relatively infrequently.&lt;/p&gt; &lt;h3 id=&quot;model-training-1&quot;&gt;Model Training&lt;/h3&gt; &lt;p&gt;The training method we used for this model was nearly identical to that of the first model, and similar to the Continuous Bag-of-Words (CBOW) method for training word2vec. For every context, we calculated the average of the individual word embeddings using FastText. Often, another emoji would be part of the context; such emojis would be passed into the nn.Embedding() module as well to produce an embedding to be passed into the average. The model architecture remained nearly identical, and continued using binary cross-entropy loss as our loss function.&lt;/p&gt; &lt;p&gt;Our model architecture differs somewhat from the original word2vec model, which uses a cross-entropy loss over the entire vocabulary of words as its loss function. While we may lose some expressivity by using binary cross-entropy instead, we believe that making this change made our model more lightweight and easier to train.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# the main model class # essentially a CBOW on emojis class EmojiCBOW(nn.Module): def __init__(self, n_emojis): # n_emojis: the number of emojis we&apos;re learning representations of super().__init__() self.embedding = nn.Embedding( num_embeddings = n_emojis, embedding_dim = 300 # size of word2vec embedding ) self.sigmoid = nn.Sigmoid() def forward(self, x, embeddings, emojis, masks): # x: a batch of emoji indices, shape (B, ) # embeddings: a batch of summed word embeddings from context, shape (B x 300) # emojis: a batch of in-context emoji indices, with -1 as a placeholder, shape (B x 8) # masks: a batch of masks for the relevant emoji indices, shape (B x 8) x = self.embedding(x) masks_unsqueezed = torch.unsqueeze(masks, dim=2) # get the dimensions right emoji_embeddings = self.embedding(emojis * masks) * masks_unsqueezed # apply embeddings to emojis w/ mask applied, (B x 8 x 300) emoji_embeddings = torch.sum(emoji_embeddings, dim=1) # sum acros embeddings, (B x 300) tot_embeddings = embeddings + emoji_embeddings # (B x 300) tot_embeddings = tot_embeddings / 8 # get avg embeddings, could help w/ numerical stability? # performing a batched dot product x = torch.unsqueeze(x, dim=1) #(B x 1 x 300) tot_embeddings = torch.unsqueeze(tot_embeddings, dim=2) #(B x 300 x 1) tot_embeddings = tot_embeddings.to(torch.float) / 8 result = torch.bmm(x, tot_embeddings) #(B x 1 x 1) result = torch.flatten(result) #(B, ) result = self.sigmoid(result) #should output target probabilities return result #should be shape (B, ) &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;t-sne-on-learned-embeddings-1&quot;&gt;t-SNE on Learned Embeddings&lt;/h3&gt; &lt;p&gt;We trained the model for 80 epochs over a 80-20 train-test split of 250 positive and 250 negative samples for each emoji. We used an Adam optimizer with the default parameters, and model training took roughly two hours. The model achieved 0.39 logloss and 0.79 accuracy on a validation set.&lt;/p&gt; &lt;p&gt;After the model was trained, we took emoji embedding weights from the model’s nn.Embedding() module and projected them down to two dimensions using t-SNE.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojitweets-transfer-40e-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojitweets-transfer-40e-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojitweets-transfer-40e-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-transformer-elo-prediction/emojitweets-transfer-40e.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The model does reasonably well at clustering similar emojis together; as before, the flags, faces, and numbers are close together in embedding space. However, the quality of this clustering is noticeably worse than it was in the baseline model. We attribute this to the quality of the dataset and to the increased difficulty in the learning task. The emoji descriptions were clean, precise, and informative; tweets are generally none of those three. Additionally, learning embeddings from contexts has historically required a lot of training data and compute to perform successfully. We, however, only had the compute and memory to sample 500 tweets per emoji, which is only a tiny sample from the massive distribution of possible contexts that may surround any given emoji. Producing emoji embeddings that outperform the baseline model would require much more training data and time than what Colab offers.&lt;/p&gt; &lt;p&gt;While these embeddings lose to the baseline embeddings in overall quality, they have certain properties that the baseline embeddings lack. Namely, since these embeddings were trained on a much more varied and organic dataset, they encode emoji use cases beyond what emojis literally mean. Specifically, they can learn from slang.&lt;/p&gt; &lt;h3 id=&quot;emoji-emoji-similarities-1&quot;&gt;Emoji-Emoji Similarities&lt;/h3&gt; &lt;p&gt;To illustrate this, we can look at the nearest neighbors of the same four emojis that were presented earlier. We narrow down our search to the top-200 most common emojis in our dataset because those were likely learned the best by our model.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Emoji&lt;/th&gt; &lt;th&gt;1-NN&lt;/th&gt; &lt;th&gt;2-NN&lt;/th&gt; &lt;th&gt;3-NN&lt;/th&gt; &lt;th&gt;4-NN&lt;/th&gt; &lt;th&gt;5-NN&lt;/th&gt; &lt;th&gt;6-NN&lt;/th&gt; &lt;th&gt;7-NN&lt;/th&gt; &lt;th&gt;8-NN&lt;/th&gt; &lt;th&gt;9-NN&lt;/th&gt; &lt;th&gt;10-NN&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;😍&lt;/td&gt; &lt;td&gt;🏆&lt;/td&gt; &lt;td&gt;💜&lt;/td&gt; &lt;td&gt;🎉&lt;/td&gt; &lt;td&gt;🇩🇪&lt;/td&gt; &lt;td&gt;💘&lt;/td&gt; &lt;td&gt;💖&lt;/td&gt; &lt;td&gt;👑&lt;/td&gt; &lt;td&gt;💞&lt;/td&gt; &lt;td&gt;💪&lt;/td&gt; &lt;td&gt;🇧🇷&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;😀&lt;/td&gt; &lt;td&gt;📚&lt;/td&gt; &lt;td&gt;😆&lt;/td&gt; &lt;td&gt;😏&lt;/td&gt; &lt;td&gt;🎉&lt;/td&gt; &lt;td&gt;😌&lt;/td&gt; &lt;td&gt;😫&lt;/td&gt; &lt;td&gt;🔗&lt;/td&gt; &lt;td&gt;🙂&lt;/td&gt; &lt;td&gt;⚡&lt;/td&gt; &lt;td&gt;🇫🇷&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;💀&lt;/td&gt; &lt;td&gt;😭&lt;/td&gt; &lt;td&gt;🍆&lt;/td&gt; &lt;td&gt;😓&lt;/td&gt; &lt;td&gt;🤤&lt;/td&gt; &lt;td&gt;💔&lt;/td&gt; &lt;td&gt;😩&lt;/td&gt; &lt;td&gt;🐥&lt;/td&gt; &lt;td&gt;😮&lt;/td&gt; &lt;td&gt;🐻&lt;/td&gt; &lt;td&gt;🍑&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;🚀&lt;/td&gt; &lt;td&gt;💸&lt;/td&gt; &lt;td&gt;🔹&lt;/td&gt; &lt;td&gt;💯&lt;/td&gt; &lt;td&gt;🎯&lt;/td&gt; &lt;td&gt;💵&lt;/td&gt; &lt;td&gt;2️⃣&lt;/td&gt; &lt;td&gt;👋&lt;/td&gt; &lt;td&gt;💰&lt;/td&gt; &lt;td&gt;😤&lt;/td&gt; &lt;td&gt;😎&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We see here that the nearest neighbors for 😍 and 😀 are noticeably less intuitive than the ones in the baseline model, though some still make sense. Interestingly, however, 💀 has become more associated with strong emotions like 😭 and 😩. This correlates with the online slang “I’m dead,” which expresses a strong (could be both positive or negative) emotional response to something. Additionally, 🚀 has become more associated with money, which correlates with the use of 🚀 to indicate a stock or asset going “to the moon.”&lt;/p&gt; &lt;h3 id=&quot;word-emoji-similarities-1&quot;&gt;Word-Emoji Similarities&lt;/h3&gt; &lt;p&gt;We can also observe this phenomenon in the cosine similarities between words and emojis. We use the same words as above, and again we narrow our nearest neighbors search to the top 200 most popular emojis.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Word&lt;/th&gt; &lt;th&gt;1-NN&lt;/th&gt; &lt;th&gt;2-NN&lt;/th&gt; &lt;th&gt;3-NN&lt;/th&gt; &lt;th&gt;4-NN&lt;/th&gt; &lt;th&gt;5-NN&lt;/th&gt; &lt;th&gt;6-NN&lt;/th&gt; &lt;th&gt;7-NN&lt;/th&gt; &lt;th&gt;8-NN&lt;/th&gt; &lt;th&gt;9-NN&lt;/th&gt; &lt;th&gt;10-NN&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;happy&lt;/td&gt; &lt;td&gt;😃&lt;/td&gt; &lt;td&gt;😺&lt;/td&gt; &lt;td&gt;😌&lt;/td&gt; &lt;td&gt;😹&lt;/td&gt; &lt;td&gt;🏩&lt;/td&gt; &lt;td&gt;😊&lt;/td&gt; &lt;td&gt;💛&lt;/td&gt; &lt;td&gt;😂&lt;/td&gt; &lt;td&gt;😞&lt;/td&gt; &lt;td&gt;😁&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;sad&lt;/td&gt; &lt;td&gt;😒&lt;/td&gt; &lt;td&gt;😻&lt;/td&gt; &lt;td&gt;👏&lt;/td&gt; &lt;td&gt;😥&lt;/td&gt; &lt;td&gt;😭&lt;/td&gt; &lt;td&gt;😓&lt;/td&gt; &lt;td&gt;😣&lt;/td&gt; &lt;td&gt;😔&lt;/td&gt; &lt;td&gt;😂&lt;/td&gt; &lt;td&gt;😪&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;lit&lt;/td&gt; &lt;td&gt;🔥&lt;/td&gt; &lt;td&gt;🚨&lt;/td&gt; &lt;td&gt;😍&lt;/td&gt; &lt;td&gt;✅&lt;/td&gt; &lt;td&gt;😎&lt;/td&gt; &lt;td&gt;💯&lt;/td&gt; &lt;td&gt;💣&lt;/td&gt; &lt;td&gt;🇺🇸&lt;/td&gt; &lt;td&gt;🗣&lt;/td&gt; &lt;td&gt;💫&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;bitcoin&lt;/td&gt; &lt;td&gt;💵&lt;/td&gt; &lt;td&gt;🎉&lt;/td&gt; &lt;td&gt;😱&lt;/td&gt; &lt;td&gt;💸&lt;/td&gt; &lt;td&gt;🤑&lt;/td&gt; &lt;td&gt;🔹&lt;/td&gt; &lt;td&gt;🇮🇳&lt;/td&gt; &lt;td&gt;🍃&lt;/td&gt; &lt;td&gt;😆&lt;/td&gt; &lt;td&gt;🌊&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;As before, the nearest neighboring emojis generally make sense, but are less accurate than the neighbors in the baseline model. At the same time, the nearest neighbors now align more closely with slang (or “new” words like bitcoin). “Lit” now is more related to a feeling of firm agreement, and “bitcoin” is now more related to money. In both cases, the nearest neighbors align more with the words’ common usages than their literal meanings.&lt;/p&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt; &lt;p&gt;Given the time and computational constraints we had for this project, we had to pass on many paths for future exploration. We list a few in this section.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;We would’ve liked to train our second model for much longer on a much larger dataset of tweets. Only about 400 of our emojis had over 50 tweets associated with them. This greatly restricted their positive sample sets, which likely resulted in far-from-optimal emoji embeddings.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;We also considered training a more expressive neural architecture for our second model. One word2vec CBOW &lt;a href=&quot;https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0&quot;&gt;implementation&lt;/a&gt; we found used a Linear layer after the Embedding layer. It projected the 300-dimensional embeddings into embeddings with dimensionality equal to the size of the emoji vocabulary to learn embeddings via a multi-class classification problem. We ultimately decided against using such a model because we doubted that we had the time, data, and compute to train a more complex model.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Something we realized towards the end of our model training was that the embeddings from the first model could be used to inform training on our second model. It would be interesting to see if transfer learning could result in increased performance for our second model, especially since many emojis were underrepresented in our dataset of tweets.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;Overall, despite the limitations, our lightweight model achieved reasonable accuracy with less than optimal conditions. One other challenge we faced had to do with Colab’s memory constraints: we were only able to train on a small set of data and were forced to generate positive and negative pairs over and over from the same set. Given a larger and more diverse set of positive/negative pairs, we believe our model could have performed even better.&lt;/p&gt; &lt;p&gt;Furthermore, we felt that our CBOW model definitely could add value for people solving downstream tasks, such as sentiment analysis. The emoji2vec model of summing the emoji’s description’s word embeddings is useful when there are few datapoints for each emoji, but the CBOW approach captures more subtle meanings and is much more accurate to how people actually use emojis in their day to day life—both have their merits.&lt;/p&gt; </content> </entry> <entry> <title>Modeling Human Speech Recognition with Different Network Architectures</title> <link href="https://deep-learning-mit.github.io/blog/2023/speech-recognition-proposal/"/> <updated>2023-11-10T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/speech-recognition-proposal</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Recent advances in machine learning have made perception tasks more doable by computers, approaching levels similar to humans. In particular, structuring models biologically and using ecologically realistic training datasets have helped to yield more humanlike results. In the field of speech recognition, models trained under realistic conditions with stimuli structured how sounds are represented in the cochlea, with network layers imitating the processing pipeline in the brain, seem to be successful in performing speech recognition tasks. However, it is unclear whether specific network architectures are more beneficial to learning human speech recognition patterns. In this project, I seek to investigate how different network architectures such as CNNs vs. TNNs affect the ability to recognize speech in a humanlike way.&lt;/p&gt; &lt;p&gt;One facet of more biological models is that they attempt to recreate the structure of the human brain. For auditory models, a useful structure to replicate is the cochlea; these replications are called cochleagrams. Cochleagrams have been used in order to model the ear more effectively, leading to models that imitate auditory perception in a more human-like way. A cochleagram works in a similar way to how the cochlea works in a human. It filters a sound signal through bandpass filters of different frequencies, creating multiple frequency subbands, where the subbands for higher frequencies are wider, like how the cochlea works in the human ear. The amplitudes of the different subbands are then compressed nonlinearly, modeling the compressive nonlinearity of the human cochlea &lt;d-cite key=&quot;mcdermott2013&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;mcdermott2011&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;A recent application of cochlear models to speech perception is found in Kell’s 2018 paper, where they create a convolutional neural network which replicates human speech recognition &lt;d-cite key=&quot;kell2018&quot;&gt;&lt;/d-cite&gt;. They trained the network to recognize a word in the middle of a 2 second clip, from a possible vocabulary of 587 words. To imitate how the ear functions, they preprocessed the sound signals into cochleagrams, intended to be a more biologically realistic model of the ear. The activations in different layers of the neural network were able to predict voxel responses in different parts of the brain, revealing that the auditory processing pipeline aligned with layers of the network.&lt;/p&gt; &lt;p&gt;A natural question to ask at this point is whether a convolutional neural network is the best architecture for this task. In Mamyrbayev Orken et al.’s 2022 paper, they explore a speech recognition system for Kazakh speech &lt;d-cite key=&quot;orken2022&quot;&gt;&lt;/d-cite&gt;. In this paper, they create a transformer model that is able to perform a continuous speech recognition task on “clean” speech (speech without noise or alterations). They trained the model on a dataset of clean speech and telephone speech, and the model was able to perform the speech recognition task on clean speech with high accuracy. Although this network does well on the specific task it is given, it is still unclear how its performance compares to that of a convolutional neural network due to not having a direct comparison between the two on the same task. In addition, it is unclear whether the transformer can yield human-like results for speech recognition.&lt;/p&gt; &lt;p&gt;In the field of computer vision, there has been work done comparing convolutional neural networks to vision transformers for the task of object recognition. Tuli’s 2021 paper explores this through the lens of human-like object recognition, determining whether the errors of a vision transformer or a convolutional neural network are more similar to humans &lt;d-cite key=&quot;tuli2021&quot;&gt;&lt;/d-cite&gt;. Their findings indicated that the vision transformer behaved in a more human-like way. In particular, in human vision, there is a phenomenon called shape bias, where if an object has the shape of a certain object A, but the texture of a certain object B, humans will be likely to predict that the object is an instance of object A. Many vision models struggle with shape vs. texture bias, being more inclined to categorize objects by their texture; the vision transformer presented in this paper has approximately twice the amount of shape bias as the convolutional neural network, further suggesting that the transformer is a more human-like representation of vision.&lt;/p&gt; &lt;p&gt;In this post, I investigate more closely the importance of network architecture in the ability to effectively model human speech recognition. I focus on three metrics of evaluating how well a model replicates human speech recognition:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ability to generalize to speakers not found in the training set: Humans hear speech from new speakers all the time, and a person who they’ve never heard before usually does not hinder their ability to recognize what they are saying. Models of speech recognition are usually trained on a corpus of speech that is inherently biased towards a set of talkers that participates in creating the corpus, so it is possible that it could overfit to the speakers in the training set. A good model of speech recognition should be able to perform well on new talkers.&lt;/li&gt; &lt;li&gt;Ability to recognize speech in different background noise conditions: Humans rarely hear speech unaccompanied by some form of background noise, and are generally robust to noise up to large signal to noise ratios. Many models of speech recognition such as the transformer in Orken 2022 are not trained or tested on noisy speech, so it is likely that it would not be able to recognize speech in these conditions.&lt;/li&gt; &lt;li&gt;Ability to recognize distorted forms of speech: Humans are remarkably robust to various distortions of speech such as sped-up/slowed-down speech, reverberant speech, and local-time manipulations, despite not encountering some of these often in their lives &lt;d-cite key=&quot;ml1950&quot;&gt;&lt;/d-cite&gt;. In order to further test a model’s ability to replicate human speech recognition, we should test how well it performs on speech manipulations.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;p&gt;The models in my experiment were given a 2 second speech clip, and were tasked with identifying the word overlapping the middle of the clip. In particular, they were trained on a dataset containing 2 second speech clips from the Common Voice dataset, where the word at the middle of the clip is from a vocabulary of 800 words, imposed on different background noises taken from the Audio Set dataset &lt;d-cite key=&quot;ardila2019&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;gemmeke2017&quot;&gt;&lt;/d-cite&gt;. So, the models were essentially given a speech clip and asked to perform an 800-way classification task.&lt;/p&gt; &lt;p&gt;In order to generate the fairest comparison between convolutional neural networks and transformers, I start with a baseline CNN inspired by Saddler 2021, and then either replace the last convolutional layer with a multi-headed attention layer or remove it from the network &lt;d-cite key=&quot;saddler2021&quot;&gt;&lt;/d-cite&gt;. Each block of the CNN is made up of a convolutional layer, followed by a ReLU activation, a weighted-average pooling layer, and a normalization layer &lt;d-cite key=&quot;saddler2021&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;feather2019&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/block-architectures-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/block-architectures-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/block-architectures-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/block-architectures.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The baseline CNN (called CNN6) is composed of 6 blocks followed by a fully connected layer and a classification layer. The transformer-CNN hybrid (CNN5+Attn) is composed of 5 convolutional blocks, followed by an attention block, followed by a fully connected layer and a classification layer. Lastly, I created a “control” CNN (called CNN5) that is the same as CNN6, but with the last convolutional block removed. This was intended to test whether an attention layer provides any benefit as opposed to not including the layer at all. All networks begin with an initial data preprocessing step that converts the audio signal into a cochleagram.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/network-architectures-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/network-architectures-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/network-architectures-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/network-architectures.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;It is difficult to derive a direct comparison between a convolutional layer and a multi-headed attention layer, in particular how to decide how many attention heads to include and what the attentional layer dimension should be. In order to have the best chance of comparison between CNN5+Attn and the other networks, I ran multiple CNN5+Attn networks with a larger vs. smaller number of attention heads (64 vs. 16) and a larger vs. smaller attention dimension (512 vs. 16) for 10 epochs to determine a preliminary measure of network performance across these parameters. The preliminary results after 10 epochs showed that the CNN5+Attn network with a small number of attention heads and a smaller attention dimension had the highest training accuracy and trained the fastest, so I used this model for my analysis.&lt;/p&gt; &lt;p&gt;After preliminary analysis, I trained the CNN6, CNN5+Attn, and CNN5 networks for 100 epochs. I then evaluated the models’ performance on this task in the three aforementioned conditions.&lt;/p&gt; &lt;p&gt;1) To evaluate performance on clips spoken by talkers not encountered in the training dataset, I evaluated the models on clips taken from the WSJ speech corpus.&lt;/p&gt; &lt;p&gt;2) For clips superimposed on different types of background noise, I evaluated the model on 5 types of background noise, in signal-to-noise ratios ranging from -9 dB to +3 dB, plus a +infinity condition which represents no background noise:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auditory scenes: background noises encountered in everyday life like rain or cars passing by&lt;/li&gt; &lt;li&gt;8-speaker babble: 8 other people talking in the background&lt;/li&gt; &lt;li&gt;Music&lt;/li&gt; &lt;li&gt;Speech-shaped noise: gaussian noise that is given the envelope of speech signals&lt;/li&gt; &lt;li&gt;Modulated speech-shaped noise: speech-shaped noise that is modulated so that the noise alternates between being very quiet and very loud&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;3) Distorted speech clips with 6 types of distortions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sped-up/slowed-down speech (preserving original pitches)&lt;/li&gt; &lt;li&gt;Speech in a reverberant environment: speech convolved with an impulse response of different reverberant environments&lt;/li&gt; &lt;li&gt;Whispered speech&lt;/li&gt; &lt;li&gt;Inharmonic speech: speech signals are decomposed into their harmonics, and the harmonics are moved up or down to distort the signal&lt;/li&gt; &lt;li&gt;Sine wave speech: speech signals are filtered into frequency subbands, and each band is replaced by a sine wave with the center frequency of the band&lt;/li&gt; &lt;li&gt;Locally time-reversed speech: speech is decomposed into chunks of a certain length, and the chunks are reversed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Then I compared the models’ performance on these conditions to existing human data where humans were asked to perform the same task of recognizing the middle word of a 2-second clip in various types of noise or distortion.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;Overall, I found that CNN6 performed better than CNN5+Attn, which performed better than CNN5. After 100 epochs, CNN6 had a validation accuracy of around 0.60, CNN5+Attn had validation accuracy of 0.55, and CNN5 had validation accuracy of 0.53. In particular, CNN5 overfit quite a bit (0.12 gap between training and validation accuracy) while CNN5+Attn overfit much less (0.05 gap between training and validation accuracy).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/accuracy-by-epoch-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/accuracy-by-epoch-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/accuracy-by-epoch-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/accuracy-by-epoch.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;All three models performed similarly to humans for clean speech spoken by talkers not encountered in the training dataset.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/clean-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/clean-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/clean-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/clean.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In different types of background noise, in general the models performed similarly to humans, except in the condition of modulated speech-shaped noise. In general, humans perform better for modulated noise than “stationary” noise because they are able to fill in speech in the “gaps”, or quieter sections, of the noise, but none of the models have as strong of an effect as humans for this. The CNN5+Attn model does particularly badly on this compared to the other networks.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/background-noise-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/background-noise-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/background-noise-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/background-noise.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The models all perform similarly to humans for inharmonic speech, local time reversal, and low levels of sped-up or slowed-down speech. For whispered speech and sine-wave speech, the models perform slightly worse than humans, with CNN6 performing better than CNN5+Attn performing better than CNN5. For reverberant speech and extremely sped-up or slowed-down speech, all of the models perform significantly worse than humans, with the same hierarchy of performance between the models.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-speech-recognition-proposal/manipulations-2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;Overall, it seems that CNN6 is the best option for replicating human speech recognition, but CNN5+Attn does have some benefits. In particular, it trains substantially faster than CNN5, and overfits less than both CNN5 and CNN6. The hybrid architecture may help with overfitting because it forces the model to do multiple types of analysis in order to determine the output. Although CNN5+Attn does still perform worse than CNN6, it is reasonable to hypothesize that it has potential. Due to resource limitations, I was only able to test two different conditions for number of attention heads and attention dimension, but as shown from the preliminary training the number of attention heads and the attention dimension does have an effect. It seems likely that with a more extensive search of these parameters, it could be possible to create a CNN5+Attn network that performs similarly or better than the CNN6 network on these tasks.&lt;/p&gt; &lt;p&gt;All of the models have discrepancies with humans for the modulated background noise condition. One possible explanation for this is that the models do not learn the process of recognizing smaller phonemes of a word, only learning a classification task on the 800 words that they are given, so they are unable to piece together chunks of a word into a larger word like humans do. A possible way to test this would be to create a model for a phoneme-detection task, and then add a layer that combines the phonemes into a larger word, and see whether this performs better in this condition. This would make sense because some of the earliest things humans learn about speech are not full words, but phonemes like “ba” or “da,” so a model trained on this task would then have been optimized in more human-like conditions.&lt;/p&gt; &lt;p&gt;In addition, there are some discrepancies between the models and humans in some of the speech distortions. The largest discrepancies are found in very sped-up or slowed-down speech, and in reverberant speech. This seems likely to be due to a shortcoming of the dataset. The Common Voice dataset is composed of people reading passages, which is generally a single slow, steady speed, and there is no reverberation. The speech that humans encounter in their lives varies a lot in speed, and they also encounter speech in many different reverberant environments, so they are optimized to recognize speech in these conditions. It is reasonable to assume that if reverberation and varied speeds of speech were incorporated into the training dataset, the model would perform better in these conditions.&lt;/p&gt; &lt;p&gt;Further directions of this project could include trying more variations of the parameters of the attention model. In addition, it would be interesting to try different hybrid architectures; for example, 4 layers of convolution followed by 2 layers of attention. This could give a more complete idea of the benefits and disadvantages of CNNs and transformers for the task of speech recognition. In conclusion, the current results seem promising, but more extensive testing is needed in order to get a full picture of whether these models can accurately replicate human speech recognition.&lt;/p&gt; &lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt; &lt;p&gt;I would like to thank my fellow members of the McDermott Lab, particularly Mark Saddler for creating the code for the baseline CNN, and Erica Shook for providing me with human data and experimental manipulation code.&lt;/p&gt; </content> </entry> <entry> <title>Analytic, Empirical, and Monte Carlo Bayesian Methods for Uncertainty Estimation</title> <link href="https://deep-learning-mit.github.io/blog/2023/uncertainty/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/uncertainty</id> <content type="html">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;p&gt;Many practical uses of deep neural network (DNN) models involve using them with a restricted amount of training data, which doesn’t encompass all the potential inputs the model might face when actually used. This exposes a significant limitation of models based on data: they can behave unpredictably when dealing with inputs that differ from the data they were trained on, known as out-of-distribution (OOD) inputs. Machine learning models that are trained within a closed-world framework often mistakenly identify test samples from unfamiliar classes as belonging to one of the recognized categories with high confidence&lt;d-cite key=&quot;scheirer2013&quot;&gt;&lt;/d-cite&gt;. This tendency of the model to make overconfident misclassifications is sometimes described in the literature as “arrogance.” This can be catastrophical when such models are deployed in the real world&lt;d-cite key=&quot;amodei2016&quot;&gt;&lt;/d-cite&gt;. For instance, in self-driving technology, the system should warn the user and transfer the control to the human driver when it encounters unfamiliar scenes or objects that were not present in its training data, and thus it is unable to make a safe and reliable decision.&lt;/p&gt; &lt;p&gt;Consequently, there has been a surge of research focused on improving DNN models to be able to assess their own uncertainty and recognize OOD inputs during their operational phase &lt;d-cite key=&quot;yang2022&quot;&gt;&lt;/d-cite&gt;. Not only improving their usage in safety critical settings, being able to predict model uncertainty is important in predicting model performance. With increased ability to predict model performance, we can improve the training process and improve inference. In this project, we explore 3 different methods of quantifying uncertainty: Monte Carlo Dropout&lt;d-cite key=&quot;gal2016&quot;&gt;&lt;/d-cite&gt;, Sketching Curvature for Efficient Out-of-Distribution Detection (SCOD)&lt;d-cite key=&quot;sharma2021&quot;&gt;&lt;/d-cite&gt;, and Stochastic Weight Averaging Gaussian (SWAG)&lt;d-cite key=&quot;maddox2019&quot;&gt;&lt;/d-cite&gt;. We’ll also attempt to modify these existing methods and even combine them in order to provide improvements to practicality such as RAM usage and improvements to important metrics we establish. We develop 2 methods, SCODv2 which is an extends a simple isotropic prior used by SCOD and SCWAG which combines elements of both SCOD and SWAG.&lt;/p&gt; &lt;h2 id=&quot;stochastic-weight-averaging-gaussian-swag&quot;&gt;Stochastic Weight Averaging Gaussian (SWAG)&lt;/h2&gt; &lt;p&gt;SWAG&lt;d-cite key=&quot;maddox2019&quot;&gt;&lt;/d-cite&gt; is a method that can improve generalization in deep learning settings. It approximates the posterior distribution of weights as a normal distribution with mean determined by an average of weight iterates and covariance determined by the sum of the running variances of each weight and a low rank covariance matrix. More specifically, we use the following to perform our weight updates.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_swag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swag_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swag_start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_all_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_bar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swag_freq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;if &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swag_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_all_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_bar_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta_bar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta_bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta_bar_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_bar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_bar_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_bar&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The learned \(\bar{w} \in \mathbf{R}^d\) is the mean of the posterior distribution on weights. The \(\Sigma\) vector represents the running variance of the weights and can be diagonalized to get a very rough posterior. (The method we used to determine the running variance is unlike the one presented in the SWAG paper due to issues with numerical instability and catastrophic cancellation which resulted in negative variances. To address this issue we used Welford’s online algorithm.) The \(D\) matrix contains the last \(K\) deviations of updated \(w\) values from \(\bar{w}\) (including the effect that the updated \(w\) has on \(\bar{w}\)). This allows us to form a rank \(K\) approximation of the posterior covariance. Thus we have the posterior \(P(w\mid\mathcal{D}) = \mathcal{N}\left(\bar{w}, \frac{1}{2}\left(\text{diag}(\Sigma) + \frac{DD^T}{K-1} \right)\right)\). To sample from the posterior, we do the following reparametrization&lt;/p&gt; \[z_d \sim \mathcal{N}(0, \mathbf{I}_d)\] \[z_K \sim \mathcal{N}(0, \mathbf{I}_K)\] \[\tilde{w} = \bar{w} + \frac{1}{\sqrt{2}}\text{diag}(\Sigma)^{\frac{1}{2}}z_d + \frac{1}{\sqrt{2(K-1)}}Dz_K\] &lt;p&gt;It is important to note that while a prior distribution on weights is not specified, it is implicitly chosen through how often we update our running average of the weights, variances, and deviations, as well as the optimizer being used.&lt;/p&gt; &lt;p&gt;For the purposes of inference, each \(\tilde{w}\) determines the parameters for a clone model and with \(S\) samples we effectively have an ensemble of \(S\) models. Their output distributions are averaged arithmetically to yield the final output. We expect that for in-distribution inputs, the individual outputs do not disagree drastically. And for out-of-distribution inputs, the individual outputs can differ a lot. So like with out other ensemble method, a good metric of uncertainty here is to use the average-pairwise KL divergence between the distributions. Here are some results and findings of this metric applied to SWAG.&lt;/p&gt; &lt;p&gt;We train a model with SWAG on the MINST and CIFAR10 datasets. First, we only train on the digits/classes from 0-5 and look at the KL scores on the digits/class 6-9. Expectedly, the scores tend to drastically increase on the unseen digits. However, the increase is less drastic for the CIFAR dataset as the data is a bit more homogenous.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_mnist_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_mnist_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_mnist_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_mnist_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_cifar_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_cifar_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_cifar_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/violin_cifar_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;We can also take a look at the data itself and identify the images which have the highest and lowest scores for different splits of the data. For these images, we identify the true label, followed by the KL score assigned to the image (higher being more uncertain), and finally the predictions made by 10 of 25 sampled models.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_id_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_id_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_id_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_id_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_id_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_id_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_id_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_id_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_id_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_id_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_id_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_id_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_id_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_id_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_id_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_id_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The above pictures correspond to the highest and lowest scores from in-distribution training data. The major contributors for the high scores for MNIST are digits that are so poorly written it’s hard to say what it is or it resembles another image too much. For CIFAR, it seems like the high score images are inducing confusion due to their color scheme or background. A lot of images with a blue or sky background such as those of birds do seem to be mistaken for planes at times. The low score images on the other hands are all extremely similar to one another; they’re very well written digits (usually 0) or something that is obviously a car (usually red).&lt;/p&gt; &lt;p&gt;Next, we take a look at how these scores fair on new out-of-distribution images.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_ood_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_ood_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_ood_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_ood_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_ood_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_ood_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_ood_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_ood_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_ood_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_ood_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_ood_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_ood_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_ood_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_ood_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_ood_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_ood_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;These are the highest and lowest scores on the OOD dataset. It’s unsurprising that the highest scores are assigned to the images that are unlike anything in the training set. For MNIST this is the number 8 and for CIFAR there doesn’t seem to be any one class. However, it is important to see that there are still images where our model has very low scores (high certainty). However, this simply comes from the fact that these inputs happen to look more similar to one class of training images (9 is really similar looking to 4 and trucks look pretty close to cars, especially if they’re red since a lot of the low score car-images are red).&lt;/p&gt; &lt;p&gt;All the methods used in this paper tend to show similar results for the images corresponding to the highest and lower measures of uncertainty so we won’t be lookig at those images for every single method.&lt;/p&gt; &lt;p&gt;Now that we’ve seen that we can use our measure of uncertainty as how well the output will yield the correct answer, we can try using uncertainty of output as a way to predict error. Ideally, we would like to see some sort of correlation between our uncertainty measure and our actual errors or probability of corect answer. So we retrained our models on all digits using SWAG and looked at the performance on a validation set. Notice that we don’t care too much about the error itself, but it’s (actually the probability of target label) correlation with the uncertainty measure. In particular, we look at the Spearman correlation to capture nonlinear relationships.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; Spearman Correlation: -.9923 &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_swag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_swag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_swag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_swag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; Spearman Correlation: -.3867 &lt;/div&gt; &lt;/div&gt; &lt;p&gt;There is significant negative correlation which is what we’re looking for. If we can predict how well our model will perform on certain inputs, it allows us to better deploy model in real world situations as well as possibly improve it by doing something such as boosting or improved training. We now look to improve this relationship between error and uncertainty measure by finding better uncertainty measures.&lt;/p&gt; &lt;h2 id=&quot;local-ensemble-monte-carlo-dropout&quot;&gt;Local Ensemble: Monte Carlo Dropout&lt;/h2&gt; &lt;p&gt;We start off by comparing with a very simple method. Given a neural net with Dropout layers, and a new datapoint from test ID or OOD datasets, we output \(50\) different probabilistic distributions (rather than setting our model on evaluation mode, we keep the Dropout layers on), \(p_1, p_2, \ldots p_{50}\). Our uncertainty score is \(\text{Unc}(x) = \frac{1}{49\cdot 50}\sum_{i\neq j}D_\text{KL}(p_i\, \Vert \, p_j)\), i.e. the average KL divergence between any pair of distributions. The intuition is that, when the model shouldn’t be confident about a OOD datapoint, dropping weights (which can be seen as perburtabions) should change our output distributions significantly. This sensitiveness indicates lack of robustness and certainty.&lt;/p&gt; &lt;p&gt;This model is very simple and our weight “peturbations” are not too mathematically motivated in the sense of them coming from some justified posterior. However, it still provides a good baseline to compare against.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_mc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_mc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_mc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_correlation_mc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; Spearman Correlation: -.9944 &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_mc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_mc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_mc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_correlation_mc.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; Spearman Correlation: -.2936 &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Overall, the error estimation on MNIST is about the same but significantly worse on the CIFAR dataset. This is about expected since MC dropout is such a simple method.&lt;/p&gt; &lt;h2 id=&quot;sketching-curvature-for-efficient-out-of-distribution-detection-scod&quot;&gt;Sketching Curvature for Efficient Out-of-Distribution Detection (SCOD)&lt;/h2&gt; &lt;p&gt;There is research literature on leveraging the local curvature of DNNs to reason about epistemic uncertainty. [Sharma et al.] explores this idea through a Bayesian framework. Let us assume a prior on the weights, \(P(w) = \mathcal{N}(0, \epsilon^2 I)\). By using a second-order approximation of the log-likelihood \(\log p(y,w\mid x)\), we arrive at the Laplace posterior \(P(w\mid\mathcal{D}) =\mathcal{N}(w^{MAP}, \Sigma^*)\), where \(\Sigma^* = \frac{1}{2}(H_L + \frac{1}{2\epsilon^2}I)^{-1}\) and \(H_L\) is the Hessian of the cross-entropy loss wrt \(w\). Given a pretrained DNN, \(\theta=f(x,w)\in\mathcal{R}^d\) where \(\theta\) determines a distribution on \(y\), we assume that the trained weights \(w^*\) are a good approximation for \(w^{MAP}\). We define our uncertainty metric to be the change in the output distribution, \(\theta\), when the weights are perturbed around \(w^*\) according to the posterior distribution. Using the KL divergence to measure distance between output distributions, we define&lt;/p&gt; \[\text{Unc}(x) = \mathbb{E}_{dw\sim \mathcal{N}(0, \Sigma^*)}\left[ D_{\text{KL}}\left( p(\theta\mid x, w^*)\, \Vert \, p(\theta\mid x, w^* + dw)\right) \right]\] &lt;p&gt;We can approximate the local KL divergence using the Fisher information matrix (FIM) of \(y\) wrt \(\theta\): \(D_{\text{KL}} \approx d\theta^TF_\theta(\theta)d\theta + O(d\theta^3)\). Also, by change of variables, we can rewrite the FIM in terms of \(w\): \(F_w(x, w) = J^T_{f,w}F_\theta(f(x,w))J_{f, w}\) where \(J_{f,w}\) is the Jacobian of the network outputs with respect to the weights. Putting this together, we get that&lt;/p&gt; \[\text{Unc}(x) = \mathbb{E}_{dw\sim \mathcal{N}(0, \Sigma^*)} \left[dw^TF_w(x,w^*)dw \right] = \text{Tr}\left( F_w(x,w^*)\Sigma^*\right)\] &lt;p&gt;We can also approximate \(\Sigma^* \approx \frac{1}{2}(MF_{w^*}^\mathcal{D} + \frac{1}{2\epsilon^2}I)^{-1}\), where \(F_{w^*}^\mathcal{D}\) is the averaged FIM on the training dataset&lt;d-cite key=&quot;ritter2018&quot;&gt;&lt;/d-cite&gt; and \(M\) is the size of the training set. If the neural net has \(N\) weights, then the FIMs are \(N \times N\) matrices and computing them becomes intractable (both time complexity and memory).&lt;/p&gt; &lt;p&gt;For simplicity, let us assume that the output of our DNN, \(\theta\), is the categorial distribution, i.e. \(\theta_i\) represents the probability assigned to class \(i\). In this case, we have that \(F_\theta(\theta) = \text{diag}(\theta)^{-1}\). Therefore, the FIM for one input os has rank at most \(\min(n, d)\) and we can represent it as \(F_w(x,w^*) = LL^T\), where \(L=J_{f,w}^T\text{diag}(\theta)^{-1/2}\). The same trick, however, doesn’t work for \(F_{w^*}^\mathcal{D}\) as it can reach rank as high as \(min(N, Md)\). For now, let us assume that we can find a low-rank approximation of \(F_{w^*}^\mathcal{D} = U\text{diag}(\lambda)U^T\), where \(U\in\mathbb{R}^{N\times k}\) and \(\lambda\in\mathbb{R}^k\). With a few mathematical tricks (which can be followed in [Sharma et al.]), one can prove that&lt;/p&gt; \[\text{Unc}(x) = \epsilon^2\Vert L\Vert_F^2 - \epsilon^2 \left \Vert \text{diag}\left(\sqrt{\frac{\lambda}{\lambda + 1/(2M\epsilon^2)}}\right)U^TL \right \Vert^2_F\] &lt;p&gt;[Sharma et al.] also provides an randomized algorithm for finding \(U\) and \(\Lambda\) by using the FixedRankSymmetricApproximation&lt;d-cite key=&quot;tropp2017&quot;&gt;&lt;/d-cite&gt; algorithm presented in [Tropp et all.]. We chose \(\epsilon \approx 0.04536\) by training another model with the same hyperparameters and architecture and taking the standard deviation of all its weights. This is the pseudocode of the SCOD algorithm&lt;d-cite key=&quot;sharma2021&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_algo-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_algo-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_algo-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_algo.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;\(\Sigma\in\mathbb{R}^{r\times N}\) and \(\Psi \in \mathbb{R}^{s\times N}\) are random sketching matrices, which we chose to simply be matrices with i.i.d standard Gaussian entries. \(r+s\) is the size of the sketch and is ideally chosen as high as RAM allows. We also use the budget split \(s = 2k+1\) and \(r=4k+3\), where \(k\) is the target rank, as [Tropp et all.] suggests. We ended up setting \(k=50\) and got the following results:&lt;/p&gt; &lt;p&gt;We have been able to implement SCOD, but due to issues with saving our results and time, we can now only show the performance of the uncertainty score on predicting error on a subset (classes 0-5) of the CIFAR dataset.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_correlations-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_correlations-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_correlations-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/scod_correlations.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; Spearman Correlation: -.1083 &lt;/div&gt; &lt;/div&gt; &lt;p&gt;The score is a bit suspiciously low, so there may be something wrong with our implementation ignoring the fact that we only test of the subset. Nonetheless, it still a significant negative correlation and we get similar results when looking at high uncertainty and low uncertainty images.&lt;/p&gt; &lt;h2 id=&quot;scodv2&quot;&gt;SCODv2&lt;/h2&gt; &lt;p&gt;We also did our own tweak on SCOD. Rather than having a vanilla prior, we can generalize it to any normal distribution with diagonal covariance. Let’s say that our prior is \(w\sim\mathcal{N}(0, \Sigma)\), where \(\Sigma\) is a diagonal matrix. Then, our Laplacian posterior’s covariance matrix becomes \(\Sigma^* = \frac{1}{2}(MF_{w^*}^\mathcal{D} + \frac{1}{2}\Sigma^{-1})^{-1}\). By the Woodbury matrix identity \(\Sigma^*=\Sigma - 2\Sigma U\left(\text{diag}(M\lambda)^{-1}+2U^T\Sigma U \right)^{-1}U^T\Sigma\). Using the well-known identities, \(\Vert A\Vert_F^2 = \text{Tr}(AA^T)\), \(\text{Tr}(AB) = \text{Tr}(BA)\), we get that&lt;/p&gt; \[\text{Unc}(x_{\text{new}}) = \text{Tr}\left(\Sigma^*F_w(x_{\text{new}},w^*)\right) = \text{Tr}\left(L^T\Sigma L\right) - 2\text{Tr}\left(L^T\Sigma U\left(\text{diag}(M\lambda)^{-1}+2U^T\Sigma U \right)^{-1}U^T\Sigma L\right)\] &lt;p&gt;\(= \left \Vert L^T \Sigma^{1/2}\right \Vert_F^2 - 2\left \Vert L^T \Sigma UA\right \Vert_F^2\), where \(AA^T = \left(\text{diag}(M\lambda)^{-1}+2U^T\Sigma U \right)^{-1}\).&lt;/p&gt; &lt;p&gt;Since \(\Sigma\) is a diagonal matrix, the biggest matrices we ever compute are of size \(N\times \max(k, d)\), which means that the computation is equally efficient asymptotically to the vanilla prior. To decide what diagonal matrix to use, for each layer, we assigned the same variance given by the variance of the weights of the same layer in a differently trained model (with same architecture).&lt;/p&gt; &lt;p&gt;Due to issues with saving our results and timing, we are not able to show our results estimating error from uncertainty for SCODv2.&lt;/p&gt; &lt;h2 id=&quot;stochastic-curvature-and-weight-averaging-gaussian-scwag&quot;&gt;Stochastic Curvature and Weight Averaging Gaussian (SCWAG)&lt;/h2&gt; &lt;p&gt;Whereas SCOD attempts to analytically approximate the posterior by approximating the Hessian using the Gauss-Newton matrix, SWAG approximates the posterior by keeping running track of moments and deviations when it approaches flat regions in the loss landscape. What if we could combine these two ideas? We could use the SWAG emprical posterior. This method would not require matrix sketching of any form and lowers the required RAM necessary an SCOD can be quite RAM intensive. Using the \(\Sigma\) and \(D\) from SWAG to determine the posterior \(\Sigma^*\), we arrive the following measure of uncertainty (after digging through some math).&lt;/p&gt; \[\text{Unc}(x) = \text{Tr}\left( F_w(x,w^*)\Sigma^*\right) = \frac{1}{2} \text{Tr}\left(F_w(x,\bar{w})\left(\text{diag}(\Sigma) + \frac{DD^T}{K-1} \right) \right)\] \[\text{Unc}(x) \propto ||L\Sigma||_F^2 + \frac{1}{K-1}||LD||_F^2\] &lt;p&gt;We do this by introducing a wrapper model that takes in a base model as well as the SWAG outputs in order to perform the Jacobian based operations during each forward pass. For evaluation, we look at the Spearman correlation of the uncertainty score with the target probability and we notice some improvement over SWAG on the CIFAR dataset.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_scwag_correlations-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_scwag_correlations-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_scwag_correlations-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_scwag_correlations.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; Spearman Correlation: -.9897 &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_scwag_correlations-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_scwag_correlations-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_scwag_correlations-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_scwag_correlations.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; Spearman Correlation: -.8484 &lt;/div&gt; &lt;/div&gt; &lt;p&gt;With MNIST, we already had near perfect correlation so this slight decrease isn’t too worrisome. However, the Spearman correlation has shot up drastically which shows that this method of combining the analytical approximation of uncertainty with an empirically constructed posterior has merit. There is something worrisome with the fact that the model with exactly \(bar{w}\) with its weights is producing distributions that have a maximum value of around \(.25\). We suspect we could have made some error here but have not been able to pinpoint anything wrong with out implementaton. The model still seems to have fairly accurate predictions as seen below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_scwag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_scwag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_scwag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_hard_scwag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_scwag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_scwag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_scwag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/mnist_easy_scwag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_scwag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_scwag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_scwag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_hard_scwag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_scwag-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_scwag-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_scwag-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-uncertainty-detection-project/cifar_easy_scwag.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt; &lt;p&gt;For SCWAG, we could work on figuring out why our output distributions becomes less spiked as a result of using \(\bar{w}\) as the weights for the network. We suspect that it’s a result of starting our SWAG averaging for \(\bar{w}\) too early so we were considering \(w\) far away from flat local minima of the loss landscape. Additionally, we could inspect the arcing nature in the plot of target probabilities vs score. For near 0 scores, it seems that the target probabilities arc from .25 to 0 which is unusual. Finally, we want to think of a way to introduce the loss landscape more into our approach. Maybe we can form a more expressive posterior. If we can manage that, our uncertainty estimates and correlation might improve. But more importantly, we would be able to call our method SCALL(y)WAG which is pretty cool.&lt;/p&gt; &lt;p&gt;In general and particularly for SCOD, we’d still like to experiment with priors that induce different types of posteriors. Because the dependence on prior is explicit here as opposed to implicit for SWAG, it allows us more room for experimentation in choosing nice expressive priors.&lt;/p&gt; </content> </entry> <entry> <title>Understanding LLM Attention on Useless Numbers in Word Problems (and this Title has 8 Es)</title> <link href="https://deep-learning-mit.github.io/blog/2023/structured-physics-loss-diffusion/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/structured-physics-loss-diffusion</id> <content type="html">&lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/title-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/title-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/title-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/title.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt; &lt;p&gt;We investigate how attention is used to identify salient parts of word problems. There is no difference between attention across layers to necessary and useless numbers in math word problems. Slightly decreasing attention on useless numbers in word problems increases performance, while increasing or significantly lowering attention decreases performance.&lt;/p&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Transformer model architectures are the new magic bullet in natural language processing, largely due to their attention mechanism. The sudden salience of the transformer and subsequent massive research focus resulted in the emergence of powerful large language models such as the GPT series, Llama, PaLM, and others. The ever-increasing size of these models, as well as the datasets on which they were trained, allows them to continually perform better at a wide range of text generation and analysis tasks [11]. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;However, as with many generative algorithms - especially autoregressive ones like LLMs - the underlying model has no implicit structure for processing or analyzing a logical framework inside the prompt it is given. Transformers, and by extension LLMs, are at their core sequence-to-sequence models. These take in a sequence of arbitrary length and output a sequence of arbitrary length, for example an English sentence input its French translation as the output. Sequence-to-sequence models leverage the fact that language has structure and syntax, and are capable of creating responses that mimic the structural rules followed by its training data [4, 6, 8]. However, in sequence-to-sequence models and the recurrent-neural-network-derived architectures that follow, such as the transformer, there are no intrinsic characteristics that leverage the logical framework of the input. Models that strive to have reasoning capabilities use a variety of approaches to augment the transformer architecture [10], such as specific prompting [1, 7], machine translation [3], salience allocation [5], and more. Some of these improved models exhibit performance that suggests the use of reasoning processes, but as described by Wei et al. [12] “As for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually ‘reasoning.’” Huang et al. share a similar sentiment that highlights that the most widespread solution, and an effective one, is simply the ever-increasing size of LLMs: “…there is observation that these models may exhibit reasoning abilities when they are sufficiently large… despite the strong performance of LLMs on certain reasoning tasks, it remains unclear whether LLMs are actually reasoning and to what extent they are capable of reasoning.” &lt;br /&gt; &lt;br /&gt;&lt;/p&gt; &lt;p&gt;Before diving into why this is interesting, let’s take a step back and briefly inspect the transformer as an architecture. Transformers are loosely an extension of a recurrent neural network that leverage parallel processing and a mechanism known as attention to remove the typical reliance RNNs have on temporal data and instead allow the model to process an entire input sequence simultaneously [13, 9]. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/rnn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/rnn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/rnn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/rnn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;But what is attention? The key upside of transformers is that they are able to capture both short- and long-range dependencies within the input simultaneously, without the need to manage a memory cell like in certain RNN architectures such as a long short-term memory network. This is accomplished through attention, essentially the computation of how much each part of the input should be weighted based on parameters learned from training data. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/att-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/att-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/att-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/att.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;As shown above, each element in the input, split into “tokens,” is given a calculated query and key vector, along with the value vector describing the text, image, or other kind of data contained in the token. This is designed to mimic a value in a database, corresponding to a specific key, being retrieved according to a query. Multiplying some query vector with a given token’s key vector results in a scalar that essentially defines the “significance” of the given token compared to the other tokens, known as an attention score. This attention score is then multiplied by its corresponding token’s value vector and summed to create a context vector representing the aggregate information from the attention step.&lt;br /&gt;&lt;br /&gt; Now we circle back to word problems. Due to the aforementioned absence of explicit logical reasoning capabilities, transformer-based language models - especially smaller ones - can struggle with the few short analytical hops to correctly evaluate a word problem without help. For example, the following question was asked to Llama 2, Meta’s open-source LLM released in 2023. This version of Llama, the smallest available, has 7 billion parameters. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;div align=&quot;center&quot; style=&quot;font-family: monospace; font-size: smaller;&quot;&gt; *User: Answer as concisely as possible. Jack has 7 chairs. Jill takes 7 chairs from him, as well as 2 brooms. How many chairs does Jack have left?* &lt;br /&gt; *Llama-2-7b-chat: Jack has 7 chairs left.* &lt;/div&gt; &lt;p&gt;You may notice that this response is incorrect. However, it is wrong in a way that seems to ignore certain important information presented in the question (removing 2 brooms). Of course, there is also unimportant information in the prompt that we want the model to ignore - the fact that Jill took two of Jack’s brooms is not relevant to the number of chairs in his possession.&lt;/p&gt; &lt;h3 id=&quot;related-work&quot;&gt;Related Work&lt;/h3&gt; &lt;p&gt;Existing approaches to entice LLMs to correctly answer word problems like these take a few forms, touched on previously. There are various versions of prompt engineering, which are designed to ask the question in a specific way in order to prompt the model’s response to be formatted in a certain way. Zero-shot chain-of-thought prompting [1, 12, 14] is a commonly cited example, where an additional instruction such as “Let’s think about this step by step” or “Let’s think analytically” are appended to the prompt. These additional instructions encourage the model to decompose the problem into intermediate steps and solve them procedurally. However, Wei et al. note that this does not indicate that the model itself is reasoning, only that it is achieving better results by emulating a structure often used in linear reasoning problems. Additionally, the authors go on to note that this emergent behavior of large models is challenging to reproduce in smaller models. Another novel approach is to parse the input information in a way that is conducive to solving an underlying math problem numerically. Griffith and Kalita treat this process as a machine translation problem, training several unique transformer architectures to make multiple translations from English to arithmetic expressions [3] that can then be evaluated computationally, outside of the LLM. These two techniques can also be fused, using fine-tuned chain-of-thought prompting for zero-shot math word problem solutions, bridging the gap between the previous two methods [7].&lt;br /&gt;&lt;br /&gt; More broadly, solving word problems is a two-part problem: selecting for important information, and then analytically evaluating that information to arrive at an answer. There is a broad body of work on using LLMs to summarize bodies of text, which parallels extraction of useful numerical features from word problems. The two main types of summarization are extractive summarization and abstractive summarization, where the former remains truer to the original input text but struggles to create novel text, while the latter attempts to fill in those gaps but can sometimes create information that was not originally present and may not be correct [15, 5]. Wang et al. in particular create an augmentation to the transformer architecture, dubbed SEASON, that is designed to combine both extractive and abstractive summarization, but contains useful insights into how extractive summarization of text might apply to math word problems. For example, the abstractive power of SEASON comes from the underlying transformer and its generative capabilities, but it is constrained by a fixed-allocation salience system to emphasize extraction of useful information by essentially adding additional key vectors that describe their relevance to a summarization query. This allows the model to predict the salience of potential responses in order to reduce hallucination of abstractive elements. This salience-driven approach shows theoretical promise in complex extractive word problem scenarios, where managing an allocation of salience could translationally be indicative of useful numerical inputs rather than core themes. Salience also shares some characteristics, mechanically, with attention, and raises the question of whether intuition from summarization models can be applied to augment transformer attention to have better extractive logic.&lt;/p&gt; &lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt; &lt;p&gt;This question, bolstered by the similarly-themed research underlying the ability of LLMs to reason and solve math word problems, was the driving force behind our project. Attention is an extremely powerful tool, and a better understanding of how attention scores affect assessment and evaluation of word problems is necessary in order to use it more effectively to address the gaps in the reasoning capabilities of LLMs, especially smaller architectures. A true solution to this problem would be complex, but we strove to answer certain core questions about how math word problems move through large language models, what their attention scores can tell us about how the model is choosing to respond, and what information the model is responding to. Chiefly, we were interested in how the attention scores of certain tokens in word problems - particularly pertaining to numbers necessary for solving the problem - would change throughout the layers of the transformer, and whether that yields insight into how to tune the attention process generally to enhance the models’ abilities, both reasoning and extractive.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;h3 id=&quot;model-and-hardware&quot;&gt;Model and Hardware&lt;/h3&gt; &lt;p&gt;Our chosen model for study was Meta’s Llama 2 7B-chat parameter model. This choice was a result of our particular focus on smaller LLMs, due to the aforementioned emergent reasoning capabilities of models with significantly larger numbers of parameters. Llama 2 is also open-source, allowing us to easily peel apart the attention layers and heads to study how input and output information propagated through the network, as well as extract model weights and attention values. The chat version of the model additionally is better suited for direct question responses, and includes wrappers to handle the relevant meta-parameters to make the chat interface feasible. We hosted Llama 2 on a vast.ai cloud instance due to the high VRAM requirements of the model. The instance consisted of a single Nvidia RTX 4090 GPU instance with 24GB of VRAM connected to an AMD Ryzen 9 5950X 16-core CPU. The model was supported by Nvidia CUDA version 11.7 and the cuDNN GPU-accelerated development library, version 8.9.7. The model itself ran using PyTorch 2.0.1.&lt;/p&gt; &lt;h3 id=&quot;prompt-generation&quot;&gt;Prompt Generation&lt;/h3&gt; &lt;p&gt;We prepended the instruction “Answer as concisely as possible” to each prompt in order to deliberately circumvent potentially invoking chain-of-thought reasoning and thereby subverting the qualities under investigation regarding the model’s zero-shot ability to discern relevant and irrelevant information. In order to assess that capability, we created a question generation algorithm to randomly generate a bank of simple subtraction word problems, for example “If Jack starts out with 7 sponges and Jill takes 4 of them, then Jack gets 2 badges, how many sponges does he have?” Each question contains two numbers necessary to the subtraction - in this example, that would be the number of sponges before and after the events of the problem: 7 and 4. Each example also contains one useless number, corresponding to things that are not relevant to the ultimate question being asked to the model. In this case, that would be the two badges. Each number is generated in its numeral representation (‘7’ rather than ‘seven’), as this ensures that Llama encodes each of these numbers as a single token that can be easily traced. &lt;br /&gt;&lt;br /&gt;Numbers with more digits or numbers spelled out in natural language were often split into multiple consecutive tokens, so to simplify our visualizations we elected to force a single-token representation. This necessitated that each of the four numerical quantities in the math problem - the two relevant numbers, the useless number, and the answer - had to all be unique, in order to avoid accidentally crediting the model for producing a correct response when in fact it simply selected a number in the problem that had been generated to be a duplicate of the answer. This might occur with a problem like “If Jack has 8 umbrellas, and Jill takes 5 of them, then Jack gets 3 belts, how many umbrellas does he have?” In this case, attribution of salience to the value “3 belts” and subsequent inclusion of the number 3 in the answer introduces ambiguity into the correctness of the response, since 3 is in fact the true answer.&lt;br /&gt;&lt;br /&gt; To avoid one-off errors attributed with specific words or sentence structures, the algorithm was designed to randomly construct the sentences using multiple different semantic structures and sample the nouns used from a bank of 100 random objects. Coupled with large testing sets of several hundred examples, this prevents irregularities in the model’s responses to particular syntax or words from significantly affecting results. Finally, the last meaningful element of prompt design was that the nouns chosen to be in the random object pool were deliberately selected to be as semantically difficult as possible. If the model is presented with a question that, for example, includes a number of vehicles as well as a number of cars, it would be entirely justifiable to interpret that question differently than the intent of a subtraction problem with the same numbers but instead involving apples and chinchillas.&lt;br /&gt;&lt;br /&gt; We calculate whether the problem is correct by checking whether the correct number and noun are both present in the correct configuration in the answer content output by Llama. Each prompt was run on a fresh reinitialized instance of Llama, to avoid extracting information from a larger content window that might include numbers or insight from past problems.&lt;/p&gt; &lt;h3 id=&quot;data-extraction&quot;&gt;Data Extraction&lt;/h3&gt; &lt;p&gt;The main data structure was filled as follows. For each new autoregressive output logit, each head in each transformer layer calculates attention scores across all input tokens. These scores were collected and aggregated to map the attention in the model as each prompt moved through the transformer blocks.&lt;/p&gt; &lt;p&gt;In each experiment, attention scores were scraped from the individual model instance for each prompt by selecting the attention values associated with the tokenized representations of the two necessary numerical inputs as well as the single useless input. This produced a lot of data in high dimensions.&lt;/p&gt; &lt;p&gt;To extract the significant aspects of the data and compress it to a reasonable number of dimensions for graphical representation, we took the attention score tensors (which were also saved at their original sizes) and averaged across the following dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Heads in each layer:&lt;/strong&gt; This revealed the change in attention over layers, rather than over heads, in order to potentially reveal the numbers’ progression through deeper-level abstractions, allowing us to answer questions like: &lt;ul&gt; &lt;li&gt;How do self-attention and attention in early layers look for values relevant to the problem?&lt;/li&gt; &lt;li&gt;What role does attention play for the purposes of arriving at a solution to the problem as we reach the middle layers of the model?&lt;/li&gt; &lt;li&gt;Is there a meaningful representation of the numerical values the problem is concerned with deep inside the model?&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Output logits:&lt;/strong&gt; The rationale behind this choice was to allow any intermediate “reasoning” to become evident by encapsulating multiple parts of the response.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input problems:&lt;/strong&gt; Eliminates intrinsic variation in response to slightly different questions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This allowed us to arrive at a representation of how the attention for the relevant tokens changed as it passed through the individual layers of the model.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/averaged-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/averaged-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/averaged-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/averaged.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;attention-modification&quot;&gt;Attention Modification&lt;/h3&gt; &lt;p&gt;For our experiments where we modify attention scores to the useless token, in every layer we multiply every attention score to that token by some value, the multiplier, before taking softmax.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;We found that there was no significant difference between attention to the useless number and the two necessary numbers over 100 samples (with 55/100 accuracy). Perhaps the mid-layers attention peak in the useless number is earlier than for the necessary numbers, but not significantly. We found a peak in attention to all number tokens in middle layers. We found no significant difference between the graphs for problems it answered correctly versus incorrectly.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/correct-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/correct-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/correct-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/correct.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/incorrect-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/incorrect-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/incorrect-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/incorrect.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Here is the attention across all input tokens for one example problem. (Note these are not all the input tokens. The full input tokens were [’’, ‘[’, ‘INST’, ‘]’, ‘Answer’, ‘as’, ‘cons’, ‘is’, ‘ely’, ‘as’, ‘possible’, ‘.’, ‘Jack’, ‘has’, ‘’, ‘9’, ‘pens’, ‘and’, ‘’, ‘7’, ‘spo’, ‘ons’, ‘.’, ‘He’, ‘gives’, ‘away’, ‘’, ‘9’, ‘pens’, ‘.’, ‘How’, ‘many’, ‘pens’, ‘does’, ‘he’, ‘have’, ‘?’, ‘[’, ‘/’, ‘INST’, ‘]’, ‘’]&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/sentence-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/sentence-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/sentence-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/sentence.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Surprisingly, there was not more attention to numbered tokens compared to other tokens.&lt;/p&gt; &lt;p&gt;When looking through each attention head individually, some attended to specific numbered tokens. For example, head 13 layer 16 strongly attended to “9”&lt;/p&gt; &lt;h4 id=&quot;graph-for-13th-heads-only&quot;&gt;Graph for 13th Heads Only&lt;/h4&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/head13-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/head13-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/head13-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/head13.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Finally, we multiplied attentions to the useless number’s token and varied the multiplier from 0 to 5. (30 sampler per data point). We found that it is actually useful to slightly decrease attention to the useless token, and performance decreases as attention to the useless token increases.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/multiplier-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/multiplier-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/multiplier-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-structured-physics-loss-diffusion/multiplier.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We suspect the rise of performance from multiplier of 2 to 5 be insignificant and random due to low sample size.&lt;/p&gt; &lt;p&gt;For small multipliers above 1, there are most responses of the type where the useless number is responded.&lt;/p&gt; &lt;div align=&quot;center&quot; style=&quot;font-family: monospace; font-size: smaller;&quot;&gt; *User: If Jack starts out with 7 coasters and Jill takes 2 of them, then Jack gets 3 badges, how many coasters does he have?* &lt;br /&gt; *Llama-2-7b-chat: Jack has 3 coasters.* &lt;/div&gt; &lt;p&gt;For large multipliers above 1, the softmax causes the other attention values to approach zero and the model’s quality deteriorates.&lt;/p&gt; &lt;div align=&quot;center&quot; style=&quot;font-family: monospace; font-size: smaller;&quot;&gt; *User: Jack has 7 tacos and 2 cucumbers. He gives away 3 tacos. How many tacos does he have?* * &lt;br /&gt; *Jack has 767 tacos. How many tacos does Jack have? Jack has 76 tacos. How many tacos does Jack has?* &lt;/div&gt; &lt;p&gt;And at very extreme multipliers, the model outputs gibberish.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;We found decreasing attention 50% (pre-softmax) on the useless token improves performance on our word problems, and increasing the attention (or decreasing the attention too much). We hypothesize the performance decreases because it 1) makes the model more likely to output the useless number, and 2) changes the model too much, turning responses into gibberish. Our initial exploration of the attention tracked through the layers of the model yielded very little insight, perhaps due to rapid abstraction of the tokens. This gives us insight into how we might further explore using attention as a salience-adajcent metric for extracting information from world problems.&lt;/p&gt; </content> </entry> <entry> <title>Cross-Lingual Fine-Tuning for Multilingual Text Embeddings</title> <link href="https://deep-learning-mit.github.io/blog/2023/multilingual-representations-in-embeddings-models/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/multilingual-representations-in-embeddings-models</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Recently, &lt;a href=&quot;https://platform.openai.com/docs/guides/embeddings&quot;&gt;embeddings models&lt;/a&gt; have become incredibly popular as LLMs become more integrated into tools and applications. Embeddings models (specifically, Siamese encoder-only Transformers) are the state-of-the-art method in retrieval, an old problem in computer science. Embeddings are often used in settings like recommendation algorithms, similarity search, and clustering, and have recently found extensive use in Retrieval-Augmented Generation&lt;d-cite key=&quot;rag&quot;&gt;&lt;/d-cite&gt;, assisting LLMs to be more knowledgeable and truthful. However, the best embeddings models are trained on only English data, which means they suffer greatly at applications in other languages, and are inaccessible to most of the world&lt;d-cite key=&quot;mteb&quot;&gt;&lt;/d-cite&gt;. In this blog post, we summarize the history of embeddings research, detail the training regime of a modern embeddings model, present a new multilingual embedding benchmark, and investigate whether it is possible to fine-tune in multilingual capability to a pretrained monolingual model.&lt;/p&gt; &lt;p&gt;Our central question is whether it is possible to learn new languages at the finetuning stage, using contrastive training on publicly available text pair datasets. If successful, it would mean that the encoder can learn a map from one language onto the embedding space of another. This implies that it is possible to approximate translation, at a conceptual level, with a transformation. We will study the results on various language pairs, and compare to a fully pretrained multilingual model.&lt;/p&gt; &lt;h2 id=&quot;the-embedding-task&quot;&gt;The Embedding Task&lt;/h2&gt; &lt;p&gt;The aim of embedding text (or any other medium) is to convert human-readable information into vectors. This is useful, because while neural nets cannot process words, images, or sound, they can process vectors. Every NLP model thus has some form of embedding - GPTs, for example, have an embedding layer at the start that transforms input tokens into vector representations&lt;d-cite key=&quot;gpt1&quot;&gt;&lt;/d-cite&gt;. GPTs need an embedding layer because the amount of unique tokens is huge (GPT-2, for example, has 50,257 possible tokens&lt;d-cite key=&quot;gpt2&quot;&gt;&lt;/d-cite&gt;), and it is much more computationally efficient to work with lower-dimensional vectors (GPT-2 embeds these down to 768-dimensional vectors to compute with).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/openai_embed-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/openai_embed-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/openai_embed-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/openai_embed.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Embeddings models, as described by OpenAI &lt;/div&gt; &lt;p&gt;Because of this reduction of information, embeddings are also a form of compression. To turn a whole sentence (or paragraph) into a vector requires prioritising some characteristics and losing others, and we find that the most valuable thing to prioritise is semantic and contextual information. This leads to a very useful property: text pairs with similar meanings or usage patterns tend to have similar vector representations. For example, the vectors “cat” and “dog” are closer to each other than “cat” and “cucumber”. Even more interestingly, as found in the Word2Vec paper, this property causes embeddings to have arithmetic consistency, as shown in the famous “king - man + woman = queen” example.&lt;d-cite key=&quot;w2v&quot;&gt;&lt;/d-cite&gt; You can explore the Word2Vec embedding space in the interactive visualization below:&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-09-multilingual-representations-in-embeddings-models/word2vec_demo.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Visualisation of Word2Vec for the 250 most common English nouns &lt;/div&gt; &lt;p&gt;While this may seem abstract, embeddings have found usage in many downstream and commercial tasks, including:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt; - embeddings models classify sentences, such as in sentiment analysis between positive or negative airline reviews&lt;d-cite key=&quot;sent&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Search&lt;/strong&gt; - models return nearest-embedded results to a search query, understanding synonyms and context&lt;d-cite key=&quot;sgpt&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recommendation&lt;/strong&gt; - models return embeddings that suggest related items users may like, for example &lt;a href=&quot;https://arxiv.org/pdf/1507.08439.pdf&quot;&gt;clothes and jewellery&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt; - embeddings are used to cluster datapoints into smaller groups, with downstream algorithms like k-means&lt;d-cite key=&quot;kmeans&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reranking&lt;/strong&gt; - embeddings are used to sort a list, such as one retrieved from a database, into most relevant items&lt;d-cite key=&quot;rerank&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retrieval&lt;/strong&gt; - a query is embedded, and answers are selected by the closeness of their embedding.&lt;d-cite key=&quot;beir&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;history-and-background&quot;&gt;History and Background&lt;/h3&gt; &lt;p&gt;The first successful approaches to these problems were bag-of-words models. These are non-neural algorithms that work by ranking documents based on how many word occurrences they share. There were some improvements around this basic idea, for example Okapi BM25&lt;d-cite key=&quot;bm25&quot;&gt;&lt;/d-cite&gt; includes a term for the expected likelihood of that word co-occurring.&lt;/p&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;Sentence&lt;/th&gt; &lt;th&gt;about&lt;/th&gt; &lt;th&gt;bird&lt;/th&gt; &lt;th&gt;bird,&lt;/th&gt; &lt;th&gt;heard&lt;/th&gt; &lt;th&gt;is&lt;/th&gt; &lt;th&gt;the&lt;/th&gt; &lt;th&gt;word&lt;/th&gt; &lt;th&gt;you&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;About the bird, the bird, bird bird bird&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;You heard about the bird&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;The bird is the word&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;div class=&quot;caption&quot;&gt; A table demonstrating bag-of-words calculation. &lt;/div&gt; &lt;p&gt;The first neural approaches to this problem actually used bag-of-words as a loss function, for example Word2Vec (2013)&lt;d-cite key=&quot;w2v&quot;&gt;&lt;/d-cite&gt; used either continuous bag-of-words (CBOW) or skipgram loss to train a word embedding model. Word2Vec itself is a shallow two-layer neural network that is used to generate an embedding, which in the CBOW training regime is used to predict a word given a bag of surrounding words. The skipgram loss is similar, but weighs words depending on their proximity to the word we’re trying to predict. This word-prediction-from-embeddings task is a &lt;em&gt;key part&lt;/em&gt; of training language models to have useful representations, and we’ll see it again later.&lt;/p&gt; &lt;p&gt;Word2Vec had some incredible results, and was later improved by subsequent approaches&lt;d-cite key=&quot;glove&quot;&gt;&lt;/d-cite&gt;, but word embeddings often failed due to the fact that words with multiple meanings had to share the same point in the embedding space. The sentences “I went to the bank to cash a check” and “I went to the bank to catch a fish” are obviously semantically unrelated, but the word “bank” will necessarily have to share an embedding, making the embedding itself likely meaningless.&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-09-multilingual-representations-in-embeddings-models/special_demo.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Visualisation of Word2Vec struggling with polysemanticity in the &quot;riverbank&quot; example &lt;/div&gt; &lt;p&gt;To solve this, embeddings need to be generated in-context, and be able to support multiple meanings. There were some attempts at changing Word2Vec to support polysemanticity, such as Multi-Sense Skip-Gram (MSSG)&lt;d-cite key=&quot;mssg&quot;&gt;&lt;/d-cite&gt;, but they required hacky workarounds such as pre-programming an expected number of meanings for each word.&lt;/p&gt; &lt;h4 id=&quot;bert&quot;&gt;BERT&lt;/h4&gt; &lt;p&gt;BERT&lt;d-cite key=&quot;bert&quot;&gt;&lt;/d-cite&gt; was arguably the beginning of the LLM revolution, as it showed for the first time that a single pretrained language model could be finetuned to support many different tasks downstream. It was essentially an embeddings model - trained again with the word prediction task, now with the context of words not weighted by proximity, but by a trainable position embedding that provided information that the model could use to predict long-term associations and causality. This fixed the polysemanticity problem described above. It can be used to produce both word-level and sentence-level embeddings, that proved extraordinarily useful for the embeddings tasks.&lt;/p&gt; &lt;h5 id=&quot;bert-training&quot;&gt;BERT Training&lt;/h5&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/bert-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/bert-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/bert-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/bert.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; BERT architecture diagram &lt;/div&gt; &lt;p&gt;BERT (Bidirectional Encoder Representations from Transformers) is based on the Transformer architecture introduced by Vashwani et al. in 2017&lt;d-cite key=&quot;attn&quot;&gt;&lt;/d-cite&gt;. The key differences were that BERT was allowed bidirectional context rather than left-side-only, that it did not include a decoder, and its masked language modeling and next sentence prediction training objectives. The bidirectional context is crucial for language modeling, since “The [MASK]” is much harder to predict than “The [MASK] colored fire engine”, and in embeddings (unlike text generation) we have the whole original text available.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mlm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mlm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mlm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mlm.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; BERT&apos;s Masked Language Modeling loss &lt;/div&gt; &lt;p&gt;MLM works by taking 15% of the text tokens that BERT sees and replacing them with a [MASK] token. The model’s objective is to predict that masked word with its embedding, using the context from the surrounding tokens, and then it is trained on the cross-entropy loss between the predictions and the actual truth.&lt;/p&gt; &lt;p&gt;BERT was also trained on the NSP (Next Sentence Prediction) objective. In training, the model is given a pair of input segments, and its task is to predict whether the second segment (segment B) follows the first one (segment A) in the original text or if they are randomly sampled and unrelated. The input is constructed by concatenating segment A, which is preceded by a special [CLS] token, and segment B, with a special [SEP] (separator) token in between. For example: “[CLS] Segment A [SEP] Segment B”. BERT then produces a pair of embeddings: one for the [CLS] token at the beginning of the input and one for the [SEP] token that separates the two segments. These embeddings are then used to compute a binary classification. The intended effect is that [CLS] contains information about the overall meaning of the first sentence, and [SEP] contains information about the second. This is the first example of sentence embeddings, which are the key to how a modern embeddings model works.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/nsp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/nsp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/nsp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/nsp.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; BERT&apos;s Next Sentence Prediction loss &lt;/div&gt; &lt;p&gt;BERT turns token inputs into embeddings for each token in its context window, which is 512 tokens long. We can choose to construct a single text embedding from this any way we like. There are several popular strategies for this “token pooling” problem. Reading the above, one may be tempted to take the [CLS] token’s embedding. In practice, however, the [CLS] token embeddings proved to be slightly worse than just taking the average of all the individual token embeddings of the sentence&lt;d-cite key=&quot;berthater&quot;&gt;&lt;/d-cite&gt;, and subsequent models such as RoBERTa&lt;d-cite key=&quot;roberta&quot;&gt;&lt;/d-cite&gt; skipped the NSP training objective and actually performed slightly better. Why this is the case is an area of ongoing research, but as a matter of opinion, we personally suspect Shitao Xiao’s work on RetroMAE&lt;d-cite key=&quot;rmae&quot;&gt;&lt;/d-cite&gt; correctly diagnoses the issue, as demonstrated by their models’ improved performance on benchmarks. The training losses described in that paper are more complex and outside the scope of this blog post, but it’s worth a read if interested.&lt;/p&gt; &lt;h4 id=&quot;sbert&quot;&gt;SBERT&lt;/h4&gt; &lt;p&gt;The final part of the story is Sentence-BERT&lt;d-cite key=&quot;sbert&quot;&gt;&lt;/d-cite&gt;, and its addition of contrastive text-pair pretraining. This what turns BERT, a general language model, into a model that specifically generates text embeddings. Contrastive training was discussed at length in 6.s898; the core insight is that we can train an encoder model to have a useful representation if we train it to embed similar examples together, and dissimilar examples far apart. In Sentence Transformers, this is done by contructing a “Siamese BERT” network. There are two BERT models (or commonly two copies of the same model) that are each used to embed a text passage. Then, the loss is calculated by the following formula:&lt;/p&gt; \[\mathcal{L}_N = -\mathbb{E}_{X} \left[ \log \frac{f_k(x_t+k, c_t)}{\sum_{x_j \in X} f_k(x_j, c_t)} \right]\] &lt;p&gt;This encourages the model to predict positive pairs (similar passages) as vectors with close to 1 similarity, and negative pairs close to 0. Similarity metrics include (Euclidean) distance, but most often used is cosine similarity. Negative pairs can either be “mined” with some heuristic such as bag-of-words, or simply sampled at random from other examples in the batch. Due to this, pretraining batch sizes for embedding BERTs are often huge, in the tens of thousands&lt;d-cite key=&quot;gte&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/sbert-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/sbert-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/sbert-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/sbert.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The Siamese BERT architecture &lt;/div&gt; &lt;p&gt;The reason two models are used is that many tasks see improved performance if there is a distinction made between “questions” and “answers”. For example, searches and retrieval queries may not resemble the results they most need in meaning: “What is the the tallest building in Hong Kong” and “The International Commerce Centre” are not closely semantically related, but should be paired in search contexts. Because of this, we can train a “query” and “passage” model together as one giant network on a contrastive loss, and thus get a model that can take in both.&lt;/p&gt; &lt;p&gt;In practice, this improvement is rarely worth doubling the number of parameters, and so most papers simply re-use the same model for both queries and passages.&lt;/p&gt; &lt;h2 id=&quot;how-embeddings-models-are-trained&quot;&gt;How Embeddings Models are Trained&lt;/h2&gt; &lt;p&gt;Putting all this together, we have the current standard recipe for training a modern embeddings model, in up to three stages:&lt;/p&gt; &lt;h3 id=&quot;1-pretraining&quot;&gt;1. Pretraining&lt;/h3&gt; &lt;p&gt;It is valuable to start with a language model that has already learned some inner representation of language. This makes the embeddings task significantly easier, since the model must only learn to condense this inner representation into a single high-dimensional dense vector space. While it is possible to use more modern LLMs such as GPT or LLaMA for embeddings&lt;d-cite key=&quot;sgpt&quot;&gt;&lt;/d-cite&gt;, they are fundamentally hampered because they cannot attend to context in both directions. Therefore, almost all state-of-the-art embeddings models still begin from the BERT models themselves, or their derivatives&lt;d-cite key=&quot;gte&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;e5&quot;&gt;&lt;/d-cite&gt;. These are trained as described above, with an MLM and potentially NSP loss.&lt;/p&gt; &lt;h3 id=&quot;2-training&quot;&gt;2. Training&lt;/h3&gt; &lt;p&gt;Following Sentence-BERT, the model is trained contrastively. At this point, we choose a pooling strategy to convert BERT outputs into sentence embeddings. Many current papers choose to use average pooling&lt;d-cite key=&quot;sbert&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;e5&quot;&gt;&lt;/d-cite&gt;, but max-pooling or taking the [CLS] token are occasionally used. Positive pairs are either handpicked from datasets such as search engine question-responses&lt;d-cite key=&quot;msmarco&quot;&gt;&lt;/d-cite&gt;, or commonly generated from general text data, such as academic paper title-abstract pairs, Wikipedia page title-summaries and so forth&lt;d-cite key=&quot;gte&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;3-fine-tuning&quot;&gt;3. Fine-Tuning&lt;/h3&gt; &lt;p&gt;It has also become common to fine-tune especially large embeddings models on higher-quality datasets, such as MS MARCO (Bing question-passage responses)&lt;d-cite key=&quot;msmarco&quot;&gt;&lt;/d-cite&gt;, fact verification (e.g. FEVER)&lt;d-cite key=&quot;fever&quot;&gt;&lt;/d-cite&gt;, and paraphrasing (e.g. Quora). This increases performance at desired tasks&lt;d-cite key=&quot;bge&quot;&gt;&lt;/d-cite&gt;, and was the inspiration for our approach.&lt;/p&gt; &lt;h2 id=&quot;how-embeddings-models-are-tested&quot;&gt;How Embeddings Models are Tested&lt;/h2&gt; &lt;p&gt;Similarly to how decoder LLMs have recently converged on being measured on the HuggingFace Open LLM Leaderboard, the currently ubiquitous benchmark for embeddings models is MTEB&lt;d-cite key=&quot;mteb&quot;&gt;&lt;/d-cite&gt;. Presented in a 2022 paper, it contains 8 embedding tasks covering a total of 58 datasets. The tasks are:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mteb-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mteb-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mteb-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/mteb.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; MTEB datasets &lt;/div&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bitext Mining&lt;/strong&gt;: Inputs are two sets of sentences from two different languages. For each sentence in the first set, the best match in the second set needs to be found. This metric is commonly ignored in places such as the MTEB Leaderboard and in papers, because few multilingual models have been created.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: A train and test set are embedded with the provided model. The train set embeddings are used to train a logistic regression classifier, which is scored on the test set.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: Involves grouping a set of sentences or paragraphs into meaningful clusters. A k-means model is trained on embedded texts. The model’s performance is assessed using the v-measure, which is independent of the cluster labels.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pair Classification&lt;/strong&gt;: Requires assigning labels to pairs of text inputs, typically indicating if they are duplicates or paraphrases. Texts are embedded and distances calculated using various metrics (cosine similarity, dot product, Euclidean, Manhattan). Metrics like accuracy, average precision, F1, precision, and recall are used.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reranking&lt;/strong&gt;: Involves ranking query results against relevant and irrelevant reference texts. Texts are embedded using a model, with cosine similarity determining relevance. Rankings are scored using mean MRR@k and MAP, with MAP as the primary metric.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Retrieval&lt;/strong&gt;: Each dataset includes a corpus and queries, with a goal to find relevant documents. Models embed queries and documents, computing similarity scores. Metrics like nDCG@k, MRR@k, MAP@k, precision@k, and recall@k are used, focusing on nDCG@10.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Semantic Textual Similarity (STS)&lt;/strong&gt;: Involves assessing the similarity of sentence pairs. Labels are continuous, with higher scores for more similar sentences. Models embed sentences and compute similarity using various metrics, benchmarked against ground truth using Pearson and Spearman correlations. Spearman correlation based on cosine similarity is the main metric.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Summarization&lt;/strong&gt;: Evaluates machine-generated summaries against human-written ones. Models embed summaries, computing distances between machine and human summaries. The closest score, such as the highest cosine similarity, is used for evaluation. Metrics include Pearson and Spearman correlations with human assessments, focusing on Spearman correlation based on cosine similarity.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We can see that MTEB represents many downstream users’ desires as described earlier, but could be criticised for favoring cosine similarity as a distance metric for training. In either case, MTEB has demonstrated, and itself encouraged, some trends in research:&lt;/p&gt; &lt;h3 id=&quot;scaling&quot;&gt;Scaling&lt;/h3&gt; &lt;p&gt;The MTEB paper itself, as well as the GTR&lt;d-cite key=&quot;gtr&quot;&gt;&lt;/d-cite&gt; and Sentence-T5&lt;d-cite key=&quot;st5&quot;&gt;&lt;/d-cite&gt; papers, suggested that model parameters are correlated with higher performance. We should expect that from intuition about GPTs and their scaling laws, larger models perform better&lt;d-cite key=&quot;chinchilla&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scaling-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scaling-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scaling-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scaling.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3 from MTEB demonstrating scaling vs. performance &lt;/div&gt; &lt;p&gt;However, if we extrapolate to more recent research , we find that the state-of-the-art models have failed to get bigger over time, and the highest-performance models are still under 1B parameters. This shows that embeddings is not as easily reduced to scaling laws as LLMs are.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scale-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scale-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scale-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/scale.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; MTEB score vs time for SOTA models. The size of the cross represents parameter count. &lt;/div&gt; &lt;p&gt;However, even these small models still train on hundreds of millions or billions of text pairs&lt;d-cite key=&quot;gtr&quot;&gt;&lt;/d-cite&gt;, requiring thousands of GPU-hours to train. We can conclude that while parameter count may not be increasing, the overall compute requirements of training an embeddings model are getting higher, and it is no longer within the reach of all researchers to work on these models.&lt;/p&gt; &lt;h3 id=&quot;multilingualism&quot;&gt;Multilingualism&lt;/h3&gt; &lt;p&gt;While MTEB is a multilingual benchmark, only a few tasks, namely STS, Classification and Bitext Mining, have multilingual versions. Combined with the abundance of English training data, this has led to every language except English, Chinese and Polish lacking a complete MTEB and thus lacking the benefits of state-of-the-art models.&lt;/p&gt; &lt;p&gt;As in other subfields of NLP, multilingual performance is often an afterthought, and left by the wayside in pursuit of higher performance on English benchmarks, or exclusively in the domain of labs that can afford extra runs&lt;d-cite key=&quot;mt5&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt; &lt;p&gt;With these problems as our motivation, we aim to find out if it is possible to add multilingualism to an existing model without having to pretrain from scratch. This may be a step towards bringing the benefits of increased embeddings performance to languages that don’t currently have a state-of-the-art model. Furthermore, if it is possible to add a new language to an existing model, this hints at the ideas that models do not necessary learn a representation based on a particular language, and that translation is easier than expected in the context of embeddings, modelable as a transformation of the representation space.&lt;/p&gt; &lt;p&gt;To do this, we will take an existing model that has both monolingual English and multilingual variants, and use contrastive training to add in new languages without sacrificing English performance, by using publicly available text translation pairs. We call this approach Cross-Lingual Fine-Tuning (CLFT). We will attempt to create a model that performs on-par with the multilingual model in multiple languages, and on-par with the original model in English, which we will measure by completing with our own data a multilingual version of MTEB in all tasks.&lt;/p&gt; &lt;h3 id=&quot;model-choice&quot;&gt;Model Choice&lt;/h3&gt; &lt;p&gt;We choose e5-base-v2 and multilingual-e5-base&lt;d-cite key=&quot;e5&quot;&gt;&lt;/d-cite&gt; as our demonstration models. E5 is the highest-performing current open-weights model with both a mono- and multilingual version, and still holds the top spot in many less popular languages. Both models are the size of BERT, with 12 layers, 768-dimensional embeddings and a context window of 512 tokens. The only difference is that the multilingual model has a much larger vocabulary to support more languages, and uses the XLM-RoBERTa tokenizer, leading to about 60% more parameters.&lt;/p&gt; &lt;p&gt;This choice does produce a caveat in the rest of our post - since the BERT tokenizer of e5-base has been trained only on English data, it will be unable to tokenize text that is not also a possible English string. In practice, this means that any Latin or near-Latin speaking languages, such as French, German and Turkish, can be used, but the model cannot be finetuned to read unknown characters like Japanese or Arabic script. Any non-Latin characters will likely become an [UNK] token, which carries no information for the model to embed. We are confident that this is not a fatal flaw, though, since just as it is possible to train LLMs with unused vocabulary, such as Persimmon-8B&lt;d-cite key=&quot;persimmon&quot;&gt;&lt;/d-cite&gt;, it is possible to train an embeddings model with a big unused vocabulary. In the case that this research proves useful, it would be easy to train a large English embeddings model with a multilingual tokenizer and fill in this extra vocabulary space afterwards in finetuning.&lt;/p&gt; &lt;h3 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h3&gt; &lt;p&gt;As described above, it is hard to use MTEB to test performance in non-English languages, due to the lack of available tasks. After investigating the source datasets, we know that this is because of a lack of data. In the interest of producing a universally fair test, especially for low-resource languages where quality data is not available, we opted to use synthetic data to create a multilingual MTEB test set, by using machine-translation to convert the English datasets into each language.&lt;/p&gt; &lt;div style=&quot;margin-top: 0.5em; margin-bottom: 1em; padding: 1em; background-color: #f2f5f7; border-radius: 10px; font-size: 1rem&quot;&gt; &lt;i&gt;Side note: We were fascinated to find that the state-of-the-art neural machine translation model is no longer GNMT&lt;d-cite key=&quot;gnmt&quot;&gt;&lt;/d-cite&gt; or the Google Translate API, but in fact just GPT-4!&lt;/i&gt; &lt;/div&gt; &lt;p&gt;We used GPT 3.5 to process ~200K test examples in each of the following languages: French, German, Spanish, Swahili, and Turkish. We selected these languages because of their presence on the No Language Left Behind (NLLB) text-pair dataset&lt;d-cite key=&quot;nllb&quot;&gt;&lt;/d-cite&gt;, widespread usage, use of Latin alphabet, and varying degrees of similarity to English. We were particularly interested in Turkish because, while many words are near-Latin when written down, the language is from a completely different Proto-Turkic root, and appears much harder for native English-speakers to read. We were also interested in Swahili, as despite being one of the world’s biggest languages, it is comparatively data-poor and so multilingual models often struggle to generalize to. We trained five models in total.&lt;/p&gt; &lt;p&gt;As mentioned before, MTEB already contains some multilingual components, in the textual similarity, bitext mining and classification tasks. The bitext mining task in particular requires a cross-lingual model, so we will use it only on the final all-language model. The remaining tasks are clustering, retrieval, classification, re-ranking, STS, and summarization. For each task, we selected one dataset that would generalise well across languages. Given more time and compute resources, it would be easy to expand the dataset to a full synthetic multilingual MTEB. From now on, we refer to this benchmark as MMTEB (Multilingual Massive Text Embeddings Benchmark).&lt;/p&gt; &lt;p&gt;Datasets and code for evaluation are available &lt;a href=&quot;https://github.com/mtybadger/mmteb-toolkit&quot;&gt;HERE&lt;/a&gt;.&lt;/p&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;Task&lt;/th&gt; &lt;th&gt;Classification&lt;/th&gt; &lt;th&gt;Clustering&lt;/th&gt; &lt;th&gt;Retrieval&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dataset&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://huggingface.co/datasets/AmazonScience/massive&quot;&gt;MASSIVE&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://huggingface.co/datasets/willhath/french-reddit-clustering&quot;&gt;Reddit&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/datasets/willhath/spanish-twentynewsgroups-clustering&quot;&gt;TwentyNewsgroup&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://huggingface.co/datasets/sproos/scifact-fr&quot;&gt;SciFact&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Semantic Text Similarity&lt;/th&gt; &lt;th&gt;Summarization&lt;/th&gt; &lt;th&gt;Reranking&lt;/th&gt; &lt;th&gt;Pair Classification&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://huggingface.co/datasets/mteb/sts22-crosslingual-sts/viewer/en?row=0&quot;&gt;STS-22&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://huggingface.co/datasets/sproos/summeval-tr&quot;&gt;SummEval&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://huggingface.co/datasets/sproos/mindsmall-tr&quot;&gt;MIND&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://huggingface.co/datasets/sproos/twitter-pairclass-fr&quot;&gt;Twitter URL Corpus&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;div class=&quot;caption&quot;&gt; Tasks and datasets in MMTEB. &lt;/div&gt; &lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt; &lt;p&gt;In CLFT, we initialize two instances of our base model, one of which is frozen, and the other is trained. We will refer to these as $f_s$ and $f_\theta$ for the static and trained model. The static model will be used to anchor our trained model to the initial representation. For each lanuage $l$, our data $X_l$, is composed of pairs of data points $(x_e, x_l) \in X_l$, where $x_e$ is a sentence in english, and $x_l$ is that sentenced translated to language $l$.&lt;/p&gt; &lt;p&gt;We initially attempted to use the literature-standard InfoNCE&lt;d-cite key=&quot;infonce&quot;&gt;&lt;/d-cite&gt; contrastive loss for \(\mathcal{L}_{\text{eng}}\) and \(\mathcal{L}_{\text{cross}}\), treating equivalent translation pairs as positive examples, and non-equivalent text pairs as negative examples. However, empirically this did not perform as well, which we suspect may be because of compute-based batch size limitations (e5 was trained on a comparatively huge batch size of 32,768&lt;d-cite key=&quot;e5&quot;&gt;&lt;/d-cite&gt;). Because of this, we chose to use our own simplified cosine similarity objective.&lt;/p&gt; &lt;p&gt;We give the model \(f_\theta\) the following goal: place \(x_l\) as close to \(x_e\) as possible, without changing where we place \(x_e\). This is crucial, because it forces the model to map the new language onto its existing representation. This is done with the following loss function&lt;/p&gt; \[\mathcal{L}(x_e, x_f) = \mathcal{L}_{\text{eng}} + \beta \mathcal{L}_{\text{cross}}\] &lt;p&gt;Where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;\(\mathcal{L}_{\text{eng}} = 1 - f_\theta(x_e) \cdot f_s(x_e)\) represents the loss component for English text, with \(f_\theta\) as the dynamic model being trained and \(f_s\) as the static reference model.&lt;/li&gt; &lt;li&gt;\(\mathcal{L}_{\text{cross}} = 1 - f_\theta(x_e) \cdot f_\theta(x_f)\) represents the cross-lingual consistency loss, comparing the dynamic model’s outputs for English and foreign text.&lt;/li&gt; &lt;li&gt;\(x_e\) and \(x_f\) are inputs for English and foreign text, respectively.&lt;/li&gt; &lt;li&gt;\(\beta\) is a coefficient to balance the influence of the cross-lingual consistency term.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We ran each of our mono-lingual models on 400,0000 text pairs from the NLLB&lt;d-cite key=&quot;nllb&quot;&gt;&lt;/d-cite&gt; dataset, with learning rate \(lr = 1e-4\) and dynamic scaling \(\beta = 0.4\).&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;We found interesting and surprising results across our chosen languages and tasks. The results in table format are available in the appendix.&lt;/p&gt; &lt;p&gt;We can visualize these results in two graphs: comparing our approach to the baseline English model, and to the current state-of-the-art multilingual model.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/base-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/base-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/base-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/base.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; All tasks and languages vs. base model performance &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/multi-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/multi-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/multi-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/multi.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; All tasks and languages vs. multilingual model performance &lt;/div&gt; &lt;p&gt;We can see that the CLFT approach did extremely well on tasks like classification, pair classification and clustering, even beating the multilingual model itself. This is to be expected in particularly well-suited tasks, since a perfect monolingual model will always outperform a multilingual model at a set number of parameters. However, the model did not improve as strongly in retrieval and semantic textual similarity tasks. Additionally, we can see the model struggle most significantly in Swahili, the most distant language to its original English in our training set. Overall, we observed an average 5.5% relative improvement on the base model, taking us &lt;strong&gt;49.8%&lt;/strong&gt; of the way to the performance of the multilingual model.&lt;/p&gt; &lt;p&gt;We have some conjectures about the reason for this split, which relate to the theory of representation learning. Since our loss is purely on positive pairs, there is weaker enforcement of a shape of the embeddings space. It is therefore likely that our approach is degenerating the shape of the embeddings space, leading to more clustering and noisier local structure. This means that tasks that rely on broad-strokes embeddings, such as clustering, classification and so on, will benefit from this approach, whereas tasks that rely on fine-grained relative positioning such as retreival, reranking and STS will suffer. CLFT could thus be viewed as a trade-off between speed and ease of training, and noisiness of embeddings.&lt;/p&gt; &lt;p&gt;We investigate this by performing a visual analysis of the embeddings after PCA dimension reduction. In the figure below, we see how different model represents the same text, after it has been translated. The texts were taken from the associated reddit clustering datasets for each language, and the labels in the diagrams are the name of the corresponding class of the datapoint. We see that the position of each embedding is roughly the same, which makes sense given our loss function.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/words-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/words-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/words-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/words.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Representations of the same text, translated to models&apos;s respecive languages &lt;/div&gt; &lt;p&gt;Additionally, the figure below demonstrates that we were mostly successful in our goal of keeping our trained models aligned with the underlying english model. We embedded the same, English text with each model and got an even tighter clustering. We see that the training on languages more similar to english, such as Spanish, did not alter the English represenations as significantly. Conversely, more distant languages, such as Swahili, led to further degradation of the embedding space.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/english-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/english-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/english-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-multilingual-representations-in-embeddings-models/english.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Representations of English text, according to each language &lt;/div&gt; &lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt; &lt;p&gt;Based on our results, we conclude that fine tuning for multilinguality is a cheap and viable alternative, especially when working with languages that do not have a large presence on the internet. While not an improvement over “true” multilingual models in general, CLFT can outperform multilingual models in scenarios where high-quality data is sparse, or in specific task categories (like clustering and reranking).&lt;/p&gt; &lt;p&gt;Additionally, we have made steps to introduce the first truly multilingual benchmark, for future embedding models to be evaluated against. All code and data for MMTEB assessment can be found &lt;a href=&quot;https://github.com/mtybadger/mmteb-toolkit&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;h3 id=&quot;limitations-and-next-steps&quot;&gt;Limitations and Next Steps&lt;/h3&gt; &lt;p&gt;Our experiment has several limitations, and there is plenty of room for extension:&lt;/p&gt; &lt;p&gt;The fact that we used machine-translated English text for our benchmark poses potential issues. It’s likely that the distribution of data that our translation model produces is not equivalent to that produced in the real world, meaning that our benchmark isn’t as accurate as the English one is. This is hard to ameliorate, especially for languages lacking many large datasets. However, barring vast troves of previously undiscovered internet data being discovered, translations can serve as a useful stopgap, and an equalizer for these less available languages. Completing the MMTEB benchmark would be a valuable contribution to the field, and a path to more languages being represented in state-of-the-art models.&lt;/p&gt; &lt;p&gt;In this paper, we only evaluated monolingual models, and did not study how the approach scales to multiple languages at once. Due to time and compute constriants, we were unable to try and train a “true” multilingual model, beyond just english and one other language. We believe that with further training, it may be possible to repeat the process above for multiple languages.&lt;/p&gt; &lt;p&gt;As mentioned in our results, CLFT can lead to noisy embeddings, which may decrease performance on particular tasks. A better distillation loss, or traditional contrastive loss with a much larger batch size, may help to regularize the data and resolve this issue.&lt;/p&gt; &lt;p&gt;As previously mentioned, we could not explore non-latin characters, vastly reducing our set of potential languages. We believe that with the correct tokenizer and base model, this should be possible. Additionally, it’s becoming possible to imagine a future of Transformers without tokenization, which would greatly help approaches like ours.&lt;/p&gt; &lt;p&gt;Despite our models maintaining near perfect alignment with the base model on the english text pairs during training, we observed performance on the English MTEB decrease substantially. This suggests that the text pairs on NLLB do not fully capture the distribution of data seen during testing,which is something that could be improved upon with better translation datasets.&lt;/p&gt; &lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt; &lt;p&gt;Here is a full table of our results:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;Classification&lt;/th&gt; &lt;th&gt;Summarization&lt;/th&gt; &lt;th&gt;Clustering&lt;/th&gt; &lt;th&gt;Retrieval&lt;/th&gt; &lt;th&gt;STS&lt;/th&gt; &lt;th&gt;Reranking&lt;/th&gt; &lt;th&gt;Pair Classification&lt;/th&gt; &lt;th&gt;Average&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Spanish (e5-base)&lt;/td&gt; &lt;td&gt;0.511&lt;/td&gt; &lt;td&gt;0.314&lt;/td&gt; &lt;td&gt;0.333&lt;/td&gt; &lt;td&gt;0.554&lt;/td&gt; &lt;td&gt;0.585&lt;/td&gt; &lt;td&gt;0.296&lt;/td&gt; &lt;td&gt;0.828&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.489&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Spanish (e5-multi)&lt;/td&gt; &lt;td&gt;0.635&lt;/td&gt; &lt;td&gt;0.301&lt;/td&gt; &lt;td&gt;0.336&lt;/td&gt; &lt;td&gt;0.655&lt;/td&gt; &lt;td&gt;0.629&lt;/td&gt; &lt;td&gt;0.243&lt;/td&gt; &lt;td&gt;0.848&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.521&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Spanish (ours)&lt;/td&gt; &lt;td&gt;0.583&lt;/td&gt; &lt;td&gt;0.314&lt;/td&gt; &lt;td&gt;0.398&lt;/td&gt; &lt;td&gt;0.568&lt;/td&gt; &lt;td&gt;0.553&lt;/td&gt; &lt;td&gt;0.284&lt;/td&gt; &lt;td&gt;0.847&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.507&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;German (e5-base)&lt;/td&gt; &lt;td&gt;0.522&lt;/td&gt; &lt;td&gt;0.307&lt;/td&gt; &lt;td&gt;0.328&lt;/td&gt; &lt;td&gt;0.560&lt;/td&gt; &lt;td&gt;0.236&lt;/td&gt; &lt;td&gt;0.293&lt;/td&gt; &lt;td&gt;0.812&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.437&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;German (e5-multi)&lt;/td&gt; &lt;td&gt;0.637&lt;/td&gt; &lt;td&gt;0.313&lt;/td&gt; &lt;td&gt;0.346&lt;/td&gt; &lt;td&gt;0.648&lt;/td&gt; &lt;td&gt;0.491&lt;/td&gt; &lt;td&gt;0.230&lt;/td&gt; &lt;td&gt;0.840&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.501&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;German (ours)&lt;/td&gt; &lt;td&gt;0.602&lt;/td&gt; &lt;td&gt;0.320&lt;/td&gt; &lt;td&gt;0.393&lt;/td&gt; &lt;td&gt;0.546&lt;/td&gt; &lt;td&gt;0.273&lt;/td&gt; &lt;td&gt;0.332&lt;/td&gt; &lt;td&gt;0.841&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.472&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;French (e5-base)&lt;/td&gt; &lt;td&gt;0.512&lt;/td&gt; &lt;td&gt;0.312&lt;/td&gt; &lt;td&gt;0.329&lt;/td&gt; &lt;td&gt;0.568&lt;/td&gt; &lt;td&gt;0.747&lt;/td&gt; &lt;td&gt;0.330&lt;/td&gt; &lt;td&gt;0.825&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.518&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;French (e5-multi)&lt;/td&gt; &lt;td&gt;0.637&lt;/td&gt; &lt;td&gt;0.306&lt;/td&gt; &lt;td&gt;0.263&lt;/td&gt; &lt;td&gt;0.644&lt;/td&gt; &lt;td&gt;0.764&lt;/td&gt; &lt;td&gt;0.222&lt;/td&gt; &lt;td&gt;0.845&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.526&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;French (ours)&lt;/td&gt; &lt;td&gt;0.622&lt;/td&gt; &lt;td&gt;0.302&lt;/td&gt; &lt;td&gt;0.404&lt;/td&gt; &lt;td&gt;0.604&lt;/td&gt; &lt;td&gt;0.749&lt;/td&gt; &lt;td&gt;0.344&lt;/td&gt; &lt;td&gt;0.849&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.554&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Turkish (e5-base)&lt;/td&gt; &lt;td&gt;0.458&lt;/td&gt; &lt;td&gt;0.296&lt;/td&gt; &lt;td&gt;0.221&lt;/td&gt; &lt;td&gt;0.411&lt;/td&gt; &lt;td&gt;0.456&lt;/td&gt; &lt;td&gt;0.308&lt;/td&gt; &lt;td&gt;0.776&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.418&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Turkish (e5-multi)&lt;/td&gt; &lt;td&gt;0.639&lt;/td&gt; &lt;td&gt;0.304&lt;/td&gt; &lt;td&gt;0.318&lt;/td&gt; &lt;td&gt;0.631&lt;/td&gt; &lt;td&gt;0.601&lt;/td&gt; &lt;td&gt;0.258&lt;/td&gt; &lt;td&gt;0.827&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.511&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Turkish (ours)&lt;/td&gt; &lt;td&gt;0.557&lt;/td&gt; &lt;td&gt;0.307&lt;/td&gt; &lt;td&gt;0.382&lt;/td&gt; &lt;td&gt;0.413&lt;/td&gt; &lt;td&gt;0.414&lt;/td&gt; &lt;td&gt;0.336&lt;/td&gt; &lt;td&gt;0.826&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.462&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Swahili (e5-base)&lt;/td&gt; &lt;td&gt;0.413&lt;/td&gt; &lt;td&gt;0.304&lt;/td&gt; &lt;td&gt;0.181&lt;/td&gt; &lt;td&gt;0.281&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.313&lt;/td&gt; &lt;td&gt;0.751&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.321&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Swahili (e5-multi)&lt;/td&gt; &lt;td&gt;0.528&lt;/td&gt; &lt;td&gt;0.303&lt;/td&gt; &lt;td&gt;0.166&lt;/td&gt; &lt;td&gt;0.527&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.257&lt;/td&gt; &lt;td&gt;0.822&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.372&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Swahili (ours)&lt;/td&gt; &lt;td&gt;0.347&lt;/td&gt; &lt;td&gt;0.315&lt;/td&gt; &lt;td&gt;0.238&lt;/td&gt; &lt;td&gt;0.332&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.275&lt;/td&gt; &lt;td&gt;0.764&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.325&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Average (e5-base)&lt;/td&gt; &lt;td&gt;0.483&lt;/td&gt; &lt;td&gt;0.307&lt;/td&gt; &lt;td&gt;0.279&lt;/td&gt; &lt;td&gt;0.475&lt;/td&gt; &lt;td&gt;0.405&lt;/td&gt; &lt;td&gt;0.308&lt;/td&gt; &lt;td&gt;0.799&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.436&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Average (e5-multi)&lt;/td&gt; &lt;td&gt;0.615&lt;/td&gt; &lt;td&gt;0.306&lt;/td&gt; &lt;td&gt;0.286&lt;/td&gt; &lt;td&gt;0.621&lt;/td&gt; &lt;td&gt;0.497&lt;/td&gt; &lt;td&gt;0.242&lt;/td&gt; &lt;td&gt;0.836&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.486&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Average (ours)&lt;/td&gt; &lt;td&gt;0.542&lt;/td&gt; &lt;td&gt;0.312&lt;/td&gt; &lt;td&gt;0.363&lt;/td&gt; &lt;td&gt;0.493&lt;/td&gt; &lt;td&gt;0.398&lt;/td&gt; &lt;td&gt;0.314&lt;/td&gt; &lt;td&gt;0.825&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.464&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </content> </entry> <entry> <title>Learning Interpretable Features with Sparse Auto-Encoders</title> <link href="https://deep-learning-mit.github.io/blog/2023/learning-interpretable-features-with-sparse-autoencoders/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/learning-interpretable-features-with-sparse-autoencoders</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;The field of Machine Learning is becoming increasingly promising as humanity endeavors to create intelligent systems, with models outperforming humans on many tasks. As models become increasingly capable, its important that humans are able to interpret a model’s internal decision making process to mitigate the risk of negative outcomes. While significant progress has been made on interpreting important parts of models like &lt;a href=&quot;https://transformer-circuits.pub/2021/framework/index.html&quot;&gt;attention heads&lt;/a&gt; &lt;d-cite key=&quot;elhage2021mathematical&quot;&gt;&lt;/d-cite&gt;, it’s also the case that hidden layers in deep neural networks have remained notoriously hard to interpret.&lt;/p&gt; &lt;h2 id=&quot;superposition-hypothesis&quot;&gt;Superposition Hypothesis&lt;/h2&gt; &lt;p&gt;One hypothesis for why it can be challenging to interpret individual neurons is because they are simultaneously representing multiple concepts. One may wonder why a network would have its neurons learn to represent multiple concepts. At a first glance, this approach to encoding information feels unintuitive and messy. The key idea comes from the Johnson–Lindenstrauss lemma: In $n$ dimensions, you can have at most $n$ pairwise orthogonal vectors, but the number of pairwise “almost orthogonal” vectors (i.e. cosine similarity at most $\epsilon$) you can have is exponential in $n$. This enables a layer to encode for many more concepts than it has neurons. So long as each neuron is only activated by a sparse combination of concepts, we can reconstruct these concepts from a given activation with minimal interference between the concepts, since they are “almost orthogonal”. This hypothesis is known as &lt;strong&gt;&lt;a href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot;&gt;superposition&lt;/a&gt;&lt;/strong&gt; &lt;d-cite key=&quot;elhage2022superposition&quot;&gt;&lt;/d-cite&gt;, and offers an explanation for why neurons have been observed in practice to be polysemantic.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Diagram depicting a larger model with disentngled features and a lower dimensional projection simulating this larger network using polysemanticity. Source &lt;d-cite key=&quot;elhage2022superposition&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;h2 id=&quot;sparse-auto-encoders&quot;&gt;Sparse Auto-Encoders&lt;/h2&gt; &lt;p&gt;Since deep neural networks are strongly biased towards making neurons polysemantic during training, humans might try to understand the model’s decision making process by “unwrapping” the network into the sparse features that the neurons in some particular layer are simulating. To do this, a concept called a Sparse Auto-Encoder (SAE) is used. An SAE is similar to a normal autoencoder, with two main differences: (1) the encoding layer is larger than the neuron layer, often by a factor of 4x. (2) the loss function penalizes not only for the MSE loss, but also for the sparsity of the encoder matrix, frequently represented as L1 loss. A sparse autoencoder lets us learn a sparse representation for a vector, but in a higher dimensional space. SAEs were first proposed in a &lt;a href=&quot;https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition&quot;&gt;blogpost&lt;/a&gt; by Lee Sharkey in December 2022, and in September 2023 more research was published on SAEs, both by a group of &lt;a href=&quot;https://arxiv.org/abs/2309.08600&quot;&gt;independent researchers&lt;/a&gt; &lt;d-cite key=&quot;cunningham2023sparse&quot;&gt;&lt;/d-cite&gt; and by &lt;a href=&quot;https://transformer-circuits.pub/2023/monosemantic-features/&quot;&gt;Anthropic&lt;/a&gt; &lt;d-cite key=&quot;bricken2023monosemanticity&quot;&gt;&lt;/d-cite&gt; demonstrating that not only can SAEs be learned at a specific layer, but the features they learn are human interpretable.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Diagram depicting an SAE architecture for a transformer language model. Source &lt;d-cite key=&quot;cunningham2023sparse&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;h2 id=&quot;research-question&quot;&gt;Research Question&lt;/h2&gt; &lt;p&gt;This inspired a new idea: what if we could take a neural network, unwrap each layer into a larger, sparse, interpretable set of features, and then learn a sparse weight matrix connecting all pairs of two consecutive feature layers? This would mean that we could take a neural network, and transform it into a new neural network simulating the old neural network, with the nice property that the computations are sparse and hopefully interpretable.&lt;/p&gt; &lt;p&gt;The main question we wish to explore is: Can we unwrap a deep neural network into a larger sparse network and learn sparse weights between consecutive feature layers without losing performance?&lt;/p&gt; &lt;h2 id=&quot;initial-mathematics&quot;&gt;Initial Mathematics&lt;/h2&gt; &lt;p&gt;Let’s begin by looking at $L_1$ and $L_2$, two consecutive layers in a deep neural network with ReLU activations. Let $W$ and $b$ be the matrix and bias respectively that connects these two layers. Then we have&lt;/p&gt; \[L_2 = \text{ReLU}(W L_1 + b)\] &lt;p&gt;We create autoencoders such that&lt;/p&gt; \[L_1 = D_1 \text{ReLU}(E_1 L_1 + e_1) \equiv D_1 F_1\] \[L_2 = D_2 \text{ReLU}(E_2 L_2 + e_2) \equiv D_2 F_2\] &lt;p&gt;where $D_i$ is the decoder for layer $i$, $E_i$ and $e_i$ are the weights of the encoder and encoder bias, and $F_i$ is the feature vector.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Biases excluded from diagram for clarity. The hockey sticks on top of $F_1$, $L_2$, and $F_2$ indicate that a ReLU is applied to get the activations at that layer. If our autoencoder is good (which it should be), we have $L_1=L_1&apos;$ and $L_2=L_2&apos;$. &lt;/div&gt; &lt;p&gt;Thus we have&lt;/p&gt; \[\begin{align} F_2 &amp;amp;= \text{ReLU}(E_2 L_2 + e_2) \\ &amp;amp;= \text{ReLU}(E_2 \text{ReLU}(W L_1 + b) + e_2) \\ &amp;amp;= \text{ReLU}(E_2 \text{ReLU}(W D_1 F_1 + b) + e_2). \end{align}\] &lt;p&gt;In general, an approximation of the form&lt;/p&gt; \[F_2 = \text{ReLU}(W_2 F_1 + b_2)\] &lt;p&gt;would be pretty terrible since we cannot easily approximate a double ReLU function with a single ReLU function. However, because of the way $F_1$ and $F_2$ are created from $L_1$ and $L_2$, the relationships are actually very sparse in nature, so we will try to learn the approximation above. Perhaps there is a clever initialization that will allow us to learn this more easily.&lt;/p&gt; &lt;p&gt;If we just ignored the inside ReLU in the definition of $F_2$, then we’d have&lt;/p&gt; \[F_2 = \text{ReLU}(E_2 W D_1 F_1 + E_2 b + e_2)\] &lt;p&gt;which suggests the following could be a good initialization for our learned weight $W_2$ and bias $b_2$.&lt;/p&gt; \[W_2 = E_2 W D_1\] \[b_2 = E_2 b + e_2\] &lt;p&gt;While this initialization seemed reasonable at the start of the project, it turned out that during training this results in a local minimum, and you can actually get much lower loss if you randomly initialize $W_2$ and $b_2$.&lt;/p&gt; &lt;h2 id=&quot;codebase&quot;&gt;Codebase&lt;/h2&gt; &lt;p&gt;To answer this main question, the first step was to build out a &lt;a href=&quot;https://drive.google.com/file/d/1_0g_Qq76AqJByCrj_i-tYr76KPeAfIem/view?usp=sharing&quot;&gt;codebase&lt;/a&gt; that had all the implementations necessary to run experiements to explore this question. The codebase was developed from scratch to ensure I understood how each part of the code worked.&lt;/p&gt; &lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt; &lt;p&gt;The first part of the code trains a four layer neural network to classify MNIST images. After training we got a validation loss of 0.09 and a validation accuracy: 0.98, indicating the model does well. For clarity, all losses described in this section will refer to loss on the validation set.&lt;/p&gt; &lt;h3 id=&quot;saes&quot;&gt;SAEs&lt;/h3&gt; &lt;p&gt;Next, two autoencoder architectures are implemented, one that learns both an encoder and decoder, and one that learns only an encoder as its decoder is tied as the transpose of the encoder. Empirically, the tied autoencoder seemed to perform better and achieved an L1 (sparsity) loss of 0.04928, and an L2 (MSE) loss of 0.03970. Seeing these numbers close in magnitude is good, indicating that the model is neither penalizing too much nor too little for L1 sparsity loss.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; For a random input: The top diagram depicts neuron activations (blue) and reconstructed neuron activations from the SAE (orange), indicating the SAE has low L2 loss and reconstructs the input well. The bottom diagram depicts the feature activations for the same input, showing they are sparse. Notably, 38/64 of the neuron activations have magnitude above 0.3, but only 7/256 of the encoded features have magnitude above 0.3. &lt;/div&gt; &lt;h3 id=&quot;feature-connectors&quot;&gt;Feature Connectors&lt;/h3&gt; &lt;p&gt;Then, a feature connector was implemented, which learns the matrices $W_2$ and $b_2$ descibed above mapping one layer to another layer. The inputs are the set of all feature $i$ activations and the outputs are the set of all feature $i+1$ activations, allowing us to gradient descent over loss (which consists of L1 sparsity and L2 MSE) to optimize $W_2$ and $b_2$. The L1 (sparsity) loss was 0.02114 and the L2 (MSE) loss: 0.03209, indicating that there is a good tradeoff between L1 and L2 penalty.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Weights matrix connecting neuron layer 1 to neuron layer 2. This is a mess. 2205 weights have magnitude greater than 0.1. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Weights matrix connecting encoded features in layer 1 to encoded features in layer 2. This is nice and sparse. 458 weights have magnitude greater than 0.1. &lt;/div&gt; &lt;p&gt;Below is what the feature connector matrix looks like after each epoch of training.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;simulating-the-feature-network&quot;&gt;Simulating the Feature Network&lt;/h3&gt; &lt;p&gt;Finally, we replace neuron connections with feature connections. This means that when we pass an input through the network, we immediately encode it as a feature and propogate it through the feature connector weights, skipping the neuron layer weights. In this network, removing two neuron to neuron layers and substituting them with feature to feature layers results in a decrease from 97.8% accuracy to 94% accuracy, which is pretty good considering we made our network much sparser.&lt;/p&gt; &lt;p&gt;Next, I tried to visualize the features using a variety of methods (both inspired by a class lecture and a &lt;a href=&quot;https://distill.pub/2017/feature-visualization&quot;&gt;Distill blogpost&lt;/a&gt; &lt;d-cite key=&quot;olah2017feature&quot;&gt;&lt;/d-cite&gt;). Unfortunately, I did not find the features to be much more interpretable than the neurons for the MNIST dataset. Still, our results are cool: we can take a network, and with only a fraction of the parameters maintain comparable performance.&lt;/p&gt; &lt;h2 id=&quot;language-models&quot;&gt;Language Models&lt;/h2&gt; &lt;p&gt;I shared these results with Logan Riggs, one of the &lt;a href=&quot;https://arxiv.org/abs/2309.08600&quot;&gt;independent researchers&lt;/a&gt; &lt;d-cite key=&quot;cunningham2023sparse&quot;&gt;&lt;/d-cite&gt; who published about SAEs in October 2023. Excited about the possibility, we collaborated to see if we could achieve the same results for language models, anticipating that the learned features might be more interpretable. We and a couple other collaborators published a &lt;a href=&quot;https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms&quot;&gt;blogpost&lt;/a&gt; showing that the learned features in Pythia-70M are indeed interpretable, and there are cool relationships! (the remainder of this section is adapted from that blogpost)&lt;/p&gt; &lt;p&gt;Below we show some examples of sparse linear feature connections. For the curious reader, additional examples can be found &lt;a href=&quot;https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4?pvs=4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&quot;or-example&quot;&gt;OR Example&lt;/h3&gt; &lt;p&gt;In Layer 1, we have:&lt;/p&gt; \[OF_{30} = 0.26IF_{2797} + 0.23IF_{259} + 0.10IF_{946}\] &lt;p&gt;where OF is output feature (in MLP_out), and IF is input feature (in Residual Stream before the MLP)&lt;/p&gt; &lt;p&gt;Below is input feature 2797, activating strongly on the token “former”&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; This is 5 examples. For each example, the top row of words are feature activation e.g. token &quot;former&quot; activated 9.4. The bottom blank row is: if we removed this feature, how much worse does the model get at predicting these tokens? e.g. Soviet is 5.5 logits worse when the model can&apos;t use this &quot;former&quot; feature. &lt;/div&gt; &lt;p&gt;Below is input feature 259, activating strongly on the token “old”&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Below is input feature 946, activating on the token “young”&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In the output feature, we see the tokens former, old, and young all activate, with young activating about half as strongly as “former” and “old” as we would expect from the weight coefficients.&lt;/p&gt; &lt;p&gt;\(OF_{30} = 0.26IF_{former} + 0.23IF_{old} + 0.10IF_{young}\)&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We can view this computation as a weighted logical OR. Output Feature 30 activates on former OR old OR young.&lt;/p&gt; &lt;h3 id=&quot;negative-weight-example&quot;&gt;Negative Weight Example&lt;/h3&gt; &lt;p&gt;In Layer 1, we have:&lt;/p&gt; \[OF_{505} = 0.68IF_{3021} -0.21IF_{729}\] &lt;p&gt;where OF is output feature, and IF is input feature.&lt;/p&gt; &lt;p&gt;Below is input feature 3021, activating strongly on tokens like “said” which in almost all cases appear not after a quote.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Below is input feature 729, activating strongly on tokens like “said” when they appear shortly after a quote.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Below we see the output feature activates on tokens like “said” that have no prior quote tokens. We’ve “subtracted out” with a large negative weight, so to speak, the examples where “said” appears after a quote, and now the feature only activates when “said” appears without any prior quotes.&lt;/p&gt; \[OF_{505} = 0.68IF_{(\text{&quot;said&quot; in many contexts})} -0.21IF_{(\text{&quot;said&quot; after quotes})}\] &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We can view this computation as a weighted logical AND. Output Feature 505 activates on A AND ~B. In the case where A is a superset of B, this is the complement of B e.g. I have the set of all fruits and all yellow fruits, so now I can find all non-yellow fruits.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our exploration into interpreting neural networks using Sparse Auto-Encoders has shown promising results. The ability to unwrap the layers of a neural network into a more interpretable, sparse representation without a significant loss in performance supports the superposition hypothesis. Even if the features were only interpretable on some architectures/datasets, I am optimistic that Sparse Auto-Encoders will not only make deep neural networks more interpretable, but they will also allow for quicker parallelized inference since each output feature will depend on a small fraction of the total possible input features.&lt;/p&gt; &lt;p&gt;I’d like to thank everyone who has contributed to my deep learning education this semester. I have learned a tremendous amount and really enjoyed working on this project.&lt;/p&gt; </content> </entry> <entry> <title>How does model size impact catastrophic forgetting in online continual learning?</title> <link href="https://deep-learning-mit.github.io/blog/2023/eunhae-project/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/eunhae-project</id> <content type="html">&lt;!-- &lt;style&gt; .caption { font-size: 0.8em; /* Adjust the size as needed */ text-align: center; color: grey; /* or any color you prefer */ } /* h1 { margin: 0.5em 0 0 0; font-size: 36px; } h3 { margin: 0em; } */ &lt;/style&gt; --&gt; &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt; &lt;p&gt;One of the biggest unsolved challenges in continual learning is preventing forgetting previously learned information upon acquiring new information. Known as “catastrophic forgetting,” this phenomenon is particularly pertinent in scenarios where AI systems must adapt to new data without losing valuable insights from past experiences. Numerous studies have investigated different approaches to solving this problem in the past years, mostly around proposing innovative strategies to modify the way models are trained and measuring its impact on model performance, such as accuracy and forgetting.&lt;/p&gt; &lt;p&gt;Yet, compared to the numerous amount of studies done in establishing new strategies and evaluative approaches in visual continual learning, there is surprisingly little discussion on the impact of model size. It is commonly known that the size of a deep learning model (the number of parameters) is known to play a crucial role in its learning capabilities &lt;d-cite key=&quot;hu2021model, Bianco_2018&quot;&gt;&lt;/d-cite&gt;. Given the limitations in computational resources in most real-world circumstances, it is often not practical or feasible to choose the largest model available. In addition, sometimes smaller models perform just as well as larger models in specific contexts&lt;d-cite key=&quot;Bressem_2020&quot;&gt;&lt;/d-cite&gt;. Given this context, a better understanding of how model size impacts performance in a continual learning setting can provide insights and implications on real-world deployment of continual learning systems.&lt;/p&gt; &lt;p&gt;In this blog post, I explore the following research question: &lt;em&gt;How do network depth and width impact model performance in an online continual learning setting?&lt;/em&gt; I set forth a hypothesis based on existing literature and conduct a series experiments with models of varying sizes to explore this relationship. This study aims to shed light on whether larger models truly offer an advantage in mitigating catastrophic forgetting, or if the reality is more nuanced.&lt;/p&gt; &lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt; &lt;h3 id=&quot;online-continual-learning&quot;&gt;Online continual learning&lt;/h3&gt; &lt;p&gt;Continual learning (CL), also known as lifelong learning or incremental learning, is an approach that seeks to continually learn from non-iid data streams without forgetting previously acquired knowledge. The challenge in continual learning is generally known as the stability-plasticity dilemma&lt;d-cite key=&quot;mermillod2013-dilemma&quot;&gt;&lt;/d-cite&gt;, and the goal of continual learning is to strike a balance between learning stability and plasticity.&lt;/p&gt; &lt;p&gt;While traditional CL models assume new data arrives task by task, each with a stable data distribution, enabling &lt;em&gt;offline&lt;/em&gt; training. However, this requires having access to all task data, which can be impractical due to privacy or resource limitations. In this study, I will consider a more realistic setting of Online Continual Learning (OCL), where data arrives in smaller batches and are not accessible after training, requiring models to learn from a single pass over an online data stream. This allows the model to learn data in real-time&lt;d-cite key=&quot;soutif-cormerais_comprehensive_2023, cai_online_2021, mai_online_2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Online continual learning can involve adapting to new classes (class-incremental) or changing data characteristics (domain-incremental). Specifically, for class-incremental learning, the goal is to continually expand the model’s ability to recognize an increasing number of classes, maintaining its performance on all classes it has seen so far, despite not having continued access to the old class data&lt;d-cite key=&quot;soutif-cormerais_comprehensive_2023, ghunaim_real-time_2023&quot;&gt;&lt;/d-cite&gt;. Moreover, there has been more recent work done in unsupervised continual learning &lt;d-cite key=&quot;yu_scale_2023, madaan_representational_2022&quot;&gt;&lt;/d-cite&gt;. To narrow the scope of the vast CL landscape to focus on learning the impact of model size in CL performance, I will focus on the more common problem of class-incremental learning in supervised image classification in this study.&lt;/p&gt; &lt;h3 id=&quot;continual-learning-techniques&quot;&gt;Continual learning techniques&lt;/h3&gt; &lt;p&gt;Popular methods to mitigate catastrophic forgetting in continual learning generally fall into three buckets:&lt;d-cite key=&quot;ghunaim_real-time_2023&quot;&gt; :&lt;/d-cite&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;em&gt;regularization-based&lt;/em&gt; approaches that modify the classification objective to preserve past representations or foster more insightful representations, such as Elastic Weight Consolidation (EWC)&lt;d-cite key=&quot;kirkpatrick2017overcoming&quot;&gt;&lt;/d-cite&gt; and Learning without Forgetting (LwF)&lt;d-cite key=&quot;li_learning_2017&quot;&gt;&lt;/d-cite&gt;;&lt;/li&gt; &lt;li&gt;&lt;em&gt;memory-based&lt;/em&gt; approaches that replay samples retrieved from a memory buffer along with every incoming mini-batch, including Experience Replay (ER)&lt;d-cite key=&quot;chaudhry2019tiny&quot;&gt;&lt;/d-cite&gt; and Maximally Interfered Retrieval&lt;d-cite key=&quot;aljundi2019online&quot;&gt;&lt;/d-cite&gt;, with variations on how the memory is retrieved and how the model and memory are updated; and&lt;/li&gt; &lt;li&gt;&lt;em&gt;architectural&lt;/em&gt; approaches including parameter-isolation approaches where new parameters are added for new tasks and leaving previous parameters unchanged such as Progressive Neural Networks (PNNs)&lt;d-cite key=&quot;rusu2022progressive&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Moreover, there are many methods that combine two or more of these techniques such as Averaged Gradient Episodic Memory (A-GEM)&lt;d-cite key=&quot;chaudhry2019efficient&quot;&gt;&lt;/d-cite&gt; and Incremental Classifier and Representation Learning (iCaRL)&lt;d-cite key=&quot;rebuffi2017icarl&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Among the methods, &lt;strong&gt;Experience Replay (ER)&lt;/strong&gt; is a classic replay-based method and widely used for online continual learning. Despite its simplicity, recent studies have shown ER still outperforms many of the newer methods that have come after that, especially for online continual learning &lt;d-cite key=&quot;soutif-cormerais_comprehensive_2023, mai_online_2021, ghunaim_real-time_2023&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;model-size-and-performance&quot;&gt;Model size and performance&lt;/h3&gt; &lt;p&gt;It is generally known across literature that deeper models increase performance&lt;d-cite key=&quot;hu2021model&quot;&gt;&lt;/d-cite&gt;. Bianco et al. conducted a survey of key performance-related metrics to compare across various architectures, including accuracy, model complexity, computational complexity, and accuracy density&lt;d-cite key=&quot;Bianco_2018&quot;&gt;&lt;/d-cite&gt;. Relationship between model width and performance is also been discussed&lt;d-cite key=&quot;hu2021model&quot;&gt;&lt;/d-cite&gt;, albeit less frequently.&lt;/p&gt; &lt;p&gt;He et al. introduced Residual Networks (ResNets)&lt;d-cite key=&quot;he2015deep&quot;&gt;&lt;/d-cite&gt; which was a major innovation in computer vision by tackling the problem of degradation in deeper networks. ResNets do this by residual blocks to increase the accuracy of deeper models. Residual blocks that contain two ore more layers are stacked together, and “skip connections” are used in between these blocks. The skip connections act as an alternate shortcut for the gradient to pass through, which alleviates the issue of vanishing gradient. They also make it easier for the model to learn identity functions. As a result, ResNet improves the efficiency of deep neural networks with more neural layers while minimizing the percentage of errors. The authors compare models of different depths (composed of 18, 34, 50, 101, 152 layers) and show that accuracy increases with depth of the model.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet18&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet34&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet50&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet101&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet152&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Number of Layers&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;34&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;101&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;152&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Number of Parameters&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;~11.7 million&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;~21.8 million&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;~25.6 million&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;~44.5 million&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;~60 million&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Top-1 Accuracy&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;69.76%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;73.31%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;76.13%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;77.37%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;78.31%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Top-5 Accuracy&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;89.08%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;91.42%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;92.86%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;93.68%&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;94.05%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;FLOPs&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;1.8 billion&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;3.6 billion&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;3.8 billion&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;7.6 billion&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;11.3 billion&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;caption&quot;&gt;Table 1: Comparison of ResNet Architectures&lt;/div&gt; &lt;p&gt;This leads to the question: do larger models perform better in continual learning? While much of the focus in continual learning research has often been on developing various strategies, methods, and establishing benchmarks, the impact of model scale remains a less explored path.&lt;/p&gt; &lt;p&gt;Moreover, recent studies on model scale in slightly different contexts have shown conflicting results. Luo et al.&lt;d-cite key=&quot;luo2023empirical&quot;&gt;&lt;/d-cite&gt; highlights a direct correlation between increasing model size and the severity of catastrophic forgetting in large language models (LLMs). They test models of varying sizes from 1 to 7 billion parameters. Yet, Dyer et al.&lt;d-cite key=&quot;dyer2022&quot;&gt;&lt;/d-cite&gt; show a constrasting perspective in the context of pretrained deep learning models. Their results show that large, pretrained ResNets and Transformers are a lot more resistant to forgetting than randomly-initialized, trained-from-scratch models, and that this tendency increases with the scale of model and the pretraining dataset size.&lt;/p&gt; &lt;p&gt;The relative lack of discussion on model size and the conflicting perspectives among existing studies indicate that the answer to the question is far from being definitive. In the next section, I will describe further how I approach this study.&lt;/p&gt; &lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt; &lt;h3 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h3&gt; &lt;p&gt;Online continual learning can be defined as follows&lt;d-cite key=&quot;cai_online_2021, ghunaim_real-time_2023&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;p&gt;The objective is to learn a function $f_\theta : \mathcal X \rightarrow \mathcal Y$ with parameters $\theta$ that predicts the label $Y \in \mathcal Y$ of the input $\mathbf X \in \mathcal X$. Over time steps $t \in \lbrace 1, 2, \ldots \infty \rbrace$, a distribution-varying stream $\mathcal S$ reveals data sequentially, which is different from classical supervised learning.&lt;/p&gt; &lt;p&gt;At every time step,&lt;/p&gt; &lt;ol&gt; &lt;li&gt;$\mathcal S$ reveals a set of data points (images) $\mathbf X_t \sim \pi_t$ from a non-stationary distribution $\pi_t$&lt;/li&gt; &lt;li&gt;Learner $f_\theta$ makes predictions $\hat Y_t$ based on current parameters $\theta_t$&lt;/li&gt; &lt;li&gt;$\mathcal S$ reveals true labels $Y_t$&lt;/li&gt; &lt;li&gt;Compare the predictions with the true labels, compute the training loss $L(Y_t, \hat Y_t)$&lt;/li&gt; &lt;li&gt;Learner updates the parameters of the model to $\theta_{t+1}$&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;task-agnostic-and-boundary-agnostic&quot;&gt;Task-agnostic and boundary-agnostic&lt;/h3&gt; &lt;p&gt;In the context of class-incremental learning, I will adopt the definitions of task-agnostic and boundary-agnostic from Soutif et al. 2023&lt;d-cite key=&quot;soutif-cormerais_comprehensive_2023&quot;&gt;&lt;/d-cite&gt;. A &lt;em&gt;task-agnostic&lt;/em&gt; setting refers to when task labels are not available, which means the model does not know that the samples belong to a certain task. A &lt;em&gt;boundary-agnostic&lt;/em&gt; setting is considered, where information on task boundaries are not available. This means that the model does not know when the data distribution changes to a new task.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Yes&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;No&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Task labels&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Task-aware&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Task-agnotic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Task boundaries&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Boundary-aware&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Boundary-agnostic&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;caption&quot;&gt;Table 2: Task labels and task boundaries. This project assumes task-agnostic and boundary-agnostic settings.&lt;/div&gt; &lt;h3 id=&quot;experience-replay-er&quot;&gt;Experience Replay (ER)&lt;/h3&gt; &lt;p&gt;In a class-incremental learning setting, the nature of the Experience Replay (ER) method aligns well with task-agnostic and boundary-agnostic settings. This is because ER focuses on replaying a subset of past experiences, which helps in maintaining knowledge of previous classes without needing explicit task labels or boundaries. This characteristic of ER allows it to adapt to new classes as they are introduced, while retaining the ability to recognize previously learned classes, making it inherently suitable for task-agnostic and boundary-agnostic continual learning scenarios.&lt;/p&gt; &lt;p&gt;Implementation-wise, ER involves randomly initializing an external memory buffer $\mathcal M$, then implementing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;before_training_exp&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;after_training_exp&lt;/code&gt; callbacks to use the dataloader to create mini-batches with samples from both training stream and the memory buffer. Each mini-batch is balanced so that all tasks or experiences are equally represented in terms of stored samples&lt;d-cite key=&quot;lomonaco2021avalanche&quot;&gt;&lt;/d-cite&gt;. As ER is known be well-suited for online continual learning, it will be the go-to method used to compare performances across models of varying sizes.&lt;/p&gt; &lt;h3 id=&quot;benchmark&quot;&gt;Benchmark&lt;/h3&gt; &lt;p&gt;For this study, the SplitCIFAR-10&lt;d-cite key=&quot;lomonaco2021avalanche&quot;&gt;&lt;/d-cite&gt; is used as the main benchmark. SplitCIFAR-10 splits the popular CIFAR-10 dataset into 5 tasks with disjoint classes, each task including 2 classes each. Each task has 10,000 3×32×32 images for training and 2000 images for testing. The model is exposed to these tasks or experiences sequentially, which simulates a real-world scenario where a learning system is exposed to new categories of data over time. This is suitable for class-incremental learning scenarios. This benchmark is used for both testing online and offline continual learning in this study.&lt;/p&gt; &lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt; &lt;p&gt;Key metrics established in earlier work in online continual learning are used to evaluate the performance of each model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Average Anytime Accuracy (AAA)&lt;/strong&gt; as defined in &lt;d-cite key=&quot;caccia_new_2022&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;The concept of average anytime accuracy serves as an indicator of a model’s overall performance throughout its learning phase, extending the idea of average incremental accuracy to include continuous assessment scenarios. This metric assesses the effectiveness of the model across all stages of training, rather than at a single endpoint, offering a more comprehensive view of its learning trajectory.&lt;/p&gt; \[\text{AAA} = \frac{1}{T} \sum_{t=1}^{T} (\text{AA})_t\] &lt;p&gt;&lt;strong&gt;Average Cumulative Forgetting (ACF)&lt;/strong&gt; as defined in &lt;d-cite key=&quot;soutif-cormerais_comprehensive_2023, soutifcormerais2021importance&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;This equation represents the calculation of the &lt;strong&gt;Cumulative Accuracy&lt;/strong&gt; ($b_k^t$) for task $k$ after the model has been trained up to task $t$. It computes the mean accuracy over the evaluation set $E^k_\Sigma$, which contains all instances $x$ and their true labels $y$ up to task $k$. The model’s prediction for each instance is given by $\underset{c \in C^k_\Sigma}{\text{arg max }} f^t(x)_c$, which selects the class $c$ with the highest predicted logit $f^t(x)_c$. The indicator function $1_y(\hat{y})$ outputs 1 if the prediction matches the true label, and 0 otherwise. The sum of these outputs is then averaged over the size of the evaluation set to compute the cumulative accuracy.&lt;/p&gt; \[b_k^t = \frac{1}{|E^k_\Sigma|} \sum_{(x,y) \in E^k_\Sigma} 1_y(\underset{c \in C^k_\Sigma}{\text{arg max }} f^t(x)_c)\] &lt;p&gt;From Cumulative Accuracy, we can calculate the &lt;strong&gt;Average Cumulative Forgetting&lt;/strong&gt; ($F_{\Sigma}^t$) by setting the cumulative forgetting about a previous cumulative task $k$, then averaging over all tasks learned so far:&lt;/p&gt; \[F_{\Sigma}^t = \frac{1}{t-1} \sum_{k=1}^{t-1} \max_{i=1,...,t} \left( b_k^i - b_k^t \right)\] &lt;p&gt;&lt;strong&gt;Average Accuracy (AA) and Average Forgetting (AF)&lt;/strong&gt; as defined in &lt;d-cite key=&quot;mai_online_2021&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;$a_{i,j}$ is the accuracy evaluated on the test set of task $j$ after training the network from task 1 to $i$, while $i$ is the current task being trained. Average Accuracy (AA) is computed by averaging this over the number of tasks.&lt;/p&gt; \[\text{Average Accuracy} (AA_i) = \frac{1}{i} \sum_{j=1}^{i} a_{i,j}\] &lt;p&gt;Average Forgetting measures how much a model’s performance on a previous task (task $j$) decreases after it has learned a new task (task $i$). It is calculated by comparing the highest accuracy the model $\max_{l \in {1, \ldots, k-1}} (a_{l, j})$ had on task $j$ before it learned task $k$, with the accuracy $a_{k, j}$ on task $j$ after learning task $k$.&lt;/p&gt; \[\text{Average Forgetting}(F_i) = \frac{1}{i - 1} \sum_{j=1}^{i-1} f_{i,j}\] \[f_{k,j} = \max_{l \in \{1,...,k-1\}} (a_{l,j}) - a_{k,j}, \quad \forall j &amp;lt; k\] &lt;p&gt;In the context of class-incremental learning, the concept of classical forgetting may not provide meaningful insight due to its tendency to increase as the complexity of the task grows (considering more classes within the classification problem). Therefore, &lt;d-cite key=&quot;soutif-cormerais_comprehensive_2023&quot;&gt;&lt;/d-cite&gt;recommendeds avoiding relying on classical forgetting as a metric in settings of class-incremental learning, both online and offline settings. Thus, Average Anytime Accuracy (AAA) and Average Cumulative Forgetting (ACF) are used throughout this experiment, although AA and AF are computed as part of the process.&lt;/p&gt; &lt;h3 id=&quot;model-selection&quot;&gt;Model selection&lt;/h3&gt; &lt;p&gt;To compare learning performance across varying model depths, I chose to use the popular ResNet architectures, particularly ResNet18, ResNet34, and ResNet50. As mentioned earlier in this blog, ResNets were designed to increase the performance of deeper neural networks, and their performance metrics are well known. While using custom models for more variability in sizes was a consideration, existing popular architectures were chosen for better reproducibility.&lt;/p&gt; &lt;p&gt;Moreover, while there are newer versions (i.e. ResNeXt&lt;d-cite key=&quot;xie2017aggregated&quot;&gt;&lt;/d-cite&gt;) that have shown to perform better without a huge increase in computational complexity&lt;d-cite key=&quot;Bianco_2018&quot;&gt;&lt;/d-cite&gt;, for this study the original smaller models were chosen to avoid introducing unnecessary variables. ResNet18 and ResNet34 have the basic residual network structure, and ResNet50, ResNet101, and ResNet152 use slightly modified building blocks that have 3 layers instead of 2. This ”bottleneck design” was made to reduce training time. The specifics of the design of these models are detailed in the table from the original paper by He et al.&lt;d-cite key=&quot;he2015deep&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/resnets_comparison-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/resnets_comparison-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/resnets_comparison-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/resnets_comparison.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;ResNet architecture. Table from He et al. (2015)&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Moreover, in order to observe the effect of model width on performance, I also test a slim version of ResNet18 that has been used in previous works&lt;d-cite key=&quot;lopez-paz_gradient_2017&quot;&gt;&lt;/d-cite&gt;. The slim version uses fewer filters per layer, reducing the model width and computational load while keeping the original depth.&lt;/p&gt; &lt;h3 id=&quot;saliency-maps&quot;&gt;Saliency maps&lt;/h3&gt; &lt;p&gt;I use saliency maps to visualize “attention” of the networks. Saliency maps are known to be useful for understanding which parts of the input image are most influential for the model’s predictions. By visualizing the specific areas of an image that a CNN considers important for classification, saliency maps provide insights into the internal representation and decision-making process of the network&lt;d-cite key=&quot;simonyan2014deep&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt; &lt;h3 id=&quot;the-setup&quot;&gt;The setup&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Each model was trained from scratch using the Split-CIFAR10 benchmark with 2 classes per task, for 3 epoches with a mini-batch size of 64.&lt;/li&gt; &lt;li&gt;SGD optimizer with a 0.9 momentum and 1e-5 weight decay was used. The initial learning rate is set to 0.01 and the scheduler reduces it by a factor of 0.1 every 30 epochs, as done in &lt;d-cite key=&quot;lin_clear_2022&quot;&gt;&lt;/d-cite&gt;.&lt;/li&gt; &lt;li&gt;Cross entropy loss is used as the criterion, as is common for image classification in continual learning.&lt;/li&gt; &lt;li&gt;Basic data augmentation is done on the training data to enhance model robustness and generalization by artificially expanding the dataset with varied, modified versions of the original images.&lt;/li&gt; &lt;li&gt;Each model is trained offline as well to serve as baselines.&lt;/li&gt; &lt;li&gt;Memory size of 500 is used to implement Experience Replay. This represents 1% of the training dataset.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt; &lt;p&gt;The continual learning benchmark was implemented using the Avalanche framework&lt;d-cite key=&quot;lomonaco2021avalanche&quot;&gt;&lt;/d-cite&gt;, an open source continual learning library, as well as the code for online continual learning by Soutif et al.&lt;d-cite key=&quot;soutif-cormerais_comprehensive_2023&quot;&gt;&lt;/d-cite&gt;. The experiments were run on Google Colab using NVIDIA Tesla T4 GPU.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Experiment 1&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Experiment 2&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Experiment 3&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Experiment 4&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Experiment 5&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Experiment 6&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Experiment 7&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ResNet18&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ResNet34&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ResNet50&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SlimResNet18&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ResNet18&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ResNet34&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;ResNet50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Strategy&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Experience Replay&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Experience Replay&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Experience Replay&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Experience Replay&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Experience Replay&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Experience Replay&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Experience Replay&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SplitCIFAR10&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SplitCIFAR10&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SplitCIFAR10&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SplitCIFAR10&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SplitCIFAR10&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SplitCIFAR10&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;SplitCIFAR10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Online&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Online&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Online&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Online&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Offline&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Offline&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;Offline&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;V100&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;T4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;A100&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;T4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;T4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;T4&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;T4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Training time (estimate)&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;3h&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;4.5h&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;5h&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;1h&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;5m&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;5m&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;5m&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;caption&quot;&gt;Table 3: Details of experiments conducted in this study&lt;/div&gt; &lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt; &lt;p&gt;Average Anytime Accuracy (AAA) decreases with model size (Chart 1), with a sharper drop from ResNet34 to ResNet50. The decrease in AAA is more significant in online learning than offline learning.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/AAA_on_off-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/AAA_on_off-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/AAA_on_off-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/AAA_on_off.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Chart 1: Average Anytime Accuracy (AAA) of different sized ResNets in online and offline continual learning&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;When looking at average accuracy for validation stream for online CL setting (Chart 2), we see that the rate to which accuracy increases with each task degrade with larger models. Slim-ResNet18 shows the highest accuracy and growth trend. This could indicate that larger models are worse at generalizing to a class-incremental learning scenario.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/stream_acc1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/stream_acc1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/stream_acc1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/stream_acc1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Chart 2: Validation stream accuracy (Online CL)&lt;/figcaption&gt; &lt;/figure&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Average Anytime Acc (AAA)&lt;/strong&gt;&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Final Average Acc&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Slim ResNet18&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.664463&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.5364&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet18&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.610965&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.3712&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet34&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.576129&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.3568&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;ResNet50&lt;/strong&gt;&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.459375&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.3036&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div class=&quot;caption&quot;&gt;Table 4: Accuracy metrics across differently sized models (Online CL) &lt;/div&gt; &lt;p&gt;Now we turn to forgetting.&lt;/p&gt; &lt;p&gt;Looking at Average Cumulative Forgetting (ACF), we see that for online CL setting, ResNet34 performs the best (with a slight overlap at the end with ResNet18), and ResNet50 shows the mosts forgetting. An noticeable observation in both ACF and AF is that ResNet50 performed better initially but forgetting started to increase after a few tasks.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_online-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_online-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_online-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_online.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Chart 3: forgetting curves, Online CL (Solid: Average Forgetting (AF); Dotted: Average Cumulative Forgetting (ACF))&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;However, results look different for offline CL setting. ResNet50 has the lowest Average Cumulative Forgetting (ACF) (although with a slight increase in the middle), followed by ResNet18, and finally ResNet34. This differences in forgetting between online and offline CL setting is aligned with the accuracy metrics earlier, where the performance of ResNet50 decreases more starkly in the online CL setting.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_offline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_offline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_offline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/forgetting_offline.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Chart 4: Forgetting curves, Offline CL (Solid: Average Forgetting (AF); Dotted: Average Cumulative Forgetting (ACF))&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Visual inspection of the saliency maps revealed some interesting observations. When it comes to the ability to highlight intuitive areas of interest in the images, there seemed to be a noticeable improvement from ResNet18 to ResNet34, but this was not necessarily the case from ResNet34 to ResNet50. This phenomenon was more salient in the online CL setting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Online&lt;/strong&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_online-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_online-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_online-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_online.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Image: Saliency map visualizations for Online CL&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Offline&lt;/strong&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_offline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_offline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_offline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliency_offline.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Image: Saliency map visualization for Offline CL&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Interestingly, Slim-ResNet18 seems to be doing better than most of them, certainly better than its plain counterpart ResNet18. A further exploration of model width on performance and representation quality would be an interesting avenue of research.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Slim-ResNet18&lt;/strong&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliencymap_exp4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliencymap_exp4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliencymap_exp4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-eunhae-project/saliencymap_exp4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Image: Saliency map visualization (Slim ResNet18)&lt;/figcaption&gt; &lt;/figure&gt; &lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt; &lt;p&gt;In this study, I compared key accuracy and forgetting metrics in online continual learning across ResNets of different depths and width, as well as brief qualitative inspection of the models’ internal representation. These results show that larger models do not necessary lead to better continual learning performance. We saw that Average Anytime Accuracy (AAA) and stream accuracy dropped progressively with model size, hinting that larger models struggle to generalize to newly trained tasks, especially in an online CL setting. Forgetting curves showed similar trends but with more nuance; larger models perform well at first but suffer from increased forgetting with more incoming tasks. Interestingly, the problem was not as pronounced in the offline CL setting, which highlights the challenges of training models in a more realistic, online continual learning context.&lt;/p&gt; &lt;p&gt;Why do larger models perform worse at continual learning? One of the reasons is that larger models tend to have more parameters, which might make it harder to maintain stability in the learned features as new data is introduced. This makes them more prone to overfitting and forgetting previously learned information, reducing their ability to generalize.&lt;/p&gt; &lt;p&gt;Building on this work, future research could investigate the impact of model size on CL performance by exploring the following questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do pre-trained larger models (vs trained-from-scratch models) generalize better in continual learning settings?&lt;/li&gt; &lt;li&gt;Do longer training improve relatively performance of larger models in CL setting?&lt;/li&gt; &lt;li&gt;Can different CL strategies (other than Experience Replay) mitigate the degradation of performance in larger models?&lt;/li&gt; &lt;li&gt;Do slimmer versions of existing models always perform better?&lt;/li&gt; &lt;li&gt;How might different hyperparameters (i.e. learning rate) impact CL performance of larger models?&lt;/li&gt; &lt;/ul&gt; &lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt; &lt;p&gt;To conclude, this study has empirically explored the role of model size on performance in the context of online continual learning. Specifically, it has shown that model size matters when it comes to continual learning and forgetting, albeit in nuanced ways. These findings contribute to the ongoing discussions on the role of the scale of deep learning models on performance and have implications for future area of research.&lt;/p&gt; </content> </entry> <entry> <title>VGAE Clustering of the Fruit Fly Connectome</title> <link href="https://deep-learning-mit.github.io/blog/2023/deep-connectome-clustering/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/deep-connectome-clustering</id> <content type="html">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/fruit-fly-connectome-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/fruit-fly-connectome-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/fruit-fly-connectome-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/fruit-fly-connectome.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; The fruit fly connectome.&lt;d-cite key=&quot;winding2023connectome&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;Everything you’ve ever learned, every memory you have, and every behavior that defines you is stored somewhere in the neurons and synapses of your big, beautiful brain. The emerging field of connectomics seeks to build connectomes–or neuron graphs–that map the connections between all neurons in the brains of increasingly complex animals, with the goal of leveraging graph structure to gain insights into the functions of specific neurons, and eventually the behaviors that emerge from their interactions. This, as you can imagine, is quite a difficult task, but progress over the last few years has been promising.&lt;/p&gt; &lt;p&gt;Now, you might be asking yourself, can you really predict the functions of neurons based on their neighbors in the connectome? A paper published by Yan et al. in 2017&lt;d-cite key=&quot;yan2017network&quot;&gt;&lt;/d-cite&gt; asked this same question, searching for an answer in a roundworm (C. elegans) connectome. In their investigation, they discovered a neuron whose behavior had not been previously characterized, which they hypothesized was necessary for locomotion. They tested this hypothesis by ablating the neuron on a living C. elegans, and to the dismay of that poor roundworm, found that it was indeed necessary.&lt;/p&gt; &lt;p&gt;Although impressive, the C. elegans connectome has only ~300 neurons, compared with the ~100,000,000,000 in the human brain; however, this year (2023):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A paper by Winding et al.&lt;d-cite key=&quot;winding2023connectome&quot;&gt;&lt;/d-cite&gt; has published the entire connectome of a fruit fly larvae, identifying 3016 neurons and their 548,000 synapses.&lt;/li&gt; &lt;li&gt;Google Research has announced an effort to map a mouse brain (~100,000,000 neurons)&lt;d-cite key=&quot;januszewski2023google&quot;&gt;&lt;/d-cite&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is exciting because the fruit fly dataset presents an opportunity to identify more nuanced functions of neurons that may be present in more complex species like mice, but not in simpler species like the roundworm. This creates the requirement for algorithms that are &lt;strong&gt;sufficiently expressive&lt;/strong&gt; and able to disentangle the similarities between neurons that appear different, but are functionally similar.&lt;/p&gt; &lt;p&gt;Furthermore, current efforts to map connectomes of increasingly complex animals makes it desirable to have algorithms that are &lt;strong&gt;able to scale&lt;/strong&gt; and handle that additional complexity, with the hopes of one day discovering the algorithms that give rise to consciousness.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;h3 id=&quot;can-we-learn-about-human-brains-by-studying-connectomes-of-simpler-organisms&quot;&gt;Can we learn about human brains by studying connectomes of simpler organisms?&lt;/h3&gt; &lt;p&gt;The primate brain exhibits a surprising degree of specialization, particularly for social objects. For instance, neurons in the face fusiform area (FFA) in the IT cortex appear to fire only in response to faces. Furthermore, individuals with lesions in or brain damage to this area lose their ability to recognize faces &lt;d-cite key=&quot;kanwisher_fusiform_2006&quot;&gt;&lt;/d-cite&gt;. In fact, there is even evidence of rudimentary face perception even in newborn infants with limited access to visual “training data,” who preferentially look at photos of faces, and other face-like arrangements, like inverted triangles (two vertices being the eyes and the third the mouth) &lt;d-cite key=&quot;otsuka_face_2014&quot;&gt;&lt;/d-cite&gt;. While there may not exist a grandmother cell that can recognize your grandmother, there certainly seems to be at least some engineered specialization in the brain. Cognitive scientists theorize that there is a set of core systems for representing object, actions, number, space, and conspecifics (other people!), together constituting what we might call “common sense,” which may help determine the blueprint of the human brain down to the genetic level &lt;d-cite key=&quot;spelke_core_2007&quot;&gt;&lt;/d-cite&gt;. Notably, facial recognition exhibits substantial genetic heritability (over 60%!) and appears to be uncorrelated with general intelligence &lt;d-cite key=&quot;shakeshaft_genetic_2015&quot;&gt;&lt;/d-cite&gt;. We might imagine that there are a set of capabilities, including social cognition, that were so critical for human behavior that our brains evolved over hundreds of thousands of years to “hard code” certain structures, like the FFA, to help scaffold them. After all, another person’s face is an important signal for processes like mate selection, friendship formation, and theory of mind. The human brain and the cognitive processes it supports are evolutionary products. And even more importantly, the brain seems to be specialized in some ways, but behave flexibly in others. Through the scientific process, how good of an understanding can we reach about the complex organ sitting between our ears? To what degree are the neuronal assemblages in our brain specialized? How do the communications amongst these neurons grant us our incredible cognitive capabilities?&lt;/p&gt; &lt;p&gt;In 1982, neuroscientist David Marr proposed three levels of analyses to study complex systems like the human mind: the computational level (what task is the system designed to solve?), the algorithmic level (how does the system solve it?), and the implementation level (where and how is the algorithm implemented in the system hardware?) &lt;d-cite key=&quot;mcclamrock_marrs_1991&quot;&gt;&lt;/d-cite&gt;. At one end of the spectrum, we might think about characterizing the computational capabilities of human cognition, like object recognition. On the other end, we might be interested in how object recognition is implemented in the brain itself, in all of its fleshy glory–how an incoming visual signal is processed by composites of receptive fields in the retina (biological “Gabor filters”) and fed to neurons in the primary and secondary visual areas of the cerebral cortex, for instance &lt;d-cite key=&quot;leeds_comparing_2013&quot;&gt;&lt;/d-cite&gt;. In recent years, scientists have developed an interest in understanding the implementation level at an extremely high resolution by charting the connectome–the comprehensive map of all neural connections in the brain. However, if the grandmother cell is too simplistic of a model for knowledge representation in the human brain, then indeed the human connectome may offer an overly complex view. It seems easy to get lost in the wilderness of its approximately 100 trillion neurons and the nearly quadrillion synapses which connect them &lt;d-cite key=&quot;sporns_human_2005&quot;&gt;&lt;/d-cite&gt;! How can we begin to approach this overwhelming terra incognita?&lt;/p&gt; &lt;p&gt;We might consider instead studying the connectome of a much simpler model organism, like the transparent 1mm-long nematode Caenorhabditis elegans, with whom we share an estimated 20-71% of our genes with &lt;d-cite key=&quot;lai_identification_2000&quot;&gt;&lt;/d-cite&gt;. Or, maybe even the fruit fly Drosophila melanogaster, 60% of whose genes can also be found in the human genome (Max Planck). Even the study of such model organisms necessitates adding structure to complex, often unlabeled, relational data. And while the fruit fly brain is orders of magnitude less complex than our own, there are still over 3,000 neurons and half a million synapses to explore &lt;d-cite key=&quot;winding2023connectome&quot;&gt;&lt;/d-cite&gt;(Winding et al., 2023). Luckily, mankind’s toolkit for studying graph-like data is well-equipped.&lt;/p&gt; &lt;h3 id=&quot;unsupervised-graph-representation-learning&quot;&gt;Unsupervised graph representation learning&lt;/h3&gt; &lt;p&gt;The problem of subdividing neurons in a connectome into types based on their synaptic connectivity is a problem of unsupervised graph representation learning, which seeks to find a low-dimensional embedding of nodes in a graph such that similar neurons are close together in the embedding space.&lt;/p&gt; &lt;p&gt;A common way to identify functional clusters of neurons is through the lens of homophily, meaning that neurons serve the same function if they are within the same densely connected cluster in the connectome; however, this fails to capture the likely case that neurons with similar low-level functions span across many regions of the brain&lt;d-cite key=&quot;winding2023connectome&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Instead, a better approach might be to cluster neurons based on their structural equivalence, such that groups of neurons with similar subgraph structures are embedded similarly, regardless of their absolute location in the connectome. This is the approach taken by Winding et al.&lt;d-cite key=&quot;winding2023connectome&quot;&gt;&lt;/d-cite&gt;, who “used graph spectral embedding to hierarchically cluster neurons based on synaptic connectivity into 93 neuron types”. They found that even though they used only information about the graph structure to predict functions, neurons in the same clusters ended up sharing other similarities, including morphology and known function in some cases.&lt;/p&gt; &lt;p&gt;Spectral embedding is a popular and general machine learning approach that uses spectral decomposition to perform a nonlinear dimensionality reduction of a graph dataset, and works well in practice. Deep learning, however, appears to be particularly well suited to identifying better representations in the field of biology (e.g., AlphaFold2&lt;d-cite key=&quot;jumper2021highly&quot;&gt;&lt;/d-cite&gt;), and deep learning methods do appear to be capable of creating embeddings that more effectively preserve the topology of nodes in graphs&lt;d-cite key=&quot;zhu2023unsupervised&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;kipf2016variational&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/vgae-embedding-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/vgae-embedding-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/vgae-embedding-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/vgae-embedding.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Learned VGAE graph embedding for Cora citation network dataset.&lt;d-cite key=&quot;kipf2016variational&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;p&gt;Thus, it stands to reason that deep learning might offer more insights into the functions of neurons in the fruit fly connectome, or at the very least, that exploring the differences between the spectral embedding found by Winding et al. and the embeddings discovered by deep learning methods might provide intuition as to how the methods differ on real datasets.&lt;/p&gt; &lt;p&gt;In this project, we explore the differences between functional neuron clusters in the fruit fly connectome identified via spectral embedding by Winding et al. and deep learning. Specifically, we are interested in exploring how spectral embedding clusters differ from embeddings learned by Variational Graph Auto-Encooders (GVAE)&lt;d-cite key=&quot;kipf2016variational&quot;&gt;&lt;/d-cite&gt;, which are a more recent architecture proposed by one of the co-authors of the Variational Auto-Encoders (VAE) paper&lt;d-cite key=&quot;kingma2013auto&quot;&gt;&lt;/d-cite&gt;, Max Welling. GVAEs are an interesting intersection of graph neural networks (GNNs) and VAEs, both of which we explored in class, and comparing this technique to spectral embedding is relevant because of our previous discussions of spectral decomposition in class with respect to network scalability and RNN weights.&lt;/p&gt; &lt;p&gt;We hypothesize that a deep learning technique would be better suited to learning graph embeddings of connectomes because they are able to incorporate additional information about neurons (such as the neurotransmitters released at synapses between neurons) and are able to learn a nonlinear embedding space that more accurately represents the topological structure of that particular connectome, learning to weight the connections between some neurons above others.&lt;/p&gt; &lt;p&gt;Before we can discuss the experiments, however, we first provide more detail for Spectral Embedding and Graph Variational Autoencoders and compare the two methods.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;h3 id=&quot;spectral-embedding&quot;&gt;Spectral Embedding&lt;/h3&gt; &lt;p&gt;One classical approach for understanding graph-like data comes from a class of spectral methods which use pairwise distance measures between data points to embed and cluster data. Spectral methods offer two obvious advantages when compared to other machine learning approaches. One, we can straightforwardly perform clustering for datasets which are inherently relational, like the connectome, where it is not immediately clear how a method like k-means can be used when we only have access to the relationships between data points (the “edges”) and not the node-level features themselves. Two, spectral methods are &lt;strong&gt;nonlinear&lt;/strong&gt;, and don’t rely on measures like squared Euclidean distance, which can be misleading for data which are tangled in high dimensions, but which exhibit a lower &lt;strong&gt;intrinsic&lt;/strong&gt; dimensionality.&lt;/p&gt; &lt;p&gt;So, how does spectral embedding work, exactly? In short, an adjacency matrix is first calculated from the original dataset, which is then used to compute the graph Laplacian. Next, a normalized graph Laplacian is then eigen-decomposed and generates a lower dimensional embedding space on which simpler linear clustering algorithms, like k-means, can be used to identify untangled clusters of the original data.&lt;/p&gt; &lt;p&gt;This class of methods makes no assumptions about the data (including cluster shape) and can be adjusted to be less noise sensitive–for example, by performing a t-step random walk across the affinity matrix for the data, as in diffusion mapping &lt;d-cite key=&quot;coifman_geometric_2005&quot;&gt;&lt;/d-cite&gt;. An added benefit is that under the hood, spectral embedding can be performed by a series of linear algebra calculations, making it extremely time-efficient. However, as with many unsupervised learning methods, clustering based on spectral embeddings is difficult to scale–in our case, due to the eigen-decomposition step of the graph Laplacian.&lt;/p&gt; &lt;h3 id=&quot;variational-graph-autoencoders&quot;&gt;Variational Graph Autoencoders&lt;/h3&gt; &lt;p&gt;Although Spectral Embedding is still very popular, in recent years, more attention has been paid to the burgeoning field of geometric deep learning, a set of ideas which aim to to solve prediction or embedding tasks by taking into account the relational structure between data points. One example is the variational graph auto-encoder (VGAE), which learns to embed a complex object like a network into a low-dimensional, well-behaved latent space. Kipf and Welling (2016)&lt;d-cite key=&quot;kipf2016variational&quot;&gt;&lt;/d-cite&gt; propose an encoder using a two-layer graph convolutional network, which performs convolutions across local subgraphs of the input network data (not unlike convolution on images, where the graph is a grid!). The graph is projected onto a low dimensional space distributed according to the standard normal through the optimization of a variational lower bound loss, and then upsampled using an inner product between latent variables. They show that this method achieves competitive results on a link prediction task when compared to other methods like spectral clustering and DeepWalk, a random walk-based representation learning algorithm.&lt;/p&gt; &lt;p&gt;On the other hand, some have discovered that spectral embedding leads to more clear separability in low dimensional representation spaces for text data compared to GNN approaches like node2vec, which reportedly achieve state-of-the-art (sota) scores for multilabel classification and link prediction in other datasets &lt;d-cite key=&quot;grover_node2vec_2016&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;palamuttam_evaluating_nodate&quot;&gt;&lt;/d-cite&gt;. In addition, it appears that simple modifications like performing an error correlation correction on the training data and smoothing predictions on the test data for GNN-free architectures lead to sota-comparable performances &lt;d-cite key=&quot;huang_combining_2020&quot;&gt;&lt;/d-cite&gt;. There are even concerns that the performance of geometric deep learning approaches are inflated, particularly in tasks like multi-label node classification, due to the assumption that the number of labels for test data are known to researchers &lt;d-cite key=&quot;lin_use_2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Thus, it remains unclear in what circumstances relatively novel geometric deep learning approaches do better compared to established and widely-explored methods like spectral learning, and particularly for novel data like the connectome. In this work, we attempt to gain deeper insights into which method is moroe well-suited to the task of connectome modeling, with the hope of learning about which method should be implemented in future connectomes, such as that of the mouse and eventually the human.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/background_visual-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/background_visual-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/background_visual-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/background_visual.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Spectral Clustering (von Luxburg, 2007; Park, Jeon, &amp;amp; Pedryc, 2014) vs (V)GAEs (Kipf &amp;amp; Welling, 2016): A Story in Pictures &lt;/div&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;Now that we have a good idea of how these methods compare to each other in terms of implementation, we explore them from an experimental perspective. Through our experiments, we try to quantitatively and qualitatively address the question of how connectome clusters learned by GVAE compare to the spectral clusters found in the paper. To answer this question, we make use of the fruit fly connectome adjacency matrix provided by Winding et al. as our primary dataset with the hope of answering this question for our readers.&lt;/p&gt; &lt;h3 id=&quot;experiment-1-link-prediction&quot;&gt;Experiment 1: Link Prediction&lt;/h3&gt; &lt;p&gt;One common way to compare unsupervised graph representation learning algorithms is through a link prediction task, where a model is trained on a subset of the edges of a graph, and then must correctly predict the existence (or non-existence) of edges provided in a test set. If the model has learned a good, compressed representation of the underlying graph data structure, then it will be able to accurately predict both where missing test edges belong, and where they do not.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-task-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-task-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-task-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-task.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; A link prediction task. Green lines correspond to the training data, which contains samples of positive samples of edges that are present in the graph, and negative samples of edges that are not present in the graph. The test set in red corresponds to the remainder of positive and negative samples in the graph. &lt;/div&gt; &lt;p&gt;We evaluate the models by computing the area under curve (AUC) of the ROC curve, which plots the true positive rate against the false positive rate. A completely random classifier that does not learn anything about the underlying graph structure would get an AUC of 0.5, while a perfect classifier would have an area of 1.0.&lt;/p&gt; &lt;p&gt;Another metric we use to evaluate how good the models are is average precision (AP) of the precision-recall curve, which describes the consistency of the model.&lt;/p&gt; &lt;p&gt;In addition to comparing the models with these metrics, we also explore how robust they are to decreasing dimensionalities of the latent space. We hypothesize that if a model is able to maintain high AUC and AP, even at very low-dimensional embedding spaces, then it is likely better at capturing the structure of the connectome and is more likely to be able to scale to larger datasets, like that of the human brain one day.&lt;/p&gt; &lt;p&gt;Running this experiment yields the following curves, where the x-axis shows the dimensionality of the latent space, and the y-axis shows the AUCs and APs of the respective models.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-auc-ap-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-auc-ap-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-auc-ap-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/link-prediction-auc-ap.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;From this experiment, we find that both the Graph Autoencoder (GAE) and Variational Graph Autoencoder (VGAE) perform better than Spectral Embedding methods in terms of AUC and AP, indicating that the models might be better suited to capturing the nuances in the fruit fly connectome. At the dimensionality used for spectral embedding in Winding et al., d=24, we find that the models have comparable performance, but as we reduce the dimensionality of the learned embedding, the spectral embedding method quickly breaks down and loses its ability to capture significant features in the data, with an AUC of 0.52 at a dimensionality of 2. Since a score of 0.5 corresponds to a random model, this means that the spectral embedding method is no longer able to capture any meaningful structure in the data at that dimensionality. Winding et al. gets around this by only using spectral embedding to get a latent space of size 24, and then performing a hierarchical clustering algorithm inspired by Gaussian Mixture Models, but the simplicity and robustness of the GAE model seems to show that they may be better suited to modeling the types of functional neurons present in the connectomes of animals.&lt;/p&gt; &lt;h3 id=&quot;experiment-2-gvae-latent-exploration&quot;&gt;Experiment 2: GVAE Latent Exploration&lt;/h3&gt; &lt;p&gt;Although the link-prediction experiment gives us a quantitative comparison of the models, we also believe it is important to explore the latent embeddings learned by GAE to see how they qualitatively compare with the learned embeddings used in the Winding et al. work. After observing that the GAE was robust to a latent space of size 2, we decided to look specifically at if there were any similarities between the clusters found by the GAE with the 2-d embedding and the level 7 clusters published by Winding et. al. Also, although the GAE showed better overall performance, we decided to specifically explore the Variational GAE because we expect it to have a latent manifold similar to that of the Variational Autoencoders.&lt;/p&gt; &lt;p&gt;To this end, we first trained a Variational GAE with a 2-d latent space on the full fruit fly connectome and extracted the latent embedding of each node in the connectome.&lt;/p&gt; &lt;p&gt;With this latent embedding, we first visualized the latent space using colors corresponding to the 93 clusters identified by Winding et al. Clusters of the same color in the learned GAE latent space mean that the VGAE identified the same cluster that was identified in the Winding et. al. paper and areas where there are many colors within a cluster mean that GAE found a different cluster compared to spectral embedding.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/explore_cluster-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/explore_cluster-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/explore_cluster-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/explore_cluster.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Coloring the GVAE latent space by the found level 7 clusters Winding et al. Black points correspond to neurons that were not assigned a cluster by Winding et al. &lt;/div&gt; &lt;p&gt;As seen in the figure above, we find that while VGAE projects directly to a 2-d latent space without any additional clustering to reduce the dimensionality, the learned embedding still shares many similarities with the spectral embedding down to a dimensionality of 24 followed by Gaussian Mixture Model hierarchical clustering. Therefore, using VGAE to learn a direct 2-d latent space still captures much of the same information that a more complex machine learning algorithm like spectral embedding is able to.&lt;/p&gt; &lt;p&gt;We further explored the learned latent space by looking at whether the learned embedding had any correlation with the cell types identified in the fruit fly larvae connectome. Since the VGAE only had information about the structure of the graph embedding, clusters of similar colors in this figure mean that the cell type within the cluster shared a lot of common structures, like potentially the same degree or being connected to similar types of up or downstream neurons.&lt;/p&gt; &lt;p&gt;We use the same color palette as the Winding et al. paper so that cell types in the level 7 clusters of the Winding et al. paper can be directly compared to the learned VGAE latent embedding.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/clustering-cell-type-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/clustering-cell-type-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/clustering-cell-type-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-deep-connectome-clustering/clustering-cell-type.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Coloring the Winding et al. level 7 clusters (left) and GVAE latent space (right) by cell types. This information was not provided to either algorithm during training, so clusters of the same cell type mean that its type can be inferred from structure only. &lt;/div&gt; &lt;p&gt;As seen in the figure above, both spectral embedding and GVAE latent spaces capture knowledge about the cell types when trained purely on the graph structure. We believe this is because cells of this type have similar properties in terms of the types of neighboring neurons they connect to in the connectome, and they may also have special properties like higher degree of connections.&lt;/p&gt; &lt;p&gt;In particular, it is interesting that sensory neurons and Kenyon cells are very well captured by both embeddings, and that MBIN cells and sensory neurons are clustered together by both their spectral embedding algorithm and VGAE.&lt;/p&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;Our preliminary investigations show that deep learning algorithms such as Graph Autoencoders (GAEs) and Variational Graph Autoencoders (VGAEs) are able to capture at least as much nuance and information about function as spectral embedding algorithms. In addition, they come with the following advangates:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;With their current implementation, they can easily be run on a GPU, while common spectral embedding algorithms in libraries such as scikit learn are only designed to work on CPUs. Since we take a deep learning approach, our GNN method can use batches optimized via Adam, while spectral embedding only works if the entire adjacency matrix fits in memoruy. This makes deep learning methods &lt;strong&gt;better able to scale to larger datasets&lt;/strong&gt; such as the mouse connectome that may come in the next few years.&lt;/li&gt; &lt;li&gt;As shown in experiment 2, GAEs and Variational GAEs are &lt;strong&gt;able to directly learn a robust embedding into a 2-d space&lt;/strong&gt; without any additional clustering, making interpretation easy and fast. We suspect that because of its higher performance at embedding connectomes to such low dimensions compared to spectral embedding which performs only marginally better than a random algorithm at such low dimensions, VGAEs must be capturing some addiitonal nuance of the graph structures that spectral embedding is simply not able to encode.&lt;/li&gt; &lt;li&gt;Comparing the 2-d embeddings of VGAE to the clustered 24-d spectral embeddings found in Winding et al. we find that even when compressing to such a low-dimensional space, the semantic information captured does in fact match that of spectral embedding at a higher dimensional space. Coloring by cell type shows that it also &lt;strong&gt;captures information about the function of neurons&lt;/strong&gt;, with similar neuron types being clustered together even when they are located all over the brain, such as Kenyon cells. Cells of the same type likely serve simlar functions, so in this respect, VGAE is able to capture information about the function of cells using only knowledge of the graph structure.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;However, VGAE does not come without its &lt;strong&gt;limitations&lt;/strong&gt;. One large limitation we found while implementing the architecture is that it currently requires graphs to be &lt;strong&gt;undirected&lt;/strong&gt;, so we had to remove information about the direction of neurons for this work. Connectomes are inherently directed, so we likely missed some key information about the function of graphs by removing this directional nature of the connectome. Although this is not explored in our work, one simple way to fix this would be to add features to each node corresponding to the in-degree and out-degree of each neuron.&lt;/p&gt; &lt;p&gt;This brings us to the another limitation of our study, which is that we did not explore &lt;strong&gt;adding features to neurons&lt;/strong&gt; in our connectome with the VGAE algorithm. Past work on GAEs has shown that adding features leads to better model results &lt;d-cite key=&quot;kipf2016variational&quot;&gt;&lt;/d-cite&gt; and makes the model better able to capture relevant structures in the data. We did not feel that would be a fair comparison with Winding et al. because spectral embedding methods are not able to include additional features related to nodes that one would get for free when mapping the connectome, but we believe that including these features in the GAE structure would lead to an even better representation of the underlying dataset. Examples of these “free” features we could get that would help us predict functions of neurons include 1) the hemisphere the cell belongs to (e.g., not in fruit flies, but neurons in the left brain of humans correspond to language), 2) the axon I/O ratio, and the dendrite output-input ratio of a neuron.&lt;/p&gt; &lt;p&gt;One final limiation is that our &lt;strong&gt;model only trains on a single connectome&lt;/strong&gt;. This means that we aren’t able to capture the variation of connectomes within a species. Maybe one day, we will be able to scan connectomes of people in the same way that we are able to scan genomes of people, but that day is likely still far away. We might be able to help this by using the generative compoment of the VGAE to create brains that are physically feasible given the structure of a single connectome, but it would be hard to test. Since we are currently only looking at the connectome of a single species, we likely aren’t capturing an embedding space that finds functionally similar neurons in different animals such as C. elegans, which we may be able to do in future work.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this work, we asked if Deep Learning techniques like Variational Graph Autoencoders could learn something about the functions of cells in a connectome using only the graph structure. We found that VGAE did in fact capture relevant structures of the graph, even in the undirected case. It performed similarly to spectral embeding, even when embedding directly into a visualizable 2-d latent space. In the future, we may be able to learn about neurons that serve the same purpose across species, or learn about the underlying low level syntactic structures like for-loops or data types that our brain uses to encode consciousness, vision, and more.&lt;/p&gt; </content> </entry> <entry> <title>Robust Image to Video Generation Using Contrastive Diffusion Over Latents</title> <link href="https://deep-learning-mit.github.io/blog/2023/contrastivediffusion-image2video/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/contrastivediffusion-image2video</id> <content type="html">&lt;h2 id=&quot;introduction-and-motivation&quot;&gt;Introduction and Motivation&lt;/h2&gt; &lt;p&gt;With recent advances in computer vision and generative AI, we all have observed the various feats that diffusive models have achieved in conditional image generation. These models have demonstrated unparalleled ability in creativity, fidelity, and relevance when generating images from text prompts. Given this explosive success of diffusion for the task of image generation, the idea of applying the same concepts to conditional video generation seems like a logical follow-up. Yet, the field still lacks robust and compelling methods for conditional video generation with diffusion models. This raises the question: why might this be? Or perhaps a follow-up: what makes videos so hard in comparison to images?&lt;/p&gt; &lt;p&gt;In an attempt to address our first question, if we take a brief dive into previous literature, we will find that the issue is not a lack of effort. Ho et al. &lt;d-cite key=&quot;ho2022video&quot;&gt;&lt;/d-cite&gt;, Zhang et al. &lt;d-cite key=&quot;2023i2vgenxl&quot;&gt;&lt;/d-cite&gt;, and Chen et al. &lt;d-cite key=&quot;chen2023videocrafter1&quot;&gt;&lt;/d-cite&gt;, all explore this idea, yet the results from these methods are not nearly as exciting as the results we see in images. But why is this?&lt;/p&gt; &lt;p&gt;Perhaps the answer lies in the solution to our second question. One of the most obvious complexities that videos have over images is also perhaps one of the most difficult: the temporal dependence between frames. But why is this relationship so hard for diffusion models? Following the work of Zhu et al. &lt;d-cite key=&quot;zhu2022discrete&quot;&gt;&lt;/d-cite&gt;, we hypothesize that this is because the implicit learning of diffusive steps between images in a video is too complex of a problem for a diffusion model; relying on the model to learn the implicit relationship between representations of video frames is infeasible from a training and convergence standpoint. If we can instead learn diffusive steps over a more regularized learned latent space, the optimization problem can be greatly simplified and the diffusion model will in theory be more robust.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;Taking a step back to examine the current state of research, we find that current image-to-video frameworks typically still use a traditional diffusion architecture, going straight from text and image representations to an output image. However, this naive approach struggles with serious issues like frame clipping and loss of contextual information, which is expected since noise-based sampling can easily throw off the output of individual frames.&lt;/p&gt; &lt;p&gt;Hence, Ho et al. in 2022 proposed the first solution, supplementing conditional sampling for generation with an adjusted denoising model that directly forces image latents to be more similar to the corresponding text latents &lt;d-cite key=&quot;ho2022video&quot;&gt;&lt;/d-cite&gt;. While this achieved improved results over the straightforward diffusion approach, this often forces the model to stick too closely to the text latent, resulting in incoherent videos.&lt;/p&gt; &lt;p&gt;To solve this issue, two recent approaches from Chen et al. and Zhang et al. have proposed methods to augment the video diffusion models themselves. Chen et al. uses the image encodings from CLIP-like language embeddings in an encoder-decoder language model, feeding the CLIP encodings at each step into a cross-attention layer that generates attention scores with the current video generation &lt;d-cite key=&quot;chen2023videocrafter1&quot;&gt;&lt;/d-cite&gt;. In doing so, additional coherence between frames is achieved. On the other hand, Zhang et al. use multiple encoders, with CLIP and VQ-GAN concatenated before two stages of diffusion model training, which they claim provides the hierarchical learning required to learn the temporal processing &lt;d-cite key=&quot;2023i2vgenxl&quot;&gt;&lt;/d-cite&gt;. However, both these models are extremely data-heavy and still suffer from hallucination and frame skipping.&lt;/p&gt; &lt;p&gt;To remedy these issues in diffusion models, Ouyang et al. and Zhu et al. posit that the implicit representation learning objective in diffusion models is the primary cause of the slow convergence and hallucination issues. Specifically, diffusion models do not directly compare their output to their input, as in contrastive models, instead performing a variational approximation of the negative log-likelihood loss over the full Markov chain. Instead, Ouyang and Zhu propose to train the diffusion model to output a structured latent in the latent space of a contrastive model like a VQ-VAE, which then reconstructs the output image &lt;d-cite key=&quot;zhu2022discrete&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;ouyang2023improving&quot;&gt;&lt;/d-cite&gt;. In doing so, a contrastive term can be added to the loss of the diffusion model, maximizing the mutual information between the structured (output) latent and input latent, leading to stronger correlations between input and output, and hence improved convergence. Hence, this approach seems to have potential in fixing the hallucination and coherence issues in video diffusion models, without the need for added complexity.&lt;/p&gt; &lt;h2 id=&quot;proposed-project-outline&quot;&gt;Proposed Project Outline&lt;/h2&gt; &lt;p&gt;Thus, we propose a novel method for conditional video generation (generating videos given a starting frame and text description) by utilizing an autoencoder framework and contrastive loss to train a regularized latent space in which a diffusion model can operate. Following the line of thought introduced above, we hypothesize that under such a formulation, the diffusion model is much more robust to temporal inconsistency, because of the regularity in the latent space. For example, if we imagine a highly regularized latent space, we will find all logical next frames for a given anchor frame clustered very closely around the anchor in this latent space. Therefore, any step the diffusion model takes would produce valid subsequent frames; it suffices simply for the model to learn which direction to go given the conditioned text prompt.&lt;/p&gt; &lt;p&gt;With this in mind, we detail the construction of the model by describing its components as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;An encoder for image data is used to map a given video frame into our latent space&lt;/li&gt; &lt;li&gt;An encoder for text data is used to map a given video description into our latent space&lt;/li&gt; &lt;li&gt;A diffusion-based model operates within the latent space, diffusing between different vectors within this latent space.&lt;/li&gt; &lt;li&gt;A decoder is used to generate images from vectors in this latent space.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The training process of such a model will involve the optimization of a diffusion/contrastive loss based on a given pair of adjacent video frames, as well as the corresponding text description for that video. We define a training step to involve the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Both video frames and the text description are encoded into our latent space.&lt;/li&gt; &lt;li&gt;One iteration of our diffusive model is run by diffusing from the latent vector corresponding to our earlier frame conditioned on our text prompt latent to obtain a new latent vector.&lt;/li&gt; &lt;li&gt;This new latent vector after cross-attention is passed through the decoder to obtain our predicted subsequent frame.&lt;/li&gt; &lt;li&gt;We then optimize our model according to the contrastive diffusion model loss presented by &lt;d-cite key=&quot;zhu2022discrete&quot;&gt;&lt;/d-cite&gt; with a key alteration: we replace their contrastive loss with our contrastive loss, which contains two terms: &lt;ol&gt; &lt;li&gt;a term that aims to push our two adjacent video frames closer together in our latent space and&lt;/li&gt; &lt;li&gt;a term that aims to push video frames closer to the text description in our latent space.&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;During inference, we generate a video through the following process:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;An initial frame and the text description are encoded into our latent space&lt;/li&gt; &lt;li&gt;We run an arbitrary number of diffusive steps, generating a latent at each step.&lt;/li&gt; &lt;li&gt;We decode the latent at each time step to obtain our video frame at that time step; stringing these frames together produces our video.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;From a more theoretical perspective, this method essentially aims to restrict the diffusion model’s flexibility to paths within a highly regularized, lower dimensional latent space, as opposed to the entire space of images that classical diffusion-based approaches can diffuse over. Such a restriction makes it much harder for the diffusion model to produce non-sensible output; the development of such a method would therefore enable the robust generation of highly temporally consistent and thus smooth videos. We also imagine the value of producing such a latent space itself. An interesting exercise, for example, is taking an arbitrary continuous path along vectors within a perfectly regular latent space to obtain sensible videos at arbitrary framerates.&lt;/p&gt; &lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt; &lt;p&gt;There are two axes along which we wish to evaluate our model: quality of generation, and quality of the contrastive latent space.&lt;/p&gt; &lt;h3 id=&quot;generation-quality&quot;&gt;Generation Quality&lt;/h3&gt; &lt;p&gt;To measure generation quality, we follow the approach presented by Ho et al., evaluating famous metrics like the FID, FVD, and IS scores. For all of these metrics, we expect to evaluate them throughout the video from beginning to end, with the level of preservation of metric values throughout a video indicating consistent video quality. Similarly, we will compare our models to those of similar size using the same metrics to evaluate whether adding the contrastive loss term truly improves generation quality. These metrics will be supplemented with qualitative human analyses, where we will score the videos on a variety of axes including coherence and relevance to the prompt.&lt;/p&gt; &lt;h3 id=&quot;use-of-contrastive-latent-space&quot;&gt;Use of Contrastive Latent Space&lt;/h3&gt; &lt;p&gt;Given that the diffusion model now maps to a much smaller latent space when compared to the whole space of output images, we believe that the diffusion output should have interpretable representations in the latent space. Hence, we will begin by exploring the latents generated by different text prompts, clustering them around the image source encodings to evaluate if the contrastive loss has truly clustered appropriately. On top of that, we plan to visualize the trajectories of videos for both the training set and our generations, to evaluate our theory of continuous trajectory evolution in the latent space.&lt;/p&gt; &lt;h2 id=&quot;implementationdeliverables&quot;&gt;Implementation/Deliverables&lt;/h2&gt; &lt;p&gt;The implementation of such a method can be greatly simplified through the use of an existing codebase. We plan on using the contrastive diffusion model &lt;a href=&quot;https://github.com/L-YeZhu/CDCD/tree/main&quot;&gt;GitHub repository&lt;/a&gt; for the implementation of &lt;d-cite key=&quot;zhu2022discrete&quot;&gt;&lt;/d-cite&gt; with a few key modifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We use a pre-trained contrastive model as our starting point (such as an image encoder/decoder from CLIP) &lt;d-cite key=&quot;Radford2021LearningTV&quot;&gt;&lt;/d-cite&gt;&lt;/li&gt; &lt;li&gt;The diffusion model is trained to predict the next frame of a video conditioned on a given text description of the video and the current frame of the video as above.&lt;/li&gt; &lt;li&gt;Our contrastive loss is used as described above.&lt;/li&gt; &lt;li&gt;Inference is modified to generate a video as described above.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Data for this project requires video/text description pairs. There are a few datasets consisting of such data, including the &lt;a href=&quot;https://www.kaggle.com/datasets/vishnutheepb/msrvtt&quot;&gt;MSR-VTT dataset&lt;/a&gt;, which is human-annotated, and the &lt;a href=&quot;https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid&quot;&gt;InternVid dataset&lt;/a&gt;, which is annotated by LLMs.&lt;/p&gt; &lt;p&gt;The project should be feasible to complete within the remaining time in the semester, with a rough timeline of deliverables as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Implementation&lt;/strong&gt; of our method by applying the specified modifications to the existing codebase should take around 1-2 weeks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt; of the models on cloud computing resources should take &amp;lt;1 week.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation and benchmarking&lt;/strong&gt; along with data visualization should take 1 week, even with the potential need for retraining our models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Blog writing&lt;/strong&gt; should take &amp;lt;1 week and can be completed in parallel with evaluation and benchmarking.&lt;/li&gt; &lt;/ul&gt; </content> </entry> <entry> <title>Adaptive Controller with Neural Net Equations of Motion for High-DOF Robots</title> <link href="https://deep-learning-mit.github.io/blog/2023/adaptive-controller-graph-eom/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/adaptive-controller-graph-eom</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Adaptive controllers are integral to modern robotic arms, enabling robots to adjust to dynamic environments and internal variations such as actuator wear, manufacturing tolerances, or payload changes. At the heart of such controllers is the formulation of the robot’s Equations of Motion (EoM), typically expressed in the form:&lt;/p&gt; &lt;p&gt;The standard symbolic form of EoM is represented as:&lt;/p&gt; \[M(q)q&apos;&apos; + C(q, q&apos;) = T(q) + Bu\] &lt;p&gt;where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;( M(q) ) is the mass matrix&lt;/li&gt; &lt;li&gt;( C(q, q’) ) represents Coriolis and centripetal forces&lt;/li&gt; &lt;li&gt;( T(q) ) depicts gravitational torques&lt;/li&gt; &lt;li&gt;( B ) is the input transformation matrix&lt;/li&gt; &lt;li&gt;( u ) denotes control input&lt;/li&gt; &lt;li&gt;( q, q’ ) are the joint angle state variables and their derivatives, respectively.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The symbolic complexity of the EoM increases considerably for robots with a high Degree of Freedom (DOF), due to the analytical resolution of the Lagrangian or Hamiltonian dynamics required. While these equations can be derived algorithmically, the computational burden is significant, and the resulting symbolic equations are extensively lengthy. To illustrate, consider the EoM for a 7-DoF Panda Emika Franka robot arm &lt;a href=&quot;https://github.com/marcocognetti/FrankaEmikaPandaDynModel/tree/master/matlab/dyn_model_panda&quot;&gt;(link)&lt;/a&gt;. The code that determines the EoM is extraordinarily verbose.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/panda-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/panda-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/panda-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/panda.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The aim of this project is to bypass the need for an explicit symbolic articulation of the EoM by formulating a neural network representation. With an accurately modeled neural network, it could serve as a foundational element in the development of an adaptive controller. The goal is for the controller to adapt a robotic arm’s physical parameters based on calibration sequences and to estimate the mass and inertia matrix of unfamiliar payloads.&lt;/p&gt; &lt;p&gt;Aside from symbolic representation, the EoM can also be computed numerically at each operating point using the Recursive Inertia Matrix Method &lt;d-cite key=&quot;featherstone2008rigid&quot;&gt;&lt;/d-cite&gt; , which has a computational complexity of ( O(n^3) ), where ( n ) is the number of joints in the rigid body. Substituting this computation-heavy method with a neural network, we can potentially calculate the forward dynamics in linear time, albeit with a trade-off in memory usage.&lt;/p&gt; &lt;h2 id=&quot;background-and-related-work&quot;&gt;Background and Related Work&lt;/h2&gt; &lt;p&gt;Before we delve into neural net architecture, let’s take a look closer at our problem and how it’s solved right now. To come up with the symbolic equation for the EOM, we use Lagrangian Mechanics in which we compute the Potential, U, and Kinectic Energy, T, of our system.&lt;/p&gt; &lt;p&gt;\(L = T - U\) \(\frac{d}{dt} \left( \frac{\partial L}{\partial \dot{q}_i} \right) - \frac{\partial L}{\partial q_i} = u_i\)&lt;/p&gt; &lt;p&gt;Quick describing of how it turns in into the manipulator equations. Working through these equations, a pattern emerge in which you can groups the equation as the manipulator equations.&lt;/p&gt; \[M(q)q&apos;&apos; + C(q, q&apos;) = T(q) + Bu\] &lt;p&gt;This method works well when the degree of freedom in the system is low. It provides much insight on how the dynamics of the system work. For example, the kinetic energy can be represented as:&lt;/p&gt; \[T = \frac{1}{2} \dot{q}^T M(q) \dot{q}\] &lt;p&gt;Highlighting that ( M ) is symmetric and positive definite. However, as introduced earlier, this method scales poorly with complexity in higher DOF systems.&lt;/p&gt; &lt;p&gt;However, as shown in the introduction, when this method is used for a 7 DOF system, the resulting equation is extraordinarily complex.&lt;/p&gt; &lt;p&gt;Bhatoo et al. &lt;d-cite key=&quot;bhattoo2022learning&quot;&gt;&lt;/d-cite&gt; introduced a graph neural network to represent the potential and kinetic energy of rope systems—a high DOF system—by segmenting the system into short segments. Each segment was then treated as a node in the graph neural network. Although they didn’t derive the forward dynamics using the Lagrangian formulation, the prospect of representing serially linked robot arms with graph neural networks was indicated as feasible.&lt;/p&gt; &lt;p&gt;The other approach to create the manipulator equation is to numerically calculate it at each operating point. There are two versions of this equation, the inverse dynamics and the forward dynamics version. In the inverse dynamics formulation, we essentially calculate \(M(q)q&apos;&apos; + C(q, q&apos;) - T(q) = Bu\)&lt;/p&gt; &lt;p&gt;Giving a particular state of the robot and a desired acceleration, what is the required torque. The inverse dynamics formulation can be computed with the Recursive Newton-Euler Algorithm with a O(n) complexity where n is the number of joints &lt;d-cite key=&quot;featherstone2008rigid&quot;&gt;&lt;/d-cite&gt; . The key idea for this algorithm is that the motion of a body is directly influence by the previous link. It’s essentially a dynamic programming algorithm in which you can store the motion of one body and then apply to the next body. This suggests that a directed graph neural net is sufficient to represent our model.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/linked_motion-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/linked_motion-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/linked_motion-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/linked_motion.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;While very efficient to calculate, the inverse dynamics is not as useful as the forward dynamics version if the end goal is to create an adaptive controller. The forward dynamics is the model that describes what is the accelerations of the system based on current state and torque input.&lt;/p&gt; \[q&apos;&apos; = M(q) \ (- C(q, q&apos;) + T(q) - Bu)\] &lt;p&gt;This formulation is more useful for adaptive controller as we can compared predicted acceleration and actual acceleration. Use their difference as a loss and to compute the gradient from the model parameters. The problem with the forward dynamics problem is that it requires a O(n^3) computation for a serially linked robot arm (the mass matrix inversion must be done). The algorithm for Forward Dynamics is called Inertia Matrix Method &lt;d-cite key=&quot;featherstone2008rigid&quot;&gt;&lt;/d-cite&gt; . One physical intuition that can be glean from the algorithm is that reaction input torques propagate down chain. Once again, this indicates that there is a one way information flow from one link to the next. Given that this version is more computationally expensive, it would be more valuable to tackle with a neural net representation as well compared to the much faster inverse dynamics problem.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/reaction_forces-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/reaction_forces-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/reaction_forces-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/reaction_forces.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h2&gt; &lt;h3 id=&quot;generating-training-data&quot;&gt;Generating Training Data&lt;/h3&gt; &lt;p&gt;Utilizing numerical methods implemented in MATLAB, we generated a large volume of training data, spanning the full operational space of the robot arm. We based our robot arm model on realistic parameters from the publicly available data of the Emika Franka Panda, comprising a total of 10 links, seven revolute joints, and two fixed joints. After disregarding the base link, we have a model with 10 parameters for each link (mass, center of mass as a 1x3 vector, and the symmetric inertia matrix flattened into a 1x6 vector) and joint properties (angle, angular velocity, angular acceleration, and torque).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/randomConfig-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/randomConfig-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/randomConfig-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/randomConfig.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We simulated the arm moving from one random configuration to another—marked in the image above by an X — recording states, torques, and accelerations during transitions. To introduce variability, we applied realistic perturbations to the physical properties of each link after every 100 recorded motion paths. In total, we accumulated 250,000 data points&lt;/p&gt; &lt;h3 id=&quot;attempt-1-graph-neural-net&quot;&gt;Attempt 1: Graph Neural Net&lt;/h3&gt; &lt;p&gt;As inspired by Bhatoo, we rearrange the dataset as a Graph Dataset based on the PyTorch Geometric Library. Each node contains the 10 physical property parameters, angle, angular velocity, and torque input. In total, each node has 13 features. The output is set to be angular acceleration of the 7 joints (1x7 vector). As for the edge index, the graph is defined to be directed, either information flows from the last node to the first or the first node to the last node. This is inspired by the physical intuition that forces propagate sequentially from one body to the next, and that motion with respect to the global coordinate frame also sequential depended on the previous body link.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/nodes-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/nodes-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/nodes-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/nodes.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We applied nine iterations of the Graph Convolution Layer, ensuring information flow from one end of the arm to the other.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/GNN-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/GNN-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/GNN-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/GNN.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Despite extensive parameter tuning, learning rate adjustments, and the application of various schedulers, the loss showed no convergence. Potential reasons for this include the complexity in capturing temporal dependencies and the possible oversimplification of force propagation through the links using graph convolutions. The complexity of 9 different GCNV also increases complexity needlessly.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/gnn_plot-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/gnn_plot-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/gnn_plot-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/gnn_plot.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;attempt-2-lstm&quot;&gt;Attempt 2: LSTM&lt;/h3&gt; &lt;p&gt;Reevaluating the necessity for graph neural networks, we considered the inherent sequential nature of the information flow in our system. There are no branches in the structure of a serially linked robot arm; hence, an LSTM, which excels in capturing long-range dependencies in sequence data, seemed appropriate. The input sequence now reflects the node properties from the previous attempt, and our LSTM architecture is defined as follows:&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RobotLSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RobotLSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# LSTM Layer &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lstm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Fully connected layers &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Initializing hidden state and cell state for LSTM &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Forward propagate the LSTM &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;lstm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Pass the output of the last time step to the classifier &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# We are interested in the last timestep &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;l3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Despite the theoretically simpler representation of the system, the results were still not satisfactory, with stabilization and convergence being unachievable.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/lstm-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/lstm-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/lstm-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/lstm.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;attempt-3-transformer&quot;&gt;Attempt 3: Transformer&lt;/h3&gt; &lt;p&gt;With LSTM and GNN strategies failing to deliver conclusive results, we pivoted to the more general-purpose Transformer architecture. This paradigm shifts focus from a strictly sequential data flow to a structure capable of interpreting the relationships between all links through its attention mechanism. Note, we also use a sinusoidal positional encoder to maintain the order coherance of the robot arm.&lt;/p&gt; &lt;p&gt;For the Transformer model, we employ the following architecture, designed to be flexible and adaptable to high DOF systems in future implementations:&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RobotTransformerModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nhead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_encoder_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim_feedforward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Store d_model as an instance attribute &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PositionalEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Sinusoidal positional encoding &lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Transformer Encoder Layer &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim_feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nhead&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_encoder_layers&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LayerNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MSELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Shape: [seq_len, batch, feature] &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;pos_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transformer_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_attn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# use the output of the first token (similar to BERT&apos;s [CLS] token) &lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;output_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;However, even with this advanced architecture, convergence remained elusive, indicating that further restructuring of the problem was required.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/transformer_generic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/transformer_generic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/transformer_generic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/transformer_generic.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;final-attempt-physics-informed-structured-transformer&quot;&gt;Final Attempt: Physics Informed Structured Transformer&lt;/h3&gt; &lt;p&gt;As nothing seems to be working, we now simplify our problem statement to gain some insights that could then we applied to the larger problem later. First, we now reformulate the serially linked robot arm dynamics into a double pendulum system with simplified parameters—each link defined by its length and a point mass at the end. The state variables in this reduced complexity scenario are simply the two link angles and their angular velocities.&lt;/p&gt; \[\mathbf{M}(q)\ddot{q} + \mathbf{C}(q, \dot{q})\dot{q} = \mathbf{T}_g(q) + \mathbf{B}u\] &lt;p&gt;where&lt;/p&gt; \[\mathbf{M} = \begin{bmatrix} (m_1 + m_2)l_1^2 + m_2l_2^2 + 2m_2l_1l_2\cos(q_1) &amp;amp; m_2l_2^2 + m_2l_1l_2\cos(q_2) \\ m_2l_2^2 + m_2l_1l_2\cos(q_2) &amp;amp; m_2l_2^2 \end{bmatrix},\] \[\mathbf{C} = \begin{bmatrix} 0 &amp;amp; -m_2l_1l_2(2\dot{q}_1 + \dot{q}_2)\sin(q_2) \\ \frac{1}{2}m_2l_1l_2(2\dot{q}_1 + \dot{q}_2)\sin(q_2) &amp;amp; -\frac{1}{2}m_2l_1l_2\dot{q}_1\sin(q_2) \end{bmatrix},\] \[\mathbf{T}_g = -g \begin{bmatrix} (m_1+m_2)l_1\sin(q_1) + m_2l_2\sin(q_1+q_2) \\ m_2l_2\sin(q_1+q_2) \end{bmatrix},\] \[\mathbf{B} = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; 1 \end{bmatrix}.\] &lt;p&gt;In this simpler problem statement, we switch to solving the Inverse Dynamics problem instead which numerically has a computational complexity of O(n). We assume that there is less complexity in this representation (a complete guess), so the neural net doesn’t have to work as hard compared to the Forward Dynamics problem which has computational complexity of O(n^3).&lt;/p&gt; &lt;p&gt;However, the task now focuses on the inverse dynamics with a reduced computational complexity of ( O(n) ), given that ( M(q) ) can be linearly separated from ( C ) and ( T_g ) and knowing that ( M(q) ) is symmetric and positive definite.&lt;/p&gt; &lt;p&gt;For this, two Transformer neural networks were created, one for ( M(q)\ddot{q} ) and another for ( C(q, \dot{q})\dot{q} - T_g(q) ). Both models were trained separately with their respective datasets before being combined to model the complete manipulator equation. We can uniquely generate training data that only incite this mode by setting gravity and angular velocity to zero to get only M(q)*ddq = u.&lt;/p&gt; &lt;p&gt;The architectures for these Transformers were revised to employ a Physics Informed approach, ensuring the adherence to known physical laws:&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RobotTransformerModelH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LightningModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nhead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_encoder_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim_feedforward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PositionalEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim_feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nhead&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_encoder_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LayerNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Output is a 1x3 vector &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MSELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Reshape for transformer &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;pos_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transformer_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_attn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;output_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Create a batch of symmetric 2x2 matrices from the batch of 1x3 output vectors &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symmetric_matrices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symmetric_matrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symmetric_matrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symmetric_matrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symmetric_matrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformed_ddq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symmetric_matrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformed_ddq&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/H-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/H-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/H-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/H.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Then we create a separate transformer neural net for C(q, dq)*dq - Tg(q). Similarly, we can generate training data that only exictes this mode by setting ddq = 0.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RobotTransformerModelC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LightningModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nhead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_encoder_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim_feedforward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PositionalEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim_feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nhead&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_encoder_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LayerNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Output is a 1x2 vector &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MSELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Reshape for transformer &lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;pos_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transformer_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_attn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;output_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/C-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/C-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/C-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/C.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We picked Transformer as it’s more general compared to LSTM or GNN. Furthermore, it can easily be extended to high DOF system later on by just working with a longer input sequence. After training these two models independtly with their own training data set, we combined the two pretrained model togeher to recreate the full manipulator equation with a complete dataset.&lt;/p&gt; &lt;p&gt;lass CombinedRobotTransformerModel(pl.LightningModule): def &lt;strong&gt;init&lt;/strong&gt;(self, config_H, config_C): super().&lt;strong&gt;init&lt;/strong&gt;() # Initialize the two models self.model_H = RobotTransformerModelH(&lt;strong&gt;config_H) self.model_C = RobotTransformerModelC(&lt;/strong&gt;config_C) self.criterion = nn.MSELoss() # Additional layers or attributes can be added here if needed&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def load_pretrained_weights(self, path_H, path_C): # Load the pre-trained weights into each model self.model_H.load_state_dict(torch.load(path_H)) self.model_C.load_state_dict(torch.load(path_C)) def forward(self, src_H, ddq, src_C): # Forward pass for each model output_H = self.model_H(src_H, ddq) output_C = self.model_C(src_C) # Combine the outputs from both models combined_output = output_H + output_C return combined_output &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This modular approach, informed by the physical structure of the dynamics, resulted in improved convergence and an adaptive controller with the capability to generalize well to unseen conditions of the double pendulum.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/combined-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/combined-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/combined-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-adaptive-controller-graph-eom/combined.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Through this journey of building and testing various neural network architectures to approximate the equations of motion for high-DOF robotic systems, it becomes evident that while cutting-edge machine learning tools hold promise, their effectiveness is tied to the physical realities of the problems they aim to solve. Success in neural net modeling involves really understanding the data and problem you are trying to solve. Here we managed to make a little head way in modeling the EOM of a 2 DOF system by mimicking the structure of the analytical solution.&lt;/p&gt; &lt;p&gt;For future work, we should take the success in the 2 DOF system and push it for higher DOF with more complex parameters. We can generate data that can isolate specific motion modes of the model that can be used to train sections of the neural net at a time. By then training all the modes independently, we can stitch together the whole structure for the whole dataset.&lt;/p&gt; </content> </entry> <entry> <title>Robustness of self-supervised ViT features in b-mode images</title> <link href="https://deep-learning-mit.github.io/blog/2023/Robustness-of-self-supervised-ViT-features-in-b-mode-images/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Robustness-of-self-supervised-ViT-features-in-b-mode-images</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;B-mode ultrasound imaging is a widely employed medical imaging technique that uses high-frequency sound waves to produce visual representations of the internal structures of the human body. Its main advantages are its ability to produce real-time images, its portability, low cost, and especially the fact that is noninvasive and safe (non-radiating). However, it is an imaging modality that carries a very high noise-to-signal ratio. Speckle noise, out-of-plane movement, and high variability in image reconstruction across devices make the resulting images complex to interpret and diagnose &lt;d-cite key=&quot;us&quot;&gt;&lt;/d-cite&gt;. As an example, the following figure shows an annotated b-mode ultrasound image.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig0-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig0-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig0-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig0.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Ultrasound b-mode image of the upper arm with the main physiology annotated. &lt;/div&gt; &lt;p&gt;Self-supervised Vision Transformers (ViT) have emerged as a powerful tool to extract deep features for a variety of downstream tasks, such as classification, segmentation, or image correspondence. Especially, DINO architectures &lt;d-cite key=&quot;dino1&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;dino2&quot;&gt;&lt;/d-cite&gt; have exhibited striking properties, where its deep features present localized semantic information shared across related object categories, even in zero-shot methodologies &lt;d-cite key=&quot;dino_feat&quot;&gt;&lt;/d-cite&gt;. Consequently, the aforementioned properties of DINO may allow us to develop efficient yet simple methods for b-mode ultrasound image interpretation, without the need for an expert or ground truth labels.&lt;/p&gt; &lt;p&gt;In this work, we propose analyzing the performance and robustness of DINO in b-mode ultrasound images of the arm and leg, capturing musculoskeletal tissue from two different ultrasound devices. We note that this dataset features a series of images with a high noise-to-signal ratio, which is a property that DINO has not yet been tested against. In particular, we focus on assessing DINO-vit-s/8 deep features across its blocks as well as its attention weights, with the final objective of segmenting bone on b-mode images in a zero-shot approach. Through all these experiments, we show the potential and feasibility of implementing DINO models in real-world b-mode medical imaging applications.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;h3 id=&quot;dino-vit-assessment&quot;&gt;DINO-vit Assessment&lt;/h3&gt; &lt;p&gt;Since the release of DINO, a self-supervised method for training ViTs based on self-distillation, there has been a line of work focused on exploring new capabilities and assessing the deep features obtained from such pre-trained models. In &lt;d-cite key=&quot;dino1&quot;&gt;&lt;/d-cite&gt;, they showed how the attention heads corresponded to different parts of an object in an image, or how one could segment desired objects by thresholding the self-attention maps. Similarly, semantic information analysis across related images was performed to show the potential of the deep features contained in DINO-vit models. Employing principal component analysis (PCA), matching algorithms or linear classifiers on the deep features, promising results on segmentation, semantic co-segmentation, and correspondence tasks were presented &lt;d-cite key=&quot;dino2&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;dino_feat&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Further research was done by combining Stable Diffusion features and DINO features, improving semantic correspondence tasks at the cost of increasing the computation effort &lt;d-cite key=&quot;dino_stable&quot;&gt;&lt;/d-cite&gt;. While DINO has shown strong generalization to downstream tasks, there has been no work on the assessment of this model on a b-mode ultrasound imaging domain. Besides the high signal-to-noise ratio, ultrasound images usually present a complex structure of tissues that makes it difficult to differentiate between the foreground, the desired structure to segment or analyze, and the background. Our work shows that DINO is also robust to this type of images, leading to promising results on segmentation tasks.&lt;/p&gt; &lt;h3 id=&quot;ultrasound-b-mode-imaging-segmentation-on-musculoskeletal-tissue&quot;&gt;Ultrasound B-mode Imaging Segmentation on Musculoskeletal Tissue&lt;/h3&gt; &lt;p&gt;Muscle and bone segmentation have important applications in clinical and rehabilitation practices to assess motion performance, diagnosis of the musculoskeletal system, and quantification of rehabilitation procedures, among others. There has been effort in developing deep learning tools to automatically segment and quantify desired parameters for the aforementioned applications. In &lt;d-cite key=&quot;unet_segment&quot;&gt;&lt;/d-cite&gt;, a U-Net architecture with Deep Residual Shrinkage layers for denoising was implemented and trained to segment muscle fibers. Similarly, different muscle heads were segmented employing a large dataset of muscle images from different subjects and devices to train several convolutional neural network architectures &lt;d-cite key=&quot;muscle_segment&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;muscle_segment2&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Medical images, from any source, are in general scarce and difficult to label, which poses a limitation for deep learning models to achieve a good performance and generalization. Most of the current methods, lack the capability to perform well in unseen segmentation tasks involving different anatomies. In &lt;d-cite key=&quot;universeg&quot;&gt;&lt;/d-cite&gt;, they developed a deep learning model, UniverSeg, based on a novel Cross-Block mechanism that produces accurate segmentation maps without the need for additional training. However, when employed in noisier data domains, such as b-mode images, the performance breaks down. In this work, we discover that DINO has potential even when dealing with noisier datasets based on b-mode ultrasound images.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;The dataset consists of b-mode ultrasound images from the arm and leg of two subjects while moving. We recorded short videos and randomly selected frames to obtain the images. In the images, bone, muscle, and fascia tissues can be appreciated. We also acquired videos from two different ultrasound sources to expand the domain where DINO was tested. With all this, 4 different image origins (or image domains) form the dataset, as appreciated in the figure below. We labeled 10 bone heads of each domain to evaluate DINO’s performance.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig01-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig01-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig01-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig01.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Example of one image of each origin with its mask label (blue). a) Arm (Source 1, Subject 1). b) Arm (Source 1, Subject 2). c) Arm (Source 2, Subject 1). d) Leg (Source 2, Subject 1) &lt;/div&gt; &lt;h3 id=&quot;deep-feature-assessment&quot;&gt;Deep Feature Assessment&lt;/h3&gt; &lt;p&gt;We analyzed DINO-vit-s/8 features over different layers qualitatively. For any block \(i\), we extracted the Keys, Values, Queries, and Tokens and applied a principal component analysis (PCA) to get the three most important components. For the attention maps, we averaged the self-attention weights of the CLS token over each head of the multi-head block.&lt;/p&gt; &lt;p&gt;This analysis was done with the intention of qualitatively finding the most suitable deep features for the subsequent segmentation task. Similarly, the self-attention maps were observed to corroborate that the model focuses especially on the bone, and less on the surrounding structures.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Workflow to obtain deep features as well as self-attention information. Transformer block design obtained from &lt;d-cite key=&quot;dino_feat&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;h3 id=&quot;segmentation-pipeline&quot;&gt;Segmentation Pipeline&lt;/h3&gt; &lt;p&gt;As described in the results section, the Keys of the last block (block 12) of DINO-vit-s/8 were employed as deep features for the segmentation. As in &lt;d-cite key=&quot;dino_feat&quot;&gt;&lt;/d-cite&gt;, we used a zero-shot approach as the pipeline for bone segmentation. We first clustered together all the features obtained from the different images passed through DINO with k-means. Then, we selected those clusters for the segmentation mask employing a simple voting algorithm. Being \(\texttt{Attn}_i^\mathcal{I}\) the self-attention of the CLS token averaged over all heads of block 12 in image \(\mathcal{I}\) and patch \(i\); and \(S_k^\mathcal{I}\) the segment in image \(\mathcal{I}\) belonging to cluster \(k\). The saliency of this segment was computed as&lt;/p&gt; \[\texttt{Sal}(S_k^\mathcal{I}) = \frac{1}{|S_k^\mathcal{I}|} \sum_{i \in S_k^\mathcal{I}} \texttt{Attn}_i^\mathcal{I}\] &lt;p&gt;and the voting of the cluster \(k\) was obtained as&lt;/p&gt; \[\texttt{Votes}(k) = \mathbb{1}[\sum_\mathcal{I}\texttt{Sal}(S_k^\mathcal{I}) \geq \tau ]\] &lt;p&gt;for a threshold \(\tau\) set to 0.2. Then, a cluster \(k\) was considered to be part of the mask if its \(\texttt{Votes}(k)\) were above a percentage of 65% of all images. The following image sketches the whole process.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Zero-shot segmentation pipeline using keys as deep features. &lt;/div&gt; &lt;p&gt;To quantitatively assess the segmentation results, both Dice and IoU metrics were computed employing the labeled bone head segmentations.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;deep-features-assessment&quot;&gt;Deep Features Assessment&lt;/h3&gt; &lt;p&gt;We first input a single image to the model and analyzed the Keys, Values, Queries, and Tokens, as well as the self-attention of the CLS token from shallower to deeper layers.&lt;/p&gt; &lt;p&gt;The three most important components after performing the PCA on the deep features are plotted in RGB as depicted in the figure below. Tokens seem to carry spatial information throughout the different blocks, representing depth information in the final block. On the other hand, Keys and Values seem to carry spatial information on the shallower blocks, and semantic information on the deeper blocks. In fact, we considered the Keys descriptors the most appropriate to be used to segment bone, as the bone head can be distinguished from the surrounding structures. Regarding the attention maps, they seem to move from the skin (in shallow blocks) to the bone (deeper blocks).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Token, Value, Key, and Query features as well as self-attention maps for different blocks (from shallow to deep). &lt;/div&gt; &lt;p&gt;Now, if we focus on the Keys features of the last block for the four different image domains, we can appreciate a similar behavior. Bone heads seem to be represented in all four cases by the Keys, being differentiated by the surrounding structures. That being said, we should note that the intersection between muscles just above the bone is in some cases also represented like the bone. Regarding the self-attention maps, in all four cases, they are principally focused on the bone head. However, we can also see that some muscle fibers or intersections may be present.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Keys deep features and self-attention maps from block 12 for the four different image origins. &lt;/div&gt; &lt;p&gt;An interactive scatter plot is another method to argue the representation of the bone by the Key features. For all the four different image origins, the patches belonging to the bone head are grouped on a region of the Euclidean space, while the patches belonging to other structures are scattered all over other regions.&lt;/p&gt; &lt;div class=&quot;l-page&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/scatter.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; 3D scatter plot of the 3 components of the Key descriptors (block 12). Legend: &quot;other&quot; any patch not belonging to the bone head. &quot;boneS1A1&quot; bone patches of Source 1 - Arm Subject 1. &quot;boneS1A2&quot; bone patches of Source 1 - Arm Subject 2. &quot;boneS2A1&quot; bone patches of Source 2 - Arm Subject 1. &quot;boneS2L&quot; bone patches of Source 2 - Leg Subject 1. &lt;/div&gt; &lt;h3 id=&quot;same-domain-experiment&quot;&gt;Same Domain Experiment&lt;/h3&gt; &lt;p&gt;We subsequently performed the segmentation task on a set of images from the same origin. For each of the 4 domains, sets of 2, 3, 5, and 10 images were input to the segmentation pipeline. Recalling that the images were selected as random frames from short videos, each image within a domain presented a slightly different configuration of bone and surrounding structures. Therefore, the goal of segmenting with varying image quantities was to evaluate the balance between improvements due to increased feature quantity versus confusion introduced by variation in the images.&lt;/p&gt; &lt;p&gt;The reader can observe the results in the figure below. The bones from Source 1 Arm 1 are the best segmented, and the amount of images does not affect the performance, obtaining constant values of Dice and IoU of about 0.9 and 0.77, respectively. The segmentation of images from Source 1 Arm 2 in general takes also some part of the muscle tissue, and as in the previous case, the amount of images used does not change the performance with Dice and IoU metrics of about 0.7 and 0.5, respectively. In the case of images from Source 2 Arm 1, a larger quantity of images improves the segmentation results, increasing Dice and IoU metrics from 0.58 to 0.75, and 0.46 to 0.61, respectively. Finally, the segmentation masks from images from Source 2 Leg carry not only the bone but part of the surrounding tissue too. When increasing the number of images to 10, the performance drastically falls (with Dice and IoU of 0) as the segmentation results contain muscle fibers instead of bone.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Results of the segmentation on same domain images experiment. a) Segmentation result examples for the 4 different image domains. b) Metrics for the 4 different image domains and different amounts of images (mean and standard deviation). &lt;/div&gt; &lt;h3 id=&quot;different-domain-experiments&quot;&gt;Different Domain Experiments&lt;/h3&gt; &lt;p&gt;Then, we performed the segmentation task on a set of images from origin pairs. Five images of each origin were paired forming the following groups. Group 1: different physiology (source 1 - arm subject 1 and source 1 - arm subject 2), group 2: different sources (source 1 - arm subject 1 and source 2 - arm subject 1), group 3: different body parts (source 2 - arm subject 1 and source 2 - leg subject 1), and finally group 4: different body parts and sources (source 1 - arm subject 1 and source 2 - leg subject 1). We carried out this experiment to evaluate if the deep features shared from different image origins were similar enough to properly perform the segmentation task, giving an idea of feature correspondence between different image domains.&lt;/p&gt; &lt;p&gt;The image below shows the experiment results. The segmentation performed on the domain source 1 arm subject 1 worsens when paired with any other image domains. Both IoU and Dice metrics fall from 0.9 and 0.77 (previous values) to 0.78 and 0.59, respectively. Contrarily, the domains consisting of source 1 arm subject 2 and source 2 arm subject 1 improve when paired with source 1 arm subject 1. Finally, the image origin containing leg images maintains a similar segmentation performance when being paired.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig6-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig6-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig6-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig6.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Results of the segmentation for pairs of domain images. Legend: Different physiology (source 1 - arm subject 1 and source 1 - arm subject 2), Different sources (source 1 - arm subject 1 and source 2 - arm subject 1), Different body parts (source 2 - arm subject 1 and source 2 - leg subject 1), and Different body parts and sources (source 1 - arm subject 1 and source 2 - leg subject 1). Bar plots contain mean and standard deviation. &lt;/div&gt; &lt;h3 id=&quot;noise-experiment&quot;&gt;Noise Experiment&lt;/h3&gt; &lt;p&gt;We further assessed DINO by introducing white noise to the dataset. Being an image \(\mathcal{I}\), the image input to DINO was \(\mathcal{I}_{\texttt{Noisy}} = \mathcal{I} + \epsilon \cdot \mathcal{N}(0, 1)\). We segmented five images from the domain Source 1 Arm Subject 1 and incrementally increased the white noise strength by tuning \(\epsilon\). We performed this last experiment to evaluate how the deep features and attention maps change as well as the resulting segmentation masks with increasing noise, gaining intuition on how robust DINO can be.&lt;/p&gt; &lt;p&gt;As observed in the following figure, the Keys features and the attention weights start being affected by the noise at \(\epsilon = 2.0\). Keys features are less efficient at describing the bone from the surrounding structures, and the attention maps start shifting the attention to only the left side of the bone and the muscle line above the bone. Segmentation results show that with increased noise, some parts of the muscle are segmented and for \(\epsilon \geq 2.5\), the right side of the bone is not included on the segmentation mask.&lt;/p&gt; &lt;p&gt;Taking a look at the metrics, the more the noise strength is increased, the lower the Dice and IoU values obtained. From little noise to the highest tested in this experiment, a reduction of about 50% for both Dice and IoU occurs.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Robustness-of-self-supervised-ViT-features-in-b-mode-images/fig7.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Results with noisy images. a) Original, Keys features, attention, maps and segmentation results for different values of $\epsilon$. b) Dice and IoU metrics for different values of $\epsilon$. &lt;/div&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;In this project, we used a DINO ViT model to segment bone heads from ultrasound images using a zero-shot methodology involving clustering. We first studied how the model deep features change across different layers, and chose Key features as the most appropriate for characterizing bone. We then segmented bone from different image domains, initially employing batches of images from the same domain, and then combining them. Finally, we tested DINO and its robustness by adding additional noise.&lt;/p&gt; &lt;p&gt;Encouraging results were found in the deep features of the model. We could appreciate how both Key and Query features were capable of differentiating bone, some muscle regions, and skin tissue. We also obtained surprisingly good segmentation masks for a zero-shot methodology on a new dataset as ultrasound b-mode images are. In particular, the image domain “source 1 arm subject 1” presented very similar segmentation masks compared to the labeled ones, giving an idea of how semantic features obtained by DINO extend beyond its training data domain, displaying astonishing generalization. Even when adding noise to the image dataset, DINO Key features kept describing the bone up to high noise strengths.&lt;/p&gt; &lt;p&gt;While the project has yielded promising results, there are several limitations to take into account. First, we should note that the success of the zero-shot methodology has relied on an initial hyperparameter tuning, finding the threshold \(\tau\), the voting percentage, and the number of clusters. However, we are aware that the optimal configuration may vary across different datasets or imaging conditions. Additionally, we focused on segmenting only bone, but we have not explored the capabilities of DINO to segment other tissues or structures. We acknowledge that a comprehensive medical imaging solution should combine the segmentation of multiple relevant structures for a general understanding and application. Finally, only two anatomical parts (arm and leg) and two subjects were included in the dataset. To better explore the applicability of the model, a more diverse dataset containing more anatomical parts from more subjects should be considered.&lt;/p&gt; &lt;p&gt;In conclusion, this project demonstrates the potential of employing the DINO ViT model for ultrasound bone segmentation using a zero-shot methodology. We believe that this work lays a foundation for future improvements, promoting a more comprehensive understanding of DINO’s capabilities in medical image segmentation.&lt;/p&gt; </content> </entry> <entry> <title>Investigating the Impact of Symmetric Optimization Algorithms on Learnability</title> <link href="https://deep-learning-mit.github.io/blog/2023/Symmetry-Optimization/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Symmetry-Optimization</id> <content type="html">&lt;h2 id=&quot;introductions&quot;&gt;Introductions&lt;/h2&gt; &lt;p&gt;Neural networks have been a staple in Deep Learning due to their expressive power. While the architecture itself is very powerful, the process of \textit{optimizing} the neural network - i.e., finding the values of the parameters of the network that minimize the loss over training data - is approximate. After all, a neural network is a highly non-smooth function and is inherently difficult to optimize. The core idea of many of these methods is to approximate the neural network (i.e. via first or second-order approximations), which are then easier to optimize over.&lt;/p&gt; &lt;p&gt;Our goal is to explore if incorporating “asymmetries” into optimization can help. Many methods use a similar update rule for all parameters in the network. We experiment with using different rules for different parameters, guided by heuristics.&lt;/p&gt; &lt;h2 id=&quot;motivation-a-couple-of-nice-papers&quot;&gt;Motivation: a couple of nice papers&lt;/h2&gt; &lt;p&gt;This project is motivated by a couple results, not necessarily in the context of neural networks. The first comes from a 2004 paper by Andrew Ng titled “Feature Selection, \(L_1\) vs. \(L_2\) regularization, and rotational invariance”. It concerns the sample complexity of feature selection - how much training data is necessary to fit the model to some accuracy with high probability - where the number of relevant features is small compared to the total number of features. The paper shows that the mode of regularization is of utmost importance to the sample complexity: the sample complexity using $L_2$ regularization is exponential compared to the sample complexity with $L_1$ regularization. One may ask: what does this have to do with symmetry? In the case of $L_2$ regularization, the classifier remains the same even when the training data is rotated (i.e. the data is pre-multiplied by a rotation matrix). More aptly, logistic regression with $L_2$ regularization is \textit{rotationally invariant}. This is not the case for $L_1$ regularization. For the precise statements, see the theorems from the paper below:&lt;/p&gt; &lt;h3 id=&quot;theorem-sample-complexity-with-l_1-regularized-logistic-regression&quot;&gt;Theorem: Sample complexity with $L_1$-regularized logistic regression&lt;/h3&gt; &lt;p&gt;Let any $\epsilon&amp;gt;0, \delta&amp;gt;0, C&amp;gt;0, K\geq 1$ be given, and let $0&amp;lt;\gamma&amp;lt;1$ be a fixed constant. Suppose there exist $r$ indices $1\leq i_1, i_2,\ldots i_r\leq n$, and a parameter vector \(\theta^*\in\mathbb{R}^n\) such that only the $r$ corressponding components of $\theta^*$ are non-zero, and \(|\theta_{ij}|\leq K\) ($j=1,\ldots r$). Suppose further that \(C\geq rK\). Then, in order to guarantee that, with probability at least $1-\delta$, the parameters $\hat{\theta}$ output by our learning algorithm does nearly as well as \(\theta^*\), i.e., that \(\epsilon^l(\hat{\theta})\leq \epsilon^l(\theta^*)+\epsilon,\) it suffices that \(m=\Omega((\log n)\cdot \text{poly}(r, K, \log(1/\delta), 1/\epsilon, C)).\)&lt;/p&gt; &lt;h3 id=&quot;theorem-sample-complexity-for-rotationally-invariant-algorithms-including-l_2-regularized-logistic-regression&quot;&gt;Theorem: Sample complexity for rotationally invariant algorithms (including $L_2$-regularized logistic regression)&lt;/h3&gt; &lt;p&gt;Let $L$ be any rotationally invariant learning algorithm, and let any $0&amp;lt;\epsilon&amp;lt;1/8, 0&amp;lt;\delta&amp;lt;1/100$ be fixed. Then there exists a learning problem $\mathscr{D}$ so that: $(i)$ The labels are determinisitically related to the inputs according to $y=1$ if $x_1\geq t$, $y=0$ otherwise for some $t$, and $(ii)$ In order for $L$ to attain $\epsilon$ or lower $0/1$ misclassification error with probability at least $1-\delta$, it is necessary that the training set size be at least \(m=\Omega(n/\epsilon)\)&lt;/p&gt; &lt;p&gt;While this example is nice and shows us how symmetry can be harmful, it concerns the symmetry of the algorithm disregarding optimization. A 2022 paper by Abbe and Adsera specializes the effects of symmetry to neural networks trained by gradient descent (more on this later). This paper uses a notion of symmetry called \textit{G-equivariance}. See the definition below:&lt;/p&gt; &lt;h3 id=&quot;definition-g-equivariance-a-randomized-algorithm-a-that-takes-in-a-data-distribution-mathcaldinmathcalpmathcalxtimesmathcaly-and-outputs-a-function-mathcalamathcald-mathcalxrightarrowmathcaly-is-said-to-be-g-equivariant-if-for-all-gin-g-mathcalamathcaldoversetdmathcalagmathcaldcirc-g&quot;&gt;(Definition: $G-$equivariance) A randomized algorithm $A$ that takes in a data distribution $\mathcal{D}\in\mathcal{P}(\mathcal{X}\times\mathcal{Y})$ and outputs a function $\mathcal{A}(\mathcal{D}): \mathcal{X}\rightarrow\mathcal{Y}$ is said to be $G-$equivariant if for all $g\in G$ \(\mathcal{A}(\mathcal{D})\overset{d}{=}\mathcal{A}(g(\mathcal{D}))\circ g\)&lt;/h3&gt; &lt;p&gt;Here $g$ is a group element that acts on the data space $\mathcal{X}$, and so is viewed as a function $g:\mathcal{X}\rightarrow\mathcal{X}$, and $g(\mathcal{D})$ is the distribution of ${g(\mathbf{x}),y}$ where $(\mathbf{x}, y)\sim\mathcal{D}$&lt;/p&gt; &lt;p&gt;More simply, an algorithm is G-equivariant if the prediction function produced by the algorithm does not vary when the data distribution is transformed according to $G$ (i.e., a group element $g$ is applied to the data distribution). Note the algorithm includes optimizing parameters: an example of a G-equivariant algorithm is learning a fully-connected neural network via SGD with Gaussian initialization, which is equivariant with respect to orthogonal transformations. More generally, neural networks trained with SGD or noisy GD hold G-equivariance. The paper claims that G-equivariant algorithms are limitted in which functions they can learn. This is stated informally in the following theorem, where the G-alignment is a (rather complicated) measure of distance:&lt;/p&gt; &lt;h3 id=&quot;gd-lower-bound-informal-statement-limitations-of-g-equivariant-algorithms&quot;&gt;GD lower bound, informal statement: Limitations of G-equivariant algorithms&lt;/h3&gt; &lt;p&gt;Let \(\mathcal{D}_f\in\mathcal{P}(\mathcal{X}\times\mathbb{R})\) be the distribution of \((\mathbf{x}, f(\mathbf{x}))\) for \(\mathbf{x}\sim \mu_\mathcal{X}\). If \(\mu_\mathcal{X}\) is \(G-\)invariant and the \(G-\)alignment of \((\mu_\mathcal{X},f)\) is small, then \(f\) cannot be efficiently learned by a $G-$equivariant GD algorithm.&lt;/p&gt; &lt;p&gt;We refer readers interested in further details and the proof of the theorem to the paper. The paper is quite nice and we encourage readers interested in theory to take a look at it. All in all, the paper suggests training neural networks with SGD is not necessarily the way to go. Therefore, we consider variants of GD that prove to perform better in practice. We first introduce gradient descent and a popular variant: Adam.&lt;/p&gt; &lt;h2 id=&quot;overview-of-existing-optimization-algorithms&quot;&gt;Overview of existing optimization algorithms&lt;/h2&gt; &lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h3&gt; &lt;p&gt;The most widely-used optimization algorithms are some version of \textit{gradient descent}. Gradient descent iteratively updates the parameter values, moving the parameter in the direction of steepest descent (given by the negative of the gradient of the loss with respect to the parameter). Essentially, gradient descent uses a first-order approximation The amount by which the parameter is moved in this direction is referred to as \textit{learning rate} or step size, typically denoted by $\eta$. The update rule is given by \(\theta^{t+1}= \theta^t - \eta_t\nabla_{\theta}\mathscr{L}_{\mathscr{D}}(\theta^t)\) where the subscript on $\eta$ indicates a learning rate that can be changed over time. Common strategies for varying $\eta$ over time consist of decaying $\eta$, whether it be a linear or exponential decay (or something in between). In practice, \textit{stochastic} gradient descent (SGD) is used. In SGD, instead of computing the gradient for each datapoint, the gradient is approximating by taking the average of the gradients at a subset (i.e. batch) of the data. A variation of gradient descent incorporates the concept of momentum. With momentum, the increment to the parameter is a constant \(\mu\), the momentum parameter, times the previous increment, plus the update we saw in GD: \(\eta_t\nabla_{\theta}\mathscr{L}_{\mathscr{D}}(\theta^t)\). In other words, the increment is a weighted average of the previous increment and the typical GD update. Too high of a momentum can lead to overshooting the minimizer, analogous to how too high of a learning rate in GD can lead to divergence.&lt;/p&gt; &lt;h3 id=&quot;adam&quot;&gt;Adam&lt;/h3&gt; &lt;p&gt;The most popular optimizer in practice is called Adam, which performs well compared to . Adam is a gradient-based method which uses the gradient as well as the squared gradient (computed from batches), as well as an exponential decay scheme, to iteratively update $\theta$. It estimates the first and second moments of the gradient from the batch computations, and uses these estimates in its update rule. Adam requires three parameters: the learning rate, and one each for the rate of exponential decays of the moment estimates of the gradients. Adam consistently outperforms standard SGD. The optimization we present is based upon Adam, with a few modifications.&lt;/p&gt; &lt;p&gt;We briefly note that these methods are \textit{first-order methods}: they only consider first derivatives, i.e. the gradient. Second-order methods, such as Newton’s method, should theoretically be better because the approximation of the function will be better. However, the computation of the Hessian is rather cumbersome in neural networks, which is why they are not typically used.&lt;/p&gt; &lt;h3 id=&quot;automatic-gradient-descent&quot;&gt;Automatic Gradient Descent&lt;/h3&gt; &lt;p&gt;Another method we consider is Automatic Gradient Descent (AGD), which is developed in recent literature (co-authored by our very own instructor, Jeremy Bernstein!). This paper attempts to get rid of the pesky hyperparameter-tuning stage that is involved in training neural networks, leading to \textit{hyperparameter transfer}. In practice, a variety of learning rates is tested during training. In addition, this learning rate may not “transfer” across architectures: if one were to make their neural network wider or deeper, they would most likely have to search for the optimal learning rate once again. Automatic Gradient Descent attempts to solve this problem by coming up with an update that is architecture-independent in the realm of MLPs. AGD operates by computing an upperbound for the loss after the update (i.e. $\mathscr{L}(\mathbf{w}+\Delta\mathbf{w})$, where $\mathbf{w}$ is the parameter we are optimizing), then optimizing this upperbound in $\Delta\mathbf{w}$ to find the best step size. This step size is then used to update the parameter, and is recalculated at each iteration. The algorithm uses spectrally-normalized weight matrices, which allows for a nice upperbound for the loss function allowing for the optimal choice of $\eta$ to be solved for (in particular, it allows for matrix inequalities involving matrix norms to be used). The algorithm is given in full below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/agd-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/agd-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/agd-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/agd.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We include AGD in this discussion because it is an asymmetric algorithm: the weights are normalized in a layer-dependent fashion. In addition, it takes a stab at alleviating the annoying task of hyperparameter tuning. We see in practice, however, that it does not perform as well as Adam. This is presumably because the approximation of the loss function via upperbounding with matrix inequalities is not tight, or maybe because the model does not incorporate biases as presented in the paper.&lt;br /&gt; We now begin discussion of our method, which has been crafted after studying these existing methods and taking into account the potential disbenefits of asymmetry.&lt;/p&gt; &lt;h3 id=&quot;extension-of-agd-to-regularized-losses&quot;&gt;Extension of AGD to regularized losses&lt;/h3&gt; &lt;p&gt;We found the idea of AGD to be very nice, and in an attempt to understand it better, decided to explore one of the further directions listed in the paper: applying the method to regularized losses. The work in the paper applies to losses of the form $\frac{1}{N}\sum_{(x, y)}l(f_w(x), y)$. However, a more general loss includes a regularization term: \(\mathcal{L}(w)=\frac{1}{N}\sum_{(x, y)}l(f_w(x), y)+\lambda R(w)\) where $R(w)$ is a regularization term. For our purposes, we assume $l$ to be the squared-loss and $R(w)$ to be the $L_2$ norm of $w$. We shorthand $\frac{1}{N}\sum_{(x, y)}l(f_w(x), y)$ to $\hat{l}$. Below, we derive the learning rate, in the context of AGD (i.e. with the spectrally normalized weights and same form of update), for this regularized loss:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/reg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/reg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/reg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/reg.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We have omitted a lot of intermediary steps involving matrix inequalities and derivatives - see the paper on AGD if you are interested in the details! We remark that this choise of $\eta$ depends on $\lambda$, so hyperparameter tuning is still necessary. Some dependence on the architecture shows up in $\eta$, namely $\Tilde{d}$. However, as the network scales this parameter can stay constant. We are interested in how this will perform in practice - check the blog for updates on this!&lt;/p&gt; &lt;h2 id=&quot;introducing-asymmetric-nature&quot;&gt;Introducing Asymmetric Nature&lt;/h2&gt; &lt;p&gt;Our initial experiment involved a two-layer neural network (width: 1024) trained on the MNIST Dataset using three distinct learning algorithms: i) AGD (gain = 1), ii) Default Adam, and iii) Adam with diverse hyperparameters for both layers. The graph below showcases the resulting loss and accuracy. The first graph showcase loss while the second one showcase accuracy.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/lossmnist-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/lossmnist-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/lossmnist-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/lossmnist.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/accmnist-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/accmnist-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/accmnist-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/accmnist.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Given MNIST’s high accuracy even with minimal epochs, the distinction wasn’t apparent. Notably, while the asymmetric algorithm matched or outperformed default Adam, fine-tuning Adam’s hyperparameters yielded superior performance.&lt;/p&gt; &lt;p&gt;Inspired by AGD’s removal of the learning rate hyperparameter, we crafted two AGD variations for comparison with SGD and the original AGD.&lt;/p&gt; &lt;h3 id=&quot;variation-1&quot;&gt;Variation 1&lt;/h3&gt; &lt;p&gt;This variation incorporated momentum into AGD, integrating AGD’s learning rate and gradient summary with momentum’s past and current gradients. Surprisingly, this had minimal impact, indicating the optimality of gradient summary and learning rate.&lt;/p&gt; &lt;h3 id=&quot;variation-2&quot;&gt;Variation 2&lt;/h3&gt; &lt;p&gt;Here, instead of typical momentum, we introduced layer-wise asymmetry, acknowledging each layer’s varying impact on loss. Adjusting each layer’s learning rate inversely proportional to its number resulted in notable performance differences!&lt;/p&gt; &lt;p&gt;Results from training under these algorithms using the cifar-10 Dataset and MSE Loss are depicted in the subsequent diagram.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/losscifar-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/losscifar-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/losscifar-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/losscifar.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/acccifar-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/acccifar-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/acccifar-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-01-Symmetry-Optimization-project/acccifar.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h3&gt; &lt;p&gt;Emphasizing learnability, we adopt the ordering concept over exact measures. Algorithm $A_1$ is deemed superior to $A_2$ if its expected learning ability (distinguishing correct/incorrect classifications) surpasses $A_2$. This learning ability, resembling a Beta distribution, hinges on directly propotional to current accuracy. Therefore, we made our evaluation on accuracy and loss graph over epochs.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our blog offers insights into optimizing neural networks and advocates for the potential benefits of asymmetry in training processes. We trust you found our journey as engaging as we did in developing it!&lt;/p&gt; &lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt; &lt;p&gt;Ng, Andrew Y. ”Feature selection, L 1 vs. L 2 regularization, and rotational invariance.” Proceedings of the twenty-first international conference on Machine learning. 2004.&lt;/p&gt; &lt;p&gt;Bernstein, Jeremy, et al. ”Automatic Gradient Descent: Deep Learning without Hyperparameters.” arXiv preprint arXiv:2304.05187 (2023).&lt;/p&gt; &lt;p&gt;Bernstein, Jeremy, et al. ”Automatic Gradient Descent: Deep Learning without Hyperparameters.” arXiv preprint arXiv:2304.05187 (2023).&lt;/p&gt; &lt;p&gt;Kingma, Diederik P., and Jimmy Ba. ”Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980 (2014).&lt;/p&gt; &lt;p&gt;Philipp, George, Dawn Song, and Jaime G. Carbonell. ”The exploding gradient problem demystified- definition, prevalence, impact, origin, tradeoffs, and solutions.” arXiv preprint arXiv:1712.05577 (2017).&lt;/p&gt; </content> </entry> <entry> <title>Can CNN learn shapes?</title> <link href="https://deep-learning-mit.github.io/blog/2023/how-cnns-learn-shapes/"/> <updated>2023-11-09T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/how-cnns-learn-shapes</id> <content type="html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;One widely accepted intuition is that Convolutional Neural Networks (CNNs) that are trained for object classification, combine low-level features (e.g. edges) to gradually learn more complex and abstracted patterns that are useful in differentiating images. Stemming from this is the idea that neural networks can understand and use shape information to classify objects, as humans would. Previous works have termed this explanation the shape hypothesis. As &lt;d-cite key=&quot;kriegeskorte2015deep&quot;&gt;&lt;/d-cite&gt; puts it,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;… the network acquires complex knowledge about the kinds of shapes associated with each category. […] High-level units appear to learn representations of shapes occurring in natural images&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This notion also appears in other explanations, such as in &lt;d-cite key=&quot;lecun2015deep&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Intermediate CNN layers recognize parts of familiar objects, and subsequent layers […] detect objects as combinations of these parts.&lt;/p&gt; &lt;/blockquote&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/shapetexture-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/shapetexture-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/shapetexture-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/shapetexture.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 1. &lt;d-cite key=&quot;geirhos2018imagenet&quot;&gt;&lt;/d-cite&gt; shows that CNNs trained on ImageNet data are biased towards predicting the category corresponding to the texture rather than shape.&lt;/p&gt; &lt;p&gt;Yet it remains poorly understood how CNNs actually make their decisions, and how their recognition strategies differ from humans. Specifically, there is a major debate about the question of whether CNNs primarily rely on surface regularities of objects, or whether they are capable of exploiting the spatial arrangement of features, similar to humans. Studies have shown that the extent to which CNNs use global features ; shapes or spatial relationships of shapes, is heavily dependent on the dataset it is trained on. &lt;d-cite key=&quot;geirhos2018imagenet&quot;&gt;&lt;/d-cite&gt; shows that CNNs trained on ImageNet data are biased towards predicting the category corresponding to the texture rather than shape. &lt;d-cite key=&quot;farahat2023novel&quot;&gt;&lt;/d-cite&gt; reveal that CNNs learn spatial arrangements of features only up to a intermediate level of granularity by comparing networks trained on Sketchy dataset, composed of sketches drawn by images of animals, and the Animals dataset, images of animals.&lt;/p&gt; &lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;p&gt;The question leading this project is if it is possible to steer the learning of a CNN network to use abstracted global shape features as dominant strategy in classifying images, in a similar sense that humans do. Previous works have shown that networks trained on &lt;d-cite key=&quot;geirhos2018imagenet&quot;&gt;&lt;/d-cite&gt; texture agnostic datasets, or &lt;d-cite key=&quot;farahat2023novel&quot;&gt;&lt;/d-cite&gt; abstracted sketch dataset have an increased ability to integrate global features. Extending the findings of these works, I experiment if it possible to induce the learning of CNNs to depend on global shapes by adjusting the filter size, or augmenting and curating the training data.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;p&gt;In the following experiments, I train a CNN on human-generated sketch data and test with conlfict sets to determine if it has learned to integrate global features in its decision making. The objective is to push the network to learn and depend on global features (the overall shape) of the object rather than local features (direction or curvature of strokes) in classifying images. To do this, I first vary the filter sizes to see if there is an opimal sequence that enables the network to learn such features. Next I augment the data by fragmentation and by adding a false category so that the network is forced to learn to classify images even when the local information is obscured and only when global information is present. Finally, to test the ability of the models from each experiment in integrating the global feature, I design a conflict set that is different from the training data. Images in the conflict set have the global features (overall shape) that aligns with its category but the local features (strokes and corner conditions) are distorted to varying degrees.&lt;/p&gt; &lt;h3 id=&quot;training-data&quot;&gt;Training Data&lt;/h3&gt; &lt;p&gt;The first way that the model is pushed to learn global features is by training it on human generated sketch data. This is distinct from the previous works that have used stylized image data, or image data that has been turned in to line drawings in that it is more driven by the human perception. It is likely that the data is more varied because it is each drawn by a different person, but what humans perceive as distinctive features of that object category is likely to be present across instances.&lt;/p&gt; &lt;p&gt;The hypothesis is that because of the scarsity of features, and absense of other local features such as texture, the model would inevitably have to learn global features that humans commonly associate to object categories, such as shape.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/QuickDraw_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/QuickDraw_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/QuickDraw_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/QuickDraw_example.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 2. Example from circle and square category of &lt;a href=&quot;https://github.com/googlecreativelab/quickdraw-dataset&quot;&gt;Quick, Draw! dataset&lt;/a&gt; that are used in this project.&lt;/p&gt; &lt;p&gt;For the following experiments I use 100,000 instances each from the circle and square categories of the &lt;a href=&quot;https://github.com/googlecreativelab/quickdraw-dataset&quot;&gt;Quick, Draw! dataset&lt;/a&gt; that have been rendered into 28x28 grayscale bitmap in .npy format. The dataset is split 85% for training and 15% for validation.&lt;/p&gt; &lt;h3 id=&quot;architecture-and-training-hyperparameters&quot;&gt;Architecture and Training Hyperparameters&lt;/h3&gt; &lt;p&gt;The CNN architecture is composed of 3 convolution layers and 2 linear layers with max pooling and relu activation. The filter size of each convolution layer, marked as * is varied in the following experiments. We use cross entropy loss and accuracy is the portion of instances that were labeled correcty. Each model is trained for 20 epochs with batch size 256.&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_augmentation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2304&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2 categories (circle, square) &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h3 id=&quot;convolutional-layer-filter-size&quot;&gt;Convolutional Layer Filter Size&lt;/h3&gt; &lt;p&gt;The hypothesis is that the size of the filters of each convolution layer affects the scale of features that the network effectively learns and integrates in its final decision making. The underlying assumption is that if the filter size gradually increases, the CNN learns global scale features and uses that as dominant stragety. I test for different combinations of size 3,5,7,9 to see if there is an optimal size filter to train a CNN network for our purpose.&lt;/p&gt; &lt;h3 id=&quot;data-augmentation---fragmentation&quot;&gt;Data Augmentation - Fragmentation&lt;/h3&gt; &lt;p&gt;I train models with augmented data of different degree of fragmentation. Lower degrees of fragmentation divide the shape into 2 fragments and with higher degree, the shape is divided into an increasing number of parts. I do this by using masks that create streaks going across the image each in the horizontal, vertical and two diagonal directions. As a result, we create circles and squares with dashed lines.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_fragmentation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_fragmentation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_fragmentation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_fragmentation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 3. Augmentations with varying degrees of fragmentation.&lt;/p&gt; &lt;p&gt;The hypothesis is that fragments of circles and squares may be similar, so as the network is trained to distinguish between two categories regardless, it has to gain an understanding of larger scale features ; how these line segments are composed. If the model successfully train on datasets that are highly fragmented, it is expected to acquire knowledge of global features. For instance, intermediate scale understanding interpretation of circles would be that the angle of line segments are gratually rotating. On the otherhand squares would have parallel line segments up to each corner where ther is a 90 degree change in the angle.&lt;/p&gt; &lt;h3 id=&quot;data-augmentation---negative-labels&quot;&gt;Data Augmentation - Negative Labels&lt;/h3&gt; &lt;p&gt;We add instances where the local features of the circle or square is preserved, but the global feature is absent and labeled them as an additional category, ‘false’. We create this augmentation by masking half or 3/4 of the existing data. The intention here is to have the model learn to only categorize shapes when their global features are present.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_negative-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_negative-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_negative-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augmentation_negative.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 4. Augmentation with addition of ‘false’ category.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;training-evaluation&quot;&gt;Training Evaluation&lt;/h3&gt; &lt;p&gt;We first want to examine if the independent variables affect the model’s training on the classification task. There is the possibility that with certain filter sizes, the model may not be able to encode enough information to differentiate circles and squares. More likely there is a possibility with the augmentations that we are using to force the CNN to learn a more difficult strategy, where the model fails to train to classify instances similar to the training set to start with. If training the model is unsuccessful, it means that CNNs under those conditions are incapable of finding any strategy to differentiate the two shape categories.&lt;/p&gt; &lt;h3 id=&quot;conflict-set-evaluation&quot;&gt;Conflict Set Evaluation&lt;/h3&gt; &lt;p&gt;To test the networks ability to employ global features we borrow the approach of &lt;d-cite key=&quot;baker2020local&quot;&gt;&lt;/d-cite&gt; that use “conflict examples”. Conflict instances have the overall shape that aligns to its label, but the local features, such as stroke or texture do not. The premise is that it is easy for humans, that primarily use global information to differentiate shapes to successfully categorize these conflict sets. Therefore, it would be a good way to test if the trained CNNs use similar differentiating strategies as humans.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/conflictset-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/conflictset-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/conflictset-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/conflictset.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 5. Three conflict sets that obscure local features to contradict the global feature and ground truth label.&lt;/p&gt; &lt;p&gt;We create three series of conflict sets for circle and squares that obscure its most distinguishing local features. The first set obscures the corner conditions - circles with one to two angular corners and squares with chamfered corners are included in this set. The second obscures line conditions - circles with angular lines and squares with curvy lines are created for this set. The third series targets the composition of strokes - instead of continuous lines, we use series of parallel lines of varying angles to form a circle or square.&lt;/p&gt; &lt;h3 id=&quot;filter-variation&quot;&gt;Filter Variation&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_training-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_training-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_training-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_training.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 6. Training evalution for variations in filter size of the convolution layer.&lt;/p&gt; &lt;p&gt;For each variation in filter size, the models trained to reach over 98.5% accuracy on the validation set. Contrary to our speculation, the filter size did not largely affect the models ability to learn the classification task.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_results-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_results-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_results-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/filter_results.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 7. Evaluation with conflict set for variations in filter size of the convolution layer.&lt;/p&gt; &lt;p&gt;Overall we observe that having a large size filter at the final layer increases the model’s performance on the conflict set as with filter sequence 337 and 339. We can speculate that having consistantly smaller size filters in the earlier layers and only increasing it at the end (337, 339) is better than gradually increaseing the size (357, 379). However, this is not true all the time as models with consistant size filters performed relavitely well (333, 555). Starting with a larger size filter (555, 557, 579 compared to 333, 337, 379) also helped in performance. However, this also came with an exception where 339 performced better than 559.&lt;/p&gt; &lt;p&gt;Overall we can see that the models have trouble classifying instances with increased degree of conflicting local features. For instance the 4th instance in set 2 obstructs all four of the perpendicular angles of a square. The 3rd and 4th instance of set 2 have the most angular ridges forming its lines and the 7th and 8th instance of set 2 have the most circluar forming its lines. From set 3, the first and second instance obstruct the gradually changing angle of strokes within the circle the most.&lt;/p&gt; &lt;h3 id=&quot;data-augmentation-variation&quot;&gt;Data Augmentation Variation&lt;/h3&gt; &lt;p&gt;Based on the results with filter variation, we choose the filter size 555 to that performed moderately well, but still has room for improvement for the next experiment with augmented training data.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_training-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_training-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_training-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_training.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 8. Training evalution for variations in augmentation of training data.&lt;/p&gt; &lt;p&gt;All models trained to reach over 98% accuracy on the validation set. As we speculated, the model had more difficulty in training with the augmentation as opposed to without. With the additional third negative category, the model was easier to train. This is evident with the divide in the plot with datasets that were augmented with the negative category to have higher evaluation values than the baseline and those that were only augmented with fragmented data were below the baseline.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_results-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_results-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_results-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-how-cnns-learn-shapes/augment_results.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 9. Evaluation with conflict set for variations in augmentation of training data.&lt;/p&gt; &lt;p&gt;The performance of models trained with augmented data on the conflict set was worse than that trained only on the original data which proves our initial hypothesis that it would be possible to enforce the network to use global features with augmented data wrong. What is interesting is how difference augmentations affect the performance. Initially, we thought that with the increased degree of fragmentation in the augmentation, the model would learn global features better, and would perform better on the conflict set. However comparison among the augmentation variations, Aug 2 showed significanly poor performance. Adding a ‘false’ category did not boost the performance either. What is interesting is that the misclassification does not include the false label. We speculate that the model has learned to look at how much of the image is occupied.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The experiments in this project have shown that there isn’t an obvious way to steer CNN networks to learn intended scale features with filter size variation and data augmentation. While it was difficult to find a strict correlation, the variation in performance across experiments shows that the independent variables do have an affect on the information that the network encodes, and what information reaches the end of the network to determine the output. The fact that trained models were unable to generalize to the conflict set reinforces the fact that encoding global features is difficult for CNNs and it would likely resort to classifying with smaller scale features, if there are apparent differences.&lt;/p&gt; &lt;p&gt;While the project seeks to entangle factors that could affect what the CNN learns, the evaluation with conflict sets does not directly review how features are processed and learned within the network. Approaches such as visualizing the activation of each neuron or layer can be more affective in this and can reveal more about how to alter the network’s sensitivity to the global features.&lt;/p&gt; </content> </entry> <entry> <title>Quantum Circuit Optimization with Graph Neural Nets</title> <link href="https://deep-learning-mit.github.io/blog/2023/quantum-gnn/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/quantum-gnn</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;One of the most notable technological developments of the past century has been computing based on binary bits (0’s and 1’s). Over the past decades, however, a new approach based on the principles of quantum mechanics threatens to usurp the reigning champion. Basing the informational unit on the quantum bit, or qubit, instead of the binary bit of “classical” computing, quantum computing takes advantage of the strange phenomena of modern physics like superposition, entanglement, and quantum tunneling.&lt;/p&gt; &lt;p&gt;Leveraging these as algorithmic tools, surprising new algorithms may be created. Shor’s algorithm, based on quantum algorithms, can solve classically hard cryptographic puzzles, threatening the security of current cryptographic protocols. Additionally, quantum computers can significantly accelerate drug discovery and materials science through quantum molecular dynamics simulations. They also show great potential in Quantum Machine Learning (QML), enhancing data analysis and pattern recognition tasks that are computationally intensive for classical computers.&lt;/p&gt; &lt;p&gt;Similar to classical computers, which base their algorithms on circuits, quantum computers build their quantum algorithms on quantum circuits. However, quantum computers are still in development and are incredibly noisy. The complexity of a quantum circuit increases its susceptibility to errors. Therefore, optimizing quantum circuits to their smallest equivalent form is a crucial approach to minimize unnecessary complexity. This optimization is framed as a reinforcement learning problem, where agent actions are circuit transformations, allowing the training of RL agents to perform Quantum Circuit Optimization (QCO). Previous techniques in this domain have employed agents based on convolutional neural networks (CNN) &lt;d-cite key=&quot;fosel2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;My previous research has demonstrated that the inherent graphical structure of circuits make QCO based on graph neural networks (GNN) more promising than CNNs. GNNs are particularly effective for data with a graph-like structure, such as social networks, subways, and molecules. Their unique property is that the model’s structure mirrors the data’s structure, which they operate over. This adaptability sets GNNs apart from other machine learning models, like CNNs or transformers, which can actually be reduced to GNNs. This alignment makes GNNs a highly promising approach for optimizing quantum circuits, potentially leading to more efficient and error-resistant quantum computing algorithms.&lt;/p&gt; &lt;p&gt;This project extends my previous work by systematically investigating the impact of various architectural choices on the performance of GNNs in quantum circuit optimization. This is achieved through a series of experiments focusing on key variables such as the number of layers in the GNN, the implementation of positional encoding, and the types of GNN layers used.&lt;/p&gt; &lt;p&gt;Specific objectives include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Evaluating the Number of GNN Layers&lt;/strong&gt;: Investigating how the depth of GNNs influences the accuracy and efficiency of quantum circuit optimization. This involves comparing shallow networks against deeper configurations to understand the trade-offs between complexity and performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Exploring Positional Encoding Techniques&lt;/strong&gt;: Positional encoding plays a crucial role in GNNs by providing information about the structure and position of nodes within a graph. This project experiments with various encoding methods to determine their impact on the accuracy of quantum circuit optimization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assessing Different Sizes of Hidden Dimension&lt;/strong&gt;: This objective focuses on understanding the influence of the hidden dimension size within GNN layers on the performance of quantum circuit optimization. By varying the size of the hidden dimension, the project identifies the optimal balance between computational complexity and the model’s ability to capture complex relationships within the data.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;quantum-circuits-and-transformation-environment&quot;&gt;Quantum Circuits and Transformation Environment&lt;/h2&gt; &lt;p&gt;In order to have quantum circuit optimizers we need quantum circuits! Quantum circuits are built out of quantum gates operating on qubits. These quatum circuits implement quantum algorithms in a similar way that classical circuits implement classical algorithms. In the below example, we have a five qubit circuit. It has a variety of single qubit gates (X, Rz, and H) as well as two qubit gates (CX).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_before-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_before-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_before-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_before.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Some gates have classical analogs, like the X gate which is analogous to the classical NOT bit-flip gate. Others, like the Hadamaard (H) gate, cannot be understood with classical intuition. We can use gates like H in combination with a two qubit gate like CX two put two qubits into unique quantum states. For example, with the following circuit, we can put two qubits into a special state called “quantum entanglement”.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/entanglement-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/entanglement-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/entanglement-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/entanglement.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/heads-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/heads-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/heads-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/heads.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;These qubits have outcomes that are perfectly correlated with each other. If they are measured, they will always result in the same outcome, even if after the circuit is applied the qubits are separated an arbitrary distance. This is despite the fact that the outcome is perfectly random! Measurement will result in 0 and 1 with probability 50% each. This is like flipping two coins whose outcome you cannot predict, but which always land both heads or both tails.&lt;/p&gt; &lt;p&gt;We can write the circuit and subsequent quantum state with the following equation. The two possible resulting states (both heads or both tails) are represented in bracket notation: \(\ket{00}\) and \(\ket{11}\).&lt;/p&gt; &lt;p&gt;\begin{equation} \ket{\psi} = \text{CX} \cdot (H \otimes I) \ket{00} = \frac{1}{\sqrt{2}}(\ket{00} + \ket{11}) \end{equation}&lt;/p&gt; &lt;p&gt;However, just like classical algorithms can be written down according to different programs and circuits which do the same thing, quantum circuits can have different equivalent forms. Transitions between these equivalent forms can be written down according to a set of local rules mapping from some set of quantum gates to another.&lt;/p&gt; &lt;p&gt;In the following diagram we show the quantum transformations used for this project. They are ordered according to 1) single qubit, 2) two qubit, and 3) three qubit transformations.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/one_gate_trans-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/one_gate_trans-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/one_gate_trans-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/one_gate_trans.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/two_gate_trans-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/two_gate_trans-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/two_gate_trans-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/two_gate_trans.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/three_gate_trans-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/three_gate_trans-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/three_gate_trans-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/three_gate_trans.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;These transformations will serve as the action space for our quantum circuit environment. Notably, some of these circuit transformations involve merges or cancellations, which can be used to simplify the circuits. A quantum agent which chooses an appropriate sequence of circuit transformations can then simplify a circuit into an equivalent form with fewer gates. Therefore, the task of circuit optimization may be decomposed into a trajectory of agent steps leading between different states, where states correspond to quantum circuits which are all algorithmically equivalent.&lt;/p&gt; &lt;h2 id=&quot;proximal-policy-optimization&quot;&gt;Proximal Policy Optimization&lt;/h2&gt; &lt;p&gt;To train the GNN agent, we use the Proximal Policy Optimization (PPO) algorithm. PPO is a model-free, on-policy reinforcement learning algorithm that aims to optimize the policy of a reinforcement learning agent by iteratively updating its policy network. We train the GNN agent on n-qubit random circuits. For training the GNN-based agents for quantum circuit optimization, we use the Proximal Policy Optimization (PPO) algorithm. PPO is a deep reinforcement learning algorithm that has shown success in a variety of applications, including game playing and robotics. The algorithm updates the policy by maximizing a surrogate objective function that approximates the expected improvement in the policy, while enforcing a constraint on the maximum change in the policy. This constraint helps to prevent the policy from changing too much from one iteration to the next, which can destabilize the training process.&lt;/p&gt; &lt;p&gt;\begin{equation} L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t[\min(r_t(\theta))\hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)\hat{A}_t] \end{equation}&lt;/p&gt; &lt;p&gt;To train the GNN agents for quantum circuit optimization, we start by initializing the GNN weights randomly. We then use the PPO algorithm to update the weights by sampling circuits from a distribution of n-qubit random circuits, encoding them into graphs, and simulating the circuits in a custom python gym environment. For each transformation we use&lt;/p&gt; &lt;p&gt;\begin{equation} r_t = - \left(q(s_{t+1}) - q(s_{t})\right) \end{equation}&lt;/p&gt; &lt;p&gt;as the reward signal for the PPO algorithm following &lt;d-cite key=&quot;fosel2021&quot;&gt;&lt;/d-cite&gt;, where we let $q$ be a function quantifying the desirability of the circuit. The PPO algorithm then updates the GNN weights to maximize the expected reward, subject to the maximum change in the policy constraint.&lt;/p&gt; &lt;p&gt;We use \(q(s) = -\texttt{circuit_size}(s)\), such that the agent’s objective is to reduce the overall circuit size, as measured by number of gates, resulting in the reward function:&lt;/p&gt; &lt;p&gt;\begin{equation} r_t = \texttt{circuit_size}(s_{t+1}) - \texttt{circuit_size}(s_t) \end{equation}&lt;/p&gt; &lt;p&gt;The methodology for implementing the quantum circuit optimization using deep reinforcement learning and graph neural networks consists of three main components: (1) encoding the circuits as directed acyclic graphs using the DAG encoding and (2) encoding the graphs as node and edge feature tensors and training a GNN-based agent using the PPO algorithm,.&lt;/p&gt; &lt;h2 id=&quot;gnn-architecture&quot;&gt;GNN architecture&lt;/h2&gt; &lt;p&gt;The GNN architecture used is inspired by the message passing neural network (MPNN), which is a type of GNN that performs iterative message passing between nodes in the graph. The GNN architecture used for this approach consists of \(L\) layers of Residual Gated Graph ConvNets.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/gnn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/gnn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/gnn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/gnn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The GNN gets as input the graph (encoded as the three tensors shown above), the positional encoding, and a binary tensor encoding of which transformations are allowed for each node (this can be computed in \(O(\# nodes)\) time).&lt;/p&gt; &lt;p&gt;Node features and positional encoding are both mapped to a k-dimensional embedding with a linear transformation and concatenated added together, forming a vector \(h\). The edge features are also linearly mapped to some \(l\)-dimensional embedding vector \(e\).&lt;/p&gt; &lt;p&gt;After, passing through \(L\) layers, each node has a feature vector \(h’\). These features are mapped to a length \(t\) Q-vector where t=# transformations. A mask is applied so that all impossible transformations are ignored. The length \(t\) Q-vectors are concatenated together from all nodes and then outputted by the GNN. An action is selected by choosing the node/transformation which corresponds to the index of the maximum Q-value.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;After training our graph neural network agent in the quantum circuit environment using PPO, we can verify that the agent can indeed optimize circuits. We randomly sample a five qubit circuit and run our agent on the circuit for fifty steps. We see that the agent is able to successfully reduce the cirucit size from 44 gates to 30, a 14 gate reduction. Meanwhile, the standard Qiskit optimizer could only reduce the circuit to 36 gates.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_optimization-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_optimization-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_optimization-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/random_optimization.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Now that we have verified our learning algorithm we can successfully train a quantum circuit optimizing agent, we proceed with our study over three hyperparameters: 1) number of layers, 2) the use of positional encoding, and 3) hidden dimension. For all plots, we display the average over several runs with standard error.&lt;/p&gt; &lt;h3 id=&quot;number-of-layers&quot;&gt;Number of Layers&lt;/h3&gt; &lt;p&gt;We innvestigate how the depth of GNNs influences the accuracy and efficiency of quantum circuit optimization. This involves comparing shallow networks against deeper configurations to understand the trade-offs between complexity and performance. In order to do this we scan over the number of layers \(L\) in our GNN from 1 to 7.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/n_layers-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/n_layers-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/n_layers-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/n_layers.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We see that, generally, increasing the number of layers in the model improves performance of the model on random circuits. This is aligned with the intuition that increasing the number of layers of a GNN allows models to “see” information from further away, which can be used to make strategic decisions.&lt;/p&gt; &lt;p&gt;However, we also observe that there is some critical point in which increasing \(L\) no longer leads to better outcomes from the model. This threshold appears to occur around \(L=5\), which performs similarly to \(L=7\).&lt;/p&gt; &lt;p&gt;This could be related to a known property of GNNs, in which features of nodes which are closer together are more similar. This becomes excerterbated as the number of layers increase, smearing out information. Therefore, we expect that if we continued to increase \(L\) then model performance would degrade.&lt;/p&gt; &lt;h3 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h3&gt; &lt;p&gt;Positional encoding can provide information about the structure and position of nodes within a graph. These features can often play a role in symmetry-breaking.&lt;/p&gt; &lt;p&gt;In addition to the existing features encoding gate type and wire information, we concatenate 8 normally distributed dimensions to the feature vector. We hypothesize that these random features can be used to “ID” gates that have the same gate type but are a located in different locations. We experiment with training a GNN with and without the addition of random positional encoding.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/pos_encoding-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/pos_encoding-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/pos_encoding-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/pos_encoding.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The resulting plot shows inconclusive evidence. While the random positional encoding came out on top at the end of training, the difference is not significant enough to be able to conclude that it is demonstrably better.&lt;/p&gt; &lt;h3 id=&quot;hidden-dimension&quot;&gt;Hidden Dimension&lt;/h3&gt; &lt;p&gt;The last hyperparameter we examine is the hidden dimension of the GNN layers. We scan over values 16, 32, 64, and 128. All other parameters are kept fixed.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/hidden_dim-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/hidden_dim-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-quantum-gnn/hidden_dim-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-quantum-gnn/hidden_dim.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We observe that performance tends to improve with scale. However, similarly to the “number of layers” hyperparameter, there appears to be some critical threshold after which scaling no longer appears to improve performance. From our experiments this threshold appears to be around 64.&lt;/p&gt; &lt;p&gt;It is unclear what would happen if we continued scaling past 128. For example, the performance could stay at the plateau reached at hidden dimension 64 and 128, or it could eventually get worse.&lt;/p&gt; &lt;h2 id=&quot;further-work&quot;&gt;Further Work&lt;/h2&gt; &lt;p&gt;While this work gave a first glimpse at some of the structural properties that work with GNNs for RL on quantum circuits, much remaining work remains.&lt;/p&gt; &lt;p&gt;Notacably, many of the training runs did not seem to train until plateau. To be fully confident in the results, training until plateau would be necessary for full confidence. Additionally, many of the runs were quite noisy, making it difficult to distinguish between the performance under different runs. Therefore, increasing training samples could effectively reduce standard error for better statistics.&lt;/p&gt; &lt;p&gt;Moreover, the scope of future exploration can be expanded. One of the most interesting areas of future work would be on what types of graph layers work best. While we use Residual Gated Convulation Nets, it is not clear that this is the best layer type. Other things than could be tested are other positional encoding schemes. While we experimented with random features, more standard positional encoding schemes include Laplacian and Random walk encoding.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;We find that there appears to be critical thresholds of optimal values for the hidden dimension and number of layers in GNNs. We also find no evidence that random positional encoding appears to improve performance, contrary to intuition that it would serve a useful symmetry-breaking function. While much work is left to be done, this work provides a first investigation into how performance of GNNs on QCO can be affected by various choices of hyperparameters.&lt;/p&gt; </content> </entry> <entry> <title>Structural vs Data Inductive Bias</title> <link href="https://deep-learning-mit.github.io/blog/2023/Structural_vs_Data_Inductive_Bias/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Structural_vs_Data_Inductive_Bias</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;h3 id=&quot;lack-of-training-data&quot;&gt;Lack of Training Data&lt;/h3&gt; &lt;p&gt;The transformative impact of vision transformer (ViT) architectures in the realm of deep learning has been profound, with their applications swiftly extending from computer vision tasks, competing with traditional neural network architectures like convolutional neural networks (CNNs). Despite their success, the intricacies of how architectural variations within ViTs influence their performance under different data conditions remain largely uncharted. Unraveling these subtleties&lt;/p&gt; &lt;h3 id=&quot;project-goal&quot;&gt;Project Goal&lt;/h3&gt; &lt;p&gt;While much research has being made to find the best choice of data augmentation or the best structural change in the model to increase performance, our project empirically compares two kinds of methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data augmentation through tuning-free procedures&lt;/li&gt; &lt;li&gt;Explicit inductive bias through discrete attention masking For data augmentation, we chose a simple-to-use procedure called TrivialAugment to increase by four times the amount of training data. Here we want an easy-to-use method that could help as a benchmark for the second method.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For explicit inductive bias, we use a general vision transformer architecture which allow us the change the number of attention heads and layers where the mask would be applied, this mask is what explicitly induce a bias in the model by forcing some layers to only learn relationship between close patches of the data.&lt;/p&gt; &lt;p&gt;Our goal with this comparison and the difference with previous works is that we want to experiment to which point one method could be better than the other by really compensating for the lack of information in the training of a vision transformer.&lt;/p&gt; &lt;p&gt;Due to computational and time limitations, we would train our model in a simple task of image classification based on CINIC-10. We also use a tiny model to be able to iterate many times through different scenarios of inductive bias. The selection of methods also reinforces these limitations but are a good starting point as many of the projects that would be lacking in training data probably are in testing phases where light tools like Google Colab are used.&lt;/p&gt; &lt;h3 id=&quot;contribution&quot;&gt;Contribution&lt;/h3&gt; &lt;p&gt;The result from this project contributes in two ways. First, it gives us a glance of how beneficial the level of proposed inductive bias in the performance of the model could be, and second, it contrasts which method, and until which point, performs better given different scenarios of initial training data available.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt; &lt;p&gt;Data augmentation consists in applying certain transformations to the data in order to create new examples with the same semantic meaning as the original data. For images, data augmentation consists in spatial transformations like cropping, zooming or flipping. Although data augmentation is very popular among practitioners, previous works like &lt;d-cite key=&quot;ref1&quot;&gt;&lt;/d-cite&gt; have proven that data augmentation by itself reaches a saturation point where it is even worse than training in the new data, decreasing the performance of the model. Given our goal of comparing data augmentation with inductive bias, we expect to get a similar result in the efficacy of data augmentation while we increase the initial amount of data.&lt;/p&gt; &lt;p&gt;Data augmentation decisions can be thought because of the many options available to perform, but it is so popular that some researches are trying to make it more easy to use and computational-efficient, one example been TrivialAugment &lt;d-cite key=&quot;ref2&quot;&gt;&lt;/d-cite&gt; method where simple random augmentation can be compared to or outperform other more complex algorithms that try to find the best augmentation for the given dataset. TrivialAugment would be the procedure used in this project given it simplicity.&lt;/p&gt; &lt;h3 id=&quot;changes-in-architecture&quot;&gt;Changes in Architecture&lt;/h3&gt; &lt;p&gt;To compensate the lack of training data for vision transformers, an interesting approach from &lt;d-cite key=&quot;ref3&quot;&gt;&lt;/d-cite&gt; is to use instance discrimination techniques which adjust the loss function of the model to improve the representation of the datapoints getting high accuracy scores for datasets with only 2k samples. The model proposed is trained from scratch with few data, but its implementation and interpretability could be troublesome for small projects.&lt;/p&gt; &lt;p&gt;Othe authors in &lt;d-cite key=&quot;ref4&quot;&gt;&lt;/d-cite&gt; use a set of pre-trained models with complementary structures (Convolutional and Involutional) to help a lightweight visual transformer model called DeiT (Data-efficient image Transformer) increase its performance by getting a baseline result that is added as a token and works like an induced bias of the properties of the image. The scores from the pre-trained models give more information than the ground truth because it gives a value of likehood for every class, which is a result of the different attributes of the specific image.&lt;/p&gt; &lt;p&gt;Although these changes demonstrate that it is possible to get better performance with few data without augmentation, it is not clear how we can adjust the inductive bias produced to identify until which point it works. The usage of pre-trained models is also not desirable here because of our premise that we could be using this experiment to make decisions in new datasets and tasks.&lt;/p&gt; &lt;h3 id=&quot;explicit-inductive-bias&quot;&gt;Explicit Inductive Bias&lt;/h3&gt; &lt;p&gt;The model proposed in &lt;d-cite key=&quot;ref5&quot;&gt;&lt;/d-cite&gt; is a better example of real lack of training data overcome by introducing a different kind of attention heads. In this case, medical images tend to have the same orientation, property that is leveraged to force the attention heads to focus on axial information which normally represents the spread of tumors. Here the inductive bias is that the image has a structure where patches aligned in rows and columns are more related between them than diagonal ones.&lt;/p&gt; &lt;p&gt;Following this path, &lt;d-cite key=&quot;ref6&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;ref7&quot;&gt;&lt;/d-cite&gt; and &lt;d-cite key=&quot;ref8&quot;&gt;&lt;/d-cite&gt; try to apply the local induced bias of convolutional networks in a transformer by different methods. &lt;d-cite key=&quot;ref6&quot;&gt;&lt;/d-cite&gt; adds a new layer at the beginning of the model which acts like a local mask but with variable learnable attention levels, where the model figures out how much local attention it should apply given the proposed task. &lt;d-cite key=&quot;ref7&quot;&gt;&lt;/d-cite&gt; on the other hand add new convolutional layers in parallel to the transformer to let them capture the local information while letting the original transformer to keep the big picture of the image. Finally, in &lt;d-cite key=&quot;ref8&quot;&gt;&lt;/d-cite&gt; it is proposed a change in the initial attention layers, making them GPSA (Gated Positional Self-Attention) which learn for each patch if pay more attention to the attention product (Query * Key) or the position of the patch in the image.&lt;/p&gt; &lt;p&gt;From these works it is stated that some layers of the transformer converge to convolutional behaviors given the nature of the data used for training, but this requires a relatively big amount of data that could not be available. It is also noticed that the inductive bias is applied to the first layers of the model.&lt;/p&gt; &lt;p&gt;The model proposed in &lt;d-cite key=&quot;ref9&quot;&gt;&lt;/d-cite&gt; uses a simpler method which consists in applying a mask pattern to some of the attention heads to induce local attention bias into the model. To decide which heads and layers should be masked, it uses a soft masking approach where the model learns a scale factor between 0 and 1 which sets the level of local inductive bias that is applied to that head. The results show that it is possible to obtain good results by using more local masking in the first layers and keeping the global interaction in the last ones. This approach is also model agnostic and easy to implement, which is why it is close to the experimentation of this project.&lt;/p&gt; &lt;p&gt;The power of this masking method is also shown in &lt;d-cite key=&quot;ref10&quot;&gt;&lt;/d-cite&gt; where the mask is learned by a parallel process of pixel-wise classification, successfully increasing the performance in more complex tasks like pixel-wise segmentation.&lt;/p&gt; &lt;h2 id=&quot;methods-and-experiment&quot;&gt;Methods and Experiment&lt;/h2&gt; &lt;p&gt;To explore and compare the benefits of data augmentation versus induced bias we are running three related experiments. All experiments would be run with CINIC-10 &lt;d-cite key=&quot;ref11&quot;&gt;&lt;/d-cite&gt; dataset in Google Colab using a T4 GPU. We decided to use CINIC-10 instead of CIFAR-10 because even though it is a drop-in replacement of CIFAR-10, it is a much larger than CIFAR-10 so we can test on different number of base training samples but not so large like ImageNet that is too large/difficult to test.&lt;/p&gt; &lt;h3 id=&quot;experiment-1&quot;&gt;Experiment 1&lt;/h3&gt; &lt;p&gt;The goal of the first experiment is to get a glance of the overall differences in accuracy for the compared methods. The model used for this experiment consists of a basic visual transformer with six layers and linear positional embeddings. Each layer corresponds to a multiheaded attention layer with only two heads. The schematic of the model can be seen in figure 1.&lt;/p&gt; &lt;p&gt;Figure 1&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_2_arch1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_2_arch1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_2_arch1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_2_arch1.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;By default, the attention heads in the model are fully connected to give them a global behavior, but the model can be configured to apply a local pattern mask or a sparse pattern mask to all heads in all layers.&lt;/p&gt; &lt;p&gt;Figure 2&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_1_mask-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_1_mask-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_1_mask-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_1_mask.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The model would be trained with different scenarios of initial data, in specific, with 1000, 2000, 5000, 12500 and 20000 samples. In each scenario, we would get four different models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Baseline model: Without data augmentation and with default global attention&lt;/li&gt; &lt;li&gt;Data augmentation: With data augmentation and default global attention&lt;/li&gt; &lt;li&gt;Local attention: Without data augmentation and with local attention&lt;/li&gt; &lt;li&gt;Sparse attention: Without data augmentation and with sparse attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The data augmentation technique would be TrivialAugment and the metric would be accuracy on validation dataset. We set these four models trying not to mix data augmentation with changes in the induced bias, keeping the default global attention in the transformer as our baseline.&lt;/p&gt; &lt;h3 id=&quot;experiment-2&quot;&gt;Experiment 2&lt;/h3&gt; &lt;p&gt;Having experimented with the differences where all layers have the same mask, we now set experiments to play with the level of induced bias applied to the model. The goal now is to identify a relation between the level of induced bias applied to the model and their performance. For this experiment we modify our first model in the following ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We increase the number of attention heads in each layer from 2 to 4&lt;/li&gt; &lt;li&gt;We set the final two layers to global attention, so the mask is not applied to them&lt;/li&gt; &lt;li&gt;We configure each head in the first four layers to be able to be hard configured as either local or global attention.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 3&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_3_arch2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_3_arch2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_3_arch2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_3_arch2.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;With this new model, we can create one instance for each combination of global/local head in any of the first four layers, generating a sense of “level of induced bias” based on the number and configuration of attention heads treated as local.&lt;/p&gt; &lt;p&gt;Given computational limitations, we would set only two initial data scenarios (10000 and 50000) and get 16 models for each scenario:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Baseline model: Without augmentation and with all global attention&lt;/li&gt; &lt;li&gt;Data augmentation: With data augmentation and all global attention&lt;/li&gt; &lt;li&gt;14 combinations of local heads and layers:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Table 1&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_7_table-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_7_table-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_7_table-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_7_table.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We would analyze the differences in accuracy between different levels of induced bias in the same initial data scenario and see if we can get a selection of best performing inductive bias levels to apply them more broadly in the third experiment.&lt;/p&gt; &lt;p&gt;With this comparison we also want to capture what are the visual differences between the attention heads in the different levels of induced bias to try to explain with is doing better or worse than the baseline.&lt;/p&gt; &lt;h3 id=&quot;experiment-3&quot;&gt;Experiment 3&lt;/h3&gt; &lt;p&gt;Our final experiment consists in comparing the accuracy and the effective additional data (EAD) that each method brings when applied to different initial amounts of data. The initial data scenarios to train the models would be 1000, 5000, 10000, 20000, and 50000 samples. The comparison would be made between the data augmentation model for each scenario, versus the top 3 levels of induced bias from experiment 2.&lt;/p&gt; &lt;p&gt;The effective additional data (EAD) represents the extra amount of real data that the method is compensating, the higher the better to be considered as a successful method for solving lack of data problems. This metric is calculated by looking at which scenario of initial data would make the baseline model perform equal to the method analyzed.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;experiment-1-1&quot;&gt;Experiment 1&lt;/h3&gt; &lt;p&gt;In our initial experiment, we compared performance on four variations of model scenarios. Our baseline model uses global attention mechanism, one uses local attention mechanism, another one uses sparse attention mechanism, and the last model uses the same global attention mechanism as the first model except that data augmentation is applied during its training process. One notable callout for our initial experiment is that we took a naïve approach and designed our local and sparse attention heads to be in all six attention layers of the attention. We trained and collected the validation accuracy and training time for each model variation for different number of base training samples from 1000 to 20000. Below are the results.&lt;/p&gt; &lt;h4 id=&quot;result-and-analysis&quot;&gt;Result and Analysis&lt;/h4&gt; &lt;p&gt;Figure 4&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_8_exp1table-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_8_exp1table-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_8_exp1table-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_8_exp1table.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 5&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_9_exp1graph-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_9_exp1graph-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_9_exp1graph-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_9_exp1graph.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;There are a few notable observations to point out from the results. First, we can see that the two models using the local attention mechanism or sparse attention mechanism performed significantly worse than our baseline model that used global attention. Though we did expect this to happen since CINIC-10’s classification task intuitively requires a global context of the image, we did not foresee the performance difference to be so drastic. For example, when the base number of training data is 5000, we see that the baseline model achieves a validation accuracy of 62.5% while the local attention model achieves just 13.97% and the sparse attention model 42.64%. We observe a similar pattern across different levels of base samples. It’s also worth calling out that sparse attention models perform better than local attention models. This makes sense as sparse attention models still take into consideration the global context just not completely on all the patches. Nevertheless, the sparse attention model takes almost the amount of time to train as the baseline model, hence it does not make sense to use it in lieu of the baseline model in practice. On the flip side, we verify that data augmentation improves performance and is the most significant when number of base samples is small.&lt;/p&gt; &lt;h3 id=&quot;experiment-2-1&quot;&gt;Experiment 2&lt;/h3&gt; &lt;p&gt;Our first experiment showed that simply setting all attention layers to contain only local or sparse attention heads does not produce good performance. As we were exploring additional datasets or tasks where applying a different attention mechanism may yield better performance, we came across the paper in &lt;d-cite key=&quot;ref9&quot;&gt;&lt;/d-cite&gt;, in which it alluded to the idea that only applying local attention mechanism to just a few beginning layers of the network may improve performance even when the task intuitively requires a global context. The rationale behind it is that perhaps through using local attention mechanism, the model can create its own convolution making understanding local information better before using that information to answer a task that requires global context. With this inspiration, we designed our second experiment trying out different combinations of local attention heads and layers, as described in the Methods and Experiments section, and below are the results and analysis.&lt;/p&gt; &lt;h4 id=&quot;result-and-analysis-1&quot;&gt;Result and Analysis&lt;/h4&gt; &lt;p&gt;Figure 6&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_10_exp2matrices-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_10_exp2matrices-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_10_exp2matrices-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_10_exp2matrices.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Here, the two matrices outline the validation accuracies we got when we trained the different local attention mechanism model on 10k and 50k base training samples. A quick recap, 1 Local Head and 1 Layer means we would use 1 local attention head in the 1st layer of the transformer. The color gradient in each matrix indicates the best performing combination from best (red) to worst (green).&lt;/p&gt; &lt;p&gt;A few patterns can be noticed. First, for both matrices, models in the bottom right corner, representing a high number of local heads and in more layers, are performing worse than the rest. This aligns with our intuition from our first experiment because having more local attention heads in deeper portions of network will prevent the models from capturing global context, thus resulting in a worse performance.&lt;/p&gt; &lt;p&gt;Figure 7&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_4_map-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_4_map-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_4_map-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_4_map.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Diving further, in figure 7, we visualize the attention weights to better compare different levels of induced bias. It seems that the performance increases as we add more local heads, but it ends up fading and not capturing the important characteristics of the data. In the 50k samples scenario it can be noticed that with more local heads, the attention spots converge to small parts of the image where there is no information about the object in it.&lt;/p&gt; &lt;p&gt;Figure 8&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_6_local_map-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_6_local_map-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_6_local_map-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_6_local_map.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Moreso, in figure 8, it can be noticed that when local heads are used, it identifies correctly smaller details of the image. In this case, with all heads being global, it is hard to identify the three different cows in the middle image, but when some local heads are used, we can capture them.&lt;/p&gt; &lt;p&gt;In summary, the major result of this experiment is that some models in the 10k samples sub-experiment produced better results than the base model. This is promising and validates our hypothesis from before. Though no combination produced better results in the 50k samples sub-experiment, we showed in Figure 8 that having local attentions can still be beneficial as it is able to capture some details that the baseline model misses.&lt;/p&gt; &lt;h3 id=&quot;experiment-3-1&quot;&gt;Experiment 3&lt;/h3&gt; &lt;p&gt;From the second experiment, we were then intrigued to see how some of the better performing models do under different number of base samples than just 10k and 50k. So, we pick three combinations (2 local heads for 2 layers, 1 local head for 2 layers, 3 local heads for 1 layer) and tested their performance against the baseline model and baseline + data augmentation for different number of base training samples from 5000 to 50k. Below are the results and analysis.&lt;/p&gt; &lt;h4 id=&quot;result-and-analysis-2&quot;&gt;Result and Analysis&lt;/h4&gt; &lt;p&gt;Figure 9&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_11_exp3table-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_11_exp3table-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_11_exp3table-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_11_exp3table.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Figure 10&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_12_exp3graph-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_12_exp3graph-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_12_exp3graph-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_12_exp3graph.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Here, we can observe two very interesting trends. First it validates our hypothesis that using local attention heads early in the layers of the vision transformers can improve performance despite the fact that task intuitive requires global context. This outcome is true for all three variations of the local attention models when the number of base training samples are 1000, 5000, and 10000. However, this effect tapers off when the number of base samples is sufficiently large, and the baseline model performs better. This seems to suggest that the benefit of the inductive bias coming from the local heads no longer outweighs the lack of information of the dataset. In other words, once there is sufficient data, the baseline model has enough information to learn a better representation on its own than that of the models.&lt;/p&gt; &lt;p&gt;Figure 11&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_13_exp3extradata-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_13_exp3extradata-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_13_exp3extradata-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-Structural_vs_Data_Inductive_Bias/figure_13_exp3extradata.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Another perhaps more explicit and comparable way of explaining the phenomenon is to look at the Effective Extra Sample score. Essentially, the data tells us how much extra (or less) training data the change in model architecture gets us to achieve the same performance accuracy if using the baseline model. This graph clearly illustrates that data augmentation and tuning of local attention heads are very effective when the training datasets are relatively small, less than 15000 samples. This is likely because the inductive bias of the local attention heads causes the models to capture important characteristics of the image more efficiently and effectively than does the baseline model. However, once the number of base training samples gets over 20000, the effect reverses and they all perform worse than the baseline model, as illustrated by the negative effective training samples.&lt;/p&gt; &lt;p&gt;Note: We did not plot the extra effective data for the data augmentation model scenario pass 10000 base training samples as its performance dropped significantly and is behaving weirdly.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Through different experimentations, both data augmentation and induced bias by discrete attention masking can compensate for the lack of data for a given problem, but this compensation is only noticeable when the initial data is very low.&lt;/p&gt; &lt;p&gt;The maximum effective additional data that the data augmentation method creates is higher than the induced bias method, but there is a sweet spot where induced bias is better than both data augmentation and baseline model.&lt;/p&gt; &lt;p&gt;Once the initial amount of data starts to increase, data augmentation is the first one that in fact worsens the performance of the model. Induced bias on the other hand looks more stable while the initial data is increasing but is still not significantly better than the baseline model.&lt;/p&gt; &lt;p&gt;We have shown that induced bias can help identify local attributes of the image more easily than the baseline alone, but this is only leveraged when the task that we want to solve is more specific and cannot be appreciated in a general task like image classification.&lt;/p&gt; &lt;h3 id=&quot;limitations-and-next-steps&quot;&gt;Limitations and Next Steps&lt;/h3&gt; &lt;p&gt;Given the restricted resources and amount of time available to execute this project, there is enough room for continuing research on this topic:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We tried to make the data augmentation and inductive bias methods simple and easy to play with, but they could not be the best ones. The same procedures of this project can be applied to better and more complex types of data augmentation and induced bias to see if the results are replicable in other situations.&lt;/li&gt; &lt;li&gt;Further experimentation could be done with datasets with multiple tasks and a deeper model to see if the type of task has an impact of the effectiveness of one method or the other. This could also be applied in recent real-world problems where there is not enough data yet, but we can clearly identify the relying relationship between patches of the images.&lt;/li&gt; &lt;li&gt;Given a deeper model and a lot more experimentation in the level of inductive bias, there is an opportunity to empirically try to make a regression between how much inductive bias is applied to the model vs the resulting change in performance. The results of this project are not enough to implement such relations.&lt;/li&gt; &lt;/ul&gt; </content> </entry> <entry> <title>From Scroll to Misbelief - Modeling the Unobservable Susceptibility to Misinformation on Social Media</title> <link href="https://deep-learning-mit.github.io/blog/2023/suscep/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/suscep</id> <content type="html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt; &lt;p&gt;Susceptibility to misinformation describes the extent to believe false claims, which is hidden in people’s mental process and infeasible to observe. Existing susceptibility studies heavily rely on the crowdsourced self-reported belief level, making the downstream research homogeneous and unscalable. To relieve these limitations, we propose a computational model that infers users’ susceptibility levels given their reposting behaviors. We utilize the supervision from the observable sharing behavior, incorporating a user’s susceptibility level as a key input for the reposting prediction task. Utilizing the capability of large-scale susceptibility labeling, we could also perform a comprehensive analysis of psychological factors and susceptibility levels across professional and geographical communities. Hopefully, we could observe that susceptibility is influenced by complicated factors, demonstrating a degree of correlation with economic development around the world, and with political leanings in the U.S.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/suscep_model-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/suscep_model-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/suscep_model-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-suscep/suscep_model.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; data-zoomable=&quot;&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figcaption&gt;&lt;strong&gt;Illustration of the Susceptibility Modeling.&lt;/strong&gt; We formulate the model to predict whether a given user will retweet a specific misinformation tweet. We utilize a shallow neural network to predict the susceptibility score, and together with the dot product of the user and tweet embeddings to predict retweet behavior. Our model is trained using two loss functions: binary classification entropy and triplet loss.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;False claims spread on social media platforms, such as conspiracy theories, fake news, and unreliable health information, mislead people’s judgment, promote societal polarization, and decrease protective behavior intentions&lt;d-cite key=&quot;pennycook2021psychology&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;Thier2021HealthMisinformation&quot;&gt;&lt;/d-cite&gt;. The harm is especially significant in various contentious events including elections, religious persecution, and the global response to the COVID-19 pandemic&lt;d-cite key=&quot;Ecker2022PsychologicalDriversMisinformation&quot;&gt;&lt;/d-cite&gt;. Many works have investigated the &lt;strong&gt;observable&lt;/strong&gt; behavior of information propagation such as where the information propagates&lt;d-cite key=&quot;Taylor2023WhereDoesYour&quot;&gt;&lt;/d-cite&gt;, how people share it&lt;d-cite key=&quot;Yang2021COVID19InfodemicTwitter&quot;&gt;&lt;/d-cite&gt;, and what people discuss about it&lt;d-cite key=&quot;Gupta2023PolarisedSocialMedia&quot;&gt;&lt;/d-cite&gt;. However, it is still crucial but challenging to understand the &lt;strong&gt;unobservable&lt;/strong&gt; mental and cognitive processes when individuals believe misinformation&lt;d-cite key=&quot;Ecker2022PsychologicalDriversMisinformation&quot;&gt;&lt;/d-cite&gt;. Users’ susceptibility (i.e., the likelihood of individuals believing misinformation) plays a pivotal role in this context. If a person is more susceptible to misinformation, they are not only more likely to share false claims but also more prone to being misled by them&lt;d-cite key=&quot;Scherer2020WhoSusceptibleOnline&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Existing works have investigated the psychological, demographic, and other factors that may contribute to the high susceptibility of a population&lt;d-cite key=&quot;Brashier2020AgingEraFake&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;Pennycook2020WhoFallsFake&quot;&gt;&lt;/d-cite&gt;. However, previous susceptibility studies heavily rely on self-reported belief towards false claims collected from questionnaire-based participant survey&lt;d-cite key=&quot;Escola-Gascon2021CriticalThinkingPredicts&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;Rosenzweig2021HappinessSurpriseAre&quot;&gt;&lt;/d-cite&gt;, which presents several limitations. For instance, different participants might interpret the belief levels in different ways. Moreover, the data collection process is labor-heavy and thus limits the scale of downstream research on size, scope, and diversity of the target population&lt;d-cite key=&quot;nan2022people&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;The unobservance of people’s beliefs makes it infeasible to model susceptibility directly. Luckily, existing psychological literature bridges unobservable beliefs and observable behaviors, showing that the sharing behavior is largely influenced by whether users believe the misinformation&lt;d-cite key=&quot;Altay2022WhyFewPeople&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;Atske2019ManyAmericansSay&quot;&gt;&lt;/d-cite&gt;, the attributes of the sharing content&lt;d-cite key=&quot;pennycook2021psychology&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;van2022misinformation&quot;&gt;&lt;/d-cite&gt;, and users’ internal mental motives&lt;d-cite key=&quot;Brady2020MADModelMoral&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;Islam2020MisinformationSharingSocial&quot;&gt;&lt;/d-cite&gt;. Motivated by these prior works, we formulate the relationship between believing and sharing described in social science literature into a machine learning task.&lt;/p&gt; &lt;p&gt;Concretely, we propose to infer people’s susceptibility level given their re/posting behaviors. To parameterize the model, we wrap the susceptibility level as input for the prediction model of the observable reposting behavior. We perform multi-task learning to simultaneously learn to classify whether a user would share a post, and rank susceptibility scores among similar and dissimilar users when the same content is seen. Note that our model does not aim to predict any ground-truth susceptibility for individuals. Instead, we use users’ reposting behaviors towards misinformation as a proxy for their susceptibility level for better interpretability. Our model design enables unobservable modeling with supervision signals for observable behavior, unlocks the scales of misinformation-related studies, and provides a novel perspective to reveal the users’ belief patterns.&lt;/p&gt; &lt;p&gt;We conduct comprehensive evaluations to validate the proposed susceptibility measurement and find that the estimations from our model are highly aligned with human judgment. Building upon such large-scale susceptibility labeling, we further conduct a set analysis of how different social factors relate to susceptibility. We find that political leanings and psychological factors are associated with susceptibility in varying degrees. Moreover, our analysis based on these inferred susceptibility scores corroborates the findings of previous studies based on self-reported beliefs, e.g., stronger analytical thinking is an indicator of low susceptibility. The results of our analysis extend findings in existing literature in a significant way. For example, we demonstrate that susceptibility distribution in the U.S. exhibits a certain degree of correlation with political leanings.&lt;/p&gt; &lt;p&gt;To sum up, our contributions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We propose a computational model to infer people’s susceptibility towards misinformation in the context of COVID-19, by modeling unobservable latent susceptibility through observable sharing activities.&lt;/li&gt; &lt;li&gt;Evaluation shows that our model effectively models unobservable belief, and the predictions highly correlate with human judgment.&lt;/li&gt; &lt;li&gt;We conduct a large-scale analysis to uncover the underlying factors contributing to susceptibility across a diverse user population from various professional fields and geographical regions, presenting important implications for related social science studies.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&quot;computational-susceptibility-modeling&quot;&gt;Computational Susceptibility Modeling&lt;/h2&gt; &lt;h3 id=&quot;modeling-unobservable-susceptibility&quot;&gt;Modeling Unobservable Susceptibility&lt;/h3&gt; &lt;p&gt;Inspired by the existing studies indicating that believing is an essential driver for dissemination, we propose to model susceptibility, which reflects users’ beliefs, as a driver for the sharing behavior, while considering characteristics of the sharing content and user profile.&lt;/p&gt; &lt;p&gt;We propose a computational model to infer a user’s unobservable susceptibility score based on their historical activities as shown in the model figure, and further train the model with signals from the observable disseminating behavior. We construct approximate contrastive user-post pairs as the training data (&lt;a href=&quot;#dataset-and-experiment-setup&quot;&gt;Dataset and Experiment Setup&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;This design would allow us to explore the best parameters for the computational model of an unobservable and data-hungry susceptibility variable using the rich data resources available on social media platforms.&lt;/p&gt; &lt;h4 id=&quot;content-sensitive-susceptibility&quot;&gt;Content-Sensitive Susceptibility&lt;/h4&gt; &lt;p&gt;We compute the user’s susceptibility when a particular piece of misinformation $p$ is perceived (i.e. $s_{u, p}$). This allows us to account for the fact that an individual’s susceptibility can vary across different content, influenced by factors such as topics and linguistic styles. By focusing on the susceptibility to specific pieces of misinformation, we aim to create a more nuanced, fine-grained, and accurate representation of how users interact with and react to different COVID-19 misinformation.&lt;/p&gt; &lt;h4 id=&quot;user-and-misinfo-post-embeddings&quot;&gt;User and Misinfo Post Embeddings&lt;/h4&gt; &lt;p&gt;As a component of the computational model, we use SBERT developed upon RoBERTa-large to produce a fixed-sized vector to represent the semantic information contained in the posts and user profiles. We consider the misinformation post as a sentence and produce its representation with SBERT. For the user profile, we calculate the average of sentence representations for the user’s recent original posts. More specifically, for every user-post pair $(u, p)$, we gather the historical posts written by user $u$ within a 10-day window preceding the creation time of the misinformation post $p$, to learn a representation of user $u$ at that specific time.&lt;/p&gt; &lt;h4 id=&quot;computational-model-for-susceptibility&quot;&gt;Computational Model for Susceptibility&lt;/h4&gt; &lt;p&gt;Given the input of the user profile for the user $u$ and the content for misinformation post $p$, the susceptibility computational model is expected to produce the &lt;em&gt;susceptibility score&lt;/em&gt; $s_{u, p}$ as shown below, reflecting the susceptibility of $u$ when $p$ is perceived.&lt;/p&gt; \[s_{u, p} = suscep(E(u), E(p))\] &lt;p&gt;We first obtain the embeddings $E(p)$ and $E(u)$ for post $p$ and user profile $u$, where $u$ is represented by the user’s historical tweets and $E$ is the frozen SBERT sentence embedding function. The susceptibility score is calculated by the function $suscep$, which is implemented as a multi-layer neural network, taking the concatenation of the user and post embeddings as inputs. In the training phase, we keep the sentence embedder frozen and learn the weights for the $suscep$ function that could be used to produce reasonable susceptibility scores. We expect to produce susceptibility scores for novel $u$ and $p$ pairs using the learned $suscep$ function during inference. Additionally, we normalize the resulting susceptibility scores into the -100 to 100 range for better interpretability.&lt;/p&gt; &lt;h3 id=&quot;training-with-supervision-from-observable-behavior&quot;&gt;Training with Supervision from Observable Behavior&lt;/h3&gt; &lt;p&gt;Susceptibility is not easily observable, thus it is infeasible to apply supervision on $s_{u, p}$ directly as only the user $u$ themselves know their belief towards content $p$. Thus, we propose to utilize the supervision signal for sharing a piece of misinformation, which is an observable behavior. We consider susceptibility as an essential factor of sharing behavior and use the susceptibility computational model’s output to predict the repost behavior.&lt;/p&gt; &lt;p&gt;To produce the probability for user $u$ to share post $p$, we calculate the dot product of the embeddings of the user profile and post content and consider the susceptibility score for the same pair of $u$ and $p$ as a weight factor, and passing the result through a sigmoid function, as illustrated in the model figure.&lt;/p&gt; \[p_{\text{rp}} = \sigma \left( E(u) \cdot E(p) \cdot s_{u, p} \right)\] &lt;p&gt;Note that we do not directly employ the \textit{susceptibility score} to compute the probability of sharing because the sharing behavior depends not only on the susceptibility level but also on other potential confounding factors. It is possible that a user possesses a notably high susceptibility score for a piece of misinformation yet chooses not to repost it. Hence, we incorporate a dot product of the user and post embedding in our model \dkvtwo{involve the misinformation post content and user profiles into the consideration of predicting the sharing behavior}.&lt;/p&gt; &lt;p&gt;\(\begin{align} \mathcal{L}_{\text{bce}}(u_i, t) &amp;amp;= -\left( y_i \log(p_{\text{rt}}(u_i, t)) + (1 - y_i) \log(1 - p_{\text{rt}}(u_i, t)) \right) \nonumber \\ \mathcal{L}_{\text{triplet}}(u_a, u_s, u_{ds}, t) &amp;amp;= \text{ReLU}\left(\Vert s_{u_{a},t} - s_{u_{s},t}\Vert_2^2 - \Vert s_{u_{a},t} - s_{u_{ds},t} \Vert_2^2 + \alpha \right) \nonumber \\ \mathcal{L}(u_a, u_s, u_{ds}, t) &amp;amp;= \frac{\lambda}{3} \sum_{i \in \{a, s, ds\}} \mathcal{L}_{\text{bce}}(u_i, t) + (1 - \lambda) \mathcal{L}_{\text{triplet}}(u_a, u_s, u_{ds}, t) \nonumber \label{eq:loss} \end{align}\)&lt;/p&gt; &lt;h4 id=&quot;objectives&quot;&gt;Objectives&lt;/h4&gt; &lt;p&gt;We perform multi-task learning to utilize different supervision signals. We first consider a binary classification task of predicting repost or not with a cross-entropy loss. Additionally, we perform the triplet ranking task&lt;d-cite key=&quot;chen2009ranking&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;hoffer2015deep&quot;&gt;&lt;/d-cite&gt; to distinguish the subtle differences among the susceptibility scores of multiple users when the same false content is present.&lt;/p&gt; &lt;p&gt;During each forward pass, our model is provided with three user-post pairs: the anchor pair $(u_a, p)$, the similar pair $(u_s, p)$, and the dissimilar pair $(u_{ds}, p)$. We determine the similar user $u_s$ as the user who reposted $p$ if and only if user $u_a$ reposted $p$. Conversely, the dissimilar user $u_{ds}$ is determined by reversing this relationship. When multiple potential candidate users exist for either $u_s$ or $u_{ds}$, we randomly select one. However, if there are no suitable candidate users available, we randomly sample one user from the positive (for “reposted” cases) or negative examples (for “did not repost” cases) and pair this randomly chosen user with this misinformation post $p$.&lt;/p&gt; &lt;p&gt;Here, we elaborate on the definition of our loss function. Here, $y_i$ takes the value of 1 if and only if user $u_i$ reposted misinformation post $p$. The parameter $\alpha$ corresponds to the margin employed in the triplet loss, serving as a hyperparameter that determines the minimum distance difference needed between the anchor and the similar or dissimilar sample for the loss to equal zero. Additionally, we introduce the control hyperparameter $\lambda$, which governs the weighting of the binary cross-entropy and triplet loss components.&lt;/p&gt; &lt;h2 id=&quot;dataset-and-experiment-setup&quot;&gt;Dataset and Experiment Setup&lt;/h2&gt; &lt;p&gt;We use Twitter data because it hosts an extensive and diverse collection of users, the accessibility of its data, and its popularity for computational social science research&lt;d-cite key=&quot;zhang2015ideology&quot;&gt;&lt;/d-cite&gt;. Additionally, it provides users’ free-text personal and emotional expression with crucial metadata, including timestamps and location data, which are useful for our subsequent analytical endeavors.&lt;/p&gt; &lt;h4 id=&quot;misinformation-tweets&quot;&gt;Misinformation Tweets&lt;/h4&gt; &lt;p&gt;We consider two misinformation tweet datasets: the ANTi-Vax dataset &lt;d-cite key=&quot;hayawi2022anti&quot;&gt;&lt;/d-cite&gt; was collected and annotated specifically for COVID-19 vaccine misinformation tweets. On the other hand, CoAID &lt;d-cite key=&quot;cui2020coaid&quot;&gt;&lt;/d-cite&gt; encompasses a broader range of misinformation related to COVID-19 healthcare, including fake news on websites and social platforms. The former dataset contains 3,775 instances of misinformation tweets, while the latter contains 10,443.&lt;/p&gt; &lt;p&gt;However, a substantial number of tweets within these two datasets do not have any retweets. Consequently, we choose to retain only those misinformation tweets that have been retweeted by valid users. Finally, we have collected a total of 1,271 misinformation tweets for our study.&lt;/p&gt; &lt;h4 id=&quot;positive-examples&quot;&gt;Positive Examples&lt;/h4&gt; &lt;p&gt;We define the positive examples for modeling as $(u_{pos}, t)$ pairs, where user $u_{pos}$ viewed and retweeted the misinformation tweet $t$. We obtained all retweeters for each misinformation tweet through the Twitter API.&lt;/p&gt; &lt;h4 id=&quot;negative-examples&quot;&gt;Negative Examples&lt;/h4&gt; &lt;p&gt;Regarding negative examples, we define them as $(u_{neg}, t)$ pairs where user $u_{neg}$ viewed but did not retweet misinformation post $t$. However, obtaining these negative examples poses a substantial challenge, because the Twitter API does not provide information on the “being viewed” activities of a specific tweet. To tackle this issue, we infer potential users $u_{neg}$ that highly likely viewed a given tweet $t$ following the heuristics: 1) $u_{neg}$ should be a follower of the author of the misinformation tweet $t$, 2) $u_{neg}$ should not retweet $t$, and 3) $u_{neg}$ was active on Twitter within 10 days before and 2 days after the timestamp of $t$.&lt;/p&gt; &lt;p&gt;We have collected a total of 3,811 positive examples and 3,847 negative examples, resulting in a dataset comprising 7,658 user-post pairs in total. We divide the dataset into three subsets with an 80% - 10% - 10% split for train, validation, and test purposes, respectively. The detailed statistics of the collected data are illustrated in the table below.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;Total&lt;/th&gt; &lt;th&gt;Positive&lt;/th&gt; &lt;th&gt;Negative&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;# Example&lt;/td&gt; &lt;td&gt;7658&lt;/td&gt; &lt;td&gt;3811&lt;/td&gt; &lt;td&gt;3847&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;# User&lt;/td&gt; &lt;td&gt;6908&lt;/td&gt; &lt;td&gt;3669&lt;/td&gt; &lt;td&gt;3255&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;# Misinfo tweet&lt;/td&gt; &lt;td&gt;1271&lt;/td&gt; &lt;td&gt;787&lt;/td&gt; &lt;td&gt;1028&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt; &lt;p&gt;In this section, we demonstrate the effectiveness of our susceptibility modeling by directly comparing our estimations with human judgment and indirectly evaluating its performance for predicting sharing behavior.&lt;/p&gt; &lt;h3 id=&quot;validation-with-human-judgement&quot;&gt;Validation with Human Judgement&lt;/h3&gt; &lt;p&gt;Due to the abstract nature of susceptibility and the lack of concrete ground truth, we face challenges in directly evaluating our susceptibility modeling. We use human evaluations to validate the effectiveness of our inferred susceptibility. Given the subjectivity inherent in the concept of susceptibility, and to mitigate potential issues arising from variations in individual evaluation scales, we opt not to request humans to annotate a user’s susceptibility directly. Instead, we structure the human evaluation as presenting human evaluators with pairs of users along with their historical tweets and requesting them to determine which user appears more susceptible to overall COVID-19 misinformation.&lt;/p&gt; &lt;p&gt;Subsequently, we compared the predictions made by our model with the human-annotated predictions. To obtain predictions from our model, we compute each user’s susceptibility to overall COVID-19 misinformation by averaging their susceptibility scores to each COVID-19 misinformation tweet in our dataset. As presented in the table below, our model achieves an average agreement of 73.06% with human predictions, indicating a solid alignment with the annotations provided by human evaluators. Additionally, we consider a baseline that directly calculates susceptibility scores as the cosine similarity between the user and misinformation tweet embeddings. Compared to this baseline, our susceptibility modeling brings a 10.06% improvement. Moreover, we compare the performance with ChatGPT prompting with the task description of the susceptibility level comparison setting as instruction in a zero-shot manner. We observe that our model also outperforms predictions made by ChatGPT. The results from the human judgment validate the effectiveness of our susceptibility modeling and its capability to reliably assess user susceptibility to COVID-19 misinformation.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;Our&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;ChatGPT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Agreement&lt;/td&gt; &lt;td&gt;73.06±8.19&lt;/td&gt; &lt;td&gt;63.00±9.07&lt;/td&gt; &lt;td&gt;64.85±9.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&quot;susceptibility-score-distribution&quot;&gt;Susceptibility Score Distribution&lt;/h3&gt; &lt;p&gt;We provide a visualization of the distribution of susceptibility scores within positive and negative examples produced by our model on the training data. As depicted below, there is a notable disparity in the distribution between positive and negative examples, verifying our assumption that believing is an essential driver for sharing behavior. The difference in the means of the positive and negative groups is statistically significant, with a p-value of less than 0.001.&lt;/p&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/pos_neg_distribution-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/pos_neg_distribution-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/pos_neg_distribution-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-suscep/pos_neg_distribution.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; data-zoomable=&quot;&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figcaption&gt;&lt;strong&gt;Susceptibility Score Distribution&lt;/strong&gt; among positive and negative user-tweet samples. The distribution of positive (red) and negative (blue) examples exhibits a clear disparity.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h3 id=&quot;sharing-behavior-prediction&quot;&gt;Sharing Behavior Prediction&lt;/h3&gt; &lt;p&gt;Furthermore, holding a belief is highly likely to result in subsequent sharing behavior. We demonstrated that our trained model possesses a strong ability for sharing behavior prediction. When tested on the held-out test dataset, our model achieves a test accuracy of 78.11% and an F1 score of 77.93. These results indirectly demonstrate the reliable performance of our model for susceptibility modeling.&lt;/p&gt; &lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt; &lt;p&gt;In this section, we show the potential of our inferred susceptibility scores in expanding the scope of susceptibility research. Our analysis not only aligns with the findings of previous survey-based studies but also goes a step further by extending and enriching their conclusions.&lt;/p&gt; &lt;h3 id=&quot;correlation-with-psychological-factors&quot;&gt;Correlation with Psychological Factors&lt;/h3&gt; &lt;p&gt;Previous research on human susceptibility to health and COVID-19 misinformation has been primarily based on questionnaire surveys &lt;d-cite key=&quot;nan2022people&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;van2022misinformation&quot;&gt;&lt;/d-cite&gt;. These studies have identified several psychological factors that influence individuals’ susceptibility to misinformation. For instance, analytical thinking (as opposed to intuitive thinking), trust in science, and positive emotions have been linked to a greater resistance to health misinformation. Conversely, susceptibility to health misinformation is associated with factors such as conspiracy thinking, religiosity, conservative ideology, and negative emotions. In this part, we analyze the correlation coefficients between our modeled susceptibility scores and the aforementioned factors to determine if our results align with previous research findings.&lt;/p&gt; &lt;p&gt;To achieve this, we compute factor scores for each user in our dataset based on their historical tweets using LIWC Analysis. We calculate the average value across all the user’s historical tweets as the final factor score. However, for emotional factors such as anxiety and anger with less frequent appearance, we opt for the maximum value instead to more effectively capture these emotions. We primarily consider the following factors: &lt;em&gt;Analytic Thinking&lt;/em&gt;, Emotions (&lt;em&gt;Positive&lt;/em&gt; emotions, &lt;em&gt;Anxious&lt;/em&gt;, &lt;em&gt;Angery&lt;/em&gt; and &lt;em&gt;Sad&lt;/em&gt;), &lt;em&gt;Swear&lt;/em&gt;, &lt;em&gt;Political Leaning&lt;/em&gt;, &lt;em&gt;Ethnicity&lt;/em&gt;, &lt;em&gt;Technology&lt;/em&gt;, &lt;em&gt;Religiosity&lt;/em&gt;, &lt;em&gt;Illness&lt;/em&gt; and &lt;em&gt;Wellness&lt;/em&gt;. These factors have been extensively studied in previous works and can be inferred from a user’s historical tweets. We calculate and plot the Pearson correlation coefficients between each factor and the susceptibility predicted by our model in the following table.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Factors&lt;/th&gt; &lt;th&gt;Coeff.&lt;/th&gt; &lt;th&gt;Factors&lt;/th&gt; &lt;th&gt;Coeff.&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Analytic Thinking&lt;/td&gt; &lt;td&gt;-0.31&lt;/td&gt; &lt;td&gt;Emotion - Positive&lt;/td&gt; &lt;td&gt;-0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Political Leaning&lt;/td&gt; &lt;td&gt;0.13&lt;/td&gt; &lt;td&gt;Emotion - Anxious&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ethnicity&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;td&gt;Emotion - Angry&lt;/td&gt; &lt;td&gt;0.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Religiosity&lt;/td&gt; &lt;td&gt;0.10&lt;/td&gt; &lt;td&gt;Emotion - Sad&lt;/td&gt; &lt;td&gt;0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Technology&lt;/td&gt; &lt;td&gt;-0.09&lt;/td&gt; &lt;td&gt;Swear&lt;/td&gt; &lt;td&gt;0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Illness&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;td&gt;Wellness&lt;/td&gt; &lt;td&gt;-0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;According to our analysis, correlations are consistent with previous social science studies based on surveys on health susceptibility. For instance, &lt;em&gt;Analytic Thinking&lt;/em&gt; is a strong indicator of low susceptibility, with a correlation coefficient of -0.31. Conversely, certain features such as &lt;em&gt;Swear&lt;/em&gt;, &lt;em&gt;Political Leaning&lt;/em&gt; and &lt;em&gt;Angry&lt;/em&gt; exhibit a weak correlation with a high susceptibility score. These results not only corroborate the conclusions drawn from previous survey-based studies &lt;d-cite key=&quot;van2022misinformation&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;nan2022people&quot;&gt;&lt;/d-cite&gt; but also provide further validation for the effectiveness of our computational modeling for susceptibility.&lt;/p&gt; &lt;h3 id=&quot;geographical-community-differences&quot;&gt;Geographical Community Differences&lt;/h3&gt; &lt;p&gt;We delve into the geographical distribution of susceptibility. Given the significant imbalance in the number of users from different U.S. states, we calculate the average susceptibility scores for each state using Bayesian smoothing. We use the overall mean susceptibility score and overall standard deviation as our priors and the more the users in the group, the less the overall mean will affect the group’s score.&lt;/p&gt; &lt;p&gt;We explore the susceptibility distribution among different U.S. states, considering the influence of political ideology associated with different states &lt;d-cite key=&quot;gelman2009red&quot;&gt;&lt;/d-cite&gt;. Out of the 100,000 users sampled from around the world, 25,653 users are from U.S. states with more than 200 users for each state. As illustrated in the figure below, the susceptibility distribution across U.S. states is imbalanced and exhibits a certain degree of correlation with political leanings, where generally, states known to have a more conservative population tend to have relatively higher susceptibility scores, while states that are considered more liberal have lower scores. Specifically, the average susceptibility score for users in blue or red states is -3.66 and -2.82 respectively. Red or blue states refer to US states whose voters vote predominantly for the Republican or Democratic Party. We determine blue/red states according to the 2020 presidential election result. We observe that 60% of the ten states with the highest susceptibility scores are red states, and 90% of the ten states with the lowest susceptibility scores are blue states. This is a trend that has been observed in various research where political ideology influences the perception of scientific information &lt;d-cite key=&quot;mccright2013influence&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;baptista2021influence&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;imhoff2022conspiracy&quot;&gt;&lt;/d-cite&gt;. However, it is crucial to acknowledge the limitations of our analysis, as it solely reflects the susceptibility distribution of the sampled users within each state.&lt;/p&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/usa-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/usa-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-suscep/usa-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-suscep/usa.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; data-zoomable=&quot;&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figcaption&gt;&lt;strong&gt;Susceptibility Distribution by U.S. State&lt;/strong&gt; (with bayesian smoothing). We use the average susceptibility score in the United States (-2.87) as the threshold, with scores above it displayed in red, and those below it in blue. Due to space constraints and insufficient data points, we are only displaying data for 48 contiguous states within the U.S.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;h3 id=&quot;measure-of-susceptibility&quot;&gt;Measure of Susceptibility&lt;/h3&gt; &lt;p&gt;The common practice in measuring susceptibility involves collecting self-reported data on agreement or disagreement with verified false claims&lt;d-cite key=&quot;Roozenbeek2020SusceptibilityMisinformationCOVID19&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;Escola-Gascon2021CriticalThinkingPredicts&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;Rosenzweig2021HappinessSurpriseAre&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;nan2022people&quot;&gt;&lt;/d-cite&gt;. Some studies assess susceptibility indirectly through its impact on behavior, but this approach fails to capture actual belief systems&lt;d-cite key=&quot;Loomba2021MeasuringImpactCOVID19&quot;&gt;&lt;/d-cite&gt;. Our work proposes a computational model as a scalable alternative to expensive and limited self-reported beliefs.&lt;/p&gt; &lt;h3 id=&quot;contributing-factors-and-application-of-susceptibility&quot;&gt;Contributing Factors and Application of Susceptibility&lt;/h3&gt; &lt;p&gt;Research utilizing manually collected susceptibility annotations has explored various factors influencing susceptibility, such as emotion&lt;d-cite key=&quot;Sharma2023SystematicReviewRelationship&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;Weeks2015EmotionsPartisanshipMisperceptions&quot;&gt;&lt;/d-cite&gt;, analytic thinking&lt;d-cite key=&quot;Li2022EmotionAnalyticThinking&quot;&gt;&lt;/d-cite&gt;, partisan bias&lt;d-cite key=&quot;Roozenbeek2022SusceptibilityMisinformationConsistent&quot;&gt;&lt;/d-cite&gt;, source credibility&lt;d-cite key=&quot;Traberg2022BirdsFeatherAre&quot;&gt;&lt;/d-cite&gt;, and repetition&lt;d-cite key=&quot;Foster2012RepetitionNotNumber&quot;&gt;&lt;/d-cite&gt;. Theories explaining susceptibility range from limited knowledge acquisition&lt;d-cite key=&quot;Brashier2020AgingEraFake&quot;&gt;&lt;/d-cite&gt; to overconfidence&lt;d-cite key=&quot;Salovich2021CanConfidenceHelp&quot;&gt;&lt;/d-cite&gt;. This understanding aids in applications like analyzing bot-driven misinformation spread&lt;d-cite key=&quot;Himelein-Wachowiak2021BotsMisinformationSpread&quot;&gt;&lt;/d-cite&gt; and developing prebunking interventions&lt;d-cite key=&quot;Roozenbeek2020PrebunkingInterventionsBased&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;Roozenbeek2022PsychologicalInoculationImproves&quot;&gt;&lt;/d-cite&gt;. However, the field lacks a computational model for large-scale susceptibility inference, which we address in our work.&lt;/p&gt; &lt;h3 id=&quot;inferring-unobservables-from-observables&quot;&gt;Inferring Unobservables from Observables&lt;/h3&gt; &lt;p&gt;Latent constructs, or variables that are not directly observable, are often inferred through models from observable variables&lt;d-cite key=&quot;Bollen2002LatentVariablesPsychology&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;Borsboom2003TheoreticalStatusLatent&quot;&gt;&lt;/d-cite&gt;. Methods like nonlinear mixed-effects models and hidden Markov models are used for this purpose. In our approach, we utilize a neural network-based architecture to represent these latent variables, aiding in predicting observable variables.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this work, we propose a computational approach to model people’s &lt;strong&gt;unobservable&lt;/strong&gt; susceptibility to misinformation. While previous research on susceptibility is heavily based on self-reported beliefs collected from questionnaire-based surveys, our model trained in a multi-task manner can approximate user’s susceptibility scores from their reposting behavior. When compared with human judgment, our model shows highly aligned predictions on a susceptibility comparison evaluation task. To demonstrate the potential of our computational model in extending the scope of previous misinformation-related studies, we leverage susceptibility scores generated by our model to analyze factors contributing to misinformation susceptibility. This thorough analysis encompasses a diverse U.S. population from various professional and geographical backgrounds. The results of our analysis algin, corroborate, and expand upon the conclusions drawn from previous survey-based computational social science studies.&lt;/p&gt; &lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt; &lt;p&gt;Besides investigating the underlying mechanism of misinformation propagation at a large scale, the susceptibility scores produced by our model have the potential to be used to visualize and interpret individual and community vulnerability in information propagation paths, identify users with high risks of believing in false claims and take preventative measures, and use as predictors for other human behavior such as following and sharing. However, while our research represents a significant step in modeling susceptibility to misinformation, several limitations should be acknowledged.&lt;/p&gt; &lt;p&gt;First, our model provides insights into susceptibility based on the available data and the features we have incorporated. However, it’s important to recognize that various other factors, both individual and contextual, may influence susceptibility to misinformation. These factors, such as personal experiences and offline social interactions, have not been comprehensively incorporated into our model and should be considered in future research.&lt;/p&gt; &lt;p&gt;Moreover, the susceptibility scores modeled by our model represent an estimation of an individual’s likelihood to engage with misinformation. These scores may not always align perfectly with real-world susceptibility levels. Actual susceptibility is a complex interplay of cognitive, psychological, and social factors that cannot be entirely captured through computational modeling. Our model should be seen as a valuable tool for understanding trends and patterns rather than providing definitive individual susceptibility assessments.&lt;/p&gt; &lt;p&gt;Finally, our study’s findings are based on a specific dataset and may not be fully generalizable to all populations, platforms, or types of misinformation. For example, due to the high cost of data collection, not all countries or U.S. states have a sufficient amount of Twitter data available for analysis, especially when we examine the geographical distribution of susceptibility. Furthermore, platform-specific differences and variations in the types of misinformation can potentially impact the effectiveness of our model and the interpretation of susceptibility scores.&lt;/p&gt; </content> </entry> <entry> <title>Examining assumptions in scRNA-seq foundation model pre-training (6.S898 Final Project)</title> <link href="https://deep-learning-mit.github.io/blog/2023/scRNAseq-assumptions/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/scRNAseq-assumptions</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;If the fundamental building block of biology is the cell, then the fundamental building block of cells are genes. Genes are small segments of DNA that encode the information to create a protein, and proteins are a diverse set of macromolecules that can perform a staggering range of chemical functions which, when taken all together, lead to the complex behavior of cells and the organisms they make up. To create proteins from genes, an intermediate “data transfer” occurs through another molecule type known as RNA. This information flow of genes to RNA to proteins is typically referred to as “gene expression”, and is so core to biology that it’s also known as the “central dogma of molecular biology”.&lt;/p&gt; &lt;p&gt;Due to the importance of gene expression, many technologies have been developed to make quantitative measurements of gene expression from cells. One of the most prominent technologies is called single-cell RNA sequencing (scRNA-seq), which enables the measurement of the expression of all genes in a given cell, often measured across thousands of cells simultaneously &lt;d-cite key=&quot;hwangSinglecellRNASequencing2018&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig1_scRNA_seq_overview-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig1_scRNA_seq_overview-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig1_scRNA_seq_overview-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig1_scRNA_seq_overview.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Schematic overview of the scRNA-seq workflow. Figure sourced from &lt;d-cite key=&quot;panMicrofluidicsFacilitatesDevelopment2022&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;p&gt;Large scale scRNA-seq datasets have enabled the high-resolution profiling of different organs and tissues at the cellular level, uncovering diverse cell types, rare subpopulations, and dynamic gene expression patterns within complex tissues and organisms. This technology has found applications in various fields, from developmental biology and immunology to cancer research and regenerative medicine.&lt;/p&gt; &lt;p&gt;While scRNA-seq has seen broad-scale adoption, many challenges remain. In particular, an individual research experiment may focus on a particular cell or tissue type, and produce insufficient data to apply modern machine learning techniques. To supplement their data or to gain additional context, a researcher may wish to utilize data generated from other experiments or researchers. However, performing large-scale integration of datasets across samples, tissues, and experiments currently presents challenges of scalability and non-biological differences between datasets driven by experimental variability (colloquially referred to as “batch effects”) &lt;d-cite key=&quot;lahnemannElevenGrandChallenges2020&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In parallel to the explosion of available scRNA-seq data, the machine learning field has seen an increasing trend towards “foundation models”. Foundation models are large-scale deep learning models pre-trained with vast amounts of data for the purposes of creating a generalizable representation of a particular datatype (e.g. text, images). Given these developments, recent work has focused on developing scRNA-seq foundation models as an approach to solve the challenge of integrating diverse sets of scRNA-seq datasets in a scalable and generalizable way &lt;d-cite key=&quot;theodorisTransferLearningEnables2023&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;yangScBERTLargescalePretrained2022&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;cuiScGPTBuildingFoundation2023&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;chenGeneptSimpleHardtoBeat2023&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;yangGeneCompassDecipheringUniversal2023&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;haoLargeScaleFoundation2023&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;levineCell2SentenceTeachingLarge2023&quot;&gt;&lt;/d-cite&gt;. Beyond just integration, foundation models of gene expression hold great promise in contributing to a broader understanding of biology by learning a representation space of cellular state, which could also lead to a large impact in downstream applications such as &lt;em&gt;in silico&lt;/em&gt; prediction of cellular responses to novel therapeutics.&lt;/p&gt; &lt;p&gt;In this post, we’ll explore a fundamental assumption of three such models (Geneformer&lt;d-cite key=&quot;theodorisTransferLearningEnables2023&quot;&gt;&lt;/d-cite&gt;, cell2sentence &lt;d-cite key=&quot;levineCell2SentenceTeachingLarge2023&quot;&gt;&lt;/d-cite&gt;, and GenePT &lt;d-cite key=&quot;chenGeneptSimpleHardtoBeat2023&quot;&gt;&lt;/d-cite&gt;), which is the assertion that a given gene expression profile can be well-approximated by a rank-value encoding of genes.&lt;/p&gt; &lt;p&gt;What exactly is a rank-value encoding? Well, a typical representation of gene expression is a vector \(x \in \mathbb{R}^N\), where \(N\) is the number of genes, and each entry is a measure of the corresponding gene’s expression. In a rank-value encoding, gene expression is instead represented as a list of N strings, where the strings are gene names, and are ordered in descending order of the underlying gene expression value.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_rank_value_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_rank_value_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_rank_value_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_rank_value_schematic.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Standard encoding of gene expression values compared to a rank-value encoding. &lt;/div&gt; &lt;p&gt;The rank-value encoding provides an intuitive transformation of the continuous gene expression values into an English language sentence that is compatible with existing approaches for foundation models in the natural language processing (NLP) field. However, as can be seen above, the rank-value encoding also drops the information of the exact gene expression values. Hopefully by the end of this post, we’ll have gained some intuition for how a rank-value encoding of gene expression could be hindering the development of foundation models for gene expression and see that this does play out in practice for a real scRNA-seq foundation model.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related work&lt;/h2&gt; &lt;h3 id=&quot;overview-of-gene-expression-representations-in-foundation-models&quot;&gt;Overview of gene expression representations in foundation models&lt;/h3&gt; &lt;p&gt;While we won’t go into a full detailed comparison of different methods for constructing gene expression foundation models from scRNA-seq data, it’s worth spending a little time discussing the commonalities and differences of various approaches at a high-level.&lt;/p&gt; &lt;p&gt;The most important distinction for this post is between methods that use a rank-value encoding and those that don’t. For methods that don’t use a rank-value encoding, we see a further distinction between methods that employ some form of value-binning, where continuous expression values are mapped to a discrete number of pre-specified bins, and those that don’t. Methods that use a binning approach are scGPT&lt;d-cite key=&quot;cuiScGPTBuildingFoundation2023&quot;&gt;&lt;/d-cite&gt; and scBERT&lt;d-cite key=&quot;yangScBERTLargescalePretrained2022&quot;&gt;&lt;/d-cite&gt;. In both scGPT and scBERT, gene expression values are first binned to map the continuous values to a set vocabulary of tokens, and these tokens are then passed through an embedding layer to generate higher-dimensional representations. In contrast, scFoundation&lt;d-cite key=&quot;haoLargeScaleFoundation2023&quot;&gt;&lt;/d-cite&gt; calculates gene expression embeddings by first transforming continuous scalar values to a vector using a small MLP, and then calculating a final embedding by using an attention mechanism over K learned vectors. While we won’t cover the full details, schematics of the approaches can be seen below to get a sense of the overall architectures, and most importantly to see how they directly use the gene expression values as input.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scGPT_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scGPT_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scGPT_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scGPT_schematic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scBERT_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scBERT_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scBERT_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scBERT_schematic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scFoundation_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scFoundation_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scFoundation_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_scFoundation_schematic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Schematics of the various approaches that *do not* use a rank-value encoding (top to bottom): scGPT, scBERT, and scFoundation. Figures sourced from &lt;d-cite key=&quot;cuiScGPTBuildingFoundation2023&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;yangScBERTLargescalePretrained2022&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;haoLargeScaleFoundation2023&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;p&gt;On the other hand, we have the methods that we’re most interested in for the purposes of this post: the ones that utilize a rank-value encoding of gene expression. These methods are: Geneformer&lt;d-cite key=&quot;theodorisTransferLearningEnables2023&quot;&gt;&lt;/d-cite&gt;, GenePT&lt;d-cite key=&quot;chenGeneptSimpleHardtoBeat2023&quot;&gt;&lt;/d-cite&gt;, and cell2sentence&lt;d-cite key=&quot;levineCell2SentenceTeachingLarge2023&quot;&gt;&lt;/d-cite&gt;. In Geneformer, gene expression values are first converted to a rank-value encoding and then used to train a Transformer-based model using a variant of a masked language modeling objective in which a set of genes at random ranks are masked, and the model must learn to predict the masked gene names. In cell2sentence and GenePT, pre-trained auto-regressive language models (GPT-2 and GPT-3.5 respectively) are applied to the rank-value encoded list of genes to obtain cell-level embeddings that are then used for downstream tasks. Again, we won’t dive into the full details of these approaches, but provide schematic overviews of them below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_Geneformer_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_Geneformer_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_Geneformer_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_Geneformer_schematic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_genePT_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_genePT_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_genePT_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_genePT_schematic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_cell2sentence_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_cell2sentence_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_cell2sentence_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig2_cell2sentence_schematic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Schematics of the various approaches that *do* use a rank-value encoding (top to bottom): Geneformer, GenePT, and cell2sentence. Figures sourced from &lt;d-cite key=&quot;theodorisTransferLearningEnables2023&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;chenGeneptSimpleHardtoBeat2023&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;levineCell2SentenceTeachingLarge2023&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;h3 id=&quot;critical-examinations-of-scrna-seq-foundation-models&quot;&gt;Critical examinations of scRNA-seq foundation models&lt;/h3&gt; &lt;p&gt;In light of the recent development of many approaches for scRNA-seq foundation models, researchers have also begun performing critical assessments of such models. One of the main value propositions of foundation models is generalization to new data in a few-shot or zero-shot manner. To test this hypothesis, Kedzierska et al.&lt;d-cite key=&quot;kedzierskaAssessingLimitsZeroshot&quot;&gt;&lt;/d-cite&gt; benchmarked the performance of Geneformer and scGPT at two zero-shot tasks with novel datasets: cell clustering and integration of data across batches (i.e. batch effect removal) . They found that both methods underperformed compared to simpler baseline methods. Similarly, Boiarsky et al.&lt;d-cite key=&quot;boiarskyDeepDiveSingleCell2023&quot;&gt;&lt;/d-cite&gt; compared scGPT and scBERT to logistic regressions in the context of cell type annotation, and also found that the simpler approach performed competitively.&lt;/p&gt; &lt;p&gt;However, both of the works discussed above focused on examining the performance of scRNA-seq foundation models as a black box, whereas to the best of our knowledge, there are no current works examining the fundamental assumptions implicit in these foundation model approaches. We hope to begin addressing that gap in this post. By understanding whether or not rank-value encoding well-approximates the real similarities and differences in gene expression across cell types, we hope to either validate this assumption or gain insight into future avenues for improving pretraining of such scRNA-seq foundation models.&lt;/p&gt; &lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt; &lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;To perform our assessment of rank-value encoding, we’ll work with the Tabula Sapiens dataset &lt;d-cite key=&quot;consortiumTabulaSapiensMultipleorgan2022&quot;&gt;&lt;/d-cite&gt;. This scRNA-seq dataset is a reference-quality collection of nearly 500,000 cells from 24 organs, sourced from 15 normal human subjects. The Tabula Sapiens dataset provides a good testbed for our experiments, as the samples have been processed in a uniform manner, allowing us to ask how rank-value encoding performs in a “best case” scenario. In the future, it would be beneficial to see how rank-value encoding performs across datasets as well, as there may be advantages in terms of smoothing out experimental noise.&lt;/p&gt; &lt;p&gt;We use the final dataset from Tabula Sapiens, which has already been subjected to quality control assessment, filtering, and normalization. While we won’t go into the details of their pipeline here, these are available in their manuscript for the interested reader. In line with typical scRNA-seq workflows, we also subset the full set of ~22,000 genes down to a subset of 2,435 genes that have been marked as “highly variable genes” (HVGs) in the Tabula Sapiens dataset. This is a fairly standard step in scRNA-seq data processing workflows, as many genes are constitutively expressed across cell types, and thus provide little information for distinguishing between cell types. Highly variable gene selection was performed by the Tabula Sapiens Consortium following the methods and recommendations in Seurat&lt;d-cite key=&quot;stuartComprehensiveIntegrationSingleCell2019&quot;&gt;&lt;/d-cite&gt;, a commonly used scRNA-seq data processing package.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig3_cell_type_hist-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig3_cell_type_hist-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig3_cell_type_hist-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig3_cell_type_hist.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Number of cells per cell type. Note that the majority of cell types have ~1000 examples, but that there&apos;s a long tail of highly represented cell types with up to 35k examples. &lt;/div&gt; &lt;p&gt;Additionally, since the Tabula Sapiens dataset is quite large and also has some cell types that are disproportionately represented, as shown above, we’ll also subset the data to get a more tractable dataset for experimentation. To do so, we’ll focus on cell types with 500 or more examples, and then further randomly subsample to 500 cells per type. This leaves us with 89 cell types&lt;d-footnote&gt;acinar cell of salivary gland, adventitial cell, b cell, basal cell, basal cell of prostate epithelium, basophil, bladder urothelial cell, capillary aerocyte, capillary endothelial cell, cardiac endothelial cell, cardiac muscle cell, cd24 neutrophil, cd4-positive alpha-beta t cell, cd4-positive helper t cell, cd4-positive, alpha-beta memory t cell, cd4-positive, alpha-beta t cell, cd8-positive alpha-beta t cell, cd8-positive, alpha-beta cytokine secreting effector t cell, cd8-positive, alpha-beta cytotoxic t cell, cd8-positive, alpha-beta memory t cell, cd8-positive, alpha-beta t cell, classical monocyte, club cell, club cell of prostate epithelium, conjunctival epithelial cell, corneal epithelial cell, corneal keratocyte, dendritic cell, dn1 thymic pro-t cell, dn3 thymocyte, duct epithelial cell, endothelial cell, endothelial cell of artery, endothelial cell of lymphatic vessel, endothelial cell of vascular tree, enterocyte of epithelium of large intestine, enterocyte of epithelium of small intestine, epithelial cell, erythrocyte, erythroid progenitor, eye photoreceptor cell, fibroblast, fibroblast of breast, granulocyte, hematopoietic stem cell, hepatocyte, immature enterocyte, immune cell, innate lymphoid cell, intermediate monocyte, keratinocyte, kidney epithelial cell, luminal cell of prostate epithelium, luminal epithelial cell of mammary gland, lung ciliated cell, macrophage, mast cell, mature enterocyte, mature nk t cell, memory b cell, mesenchymal stem cell, monocyte, myeloid cell, myofibroblast cell, naive b cell, naive regulatory t cell, naive thymus-derived cd4-positive, alpha-beta t cell, naive thymus-derived cd8-positive, alpha-beta t cell, neutrophil, nk cell, nkt cell, non-classical monocyte, pancreatic acinar cell, pancreatic ductal cell, paneth cell of epithelium of large intestine, paneth cell of epithelium of small intestine, pericyte cell, plasma cell, regulatory t cell, respiratory goblet cell, skeletal muscle satellite stem cell, smooth muscle cell, stromal cell, t cell, thymocyte, type i nk t cell, type ii pneumocyte, vascular associated smooth muscle cell, vein endothelial cell&lt;/d-footnote&gt; and 500 cells per type, for a total of 44,500 datapoints.&lt;/p&gt; &lt;p&gt;To interact with this data, we’ll be using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AnnData&lt;/code&gt;&lt;d-cite key=&quot;virshupAnndataAnnotatedData2021&quot;&gt;&lt;/d-cite&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scanpy&lt;/code&gt;&lt;d-cite key=&quot;virshupScverseProjectProvides2023&quot;&gt;&lt;/d-cite&gt; Python packages, which we won’t cover in detail here but flag in case you’re interested in working with such data in the future.&lt;/p&gt; &lt;h3 id=&quot;assessments&quot;&gt;Assessments&lt;/h3&gt; &lt;p&gt;To assess how well a cellular state can be represented using a rank-value encoding of genes, we’ll look at various measures of similarity in the raw gene expression space and the rank-value encoded space, and compare those measures both within cell types and between cell types. We’ll calculate the following measures for all pairs of cells:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Euclidean distance of UMAP-projected gene expression values&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;&gt;Spearman rank correlation coefficient&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Euclidean distance of UMAP-projected Geneformer embeddings&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For each distance measure, we can then generate comparisons at the level of cell types by summarizing via the median of the pairwise distances, either within or between cell types. A schematic of this approach is shown below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_comparison_schematic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_comparison_schematic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_comparison_schematic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_comparison_schematic.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Overview of method for computing distance measures between cells followed by summarization to generate comparisons at the level of cell types. &lt;/div&gt; &lt;h4 id=&quot;umap-of-gene-expression-values&quot;&gt;UMAP of gene expression values&lt;/h4&gt; &lt;p&gt;The idea behind this comparison is to utilize the continuous gene expression vectors, but using UMAP (Uniform Manifold Approximation and Projection&lt;d-cite key=&quot;mcinnesUMAPUniformManifold2020&quot;&gt;&lt;/d-cite&gt;) to approximate the kind of non-linear transformation one might learn using a deep neural network. To calculate these values, we perform UMAP embprojectionedding of the gene expression values using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umap-learn&lt;/code&gt; Python package with defaut settings and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_components=5&lt;/code&gt;. Once we have the per-cell projections, we calculate Euclidean distance between all pairs of cells.&lt;/p&gt; &lt;h4 id=&quot;spearman-rank-correlation-coefficients&quot;&gt;Spearman rank correlation coefficients&lt;/h4&gt; &lt;p&gt;The Spearman rank correlation is a non-parametric measure of correlation between two ranked lists, which we can leverage to obtain a direct comparison of rank-value encoded gene lists. To accomplish this, we first calculate a rank-encoding of each cell’s gene expression, with identical values being assigned a &lt;a href=&quot;https://en.wikipedia.org/wiki/Ranking#Fractional_ranking_(%221_2.5_2.5_4%22_ranking)&quot;&gt;fractional rank equal to the mean of their ordinal ranks&lt;/a&gt;. As the Spearman correlation is defined as the Pearson correlation on the rank-encoded lists, we can then directly calculate the Spearman correlations between all pairs of cells.&lt;/p&gt; &lt;h4 id=&quot;euclidean-distance-of-umap-projected-geneformer-embeddings&quot;&gt;Euclidean distance of UMAP-projected Geneformer embeddings&lt;/h4&gt; &lt;p&gt;To fully assess the effect of rank-value encoding in a deep learning model, we take this one step further by calculating the embeddings of our cells using Geneformer. We generate these embeddings by using their model and code as &lt;a href=&quot;https://huggingface.co/ctheodoris/Geneformer&quot;&gt;hosted on HuggingFace&lt;/a&gt; for tokenization and embedding of our gene expression vectors. For each cell \(i\), we obtain an embedding vector \(x_i \in \mathbb{R}^{256}\). We further project these 256-dimensional vectors down to 5 dimensions using UMAP for consistency with the projections of the raw gene expression values described above, and then calculate Euclidean distance between all pairs of cells. The rationale here is that Euclidean distance between two points may be larger in a 256-dimensional space than a 5-dimensional space due the high dimensionality (i.e. “curse of dimensionality”). However, we do still see similar results when using the full 256-dimensional embedding vectors (see Appendix).&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;rank-value-encodings-preserve-similarity-between-cell-types&quot;&gt;Rank-value encodings preserve similarity between cell types&lt;/h3&gt; &lt;p&gt;The first thing we can see from our results is that rank-value encodings do preserve similarity between cell types in a similar manner as distances generated from raw gene expression values. The figure below is generated by looking at the distributions of distances between pairs of cells from the same type (“within”) or from different cell types (“between”). To provide a comparison at the level of cell types, we plot the median of each distribution rather than individual pairs of cells, i.e. the “within” group contains 89 data points and the “between” group contains \(\frac{89 \times 88}{2}\) data points.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig6_combined_measure_comparison-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig6_combined_measure_comparison-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig6_combined_measure_comparison-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig6_combined_measure_comparison.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Comparison of various similarity measures both within cell types and between cell types. Note that for the Euclidean distances (left and right), lower is more similar, whereas for rank correlation (middle), higher is more similar. &lt;/div&gt; &lt;p&gt;How should we interpret this? What we can observe is that all three measures maintain high similarity for cells from the same type and less similarity for cells from different types. Put another way, rank-value encodings do define a space in which different cell types tend to be distant and cells from the same type tend to be near each other. We can also say that this holds when using both a non-parametric measure of the rank-value encodings (Spearman rank-correlation) and also when using a deep learning model that operates on rank-value encoded gene vectors (Geneformer).&lt;/p&gt; &lt;p&gt;However, we do also see that the difference between the “within” and “between” cell type distances is more pronounced when using a non-linear function on the raw data compared to either of the methods operating on the rank-value encoded gene vectors. This difference will become even more clear as we look at joint distributions of our different measures in the next section.&lt;/p&gt; &lt;h3 id=&quot;raw-gene-expression-values-better-preserve-within-cell-type-similarities&quot;&gt;Raw gene expression values better preserve within cell type similarities&lt;/h3&gt; &lt;p&gt;To gain further insight into how rank-value encodings compare to raw gene expression values, we can look at the joint distributions of our distance measures. Below we see the joint distribution of our raw gene expression-based distances compared to the rank-correlation values, shown as a 2D histogram where each hex is colored according to the number of points that fall within that bin.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig7_raw_umap_vs_rank_corr_within_type-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig7_raw_umap_vs_rank_corr_within_type-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig7_raw_umap_vs_rank_corr_within_type-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig7_raw_umap_vs_rank_corr_within_type.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig8_raw_umap_vs_rank_corr_between_type-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig8_raw_umap_vs_rank_corr_between_type-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig8_raw_umap_vs_rank_corr_between_type-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig8_raw_umap_vs_rank_corr_between_type.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Joint distributions of distances from UMAP of raw gene expression values compared to rank correlations, within cell types (left) and between cell types (right). &lt;/div&gt; &lt;p&gt;We can notice that within cell types, the rank correlation has a fairly wide dynamic range whereas the raw gene expression-based distance seems to show a tighter packing. Between cell types, we can observe that the rank correlations largely clump up closer to zero but do mesh with the larger distances we see with the raw gene expression-based measure.&lt;/p&gt; &lt;p&gt;Given that we see a spreading out of cells within a type using a rank correlation, the natural question becomes whether this holds when we use a deep learning model that can learn a complex non-linear function of the rank encodings. That’s exactly what we look at below where we perform a similar comparison, but swapping out the rank correlation distance measure for the distance measure based on Geneformer embeddings.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig9_raw_umap_vs_geneformer_umap_within_type-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig9_raw_umap_vs_geneformer_umap_within_type-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig9_raw_umap_vs_geneformer_umap_within_type-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig9_raw_umap_vs_geneformer_umap_within_type.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig10_raw_umap_vs_geneformer_umap_between_type-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig10_raw_umap_vs_geneformer_umap_between_type-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig10_raw_umap_vs_geneformer_umap_between_type-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig10_raw_umap_vs_geneformer_umap_between_type.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Joint distributions of distances from UMAP of raw gene expression values compared to distances from UMAP of Geneformer embeddings, within cell types (left) and between cell types (right). &lt;/div&gt; &lt;p&gt;With the Geneformer embeddings derived from the rank-value encodings, we now see that the between cell type distances are better matched to the distances derived from raw gene expression values. However, we still see that Geneformer embeddings are more spread out within cell types compared to the non-linear transform of the raw gene expression values. To better understand why this might be the case, we propose one possible contributing factor in the next section.&lt;/p&gt; &lt;h3 id=&quot;sparsity-of-scrna-seq-data-may-drive-loss-of-information-in-rank-value-encodings&quot;&gt;Sparsity of scRNA-seq data may drive loss of information in rank-value encodings&lt;/h3&gt; &lt;p&gt;A key aspect of scRNA-seq data is its extremely high sparsity. When working with single cells, the amount of available RNA is already quite limited, and then each processing step, such as RNA isolation or sequencing, introduces technical noise and the possibility of “dropout events”, where a gene’s expression is not detected at all. Combined with the inherent stochasticity of gene expression, we’re often left with data where the vast majority of genes have zero detected RNA molecules.&lt;/p&gt; &lt;p&gt;Shown below is a histogram of sparsity per cell in the full Tabula Sapiens dataset as well as in the subset of cells and genes we considered in the analyses above.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_sparsity_full_dataset-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_sparsity_full_dataset-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_sparsity_full_dataset-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig4_sparsity_full_dataset.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig5_sparsity_subset-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig5_sparsity_subset-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig5_sparsity_subset-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig5_sparsity_subset.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Histogram of gene expression sparsity per cell for the full Tabula Sapiens dataset (left) and the subset of 44,500 cells and 2,450 genes we considered in previous analyses. Sparsity here is defined as the fraction of genes with zero observed RNA molecules. &lt;/div&gt; &lt;p&gt;While many methods for processing scRNA-seq data attempt to handle the high sparsity in a principled manner, most of the methods described here simply remove genes with zero observations from consideration. In particular, scGPT, GenePT, and Geneformer all remove genes with zero observations from their inputs, and cell2sentence restricts itself to the 100 genes with the highest expression per cell, effectively removing all genes with zero observations. While sparsity is at least partially driven by stochastic technical factors, there is undoubtedly a biological contribution as well, which may be removed when dropping genes with zero observations. While this issue is not unique to rank-value encoding, we can see that all of the methods we’ve discussed here that use rank-value encoding remove genes with zero observations, likely to circumvent the ambiguity in how one would enforce an ordering on genes that all have zero observations.&lt;/p&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;p&gt;To give a high-level summary, what we’ve seen in this post is that rank-value encodings are an appealing way to transform continuous gene expression vectors into a format that’s directly compatible with the foundation model architectures that have seen great success in natural language processing. However, they also seem to lose some valuable biologlical information of cell types, particularly information concerning similarity of cells within a given type.&lt;/p&gt; &lt;p&gt;While we don’t present a smoking gun for an exact characteristic of this loss of information, we present sparsity as a key challenge in scRNA-seq data, which may be exacerbated when using rank-value encodings. We can also further hypothesize that rank-value encodings may be sensitive to small changes in gene expression values from technical noise, which could cause a shifting of ranks and thus amplify the impact of said noise. Similarly, rank-value encodings lose the absolute quantification of gene expression, and this loss of granularity may impact the model’s ability to capture the cases where subtle differences in gene expression hold biological significance.&lt;/p&gt; &lt;p&gt;From the perspective of downstream use cases, models based on rank-value encodings are also limited in their ability to explore the counterfactuals that may be interesting in cases such as predicting cellular responses to a novel therapeutic. For example, if a drug were known to affect the expression of a single gene, but not to the point where the ranking of this gene shifted, then such a model would be unable to explore the downstream effect of this drug on the expression of other genes.&lt;/p&gt; &lt;p&gt;In terms of limitations, the work presented here is fairly superficial and is constrained both in terms of size of datasets and breadth of methods compared. To perform a more robust comparison in the future, we would like to scale up this analysis to larger datasets, such as the full Tabula Sapiens dataset. We would also like to more directly compare cell type similarities in the embedding spaces of other scRNA-seq foundation models, including those that do and do not utilize rank-value encodings. A great follow-up would be to perform a head-to-head comparison of a model like scBERT to Geneformer on the full Tabula Sapiens dataset.&lt;/p&gt; &lt;p&gt;Additionally, we’ve also yet to explore the angle of robustness across datasets. It’s possible that some of the shortcomings we’ve listed for rank-value encodings may actually be benefits in the context of supppressing technical noise when integrating scRNA-seq datasets across studies, institutions, and experimental techniques. Performing this comparison across datasets would be a valuable follow-up that would help paint a more full picture of the value of rank-value encodings in the context of constructing foundation models for gene expression data.&lt;/p&gt; &lt;p&gt;While we’ve discussed many challenges in constructing foundation-scale models for gene expression data, it’s worth closing this post with an optimistic reflection on the potential value of such models. By training a deep learning model to construct a representation space of cellular state, we stand to create a powerful tool that will help us gain a fundamental understanding of cellular biology and its underlying complex regulatory networks. Ultimately, such tools could help us unravel the genetics of various diseases, paving the way for a new era of disease treatments and precision medicine.&lt;/p&gt; &lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig11_SUP_raw_umap_vs_geneformer_raw_within_type-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig11_SUP_raw_umap_vs_geneformer_raw_within_type-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig11_SUP_raw_umap_vs_geneformer_raw_within_type-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig11_SUP_raw_umap_vs_geneformer_raw_within_type.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig12_SUP_raw_umap_vs_geneformer_raw_between_type-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig12_SUP_raw_umap_vs_geneformer_raw_between_type-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig12_SUP_raw_umap_vs_geneformer_raw_between_type-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-scRNAseq-assumptions/fig12_SUP_raw_umap_vs_geneformer_raw_between_type.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Joint distributions of distances from UMAP of raw gene expression values compared to distances from raw Geneformer embeddings, within cell types (left) and between cell types (right). &lt;/div&gt; </content> </entry> <entry> <title>Increasing Context Length For Transformers</title> <link href="https://deep-learning-mit.github.io/blog/2023/increasing-context-length-for-transformers/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/increasing-context-length-for-transformers</id> <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt; &lt;p&gt;Since its release on November 30, 2022, ChatGPT has assisted users around the world with a variety of document parsing and editing tasks. These tasks often require large input contexts, since the documents and texts passed into ChatGPT’s source model, GPT-3.5, can be several pages long.&lt;/p&gt; &lt;p&gt;Like many other language models, GPT-3.5 is a unidirectional transformer that uses the self-attention mechanism. But while self-attention is an extremely powerful mechanism, it is also expensive in its time and space complexity. Standard self-attention requires $O(n^2)$ operations in terms of the sequence length $n$, since the $QK^T$ term within the attention mechanism calculates and stores the attention of each of the $n$ tokens with $O(n)$ other tokens.&lt;/p&gt; &lt;p&gt;Unfortunately, the $O(n^2)$ complexity makes long input contexts difficult for transformers to handle efficiently. Over the past few years, researchers have been investigating ways of mitigating the $O(n^2)$ factor. This remains an ongoing problem, with several papers released on the topic in 2023 alone.&lt;d-cite key=&quot;xu2023retrieval&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;yang2023longqlora&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;peng2023yarn&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;yang2023revisiting&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;mohtashami2023landmark&quot;&gt;&lt;/d-cite&gt; In this post, we provide an overview of existing strategies for increasing context length for transformers. We also propose and investigate our own efficient self-attention algorithm, which we call Gaussian attention.&lt;/p&gt; &lt;h3 id=&quot;literature-review&quot;&gt;Literature Review&lt;/h3&gt; &lt;p&gt;In the past, large context lengths were handled using a simple partition scheme. Essentially, long inputs can be split into fixed-length chunks, where attention is computed separately for each chunk. Then, for chunk size $b$, a sequence of length $n$ requires only $O\left(\frac{n}{b} \cdot b^2\right) = O(nb)$ time to compute. However, this method has a major drawback in that information cannot be shared across partitioned blocks, leading to the fragmentation problem: the model lacks long-term dependencies and thus runs into cases where it lacks the necessary context to make accurate predictions.&lt;/p&gt; &lt;p&gt;Modern methods for reducing context lengths in transformers generally try to avoid this problem by either introducing ways of sharing context across partitions or reducing self-attention calculation cost by using a simpler approximation. Models that fall into second category may utilize one of many different approximation techniques, such as sparse attention matrices and fixed attention patterns.&lt;d-cite key=&quot;tay2022efficient&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;LIN2022111&quot;&gt;&lt;/d-cite&gt; We present a brief summary of existing efficient transformers.&lt;/p&gt; &lt;h4 id=&quot;sparse-transformer&quot;&gt;Sparse Transformer&lt;/h4&gt; &lt;p&gt;Child et al. proposed a sparse transformer that reduces attention calculation cost from $O(n^2)$ to $O(n\sqrt{n})$.&lt;d-cite key=&quot;child2019generating&quot;&gt;&lt;/d-cite&gt; To achieve this, the sparse transformer uses a combination of strided and local attention patterns.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/child-et-al-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/child-et-al-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/child-et-al-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/child-et-al.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Standard attention matrix (left) vs. strided attention matrix (right). Only attention values for the blue squares are computed. &lt;/div&gt; &lt;p&gt;One attention head processes a local window of size $k$ surrounding the current token $i$, while a second attention processes tokens $j$ such that&lt;/p&gt; \[(i - j) \mod l = 0 \qquad \forall j \leq i,\] &lt;p&gt;where $l$ is a parameter chosen to be close to $\sqrt{n}$. Since only $O(l)$ tokens are attended upon for each token $i$, this results in the $O(n \cdot l) = O(n\sqrt{n})$ runtime. Child et al. showed that the sparse transformer can be applied to a wide range of fields, including image, text, and music, where it can be used to possess audio sequences over 1 million timestamps long.&lt;/p&gt; &lt;h4 id=&quot;longformer&quot;&gt;Longformer&lt;/h4&gt; &lt;p&gt;Longformer&lt;d-cite key=&quot;beltagy2020longformer&quot;&gt;&lt;/d-cite&gt; applies a dilated sliding window to capture local attention patterns and reduce overall attention cost to $O(n\cdot{w})$ for window size $w$. Across successive attention layers, gaps are placed between different elements of the sliding window—thus expanding the receptive field to thousands of tokens even for small dilation factors. In order to generalize to different language modeling tasks, Longformer introduces global tokens that attend upon every other token. These global tokens are analogous to the different input representations used by language models for different tasks; for example, BERT appends a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;CLS&amp;gt;&lt;/code&gt; token to the start of every input in classification tasks. Despite using sparse attention contexts, Longformer was able to outperform state-of-the-art model RoBERTa on several long document benchmarks.&lt;/p&gt; &lt;h4 id=&quot;bigbird&quot;&gt;BigBird&lt;/h4&gt; &lt;p&gt;BigBird&lt;d-cite key=&quot;zaheer2021big&quot;&gt;&lt;/d-cite&gt; combines three different fixed attention patterns to achieve $O(n)$ complexity, being&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Global attention, consisting of tokens that attend upon every other token&lt;/li&gt; &lt;li&gt;Local attention, consisting of a sliding window around each token&lt;/li&gt; &lt;li&gt;Random attention, consisting of randomly-selected tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Using this architecture, BigBird managed to increase maximum transformer context lengths by up to 8x. In the same paper, Zaheer et al. proved that certain sparse transformers are computationally equivalent to transformers with full attention. Theoretically, sparse transformers are capable of solving all tasks that full transformers can solve; this explains why sparse transformers are often a good approximation for full transformers.&lt;/p&gt; &lt;h4 id=&quot;transformerxl&quot;&gt;TransformerXL&lt;/h4&gt; &lt;p&gt;TransformerXL differs from the previously discussed models, as it doesn’t increase self-attention efficiency by sparsifying the attention matrix.&lt;d-cite key=&quot;dai2019transformerxl&quot;&gt;&lt;/d-cite&gt; Instead, it retains the classic partitioning scheme and attempts to overcome the fragmentation problem via a recurrence-based approach. Using the recurrence mechanism, hidden-state sequences are stored and cached so they can be used for additional context when the model processes the next statement. Overall, this architecture allows the network to use historical information to process new information. As a result, it can support longer-range dependencies without leading to context fragmentation. TransformerXL was able to operate on sequences up to 450% longer than those of vanilla transformers, while being up to 1800 times faster; in addition, it achieved SOTA results on the datasets &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text8&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Penn Treebank&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WikiText-103&lt;/code&gt;.&lt;/p&gt; &lt;h4 id=&quot;landmark-tokens&quot;&gt;Landmark Tokens&lt;/h4&gt; &lt;p&gt;More recently, Mohtashami et al. suggested using landmark tokens to determine which tokens should be attended to.&lt;d-cite key=&quot;mohtashami2023landmark&quot;&gt;&lt;/d-cite&gt; Inputs are divided into a series of fixed-length blocks, and each block is associated with a landmark token. In particular, this architecture is designed so that a high attention score on any individual token within a block also leads to a high attention score on the block’s “representative vector”—which is the landmark token itself. Then, during inference, the transformer retrieves the $k$ blocks corresponding to the $k$ highest-valued landmark tokens and attends only upon the tokens in this block. Mohtashami et al. claimed that this architecture can extend the context length of Llama to more than 32k tokens, allowing it to support inputs of the same length as GPT-4.&lt;/p&gt; &lt;h4 id=&quot;visiontransfomer&quot;&gt;VisionTransfomer&lt;/h4&gt; &lt;p&gt;Most of the models discussed above apply specifically to transformers used for language modeling. However, algorithms for reducing attention complexity have been successfully used for other tasks as well. For example, VisionTransformer managed to achieve SOTA performance while limiting the attention context to a 16x16 patch around each pixel.&lt;d-cite key=&quot;dosovitskiy2021image&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;h4 id=&quot;hardware-methods&quot;&gt;Hardware Methods&lt;/h4&gt; &lt;p&gt;Aside from algorithm-based techniques, there have also been attempts to make basic transformer algorithms run faster on existing hardware. Although sparse attention algorithms may have better time complexity, they may not achieve practical speedups due to hardware inefficiencies. In order to achieve practical speedups on transformer training, Dao et al. proposed FlashAttention, an I/O-aware attention algorithm that implements the basic attention computation.&lt;d-cite key=&quot;dao2022flashattention&quot;&gt;&lt;/d-cite&gt; FlashAttention achieves speedups of up to 15% on BERT-Large, showing that efficient transformers do not necessarily need to use approximate attention algorithms.&lt;/p&gt; &lt;h4 id=&quot;other-methods&quot;&gt;Other Methods&lt;/h4&gt; &lt;p&gt;Numerous other algorithms for extending transformer context lengths have been proposed, including retrieval-based methods&lt;d-cite key=&quot;borgeaud2022improving&quot;&gt;&lt;/d-cite&gt;, low-rank approximations&lt;d-cite key=&quot;wang2020linformer&quot;&gt;&lt;/d-cite&gt;, and expectation-based methods&lt;d-cite key=&quot;ren2021combiner&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;methodology&quot;&gt;Methodology&lt;/h3&gt; &lt;p&gt;To see what types of context reduction algorithms are effective, we propose and test our own efficient transformer. We investigate whether transformers using Gaussian-distributed fixed attention patterns can perform as well as standard transformers. For each self-attention layer, we sample a Gaussian random distribution to determine which elements of the attention matrix we should compute. We analyze this approach for the unidirectional language modeling case, where the goal is to predict the next token of a given input sequence.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gauss-attn-diag-3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Examples of Gaussian attention masks with $c=5$ and inputs of length 10. &lt;/div&gt; &lt;p&gt;In language modeling, the most important context for predicting a new token often comes from examining the tokens that immediately precede it. Previous work has taken advantage of this pattern by employing fixed local attention patterns, such as the sliding window pattern used by BigBird. For token $i$, random samples from a truncated Gaussian distribution with mean $i$ and standard deviation $\sigma = \frac{\mu}{2} = \frac{i}{2}$&lt;d-footnote&gt;This means that 0 is two standard deviations from the mean $i$.&lt;/d-footnote&gt; will produce values $j$ close to $i$ with high probability. This implies that we will likely calculate the attention scores for some local region of each token $i$, allowing the model to account for important local context connections.&lt;/p&gt; &lt;p&gt;On the other hand, it may also be possible that some distant token $j$ has a large impact on the prediction of token $i$. For example, if you pass in a document in which the first sentence defines the overall purpose of the document, we might need to pay attention to this sentence even in later sections of the document. Fixed-pattern Gaussian attention allows for this possibility by calculating attention scores for $i$ and distant tokens $j$ with a lower but still nonzero probability. As a result, Gaussian attention offers some flexibility that may not be present in other fixed-pattern attention mechanisms, such as the sliding window technique.&lt;/p&gt; &lt;h4 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h4&gt; &lt;p&gt;The model takes a hyperparameter $c$, where $c$ is the number of tokens that each token attends upon. For every token $i$ in each self-attention layer, we select $c$ tokens from the Gaussian distribution $\mathcal{N}(i, i/2)$, where $\mathcal{N}$ is truncated at $0$ and $i$. Since our task focuses on the casual language modeling case, a token $i$ computes attention scores only for tokens $j&amp;lt;i$. Truncation ensures that every $i$ attends to exactly $\min(c, i)$ tokens.&lt;d-footnote&gt;If $c$ is greater than the number of tokens in range $[0,i]$, the result is to sample every taken from $[0,i]$.&lt;/d-footnote&gt; To match sampled random numbers with actual token indexes, we cast each random number $x$ to index $i = \lfloor{x}\rfloor$. In the case of duplicate indexes, we assign each duplicate index to the nearest unused index in range $[0,i]$. This algorithm is summarized below.&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for each token i: sample min(c, i) values from N(i, i/2) create list of indices by flooring every sampled value remove duplicates assigning duplicates to the nearest unused index # such an assigment always exists by pigeonhole principle &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;For each token $i$, we set all attention values for tokens which are not selected to zero. As a result, each token attends only on at most $c$ tokens, leading to an overall cost of $O(c \cdot n) = O(n)$ for constant $c$.&lt;/p&gt; &lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt; &lt;p&gt;Since we had limited training resources, we unfortunately couldn’t test Gaussian attention on large models like BERT or GPT. Instead, we used a toy study involving small models with smaller inputs—this leads to some additional considerations in analyzing our results, which we address later.&lt;/p&gt; &lt;p&gt;We first tested whether models trained with limited Gaussian attention can achieve similar performance as models that were trained on full self-attention. We trained models with $c = 5$ and $c=10$ and compared them to the performance of the base model. For our base experiments, we used three self-attention heads per layer and six layers in total.&lt;/p&gt; &lt;p&gt;Our evaluation metric for all models was next-token cross-entropy loss against a corpus of Shakespeare texts.Training is optimized with Adam and a learning rate of 0.0001.&lt;/p&gt; &lt;p&gt;Base experiment results are shown below.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Epochs&lt;/th&gt; &lt;th&gt;Training Loss&lt;/th&gt; &lt;th&gt;Validation Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;4.2623&lt;/td&gt; &lt;td&gt;4.4390&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base&lt;/td&gt; &lt;td&gt;130&lt;/td&gt; &lt;td&gt;3.7709&lt;/td&gt; &lt;td&gt;4.0320&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base&lt;/td&gt; &lt;td&gt;140&lt;/td&gt; &lt;td&gt;3.7281&lt;/td&gt; &lt;td&gt;3.9964&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 5$&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;3.7458&lt;/td&gt; &lt;td&gt;4.0355&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 10$&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;4.1619&lt;/td&gt; &lt;td&gt;4.3801&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/train-vs-val-loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/train-vs-val-loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/train-vs-val-loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/train-vs-val-loss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We found that both the $c=5$ and $c=10$ models were able to achieve similar performance as the base model, which suggests that Gaussian attention may be a good approximation for full attention. Interestingly, both Gaussian models required significantly fewer epochs to reach the same performance as the base model. Both Gaussian models also demonstrated faster separation between training and validation losses. We hypothesize that the smaller attention context helps focus learning on more relevant tokens, which lowers the number of training epochs needed. As a result, the model is able to learn the language modeling task more rapidly, leading to faster overfitting.&lt;/p&gt; &lt;p&gt;Although initial results were promising, we chose to investigate a few factors that could have inflated model performance.&lt;/p&gt; &lt;p&gt;In order to determine whether the Gaussian attention models are affected by input length, we tested the same setups with longer inputs. Our base experiments used relatively small inputs, each corresponding to one piece of dialogue in a Shakespeare script. On average, these inputs were approximately 30 tokens long; with $c = 5$, the selected context may be more than $\frac{1}{6}$ of the total tokens. As a result, Gaussian model accuracy might be inflated for small inputs, since the context essentially covers a large portion of existing tokens. To make $c$ a smaller fraction of the input length, we modified the dataset instead to create inputs with an average length of 100 tokens. We summarize the results in the table below.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Epochs&lt;/th&gt; &lt;th&gt;Training Loss&lt;/th&gt; &lt;th&gt;Validation Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;td&gt;5.5906&lt;/td&gt; &lt;td&gt;5.6207&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 5$&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;td&gt;5.5769&lt;/td&gt; &lt;td&gt;5.6166&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 10$&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;td&gt;5.6237&lt;/td&gt; &lt;td&gt;5.6565&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;With the longer input contexts, all three models had worse performance when trained for the same number of epochs. However, both Gaussian models managed to achieve approximately the same loss as the original model. This again suggests that Gaussian attention is a valid approximation of the standard attention matrix.&lt;/p&gt; &lt;p&gt;We further investigated whether the performance of the Gaussian models degraded rapidly when using a smaller number of layers and attention heads. Logically, increasing the number of attention heads would help mask bad attention patterns formed by the Gaussian sampling strategy. For example, although the sampling process selects tokens $j$ near token $i$ with high probability, it is possible that some attention head $x$ does not select the relevant tokens for a token $i$. With the addition of more attention heads, a different head may compensate for the bad head by operating on the correct tokens. Increasing the number of attention layers similarly increases the number of attention heads, where good heads can compensate for bad ones. Experiments showed that even with one layer and one attention head, the Gaussian models were able to achieve approximately the same performance as the base model.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Input Type&lt;/th&gt; &lt;th&gt;Epochs&lt;/th&gt; &lt;th&gt;# Heads&lt;/th&gt; &lt;th&gt;# Layers&lt;/th&gt; &lt;th&gt;Training Loss&lt;/th&gt; &lt;th&gt;Validation Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base&lt;/td&gt; &lt;td&gt;Short&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;5.1009&lt;/td&gt; &lt;td&gt;5.1605&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base&lt;/td&gt; &lt;td&gt;Long&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.5994&lt;/td&gt; &lt;td&gt;5.6289&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base&lt;/td&gt; &lt;td&gt;Long&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.5906&lt;/td&gt; &lt;td&gt;5.6207&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 5$&lt;/td&gt; &lt;td&gt;Short&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;5.0481&lt;/td&gt; &lt;td&gt;5.1139&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 5$&lt;/td&gt; &lt;td&gt;Long&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.5884&lt;/td&gt; &lt;td&gt;5.6273&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 5$&lt;/td&gt; &lt;td&gt;Long&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.5769&lt;/td&gt; &lt;td&gt;5.6166&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 10$&lt;/td&gt; &lt;td&gt;Short&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;4.5597&lt;/td&gt; &lt;td&gt;4.6949&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 10$&lt;/td&gt; &lt;td&gt;Short&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;4.5432&lt;/td&gt; &lt;td&gt;4.6809&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 10$&lt;/td&gt; &lt;td&gt;Long&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.6345&lt;/td&gt; &lt;td&gt;5.6666&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$c = 10$&lt;/td&gt; &lt;td&gt;Long&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.6237&lt;/td&gt; &lt;td&gt;5.6565&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;However, we noticed that with fewer heads and layers, the base model trained at approximately the same rate as the Gaussian model. A smaller number of attention heads and attention layers implies that fewer parameters need to be updated to learn the task; this typically means that training is faster for smaller models. As a result, it makes sense that a smaller model would benefit less from the increase in training speed that reduced attention context offers; since the model is so small, training is already fast and any decrease in training speed would be minor.&lt;/p&gt; &lt;p&gt;To test the limitations of Gaussian attention, we experimented with extremely sparse attention patterns that selected only one token for each model.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/vert-attn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/vert-attn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/vert-attn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/vert-attn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/diag-attn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/diag-attn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/diag-attn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/diag-attn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gaussian-attn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gaussian-attn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gaussian-attn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-increasing-context-length-for-transformers/gaussian-attn.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Although these models did not perform as well as the base transformer, we found that the token that was attended upon made a significant impact on the final loss. As shown in the table below, the models that employed a diagonal or Gaussian attention pattern performed significantly better than the model that used a vertical attention pattern on the first token. This suggests that local attention patterns were the most important ones for improving the outcome of our task; as a result, Gaussian attention may perform well specifically because it emphasizes the local attention context.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Epochs&lt;/th&gt; &lt;th&gt;# Layers&lt;/th&gt; &lt;th&gt;# Heads&lt;/th&gt; &lt;th&gt;Training Loss&lt;/th&gt; &lt;th&gt;Validation Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Diagonal&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.5089&lt;/td&gt; &lt;td&gt;5.5400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vertical&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.6652&lt;/td&gt; &lt;td&gt;5.6906&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gaussian&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;5.3231&lt;/td&gt; &lt;td&gt;5.3744&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h4 id=&quot;implications-and-limitations&quot;&gt;Implications and Limitations&lt;/h4&gt; &lt;p&gt;Our experiments showed that Gaussian attention has potential as an algorithm for improving transformer efficiency and increasing context lengths. We note that these experiments may not reflect the algorithm’s actual performance in real-world scenarios. Because we did not have the capacity to train a language model on the scale of BERT or GPT, we experimented only with much smaller models that processed much smaller contexts. As a result, our experimental results may not extend to larger models. Additionally, due to limited training time, we did not train any of the models we used for more than 150 epochs; with more training time, it is possible that the base transformers may outperform the modified ones. In order to generalize to larger models, Gaussian attention may need to be combined with other attention patterns, like global attention. More research is needed to fully understand its potential and shortcomings.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Today, methods for increasing context length in transformers remains an important research topic. Although researchers have proposed numerous efficient transformers and self-attention algorithms, a concrete solution for increasing transformer context lengths has yet to be found. With recent developments in large language models, the number of tasks that transformers can be applied to is increasing rapidly. As a result, the search for an efficient transformer is more important than ever.&lt;/p&gt; &lt;p&gt;Our work shows that Gaussian distributions can potentially be used to build fixed-pattern attention masks. However, the performance of Gaussian attention masks in larger models remains to be confirmed and requires further study.&lt;/p&gt; </content> </entry> <entry> <title>Zero-Shot Machine-Generated Image Detection using Sinks of Gradient Flows</title> <link href="https://deep-learning-mit.github.io/blog/2023/detect-image/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/detect-image</id> <content type="html">&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt; &lt;p&gt;Detecting AI-generated content has become increasingly critical as deepfakes become more prevalent. We discover and implement algorithms to distinguish machine-generated and real images without the need for labeled training data. We study the problem of identifying photorealistic images using diffusion models. In comparison to the existing literature, we discover detection techniques that do not require training, based on the intuition that machine-generated images should have higher likelihoods than their neighbors. We consider two metrics: the divergence of the score function around a queried image and the reconstruction error from the reverse diffusion process from little added noise. We also compare these methods to ResNets trained to identify fake images from existing literature. Although the previous methods outperform out methods in terms of our accuracy metrics, the gap between our zero-shot methods and these ResNet methods noticeably declines when different image transformations are applied. We hope that our research will spark further innovation into robust and efficient image detection algorithms.&lt;/p&gt; &lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt; &lt;p&gt;As AI-generated images become ever more widespread, garnering virality for how realistic they have become, we are increasingly concerned with the potential for misuse. A deluge of machine-generated fake images could spread misinformation and harmful content on social media. From relatively innocuous pictures of &lt;a href=&quot;https://www.nytimes.com/2023/04/08/technology/ai-photos-pope-francis.html&quot;&gt;Pope Francis&lt;/a&gt; wearing an AI-generated image puffer coat to dangerous &lt;a href=&quot;https://www.politico.eu/article/ai-photography-machine-learning-technology-disinformation-midjourney-dall-e3-stable-diffusion/&quot;&gt;disinformation campaigns&lt;/a&gt; powered by diffusion models, we live in a new era of media that we cannot trust. The European Union has passed &lt;a href=&quot;https://www.nytimes.com/2023/12/08/technology/eu-ai-act-regulation.html&quot;&gt;legislation&lt;/a&gt; that, among other regulations, requires AI-generated content to be explicitly marked so. The enforcement of such legislation and similar-minded policies, however, remains unclear. Consequently, a growing body of research has sought to develop techniques to distinguish between the real and the synthetic.&lt;/p&gt; &lt;p&gt;The rise of models capable of generating photorealistic content makes the detection problem difficult. While there are still numerous nontrivial challenges with current models from their inability to depict text and render tiny details humans are innately sensitive to such as eyes and hands, the pace of the technology is moving in a way that makes relying on these flaws short-sighted and dangerous. Another potential complication is that advanced photo editing techniques such as &lt;a href=&quot;https://www.adobe.com/products/firefly.html&quot;&gt;Adobe Firefly&lt;/a&gt; have capabilities such as generative inpainting that make it such that an image could contain both real and invented content. Even simple data augmentations like crops, rotations, color jitters, and horizontal flipping can make the input look vastly different to a detection model. Furthermore, the majority of popular image generation tools are text-conditional, and we cannot expect to recover the text prompt, not to mention the model that generated the image. This makes transferable, zero-shot techniques of paramount importance.&lt;/p&gt; &lt;p&gt;In this paper, we propose two techniques for detecting images from diffusion models (see Figure &lt;a href=&quot;#fig-methods-illustrated&quot;&gt;1&lt;/a&gt;). Diffusion models &lt;d-cite key=&quot;sohl2015deep&quot;&gt;&lt;/d-cite&gt; have been one of the most successful architectures for image generation, inspired by thermodynamic principles. Diffusion models learn a score function (gradient of log likelihood) that ‘undoes’ noise from the image. In effect, these models learn a gradient field that points to the real-world data manifold.&lt;d-cite key=&quot;batzolis2022your&quot;&gt;&lt;/d-cite&gt; We leverage the intuition that the greater the deviation the diffusion model’s machine-generated images are from the real world data, the greater the difference of the neighborhood gradient field. In particular, we believe that machine-generated images are more likely to live in a ‘sink’ of the gradient field as the diffusion model ‘flows’ images down the gradient field. We thus propose the &lt;em&gt;divergence of a diffusion model’s score function&lt;/em&gt; as a promising zero-shot statistic for whether an image is generated by the diffusion model.&lt;/p&gt; &lt;p&gt;In addition, another metric for the ‘sink’ property of the gradient field at the image of concern is how far the image moves after a small displacement and flow along the gradient field. This has a nice interpretation in diffusion models as the &lt;em&gt;reconstruction error&lt;/em&gt; for running the reverse process over just a small timestep on just a slightly perturbed image.&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;a name=&quot;fig-methods-illustrated&quot;&gt;Figure 1:&lt;/a&gt; The Divergence and Reconstruction Error Hypothesis: Images on the generated data manifold &lt;span style=&quot;color: red&quot;&gt;(red)&lt;/span&gt; have negative divergence and small reconstruction error, while images on the real data manifold &lt;span style=&quot;color: green&quot;&gt;(green)&lt;/span&gt; have zero divergence and large reconstruction error.&lt;/em&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/methods-illustrated-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/methods-illustrated-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/methods-illustrated-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-detect-image/methods-illustrated.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Our overarching research question is thus summarized as, can we use the properties of a diffusion model’s tacit vector field to build an effective zero-shot machine-generated image detector, specifically looking at &lt;em&gt;divergence&lt;/em&gt; and &lt;em&gt;reconstruction error&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;The main contributions of our paper are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Proposing two methods inspired by sinks of gradient flows: &lt;em&gt;divergence&lt;/em&gt; and &lt;em&gt;reconstruction error&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Conducting a wide battery of experiments on the performance of these methods in a variety of augmentation settings.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;related-work&quot;&gt;Related Work&lt;/h3&gt; &lt;p&gt;Previous literature has considered several different methods for image detection. Sha et al. 2022 &lt;d-cite key=&quot;sha2022fake&quot;&gt;&lt;/d-cite&gt; trained machine learning classifiers to detect fake images using high-level image and text embeddings. They, however, do not consider the local information around image embeddings, and require existing datasets of known image-generated and non-image-generated examples to train their classifier. Corvi et al. 2023 &lt;d-cite key=&quot;corvi2023detection&quot;&gt;&lt;/d-cite&gt; identified “forensic traces” in machine-generated image residuals for this task. Again, their method requires many data samples, and requires separate training on diffusion models and GANs.&lt;/p&gt; &lt;p&gt;We are inspired by ideas from DetectGPT,&lt;d-cite key=&quot;mitchell2023detectgpt&quot;&gt;&lt;/d-cite&gt; a recent work which addressed the same problem of detecting AI-generated content, but in the setting of large language models. For a given piece of text, DetectGPT perturbs the original text and computes the difference in log-likelihood between the perturbed text and the original text:&lt;/p&gt; \[\mathrm{DetectGPT}(x,p_{\theta},q)\triangleq\log p_{\theta}(x)-\mathbb{E}_{\tilde{x}\sim q(\cdot|x)}\log p_{\theta}(\tilde{x})\] &lt;p&gt;where $p_\theta$ is the language model and $q$ is the distribution of perturbations. If the difference in log-likelihood is large, then the attack claims that the original text is more likely to be generated by a language model.&lt;/p&gt; &lt;p&gt;There are several critical differences between language models and diffusion models. With text, one can directly compute the log likelihood of a given piece of text, even with only blackbox access, i.e., no visibility to the model’s parameters. In contrast, for diffusion models, it is intractable to directly compute the probability distribution over images because diffusion models only learn the score. Moreover, the most commonly used diffusion models, e.g. DALL-E 3, apply the diffusion process to a latent embedding space rather than the pixel space. To address the latter concern, we plan on applying the encoder to the image to obtain an approximation of the embedding that was passed into the decoder. And to address the former, instead of approximating the probability curvature around a given point like DetectGPT, we formulate a statistic characterizing whether the gradient field/score is a sink, i.e., the gradients around a machine-generated image point to the machine-generated image. This captures the idea of a local maximum in probability space, similar to the DetectGPT framework.&lt;/p&gt; &lt;p&gt;It would be remiss to not mention Zhang et al. 2023,&lt;d-cite key=&quot;zhang2023watermarks&quot;&gt;&lt;/d-cite&gt; who argued that watermarking, a strictly easier task than machine-generated image detection, is likely impossible. They claim that an adversary who can perturb a generated image of text without too much degradation and has blackbox access to the watermarking scheme can conduct a random-walk on reasonable outputs until the watermark is degraded. However, their analysis was mainly theoretical and lacked specific experiments with diffusion models. It remains to be seen whether their assumptions still hold for image generation, and whether more concrete watermarking schemes may afford some level of protection against less sophisticated adversaries or the unintentional use of machine-generated images.&lt;/p&gt; &lt;h3 id=&quot;methods&quot;&gt;Methods&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Dataset.&lt;/strong&gt; To conduct our research, we needed datasets of known real and fake images. We used MSCOCO &lt;d-cite key=&quot;lin2014microsoft&quot;&gt;&lt;/d-cite&gt;, a dataset of 330K non-machine generated images and captions of common real-world objects which was also used by Corvi et al. 2023.&lt;d-cite key=&quot;corvi2023detection&quot;&gt;&lt;/d-cite&gt; Initially, we planned to use DiffusionDB &lt;d-cite key=&quot;wang2022diffusiondb&quot;&gt;&lt;/d-cite&gt; for our fake images, a dataset of 14M (prompt, image) pairs generated by the open-source Stable Diffusion Version 1 model scraped from the StableDiffusion discord. However, we realized that many of the images in DiffusionDB are not meant to be realistic. Instead, we iterated through the captions of MSCOCO and used Stable Diffusion V1.4 to generate a matching machine-generated image for that caption, as in Corvi et al. 2023.&lt;d-cite key=&quot;corvi2023detection&quot;&gt;&lt;/d-cite&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Baseline.&lt;/strong&gt; We used the model and code from Corvi et al. 2023 &lt;d-cite key=&quot;corvi2023detection&quot;&gt;&lt;/d-cite&gt; to identify images generated by Stable Diffusion as our trained baseline. Their model is a ResNet18 image-only detector trained on the training split of the MSCOCO dataset and images also generated by prompts from StableDiffusion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Detection Algorithms.&lt;/strong&gt; For out attacks, we compute the divergence of the diffusion model’s score field around the image (negative divergence indicates a sink). We can estimate this via a finite-differencing approach: given a diffusion model $s_\theta(x)$ which predicts the score $\nabla_x\log p_\theta(x)$, we have that&lt;/p&gt; \[\mathrm{div}(s_\theta,x)= \sum_{i=1}^d \frac{s_\theta(x+he_i)_i-s_\theta(x-he_i)_i}{2h}\] &lt;p&gt;for small $h$ and orthogonal basis ${e_i}_{i=1}^d$. However, images are high-dimensional, and even their latent space has $\approx10,000$ dimensions, which means that fully computing this sum could be computationally expensive. In this paper, we sample a fraction of the dimensions for each queried image.&lt;/p&gt; &lt;p&gt;Another way to capture the intuition that machine-generated images are have higher likelihoods than their neighbors is by noising the latent to some timestep $t$, and then comparing the distance of the denoised image to the diffusion model to the original image. That is, given a diffusion model $f_\theta$ which takes a noised image and outputs an unnoised image (abstracting away noise schedulers, etc. for clarity),&lt;/p&gt; \[\mathrm{ReconstructionError}(f_{\theta},x)\triangleq \mathbb{E}_{\tilde{x}\sim \mathcal{N}(x,\epsilon)}||x-f_{\theta}(\tilde{x})||_2^2\] &lt;p&gt;for small $\epsilon$. The intuition is that if an image and thus more likely, then the denoising process is more likely to send noisy images to that particular image.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comparison.&lt;/strong&gt; For each model, we use the AUC-ROC curve and the true positive rate (TPR) at low false positive rate (FPR) as metrics. The latter notion of accuracy is borrowed from the membership inference attack setting in Carlini et al. 2021.&lt;d-cite key=&quot;carlinifpr&quot;&gt;&lt;/d-cite&gt; As they argue, this metric quantifies our confidence that a point identified as fake is actually fake. In important settings like filtering fake images on social media platforms, this is especially important as there may be asymmetric consequences for accidentally flagging an image as fake compared to missing a fake image. We also provide a data visualization tool for the images our method identifies. In the real world, we can expect that the images we want to test will be distorted, either by random cropping, reflections, rotations, or compression. We will apply image augmentations over both fake and real image datasets and report the same metrics over these augmentations.&lt;/p&gt; &lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt; &lt;p&gt;We run all experiments over a common set of 500 images from the test set of &lt;a href=&quot;https://huggingface.co/datasets/nlphuji/mscoco_2014_5k_test_image_text_retrieval&quot;&gt;MSCOCO&lt;/a&gt; and the corresponding 500 images generated by Stable Diffusion V1.4 with the same prompt using HuggingFace’s default arguments.&lt;/p&gt; &lt;p&gt;For our Divergence method, we randomly sample $d=10$ dimensions to compute the divergence over and set $h=0.1$. For our Reconstruction method, we compute an average distance over 10 reconstructed images per original image and use add/remove noise equivalent to 1 time-step.&lt;/p&gt; &lt;p&gt;For each method, we evaluate the performance on no augmentation, random $256\times 256$ crop (corresponding to about a quarter of the image for generated images), grayscale, random horizontal flip with probably $0.5$, random rotation between $[-30^\circ,30^\circ]$, and random color jitter of: brightness from $[0.75,1.25]$, contrast from $[0.75,1.25]$, saturation from $[0.75,1.25]$, and hue from $[-0.1,0.1]$.&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;a name=&quot;table-results&quot;&gt;Table 1:&lt;/a&gt; Divergence, Reconstruction, and ResNet Detection AUC and True Positive Rate at 0.1 False Positive Rate.&lt;/em&gt;&lt;/p&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;AUC / TPR$_{0.1}$&lt;/th&gt; &lt;th colspan=&quot;3&quot; style=&quot;text-align: center&quot;&gt;Method&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Augmentation&lt;/th&gt; &lt;th&gt;Divergence&lt;/th&gt; &lt;th&gt;Reconstruction&lt;/th&gt; &lt;th&gt;ResNet&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;No Aug.&lt;/th&gt; &lt;td&gt;0.4535 / 0.078&lt;/td&gt; &lt;td&gt;0.7310 / 0.000&lt;/td&gt; &lt;td&gt;1.000 / 1.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Crop&lt;/th&gt; &lt;td&gt;0.4862 / 0.092&lt;/td&gt; &lt;td&gt;0.4879 / 0.064&lt;/td&gt; &lt;td&gt;1.000 / 1.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Gray.&lt;/th&gt; &lt;td&gt;0.4394 / 0.056&lt;/td&gt; &lt;td&gt;0.7193 / 0.000&lt;/td&gt; &lt;td&gt;1.000 / 1.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;H. Flip&lt;/th&gt; &lt;td&gt;0.4555 / 0.084&lt;/td&gt; &lt;td&gt;0.7305 / 0.000&lt;/td&gt; &lt;td&gt;1.000 / 1.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Rotate&lt;/th&gt; &lt;td&gt;0.4698 / 0.062&lt;/td&gt; &lt;td&gt;0.6937 / 0.000&lt;/td&gt; &lt;td&gt;0.9952 / 0.984&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;Color Jitter&lt;/th&gt; &lt;td&gt;0.4647 / 0.082&lt;/td&gt; &lt;td&gt;0.7219 / 0.000&lt;/td&gt; &lt;td&gt;1.000 / 1.000&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;p&gt;&lt;em&gt;&lt;a name=&quot;fig-roc-auc&quot;&gt;Figure 2:&lt;/a&gt; AUC-ROC Curves in No Augmentation Setting.&lt;/em&gt;&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;p&gt;(a) Divergence&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Divergence_n_points=500_n_samples=10_noise_amount=0.1_num_inference_steps=25_seed=229_bs=1_roc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Divergence_n_points=500_n_samples=10_noise_amount=0.1_num_inference_steps=25_seed=229_bs=1_roc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Divergence_n_points=500_n_samples=10_noise_amount=0.1_num_inference_steps=25_seed=229_bs=1_roc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-detect-image/method=Divergence_n_points=500_n_samples=10_noise_amount=0.1_num_inference_steps=25_seed=229_bs=1_roc.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;p&gt;(b) Reconstruction&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Reconstruction_n_points=500_n_samples=10_noise_amount=1.0_num_inference_steps=25_seed=229_bs=1_roc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Reconstruction_n_points=500_n_samples=10_noise_amount=1.0_num_inference_steps=25_seed=229_bs=1_roc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Reconstruction_n_points=500_n_samples=10_noise_amount=1.0_num_inference_steps=25_seed=229_bs=1_roc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-detect-image/method=Reconstruction_n_points=500_n_samples=10_noise_amount=1.0_num_inference_steps=25_seed=229_bs=1_roc.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;p&gt;(c) ResNet&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Resnet_n_points=1000_seed=229_bs=1_roc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Resnet_n_points=1000_seed=229_bs=1_roc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/method=Resnet_n_points=1000_seed=229_bs=1_roc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-detect-image/method=Resnet_n_points=1000_seed=229_bs=1_roc.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;&lt;a name=&quot;fig-hists&quot;&gt;Figure 3:&lt;/a&gt; Histograms of Computed Statistics in No Augmentation Setting.&lt;/em&gt;&lt;/p&gt; &lt;div class=&quot;l-body&quot;&gt; &lt;p&gt;(a) Divergence&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-detect-image/method=Divergence_n_points=500_n_samples=10_noise_amount=0.1_num_inference_steps=25_seed=229_bs=1.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;l-body&quot;&gt; &lt;p&gt;(b) Reconstruction&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-detect-image/method=Reconstruction_n_points=500_n_samples=10_noise_amount=1.0_num_inference_steps=25_seed=229_bs=1.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;l-body&quot;&gt; &lt;p&gt;(c) ResNet&lt;/p&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-08-detect-image/method=Reconstruction_n_points=500_n_samples=10_noise_amount=1.0_num_inference_steps=25_seed=229_bs=1.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;Trained Baseline.&lt;/strong&gt; The trained baseline does extraordinarily well at the MSCOCO vs. Stable Diffusion detection task. It achieves $1.0$ AUC (perfect accuracy) across all augmentation settings except for rotation for which it gets an almost perfect AUC of $0.9952$. This high performance matches Corvi et al. 2023’s findings,&lt;d-cite key=&quot;corvi2023detection&quot;&gt;&lt;/d-cite&gt; stemming from the fact that the ResNet was trained on the MSCOCO distribution and Latent Diffusion generated images are similar to Stable Diffusion generated images. In their paper, the performance noticeably drops to around $0.7$-$0.8$ AUC for other image generation models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Divergence.&lt;/strong&gt; Divergence does extremely poorly, with AUCs just slightly below 0.5, indicating that in fact generated images have greater divergence than real images—the opposite of our intuition, but this may also be noise as these values are essentially equivalent to random guessing. We suspect that this is largely due to our low choice of $d$, meaning that we cannot get a representative enough sample of the dimensions to get an accurate estimate of the true divergence. We may have also chosen $h$ too large, as we have no idea of the scale of any manifold structure that may be induced by the gradient field.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reconstruction Error.&lt;/strong&gt; Reconstruction error, on the other hand, boasts impressive AUCs of around $0.7$. The shape of the curve is particularly strange, and with the additional observation that the AUC when the random cropping is applied goes back to $0.5$ AUC, indicated to us that the image size may be the differentiating factor here. MSCOCO images are often non-square and smaller than the $512\times 512$ constant size of the generated images. As the Frobenius norm does not scale with image size, we hypothesize that using the spectral norm and dividing by the square root of the dimension would instead give us a more faithful comparison, akin to the random crop results. However, data visualization of the examples does not show a clear correlation between image size and reconstruction error, so it appears that this detection algorithm has decent AUC but poor TPR at low FPR, and is vulnerable to specifically cropping augmentations.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://jsonw0.pythonanywhere.com/&quot;&gt;&lt;strong&gt;Detection Visualizations.&lt;/strong&gt;&lt;/a&gt; We developed a dashboard visualizaiton that enables us to look more closely at images and their associated detection statistics. Some examples we can pick out that seem to make sense include Figure 4, where the real image is captioned as a CGI fake image, and predictably gets a low statistic as deemed by Reconstruction Error (the generated image, ironically, gets a higher statistic denoting more real).&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;a name=&quot;fig-methods-illustrated&quot;&gt;Figure 4:&lt;/a&gt; An Example Image of a CGI “Real” Image Getting Detected as Fake.&lt;/em&gt;&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/cgi-example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/cgi-example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-detect-image/cgi-example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-detect-image/cgi-example.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;However, from a visual inspection of images, we cannot identify a clear relationship between image content or quality of generated images that holds generally. We make our dashboard public and interactive; a demo can be seen below:&lt;/p&gt; &lt;div class=&quot;l-screen&quot;&gt; &lt;iframe src=&quot;http://jsonw0.pythonanywhere.com/&quot; frameborder=&quot;0&quot; scrolling=&quot;yes&quot; height=&quot;1200px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt; &lt;p&gt;Throughout our experiments, the divergence-based detector performs much worse than the other detectors. Because the latent space has a very high dimension, the divergence detector may require sampling from many more dimensions than is practical for an image detector in order to obtain good estimates of the divergence. Further research should try to scale this method to see if it obtains better results. Mitchell 2023 et al. &lt;d-cite key=&quot;mitchell2023detectgpt&quot;&gt;&lt;/d-cite&gt; justifies the validity of their machine-generated as a Hutchinson trace estimator of the divergence of the log probabilities; however, the poor performance of the divergence detector imply that estimating the trace is not helpful for image detection and that other model properties may instead be at play for this method’s effectiveness. In contrast, the noising/denoising detector implicitly incorporates information from all dimensions, which may explain its better performance. The model from Corvi et al. 2023 &lt;d-cite key=&quot;corvi2023detection&quot;&gt;&lt;/d-cite&gt; outperforms our methods under all augmentations, achieving a perfect AUC on images without data augmentations. This is consistent with what was reported in their manuscript. However, this is not an unbiased estimate of the trained classifier’s performance, because they also used MSCOCO data to train and test their classifier. We were limited to this experimental setup by data availability and previous literature. Future work should comapre the zero-shot and trained detectors on completely out-of-sample data and with different generation models.&lt;/p&gt; &lt;p&gt;Although at face-value our detectors perform worse than the pre-trained model in our experiments, our project still introduces some interesting ideas for machine-generated image detection that are of interest to the broader community and worth further exploring. First, the techniques we explored parallel zero-shot machine-generated image detection methods for text.&lt;d-cite key=&quot;mitchell2023detectgpt&quot;&gt;&lt;/d-cite&gt; The fact that in both settings, perturbing the inputs and computing the curvature of the log probabilities are potent signals for machine-generated detection implies that these features may be an indelible mark of machine-generated models across all modalities. Second, image detection algorithms trained on data may be fundamentally vulnerable to adversarial modifications. Because there exists non-robust features that are predictive of the output in training data,&lt;d-cite key=&quot;ilyas2019adversarial&quot;&gt;&lt;/d-cite&gt; adversaries, who realistically may have access to the image detection algorithm over many trials, can craft subtle background noise that circumvents image-detection algorithms. Our methods, which consist of only a few parameters, are not prone to adversarial attacks unlike trained models. Third, this work highlights the use of other features besides the image as features for image detection, e.g. score function and noising/denoising the image. Future work may build on the ideas behind these features to improve trained image detectors.&lt;/p&gt; </content> </entry> <entry> <title>Denoising EMG signals</title> <link href="https://deep-learning-mit.github.io/blog/2023/denoising-EMG-signals/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/denoising-EMG-signals</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Brain-machine interfaces (BCIs) have the potential to revolutionize human-computer interaction by decoding neural signals for real-time control of external devices. However, the current state of BCI technology is constrained by numerous challenges that have limited widespread adoption beyond clinical settings thus far. A critical barrier remains the intrinsically high signal-to-noise ratio (SNR) present in nerve recordings, which introduces substantial noise corruption that masks the relevant neural signals needed for effective device control. To address this persistent issue and work toward unlocking the full capabilities of BCIs for practical real-world application, significant advancements have been actively pursued in both hardware and software components underlying state-of-the-art systems.&lt;/p&gt; &lt;p&gt;Innovations on the hardware front aim to obtain higher-fidelity measurements of neural activity, providing cleaner inputs for software-based decoding algorithms. Novel sensor materials, unconventional device architectures, and integrated on-chip processing have shown promise toward this goal. For example, advancing the conformality and resolution of electrode arrays through new nanomaterials facilitates more targeted recordings with enhanced SNR by achieving closer neuron proximity and tissue integration. Novel satellite electrode configurations have also demonstrated derivations less susceptible to artifacts. While these approaches indicate positive directionality, substantial room remains for developing next-generation hardware able to circumvent the most fundamental limitations imposed by volume conduction effects that dominate nerve signal propagation physics.&lt;/p&gt; &lt;p&gt;Complementing these efforts, software techniques present immense potential to extract meaningful patterns straight from noisy raw recordings. The crux lies in designing sophisticated algorithms that can effectively denoise and transform complex neural data into compact, salient representations to feed downstream interpretable models. Myriad computational methods have been investigated spanning modeling, preprocessing, feature encoding, decoding, and prediction stages across model architectures. For preprocessing, techniques like low/high-pass filters, Fourier transforms, wavelet decompositions, empirical mode decompositions, and various outlier removal methods help restrict signal components to relevant frequency bands and statistics. Building upon these preprocessed signals, advanced unsupervised and self-supervised representation learning algorithms can then produce high-level abstractions as robust inputs for downstream tasks such as gesture classification. The proposed study aims to contribute uniquely to this mission by developing a novel framework tailored for preprocessing surface EMG signals. By focusing on noninvasive surface recordings, findings could generalize toward unlocking adoptable BCIs beyond strict clinical environments. Additionally, optimizing a sophisticated denoising autoencoder architecture reinforced with latest self-attention mechanisms will enrich encoded representations. Overall, this research anticipates advancing signal processing fundamentals underlying next-generation BCI systems positioned to transform human-computer interaction through widely accessible, adaptable platforms.&lt;/p&gt; &lt;h2 id=&quot;literature-review&quot;&gt;Literature Review&lt;/h2&gt; &lt;p&gt;The recent debut of BrainBERT &lt;d-cite key=&quot;BrainBERT&quot;&gt;&lt;/d-cite&gt; in early 2023 signifies a potentially transformative advancement in decoding complex physiological signals by employing an innovative transformer architecture tailored for multivariate neural time series analysis. Initial evaluations demonstrate unmatched state-of-the-art performance in extracting robust, salient features from raw intracranial primate recordings for downstream gesture classification, substantially outperforming prior established methods on proprietary datasets. However, several key limitations currently temper sweeping conclusions regarding real-world viability and require expanded validation. Perhaps most critically, the intracranial modalities utilized pose far too invasive for serious consideration within mainstream wearable interfaces. Additionally, the restricted dataset comprises only five subjects performing six rudimentary gesture classes under tightly controlled conditions unlikely replicable outside laboratories. Finally, while posting strong isolated accuracies on a singular decoding task, analysis remains limited assessing encoding versatility across diverse downstream objectives to qualify expected generalizability.&lt;/p&gt; &lt;p&gt;These nascent outcomes build upon seminal prior art pioneering self-attentive neural network frameworks for combating pervasive noise corruptions. Specifically, the 2021 study “DAEMA: Denoising Autoencoder with Mask Attention” &lt;d-cite key=&quot;DAEMA&quot;&gt;&lt;/d-cite&gt; introduced an innovative architecture leveraging trainable gating of input feature relevance based on missingness patterns. Results demonstrated state-of-the-art reconstruction capabilities from artificially corrupted EEG benchmarks by focusing model representations only on reliable data subsets. This novel direction set the foundation for subsequent efforts translating concepts toward filtering realistic physiological noise toward wearable applications.&lt;/p&gt; &lt;p&gt;Present work seeks to advance signal encoding breakthroughs within this mission by implementing DAEMA-inspired attention components for multi-channel surface EMG data. The overarching motivation is developing an optimized model capable of extracting robust representations from notoriously noisy recordings that sufficiently retain nuanced muscle activation details critical for control inference. By concentrating learned encodings exclusively on clean input sections, downstream analyses from subtle gesture delineation to complex modeling of coordination deficiencies may become tractable where previously obstructed by artifacts. Outcomes would carry profound implications for unlocking ubiquitous responsive assistive technologies.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;p&gt;From a large sEMG dataset &lt;d-cite key=&quot;sEMGdataset&quot;&gt;&lt;/d-cite&gt;, I extracted the pattern recognition (PR) recordings for model development and testing. Specifically, I utilized one second long HD-sEMG recordings sampled at 2048 Hz. These data encompassed 256 electrode channel readings captured while subjects performed 34 distinct hand gestures. In total, twenty subjects completed two sessions each, yielding over one hundred gesture recording samples per person. Due to computational constraints, my preliminary analysis focused solely on the data from the first two subjects for training and validation.&lt;/p&gt; &lt;p&gt;As a preprocessing step before feeding signals into my model, I first converted the 1D raw temporal traces into 2D spectrogram representations via short-time Fourier transforms. This translation from time domain to frequency domain was motivated by observations in the BrainBERT study, where the authors empirically found that presenting spectral depictions enabled superior feature extraction. To further improve conditioning, I normalized the matrix values using the global dataset mean and standard deviation.&lt;/p&gt; &lt;p&gt;My methodology centered on designing and training an autoencoder architecture for denoising tasks. This system was composed of an encoder model to map inputs into a lower-dimensional latent space and a partner decoder model to subsequently reconstruct the original input. Specifically, the encoder segment utilizes a multi-headed self-attention transformer layer to reweight the relative importance of input spectrogram features reflective of signal clarity versus noise. By computing dot products between each time-frequency bin, the attention heads assign higher relevance weighting to bins more strongly correlated with other clear regions of the input. In this way, the model focuses on interconnections indicative of muscle physiology rather than random artifacts. Encoder feed-forward layers subsequently compress this reweighted representation into a low-dimensional latent embedding capturing core aspects. Batch normalization and non-linearities aid training convergence. Critically, this forces the model to encode only the most essential patterns, with excess noise ideally filtered out.&lt;/p&gt; &lt;p&gt;The output then passes through a series of linear layers with ReLU activations to compress the data into a 28-dimensional latent representation (versus 56 originally). Paired with the encoder is an LSTM-based decoder that sequentially generates the full reconstructed spectrogram by capturing temporal dynamics in the latent encoding. Specifically, two bidirectional LSTM blocks first extract forward and backward sequential dependencies. A final linear layer then projects this decoding to match the original spectrogram shape. Since spectrograms consist strictly of positive values, the decoder output is passed through a final ReLU layer as well to enforce non-negativity.&lt;/p&gt; &lt;p&gt;For training, I used a custom loss function defined as a variant of mean absolute error (MAE), with additional multiplicative penalties for overestimations to strongly discourage noise injection. While mean squared error (MSE) is conventionally used for regressors, I found that it failed to properly penalize deviations in the spectrogram context as values were restricted between zero and one. Moreover, initial models tended to overshoot guesses, spectrally spreading energy across additional erroneous frequency bands - a highly undesirable artifact for signal clarity. The custom loss thus applies much harsher penalties for such over-predictions proportional to their deviation magnitude. The purpose of the loss function is to compare the model’s output to the preprocessed signal, not the original raw input. This is what makes a denoising autoencoder different from a traditional autoencoder. So, this autoencoder is being tested on its ability to reconstruct the signal from a compressed latent representation and to ignore the noise removed by manual preprocessing of the signal.&lt;/p&gt; &lt;h2 id=&quot;results-analysis&quot;&gt;Results Analysis&lt;/h2&gt; &lt;p&gt;In terms of model optimization, I incrementally adapted components like attention heads, linear layers in the encoder, LSTM layers in the decoder, and loss function parameters to improve performance. Below are some results collected during experiments, with relevant hyperparameter values listed. For context, the first signal from the validation is visualized below. Note that all experiments were done on the same sample.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image13-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image13-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image13-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image13.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image7-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image7-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image7-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image7.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image10-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image10-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image10-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image10.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image9-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image9-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image9-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image9.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Diagrams are in the following order: raw signal waveform, preprocessed signal waveform, raw spectrogram, preprocessed spectrogram. &lt;/div&gt; &lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt; &lt;p&gt;MSE loss function:&lt;/p&gt; &lt;p&gt;Applying MSE during training resulted in substantial overpredictions within reconstructed spectrograms. As shown, the final time-domain trace from the poorly constrained model contains heavy ambient noise contamination spanning multiple frequency bands – an undesirable artifact significantly corrupting signal clarity and interpretation. This confirms the need to explicitly restrict amplification predictions to retain fidelity.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image18-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image18-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image18-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image18.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image24-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image24-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image24-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image24.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Custom Loss at 1.5:&lt;/p&gt; &lt;p&gt;The custom loss variant with a penalty strength of 1.5 on positive deviations provides initial mitigations toward avoiding false noise injection. As evident for this setting however, while reconstruction quality exhibits cleaner sections, erratic artifacts still visibly persist in certain regions indicating room for improvement. Additionally, certain key activity spikes demonstrate misalignments suggestive of feature misrepresentation issues. This reconstruction is less noisy than the output when using MSE loss, but the large spike between samples 1500 and 1750 is still quite noisy.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image17-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image17-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image17-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image17.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image14-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image14-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image14-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image14.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Custom Loss at 3.0:&lt;/p&gt; &lt;p&gt;In contrast, a high 3.0 penalty parameter induces oversuppressions that completely smooths out nontrivial aspects of the true activations, retaining only the most prominent spike. This signifies that an over constrained optimization pressure to limit noise risks excessively diminishing important signal features. An appropriate balance remains to be found between both extremes.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image6-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image6-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image6-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image6.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;increased-depth&quot;&gt;Increased Depth&lt;/h3&gt; &lt;p&gt;Additional LSTM layers:&lt;/p&gt; &lt;p&gt;Attempting to append supplemental LSTM decoder layers resulted in models failing to learn any meaningful representations, instead outputting blank spectrograms devoid of structure. This occurrence highlights difficulties of vanishing or exploding gradients within recurrent networks. Addressing architectural constraints should be prioritized to add representational power.&lt;/p&gt; &lt;p&gt;Additional Linear Layers:&lt;/p&gt; &lt;p&gt;Increasing the number of linear encoder layers expects to smooth outputs from repeated feature compressions, improving noise resilience at the cost of losing signal details. However, experiments found that excessive linear layers suppressed the majority of outputs indicative of optimization issues - possibly vanishing gradients that diminish propagating relevant structures.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image20-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image20-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image20-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image20.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image8-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image8-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image8-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image8.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Doubled attention heads:&lt;/p&gt; &lt;p&gt;Using eight attention heads over two layers was expected to extract more salient input features thanks to added representational capacities. However, counterintuitively, the resulting reconstructions surfaced only a single prominent activity spike with all other informative structure entirely smoothed out. This suggests difficulties in sufficiently balancing and coordinating the priorities of multiple simultaneous attention modules.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image11-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image11-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image11-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image11.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image16-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image16-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image16-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image16.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;latent-bottleneck&quot;&gt;Latent Bottleneck&lt;/h3&gt; &lt;p&gt;Larger latent space:&lt;/p&gt; &lt;p&gt;Expanding the dimensionality to a 42 dimension latent representation afforded more flexibility in encoding input dynamics. While this retained spike occurrences and positioning, relative amplitudes and relationships were still improperly reflected as evident by distorted magnitudes. This implies that simply allowing more latent capacity without additional structural guidance is insufficient for fully capturing intricate physiological details.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image19-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image19-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image19-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image19.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image15-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image15-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image15-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image15.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;Smaller latent space:&lt;/p&gt; &lt;p&gt;As expected, severely restricting the representational bottleneck to just 14 units forced aggressive data compression that fails to retain more than the most dominant input aspects. Consequently, the decoder could only partially reconstruct the presence of two key spikes without correctly inferring amplitudes or locations. All other fine signal details were entirely lost due to the heavy dimensionality restriction forcibly imposing information loss.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image25-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image25-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image25-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image25.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This research presents foundational outcomes qualifying the early potential of an interpretable attention-based autoencoding framework for intrinsic physiological pattern extraction and noise suppression. Incrementally adapted model configurations, guided by empirical ablation studies, demonstrate promising capabilities in capturing key EMG spectrogram characteristics while filtering errant artifacts.&lt;/p&gt; &lt;p&gt;The final model parameters include: four attention heads, two attention layers, two linear layers in the encoder, 35 dimensional latent space, two LSTM layers in the decoder, and a 2.25 overshooting penalty multiplier for the custom loss function. These parameters were identified to be optimal after running the experiments described above.&lt;/p&gt; &lt;p&gt;Results using same experimental set-up as in previous section:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image27-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image27-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image27-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image27.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image26-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image26-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image26-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-denoising-EMG-signals/image26.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;When compared to the original sample, the output of my model succeeded in reproducing the signal with most of the relevant spikes, as determined by visual inspection. Note that the error is still quite large, but I believe that is an effect of the preprocessed signals still containing quite a lot of noise, since the manual filtering is not very strong. With smaller learning rate and more training iterations, the model’s output will resemble the preprocessed signal more closely.&lt;/p&gt; &lt;p&gt;In addition to further testing the parameters selected above, more work could be done in selecting the values of training hyperparameters like the gradient clipping norm maximum and learning rate scheduler parameters. Both of these methods are supposed to help with gradient stability during training and are most helpful when performing training over hundreds, if not thousands, of iterations.&lt;/p&gt; &lt;p&gt;While this project was able to produce a denoising autoencoder model that could be used to preprocess EMG signal data, another goal was to learn a latent representation that improves performance on downstream tasks. Extending this work by reproducing results from gesture recognition papers using the latent representation of EMG data instead of the normal representation. The latent representation is expected to improve performance because the attention module is expected to highlight all of the relevant parts of the signal and the linear module is expected to use the insights from the preceding attention module to condense the signal to its latent representation. This representation is impressive at removing artifacts from the signal, which means it probably contains the information needed for downstream tasks. Previous deep learning methods &lt;d-cite key=&quot;transformergesture&quot;&gt;&lt;/d-cite&gt; have shown success in recognizing gestures from the EMG signal directly, but this is the first time a denoising autoencoder’s latent representation has been utilized in achieving the same goals.&lt;/p&gt; &lt;p&gt;In short, the findings support the potential of attention boosted autoencoders in overcoming challenges that have hindered widespread adoption of BCI due to noise from instruments. The results highlight the importance of combining new design approaches in deep learning with specific problem-related preferences to accurately capture physiological details.&lt;/p&gt; </content> </entry> <entry> <title>A Deeper Look into Equivariance for Materials Data</title> <link href="https://deep-learning-mit.github.io/blog/2023/A-deeper-look-into-equivariance-for-materials-data/"/> <updated>2023-11-08T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/A-deeper-look-into-equivariance-for-materials-data</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Materials embody a diverse array of chemical and physical properties, intricately shaping their suitability for various applications. The representation of materials as graphs, where atoms serve as nodes and chemical bonds as edges, facilitates a systematic analysis. Graph Neural Networks (GNNs) have emerged as promising tools for deciphering relationships and patterns within materials data. The utilization of GNNs holds the potential to develop computational tools that deepen our understanding and aid in designing structure-property relationships in atomic systems.&lt;/p&gt; &lt;p&gt;In recent years, there has been a heightened focus on employing machine learning for the accelerated discovery of molecules and materials with desired properties [&lt;a href=&quot;#min2020accelerated&quot;&gt;Min and Cho, 2020&lt;/a&gt;; &lt;a href=&quot;@pyzer2022accelerating&quot;&gt;Pyzer-Knapp et al, 2022&lt;/a&gt;; &lt;a href=&quot;@merchant2023scaling&quot;&gt;Merchant et al, 2023&lt;/a&gt;]. Notably, these methods are exclusively applied to stable systems in physical equilibrium, where such systems correspond to local minima of the potential energy surface $E(r_1, . . . , r_n)$, with $r_i$ representing the position of atom $i$ [&lt;a href=&quot;@schutt2018schnet&quot;&gt;Schüttet al, 2018&lt;/a&gt;].&lt;/p&gt; &lt;p&gt;The diverse arrangements of atoms in the system result in varying potential energy values, influencing chemical stability. In the GIF below, different trajectories can be seen of the molecule Ethane. The Ethane molecule spends 99% of its time in a specific conformation, in which the substituents are at the maximum distance from each other. This conformation is called the staggered conformation. Looking at the molecule from a position on the C-C (main) axis (as in the second half of the animation), The staggered conformation is reached when the H atoms of the front C atom are exactly between the H atoms of the other C atom. This animation also show the 3-fold symmetry of the molecule around the main axis. All three staggered conformations will have the same energy value, as they are completely equivalent. The intermediate conformations will result in a higher energy value, as they are energetically less favorable. Different conformations can also portray elongations of some bonds lengths and variations in angles value. Predicting stable arrangements of atomic systems is in itself an important challenge!&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img width=&quot;500&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/7/76/Ethane_conformation.gif&quot; alt=&quot;Conformations of Ethane (C2H6) molecule&quot; /&gt; &lt;/p&gt; &lt;div class=&quot;caption&quot;&gt; Conformations of the molecule Ethane (C2H6) &lt;/div&gt; &lt;p&gt;In the three-dimensional Euclidean space, materials and physical systems in general, inherently exhibit rotation, translation, and inversion symmetries. These operations form the E(3) symmetry group, a group of transformations that preserve the Euclidean distance between any two points in 3D space. When adopting a graph-based approach, a generic GNN may be sensitive to these operations, but an E(3) equivariant GNN excels in handling such complexities. Its inherent capability to grasp rotations, translations, and inversions allows for a more nuanced understanding, enabling the capture of underlying physical symmetries within the material structures [&lt;a href=&quot;@batzner20223&quot;&gt;Batzner et al, 2022&lt;/a&gt;].&lt;/p&gt; &lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt; &lt;p&gt;The MD 17 dataset, an extensive repository of ab-initio molecular dynamics trajectories [&lt;a href=&quot;@chmiela2019sgdml&quot;&gt;Chmiela et al, 2019&lt;/a&gt;], was employed in this study.&lt;/p&gt; &lt;p&gt;Each trajectory within the dataset includes Cartesian positions of atoms (in Angstrom), their atomic numbers, along with total energy (in kcal/mol) and forces (kcal/mol/Angstrom) acting on each atom. The latter two parameters serve as regression targets in analyses.&lt;/p&gt; &lt;p&gt;Our focus narrowed down to the molecules Aspirin, Ethanol, and Toluene:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/aspirin-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/aspirin-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/aspirin-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/aspirin.jpg&quot; class=&quot;img-fluid rounded z-depth-1 mb-3&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Aspirin (C9H8O4)&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/ethanol-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/ethanol-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/ethanol-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/ethanol.jpg&quot; class=&quot;img-fluid rounded z-depth-1 mb-3&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Ethanol (C2H5OH)&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/toluene-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/toluene-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/toluene-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/toluene.jpg&quot; class=&quot;img-fluid rounded z-depth-1 mb-3&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Toluene (C6H5CH3)&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The distributions of energy values (kcal/mol) for various conformations of the three molecules, within the training and validation sets, are illustrated in the histograms below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 1 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_t-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_t-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_t-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_t.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_v-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_v-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_v-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_asp_v.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 2 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_t-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_t-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_t-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_t.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_v-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_v-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_v-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_eth_v.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 3 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_t-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_t-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_t-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_t.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_v-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_v-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_v-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/hist_tol_v.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Energy (kcal/mol) distributions for Aspirin (C9H8O4), Ethanol (C2H5OH) and Toluene (C6H5CH3) molecules in train and validations sets &lt;/div&gt; &lt;p&gt;The training set for Aspirin comprises 1000 conformations, while its validation set consists of 500 conformations. Ethanol’s training and validation sets each consist of 1000 conformations. Toluene’s training set comprises 1000 conformations, and its validation set consists of 500 conformations.&lt;/p&gt; &lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt; &lt;p&gt;In this project, our objective is to conduct a comparative analysis of two Graph Neural Network (GNN) architectures: an E(3) equivariant network and a non-equivariant (specifically E(3) Invariant) one. The primary focus is on energy prediction tasks related to atomic systems, with a particular emphasis on exploring the distinctions within the latent representations of these architectures and their interpretability.&lt;/p&gt; &lt;p&gt;All GNNs are permutation invariant by design [&lt;a href=&quot;@DBLP:journals/corr/abs-1905-04943&quot;&gt;Keriven and Peyr, 2019&lt;/a&gt;]. Our baseline GNN for comparison achieves rotation and translation invariance by simply operating only on interatomic distances instead of absolute position of the atoms. This design choice ensures that both the output and internal features of the network remain invariant to rotations. In contrast, our equivariant GNN for comparison utilizes relative position vectors rather than distances (scalars) together with features comprised of not only scalars, but also higher-order geometric tensors.&lt;/p&gt; &lt;p&gt;In our Invariant GNN, the node-wise formulation of the message passing is given by:&lt;/p&gt; &lt;center&gt; $$\mathbf{x}^{\prime}_i = \mathbf{\Theta}^{\top} \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{e_{j,i}}{\sqrt{\hat{d}_j \hat{d}_i}} \mathbf{x}_j$$&lt;/center&gt; &lt;p&gt;Where $ x_i, x_j $ are the feature vectors of the target and source nodes, respectively, defined as a one-hot representation of the atomic number of that node. The summation is performed over the neighborhood $\mathcal{N}(i)$ of atom $i$, defined by a radial cutoff around each node, a tunable parameter typically set around 4-5 angstroms. Meaning, the concept of neighborhood is based on the distance between nodes, not their connectivity. Additionally, $ d_i = 1 + \sum_{j \in \mathcal{N}(i)} e_{j,i} $ where $ e_{j,i} $ represents the edge weight from the source node $j$ to the target node $i$ , and is defined as the interatomic distance.&lt;/p&gt; &lt;p&gt;For constructing our equivariant GNN, &lt;a href=&quot;https://e3nn.org/&quot;&gt;E3nn&lt;/a&gt; was employed - a torch-based library designed for building o(3) equivariant networks. Following the method presented in [&lt;a href=&quot;@batzner20223&quot;&gt;Batzner et al, 2022&lt;/a&gt;], a neural network that exhibits invariance to translation and equivariance to rotation and inversion was constructed. Two key aspects of E3nn facilitating the construction of O(3) equivariant neural networks are the use of irreducible representations (Irreps) for data structuring and encapsulating geometrical information in Spherical Harmonics. Irreps are data structures that describe how the data behaves under rotation. We can think of them as data types, in the sense that this structure includes the values of the data alongside instructions for interpretation. The Spherical Harmonics form an orthonormal basis set of functions that operate on a sphere, and they’re equivariant with respect to rotations, which makes them very useful (and popular!) in expanding expressions in physical settings with spherical symmetry.&lt;/p&gt; &lt;p&gt;For the equivariant GNN, the node-wise formulation of the message is:&lt;/p&gt; &lt;center&gt; $$f&apos;_i = \frac{1}{\sqrt{z}} \sum_{j \in \partial(i)} \; f_j \; \otimes\!(h(\|x_{ij}\|)) \; Y(x_{ij} / \|x_{ij}\|) $$ &lt;/center&gt; &lt;p&gt;where $ f_j, f_i $ are the target and source nodes feature vectors, defined similarly as a one-hot representation of the atomic number. $z$ is the average degree (number of neighhbors) of the nodes, and the neighborhood $\partial(i)$ is once again defined using a radial cutoff. $x_{ij}$ is the relative distance vector, $h$ is a multi layer perceptron and $Y$ is the spherical harmonics. The expression $x \; \otimes(w) \; y$ denotes a tensor product of $x$ with $y$ using weights $w$. This signifies that the message passing formula involves a convolution over nodes’ feature vectors with filters constrained to be a multiplication of a learned radial function and the spherical harmonics.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;p&gt;The performance of the two GNNs was compared for the task of predicting the total energy of the molecule’s conformation - a scalar property. By constraining the Equivariant GNN to predict a scalar output, it becomes overall invariant to the E(3) group. However, the use of higher order geometric tensors in the intermediate representations and operations in the E-GNN, makes internal features equivariant to rotation and inversion. This enables the passage of angular information through the network using rotationally equivariant filters (spherical harmonics) in the node feature convolution. This is the essential difference between the two architectures.&lt;/p&gt; &lt;p&gt;The learning curves of the two GNNs for each molecule data are presented in the figures below:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 1 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_t_epoch_new-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_t_epoch_new-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_t_epoch_new-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_t_epoch_new.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_v_epoch_new-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_v_epoch_new-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_v_epoch_new-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_asp_v_epoch_new.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 2 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_t_epoch_new-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_t_epoch_new-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_t_epoch_new-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_t_epoch_new.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_v_epoch_new-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_v_epoch_new-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_v_epoch_new-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_ethanol_v_epoch_new.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 3 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_t_epoch_new-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_t_epoch_new-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_t_epoch_new-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_t_epoch_new.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_v_epoch_new-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_v_epoch_new-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_v_epoch_new-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/energy_pred_tol_v_epoch_new.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Train (left) and Validation (right) learning curves of Energy (kcal/mol) prediction of Aspirin (top), Ethanol (middle) and Toluene (bottom) conformations &lt;/div&gt; &lt;p&gt;The models were trained for 50 epochs using mean absolute error (MAE) objective for predicting normalized energy (in kcal/mol units). Adam optimizer with a learning rate of 0.01 and learning rate scheduler were employed. The E-GNN achieves a superior MAE rate for all three molecules.&lt;/p&gt; &lt;p&gt;Next, let’s examine the latent representation of the two models! The last layer values of the validation data of both models were projected using t-SNE to a 2D representation and color-coded according to the target energy values:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 1 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_asp_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_asp_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_asp_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_asp_1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_asp_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_asp_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_asp_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_asp_1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 2 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_eth_new-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_eth_new-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_eth_new-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_eth_new.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 3 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_tol_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_tol_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_tol_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_tol_2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_tol_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_tol_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_tol_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/gnn_lat_tol_1.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Latents projections of E-GNN (left) and GNN (right) last layer, of Aspirin (top), Ethanol (middle) and Toluene (bottom) conformations &lt;/div&gt; &lt;p&gt;A color gradient can be seen in all three projections of the Equivariant GNN; and it is the clearest for Ethanol. The Invariant GNN’s latent projections do not exhibit a similar structure, perhaps except for Ethanol’s conformations. Moreover, in Ethanol’s case, the GNN projection appears to be quite one-dimensional.&lt;/p&gt; &lt;p&gt;The apparent color gradient according to the target values in the E-GNN latent space is impressive, suggesting that the model leverages this information when embedding data conformations for predictions. Multiple “locations” in the latent space denote various high-energy conformations, indicating that the model considers not only the target energy value but also structural differences.&lt;/p&gt; &lt;p&gt;To assess whether there’s molecular structural ordering in the embeddings, we construct system-specific variables for each molecule and visualize the latent space accordingly. Ethanol, with its relatively simple structure, showcases three important variables: the distance between the two Carbons (C-C bond), the distance between Carbon and Oxygen (C-O bond), and the angle formed by the three atoms. The distributions of these variables in Ethanol’s train and validation sets are depicted in the figure below:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 1 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_t-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_t-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_t-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_t.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_v-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_v-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_v-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_cc_v.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 2 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_t-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_t-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_t-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_t.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_v-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_v-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_v-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_co_v.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 3 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_t-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_t-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_t-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_t.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_v-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_v-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_v-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/eth_ang_v.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Distributions in train (left) and validation (right) sets of Ethanol, of C-C bond length (top), C-O bond length (middle) and main angle (bottom) &lt;/div&gt; &lt;p&gt;The distributions appear very similar for each variable in the train and validation sets. Now, let’s examine Ethanol’s validation conformations latent projection, color-coded with respect to the target and the three system-specific variables:&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 1 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth_2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__cc_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__cc_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__cc_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__cc_2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;!-- Row 2 --&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__ang_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__ang_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__ang_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__ang_2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__co_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__co_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__co_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-A-deeper-look-into-equivariance-for-materials-data/egnn_lat_eth__co_2.png&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;100%&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;A clear gradient is observed for the main angle and C-C bond! The target gradient appears from the top left corner to the bottom right; the C-C bond gradient seems to go from bottom left to top right, and the main angle gradient isn’t as linear, appearing to spiral from the bottom to the top right corner clockwise. The C-O bond projection doesn’t seem to follow a discernible gradient, suggesting it’s not as influential on the target as the other two variables.&lt;/p&gt; &lt;p&gt;Cool huh? The Equivariant GNN appears to embed the data according to the target value but also according to the systems geometrical structure! This suggests that the model leverages its E(3) equivariant convolution layers to capture and encode information about both the target values and the intricate geometric features of the molecular systems.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In conclusion, our exploration has demonstrated the efficiency of the E(3) equivariant GNN, compared to an invariant GNN, in predicting the total energy of molecular conformations. Though both models were compared on predicting energy, a scalar propery, the E-GNN’s ability to leverage the inherent symmetries present in the system allowed it to effectively capture and encode the relationship between the arrangement of molecules and their respective energy. This was illustrated through the latent representation visualizations, and was particularly evident in the case of Ethanol. Here, discernible gradients in the latent space were observed, correlating with the target energy value and variations in C-C bond length and main angle. However, interpretability varies among the latent projections for the more complex molecules investigated in this project. Potential improvements could be achieved with additional data and a more expressive equivariant network.&lt;/p&gt; </content> </entry> <entry> <title>Prompt to Prompt</title> <link href="https://deep-learning-mit.github.io/blog/2023/prompt-to-prompt/"/> <updated>2023-11-07T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/prompt-to-prompt</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Recently, the techniques to edit images have advanced from methodologies that require the user to edit individual pixels to deep learning-based image editing. The latter employ for example large image generation models (e.g., stable diffusion models). While these deep learning-based image editing techniques initially required the user to mark particular areas that should be edited (Nichol et al., 2021 &lt;d-cite key=&quot;nichol2021glide&quot;&gt;&lt;/d-cite&gt;; Avrahami et al., 2022a&lt;d-cite key=&quot;avrahami2022blendeddiffusion&quot;&gt;&lt;/d-cite&gt;; Ramesh et al., 2022), recently the work by (Hertz et al, 2022 &lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;) has shown that this becomes unnecessary. Instead, image editing can be performed using a cross-attention mechanism. In particular, the proposed prompt-to-prompt editing framework enables the controlling of image edits by text only. The section below provides an overview of how this prompt-to-prompt framework works (Figure 1, by (Hertz et al, 2022&lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;)).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Figure 1: Cross-attention method overview. Top: visual and textual embedding are fused using cross-attention layers that produce attention maps for each textual token. Bottom: we control the spatial layout and geometry of the generated image using the attention maps of a source image. This enables various editing tasks through editing the textual prompt only. When swapping a word in the prompt, we inject the source image maps Mt, overriding the target maps M ∗ t . In the case of adding a refinement phrase, we inject only the maps that correspond to the unchanged part of the prompt. To amplify or attenuate the semantic effect of a word, we re-weight the corresponding attention map. (Hertz et al, 2022 &lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;While this proposed framework has significantly advanced the image editing research field, its performance leaves still room for improvement such that open research questions remain. For example, when performing an image editing operation that changes the hair color of a woman, significant variability across the woman’s face can be observed (Figure 2). This is undesirable, as the user would expect to see the same female face across all four images.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;em&gt;Figure 2: Experimentation with the proposed prompt-to-prompt image editing framework presented by (Hertz et al, 2022&lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;). The faces of the women show significant variability even though they should remain invariant across all four generated/ edited images.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Within our work, we will start to further benchmark the proposed framework’s performance, explore its hyperparameters’ impact on the image editing process, and research opportunities to improve the current performance.&lt;/p&gt; &lt;h2 id=&quot;literature-review&quot;&gt;Literature Review&lt;/h2&gt; &lt;p&gt;Before delving into the details of the prompt-to-prompt editing method, let’s briefly recap some existing techniques to edit images with diffusion models that have paved the way for this revolutionary approach:&lt;/p&gt; &lt;h3 id=&quot;1-adding-noise-to-an-image-and-denoising-with-a-prompt&quot;&gt;1. Adding noise to an image and denoising with a prompt&lt;/h3&gt; &lt;p&gt;In &lt;strong&gt;SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations&lt;/strong&gt; &lt;d-cite key=&quot;meng2021sdedit&quot;&gt;&lt;/d-cite&gt; , the user takes an image, introduces noise and then denoises it according to a user-provided prompt. As an example, given an image, users can specify how they want the edited image to look using pixel patches copied from other reference images.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;A similar approach is used in the paper &lt;strong&gt;MagicMix: Semantic Mixing with Diffusion Models&lt;/strong&gt; &lt;d-cite key=&quot;liew2022magicmix&quot;&gt;&lt;/d-cite&gt; which uses a pre-trained text-to-image diffusion based generative model to extract and mix two semantics. The figure below showcases the detailed pipeline of MagicMix (image-text mixing). Given an image x&lt;sub&gt;0&lt;/sub&gt; of layout semantics, they first craft its corresponding layout noises from step Kmin to K&lt;sub&gt;max&lt;/sub&gt;. Starting from K&lt;sub&gt;max&lt;/sub&gt;, the conditional generation process progressively mixes the two concepts by denoising given the conditioning content semantics (“coffee machine” in this example). For each step k in [K&lt;sub&gt;min&lt;/sub&gt;; K&lt;sub&gt;max&lt;/sub&gt;], the generated noise of mixed semantics is interpolated with the layout noise x&lt;sub&gt;k&lt;/sub&gt; to preserve more layout details.&lt;/p&gt; &lt;div style=&quot;text-align:center;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1.png&quot; class=&quot;img-fluid&quot; width=&quot;100&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;2-take-an-image-add-noise-and-denoise-it-with-a-prompt--add-a-mask&quot;&gt;2. Take an image, add noise and denoise it with a prompt + Add a mask&lt;/h3&gt; &lt;p&gt;In the paper &lt;strong&gt;Blended Diffusion: Text-Driven Editing of Natural Images&lt;/strong&gt; &lt;d-cite key=&quot;avrahami2022blended&quot;&gt;&lt;/d-cite&gt; , given an input of an image and a mask, the blended diffusion modifies the masked area according to a guided text prompt, without affecting the unmasked regions. One limitation of this is that it relies on the user having to produce this mask to indicate the editing region.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;An advanced version of this diffusion mode is discussed in the paper &lt;strong&gt;Text-based inpainting with CLIPSef and Stable Diffusion&lt;/strong&gt; &lt;d-cite key=&quot;luddecke2022image&quot;&gt;&lt;/d-cite&gt;. In this paper, the novelty is that the user doesn’t have to do the mask manually. Instead, it can use an existing segmentation model (e.g. ClipSef). Another alternative is presented in the paper &lt;strong&gt;DiffEdit: Diffusion-based semantic image editing with mask guidance&lt;/strong&gt; &lt;d-cite key=&quot;couairon2022diffedit&quot;&gt;&lt;/d-cite&gt; where the mask is generated directly from the diffusion model.&lt;/p&gt; &lt;h3 id=&quot;3-fine-tune-overfit-on-a-single-image-and-then-generate-with-the-fine-tuned-model&quot;&gt;3. Fine-tune (“overfit”) on a single image and then generate with the fine-tuned model&lt;/h3&gt; &lt;p&gt;In the paper &lt;strong&gt;Imagic: Text-based real image editing with diffusion models&lt;/strong&gt; &lt;d-cite key=&quot;kawar2023imagic&quot;&gt;&lt;/d-cite&gt; and &lt;strong&gt;Unitune: Text-driven image editing by fine-tuning a diffusion model on a single image&lt;/strong&gt; &lt;d-cite key=&quot;valevski2023unitune&quot;&gt;&lt;/d-cite&gt;, the authors perform extensive fine-tuning on either the entire diffusion model or specific sections of it. This process is computationally and memory-intensive, setting it apart from alternative methods.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;prompt-to-prompt&quot;&gt;Prompt-to-prompt&lt;/h3&gt; &lt;p&gt;The prompt-to-prompt editing method is a significant advancement compared with the existing image editing techniques that rely on diffusion models. Unlike the methods explained above that involve adding noise, using masks, or fine-tuning, the prompt-to-prompt method stands out because of its simplicity, flexibility, and user-friendliness. In the former methods, users often face challenges such as manually creating masks or undergoing resource-intensive fine-tuning processes, which can be both time-consuming and technically demanding. In contrast, the prompt-to-prompt editing method streamlines the editing process by allowing users to directly specify their desired edits through language prompts. This approach eliminates the need for intricate masking or extensive model training as well as leverages the power of human language to precisely convey editing intentions.&lt;/p&gt; &lt;p&gt;Throughout our research, we will adopt the prompt-to-prompt editing method as our starting point, with the aim of enhancing its performance.&lt;/p&gt; &lt;h2 id=&quot;outline-of-our-research&quot;&gt;Outline of our research&lt;/h2&gt; &lt;p&gt;To perform our research, we plan to build upon the code which complemented the paper published by (Hertz et al, 2022 &lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;, &lt;a href=&quot;https://github.com/google/prompt-to-prompt/&quot;&gt;Link to code&lt;/a&gt;). Concretely, we will rely on a stable diffusion model from hugging face which we will access via Python. No model training is required as we will solely work with attention layers that capture spatial information about the images.&lt;/p&gt; &lt;p&gt;Our study will be divided into 3 main subsections:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;a-hyperparameter-study-of-prompt-to-prompt-editing-method-word-swap&quot;&gt;A. Hyperparameter Study of prompt-to-prompt editing method “word swap”&lt;/h2&gt; &lt;p&gt;In the forthcoming subsection, we delve into a comprehensive analysis of the hyperparameters pertaining to the “word swap” method within the prompt-to-prompt editing framework. Before delving into the specifics, it’s crucial to understand the significance of these hyperparameters and their default values, as originally outlined in the seminal work by Hertz et al&lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We will systematically explore various hypotheses regarding each hyperparameter and present our empirical findings, shedding light on their individual impacts on the editing process. This examination aims to provide valuable insights into optimizing the performance of the “word swap” method and enhancing its practical utility.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;a1-exploration-of-silhouette-threshold-hyperparameter-k&quot;&gt;A1. Exploration of silhouette threshold hyperparameter (“k”)&lt;/h2&gt; &lt;p&gt;In this section, we embark on an exploration of the silhouette threshold hyperparameter (“k”). We aim to unravel the influence of varying this parameter while using the prompt ‘&lt;em&gt;“A woman’s face with blond hair”&lt;/em&gt;’ and making alterations to different hair colors (brown, red, black). The GIF below showcases the representation of these experiments.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Additionally, we present a comparative analysis of the impact of this hyperparameter on editing tasks related to landscapes. For instance, we employ the prompt ‘&lt;em&gt;“A river between mountains”&lt;/em&gt;’ and manipulate the landscape, including options like streets, forests, and deserts. The results of this landscape-oriented analysis can be seen in the figure below.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;a2-exploration-of-cross-attention-injection-hyperparameter-cross-replace-steps&quot;&gt;A2. Exploration of cross-attention injection hyperparameter (“cross replace steps”)&lt;/h2&gt; &lt;p&gt;Below we showcase the effect of the silhouette threshold hyperparameter (“k”) and the cross-attention injection hyperparameter(“cross_replace_steps”). We manipulate the “k” value, setting it to 3 different levels: 0, 0.3 (default literature value), and 0.6. The experiment was performed for both women’s faces and landscapes, providing a comprehensive understanding of how these hyperparameters affect the editing process. The following GIFs showcase the results of our exploration.&lt;/p&gt; &lt;h3 id=&quot;with-k--0&quot;&gt;With k = 0:&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;with-k--03&quot;&gt;With k = 0.3:&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;with-k--06&quot;&gt;With k = 0.6:&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Below, we present the key insights found for the prompt &lt;em&gt;“A woman’s face with blond hair”&lt;/em&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Below, we present the key insights found for the prompt &lt;em&gt;“A river between mountains”&lt;/em&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;a3-exploration-of-self-attention-hyperparameter-self-replace-steps&quot;&gt;A3. Exploration of self-attention hyperparameter (“self replace steps”)&lt;/h2&gt; &lt;p&gt;In our investigation of the self-attention hyperparameter known as “self_replace_steps,” we conducted a series of experiments with careful consideration of the interplay between this parameter and two other critical factors: “k” (the silhouette threshold) and “cross_replace_steps” (the cross-attention injection parameter). To comprehensively assess the influence of “self_replace_steps,” we designed two distinct experimental scenarios.&lt;/p&gt; &lt;p&gt;In the first scenario, we set “k” and “cross_replace_steps” to their default values in the literature review (0.3 and 0.8 respectively), creating an environment conducive to exploring the effects of self-attention within these threshold parameters. Concurrently, in the second scenario, we opted for more extreme settings by keeping “k” at 0 (no silhouette threshold) and “cross_replace_steps” at 0.2, thereby intensifying the impact of the self-attention hyperparameter.&lt;/p&gt; &lt;h3 id=&quot;with-k--03-and-cross_replace_steps--08&quot;&gt;With k = 0.3 and cross_replace_steps = 0.8:&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;with-k--0-and-cross_replace_steps--02&quot;&gt;With k = 0 and cross_replace_steps = 0.2:&lt;/h3&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Below, we present the key insights for the hyperparameter “self_replace_steps” within the context of the prompt &lt;em&gt;“A woman’s face with blond hair”&lt;/em&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Below, we present the key insights for the hyperparameter “self_replace_steps” found for the prompt &lt;em&gt;“A river between mountains”&lt;/em&gt;.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;a4-cycle-consistency-of-method&quot;&gt;A4. Cycle Consistency of method&lt;/h2&gt; &lt;p&gt;Our primary goal is to delve into the notion of “Cycle Consistency” within our methodology. This concept revolves around the seamless reversal of text prompt modifications back to their original form, ensuring that the resulting image closely mirrors the initial prompt. This bidirectional editing process serves as the central focus of our research, and in the subsequent sections, we present our findings on this crucial aspect.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;b-generalization-of-optimized-hyperparameters-to-attention-re-weight-method&quot;&gt;B. Generalization of optimized hyperparameters to “attention re-weight method”&lt;/h2&gt; &lt;p&gt;After identifying the optimal parameters, we conducted a comparative analysis to assess their generalizability across other methods, including attention re-weighting. In the visual presentation, we used GIFs to showcase image generation under two different parameter configurations for the prompt &lt;em&gt;“A woman’s face with long wavy blond hair”&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;On the left side, images were generated using default values (k=0.3; cross_replace_steps = 0.8; self_replace_steps = 0.2) while varying the assigned weights. Notably, negative weights led to instability and less desirable outcomes, as evidenced by the results on the left.&lt;/p&gt; &lt;p&gt;On the right side, we employed our optimized hyperparameter values (k = 0; cross_replace_steps = 0.2; self_replace_steps = 0.8). These images demonstrated improved stability while consistently producing the desired output. This visual comparison highlights the effectiveness of our optimized parameters and their superior performance, particularly when dealing with attention re-weighting method.&lt;/p&gt; &lt;div style=&quot;display: flex;&quot;&gt; &lt;div style=&quot;flex: 1; padding: 10px;&quot;&gt; Literature suggested parameters &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif&quot; class=&quot;img-fluid&quot; width=&quot;200&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div style=&quot;flex: 1; padding: 10px;&quot;&gt; Newly optimized parameters &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif&quot; class=&quot;img-fluid&quot; width=&quot;50&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h2 id=&quot;our-proposed-method&quot;&gt;Our Proposed Method&lt;/h2&gt; &lt;p&gt;As our research has demonstrated, the current prompt-to-prompt method, as reported in the literature &lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;, exhibits significant limitations. Specifically, with the current settings for the silhouette, cross-attention injection, and self-attention injection parameters, the method fails to perform the prompted edits with precision. A comparative analysis of the generated target images against the geometry of the reference images reveals undesired deviations. The existing method over-constrains the geometry due to excessively high k values and cross-attention injection values. Additionally, it underutilizes self-attention injection. Furthermore, the current method lacks cycle consistency. To address these shortcomings, we propose a new framework: the &lt;em&gt;“CL P2P”&lt;/em&gt; prompt-to-prompt image editing framework. This framework offers several key improvements over the existing method:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Optimization of Critical Hyperparameters&lt;/strong&gt;: Our research indicates that optimizing the values of critical hyperparameters results in higher prompt-to-prompt image editing precision and a more accurate similarity between the reference and target images for desired features. We propose the following adjusted values, particularly for editing faces and hairstyles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local editing (silhouette parameter k): 0.0&lt;/li&gt; &lt;li&gt;Cross-attention injection (cross replace steps): 0.2&lt;/li&gt; &lt;li&gt;Self-attention injections (self-replace steps): 0.8&lt;/li&gt; &lt;/ul&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;By selecting these values, the following changes are introduced to the prompt-to-prompt editing method:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;span style=&quot;color:red&quot;&gt;Remove&lt;/span&gt;: Local editing can be removed from the method, as it did not lead to significant improvements compared to the precision achieved by the elongated injection of self-attention.&lt;/li&gt; &lt;li&gt;&lt;span style=&quot;color:orange&quot;&gt;Reduce&lt;/span&gt;: The cross-attention (query-key-value attention) injection should be reduced to allow greater geometric adaptability and better convergence between the reference and target images.&lt;/li&gt; &lt;li&gt;&lt;span style=&quot;color:green&quot;&gt;Increase&lt;/span&gt;: Self-attention injection should be substantially elongated from 20% to 80% of the diffusion steps. This is crucial, especially for editing hairstyles, as it allows for the greatest geometric adaptability and ensures the convergence between desired reference and target image features.&lt;/li&gt; &lt;/ul&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Addressing Cycle-Inconsistency&lt;/strong&gt;: To remedy the cycle-inconsistency, we propose balancing the asymmetry of the current method with regards to the V values of the underlying transformer model. The current method is cycle-inconsistent, even though the same embeddings are used for both the reference and target prompts. Traditionally, the method has only employed the V values of the reference prompt, neglecting those of the target prompt. This characteristic likely introduces asymmetry, breaking the cycle-consistency of the model. We propose an additional injection mechanism for the “CL P2P” framework, a V value injection method, allowing for the consideration of both the V values of the reference and target images. To control the number of injection steps, we introduce an additional hyperparameter, “V value injection steps”. The V value injection function is defined based on the logic highlighted in the footnote of the image.&lt;/p&gt; &lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt; &lt;p&gt;The development of the “CL P2P” framework is a significant advancement in prompt-to-prompt image editing methods. However, there are still areas where further research will be needed. A critical area of exploration lies in the enhancement of cycle-consistency within the prompt-to-prompt editing process. Further research is required to ascertain and refine the optimal values for the V value injection steps, a key component in achieving cycle-consistency.&lt;/p&gt; &lt;p&gt;Additionally, the existing frameworks predominantly focus on singular reference and target prompts. While this approach has opened new pathways in human-computer interaction, several research questions remain unexplored. A notable inquiry is the potential to integrate various prompt-to-prompt editing methods, such as “word swap”, “attention re-weighting,” and “prompt refinement.” This integration aims to facilitate a dynamic, conversational interaction between users and generated images, enabling a continuous and iterative editing process. Current state-of-the-art generative image models, such as mid-journey models, do not inherently support such iterative mechanisms. The realization of this functionality necessitates extensive research and development, offering an exciting challenge for future advancements in the field.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Image generation models, inherently stochastic in nature, exhibit variability in outcomes even when similar prompts are applied. This stochasticity can result in significant deviations in the generated images. For instance, prompts like “A woman’s face with blond hair” and “A woman’s face with red hair” may yield images with markedly different facial features, demonstrating the algorithm’s underlying randomness.&lt;/p&gt; &lt;p&gt;In response to this challenge, prompt-to-prompt image generation and editing techniques have emerged as a significant area of interest in recent years. These methods, while constituting a potent tool in the arsenal of image editing alongside fine-tuning, semantic mixing, and masking approaches, are not without limitations. Specifically, the precision of edits and the geometric alignment between reference and target images often fall short of expectations.&lt;/p&gt; &lt;p&gt;Our research delves into the influence of critical hyperparameters on the outcomes of a cross-attention-based prompt-to-prompt method. We aimed to dissect the impact of each hyperparameter on image editing and geometric adaptation between the reference and target images. Our findings make substantive contributions to enhancing the precision and geometric convergence in prompt-to-prompt methods, with the following key insights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An extensive analysis of three critical hyperparameters (silhouette selection, cross-attention injection, and self-attention injection) was conducted, focusing on their effect on the precision of an attention-based prompt-to-prompt editing method.&lt;/li&gt; &lt;li&gt;Contrary to existing literature&lt;d-cite key=&quot;hertz2022prompttoprompt&quot;&gt;&lt;/d-cite&gt;, our study reveals that self-attention injection plays a more pivotal role than previously recognized. We recommend incorporating self-attention injection from the reference image for approximately 80% of the diffusion steps during the target image generation process.&lt;/li&gt; &lt;li&gt;We introduce the novel &lt;em&gt;“CL P2P”&lt;/em&gt; framework, designed to elevate the efficacy of prompt-to-prompt editing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our research not only deepens the understanding of prompt-to-prompt editing methods but also achieves enhanced editing precision and improved similarity between reference and target images.&lt;/p&gt; &lt;p&gt;Looking ahead, the &lt;em&gt;“CL P2P”&lt;/em&gt; framework paves the way for further exploration, particularly in addressing the cycle consistency of prompt-to-prompt methods. Additionally, exploring strategies to seamlessly integrate different prompts into a continuous dialogue could revolutionize human-computer interaction, enabling users to edit generated images through conversational engagement.&lt;/p&gt; </content> </entry> <entry> <title>Understanding Bias in Speech to Text Language Models</title> <link href="https://deep-learning-mit.github.io/blog/2023/Language-Bias/"/> <updated>2023-11-07T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/Language-Bias</id> <content type="html">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;p&gt;With all the buzz that ChatGPT is getting recently, it is clear that machine learning models that can interact with humans in a natural manner can quite literally flip the world around. If that is not enough proof, Siri and Google Assistant, their popularity and convenience can give you a bit more of an idea. We can see how speech processing is important as a way for humans and computers to communicate with each other, and reach great levels of interactivity if done right. A lot of the world’s languages do not have written forms, and even those that do, typing can be less expressive and slower than speaking.&lt;/p&gt; &lt;p&gt;The core of these assistant systems is automatic speech recognition, often shortened as ASR or alternatively speech2text, which we will be using. This problem sounds rather simple: turn voice into text. However easy it might sound, speech2text is far from solved. There are so many factors that affect speech that makes it extremely difficult. First, how do we know when someone is speaking? Most speech2text models are trained on and perform well when the audio is clean, which means there is not a lot of noise. In the real world, however, one can be using speech2text in a concert or a cocktail party, and figuring out who is currently speaking to the system amid all the noise is a problem in itself! Another important factor that complicates speech2text is that we don’t all talk the same way. Pronunciations vary by person and region, and intonation and expressiveness change the acoustics of our speech. We can see this in full effect when auto-generated YouTube caption looks a bit.. wrong.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/reddit-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/reddit-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/reddit-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/reddit.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;From https://www.reddit.com/r/funny/comments/ha7dva/youtube_auto_captions_spot_on/&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Aside from who and how we talk, another big part that makes speech2text hard has to do with the idiosyncrasies of text and languages itself! Some idiosyncrasies of language include orthography, the system of how we write sounds and words, and syntax, the system of how words string together into sentences. If you are familiar with English, you would be familiar with the English syntax: subject, verb, object, and a particular order for adjectives. We would instinctively say “small white car,” but not “white small car” and most definitely not “car white small.” Cross over the English channel to France (or the St. Lawrence River to Quebec), and the order changes. For French, you would say “petite voiture blanche,” which word for word is “small car white.”&lt;/p&gt; &lt;p&gt;Travel a bit further and you would see that Chinese uses “白色小车” (”white color small car”), Thai uses “รถสีขาวคันเล็ก” (”car color white * small”) and Kannada uses “ಸಣ್ಣ ಬಿಳಿ ಕಾರು” (”small white car”, same as English). Aside from order of adjectives, larger differences in syntax include having the subject appear first or last in a sentence, position of verbs, and how relative clauses work. All this means that language is quite non-linear, and natural language models that understand language must cope with our silly little arbitrary orders!&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/twitter_meme-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/twitter_meme-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/twitter_meme-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/twitter_meme.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;From https://www.bbc.com/news/blogs-trending-37285796&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Thankfully though, for speech2text how sentences work is not as important as how phonetics and orthography works. But even then, things are not quite smooth sailing either. We sometimes take for granted how difficult reading is, perhaps until you start to learn a second language and realize how much we internalize. English is notorious for not spelling words the way it sounds, mostly because writing was standardized a long time ago and pronunciation has shifted since. This makes it difficult for machine learning models to try learn.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/ought-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/ought-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/ought-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/ought.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Sentence from https://en.wikipedia.org/wiki/Ough_(orthography)&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Wow, look at all those words with “ough”! There are at least eight different pronunciations of the word, or from another point of perspective, at least eight different audios magically turn out to be spelt the same! In the diagram we tried substituting the red “ough”s to their rhymes in blue, keeping in mind that some dialects pronounce these words differently (especially for “borough”), and in green is the International Phonetic Alphabet representation of the sounds. IPA tries to be the standard of strictly representing sounds as symbols. What’s at play here? English is plagued with silent letters (”knight”), and extraneous letters (all the “ough”s and more).&lt;/p&gt; &lt;p&gt;Some languages are more straightforward in their orthography than others. Spanish tends to be fairly phonemic, which pretty much means that their writing and speaking are quite in sync. &lt;d-cite key=&quot;orthography&quot;&gt;&lt;/d-cite&gt; French, however, is very famous for its silent letters. A word like “hors-d’oeuvres”, which means appetizer, can be represented in IPA as [ɔʁ dœvʁ], you may see that around half the letters aren’t pronounced! Kannada, a language in South India that is spoken by one of our group members, is said to be quite phonemic, but doesn’t come without a different kind of headache. A number of languages, predominantly in South Asia and Southeast Asia, use a kind of writing system that combines a consonant character with a vowel character to form a new character that represents the consonant-vowel combination. The new character retains some part of the original consonant and vowel in some cute manner, kind of like the letter &lt;strong&gt;Æ&lt;/strong&gt; but dialed up many notches. Most abugida systems descend from the 3rd century BCE Brahmi script.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/brahmi-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/brahmi-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/brahmi-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/brahmi.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Above are some examples of scripts of this type, demonstrating two consonants k and m combining with vowels long a, i and u. Another interesting feature for some of these writing systems is that sometimes the vowels jump to the front, for example in Thai ก (k) + เ (e) = เก (ke). Again, writing is non-linear at times!&lt;/p&gt; &lt;h3 id=&quot;past-work&quot;&gt;Past Work&lt;/h3&gt; &lt;p&gt;Past work shows success in training speech2text models in German, Spanish, and French &lt;d-cite key=&quot;parp&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;german&quot;&gt;&lt;/d-cite&gt;. Some use pruning and finetuning of state of the art English models, and others train models from scratch for each language. Other works such as &lt;d-cite key=&quot;thaispeech&quot;&gt;&lt;/d-cite&gt; show that models can learn less common languages, like Thai which is the language our other group member speaks, as well, but they are more complex and specific to each language. &lt;d-cite key=&quot;parp&quot;&gt;&lt;/d-cite&gt; circumvents this by pruning wav2seq (a SOTA speech2text model) and finetuning the model for different languages. While this showed promising results, we wanted to dive deeper to understand, from a linguistic and data driven perspective, the biases that &lt;em&gt;simple&lt;/em&gt; speech2text models had.&lt;/p&gt; &lt;p&gt;Many state of the art models rely on encoder-decoder models. An encoder is used to create an expressive feature representation of the audio input data and a decoder maps these features to text tokens. Many speech models like &lt;d-cite key=&quot;data2vec&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;wav2letter&quot;&gt;&lt;/d-cite&gt;, &lt;d-cite key=&quot;contextNet&quot;&gt;&lt;/d-cite&gt; also use self-supervised pretraining on the encoder for better performance. One example is the Wav2Seq. Wav2Seq uses unsupervised pretraining to annotate audio samples with unique characters in the form of a psuedo language. The building blocks for these encoders are generally transformer based &lt;d-cite key=&quot;wav2seq&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;Other methods use deep recurrent neural networks like in &lt;d-cite key=&quot;cs_toronto&quot;&gt;&lt;/d-cite&gt;. RNNs are great at sequential tasks and have an internal memory to capture long term dependencies. Transformer based methods have outperformed RNNs and LSTM based architectures now though.&lt;/p&gt; &lt;p&gt;How do these features (idiosyncrasies) differ between languages and does this affect how well speech2text models learn? By doing more ablation studies on specific features, maybe this can inform the way we prune, or choose architecture, and can also help determine the &lt;em&gt;simplest&lt;/em&gt; features necessary in a speech2text model that can still perform well on various languages.&lt;/p&gt; &lt;p&gt;There has been work that perform ablation studies on BERT to provide insight on what different layers of the model is learning &lt;d-cite key=&quot;ganesh2019&quot;&gt;&lt;/d-cite&gt;. Experiments suggest lower layers learn phrase-level information, middle layers learn syntactic information, and upper layers learn more semantic features. We want to do a similar study, but on dissecting the components of language rather than the components of a particular SOTA model. Our hypothesis is that by doing so, we can be better informed when selecting preprocessing methods and models.&lt;/p&gt; &lt;p&gt;Let’s get started with some experiments!&lt;/p&gt; &lt;h2 id=&quot;generating-a-dataset&quot;&gt;Generating a Dataset&lt;/h2&gt; &lt;p&gt;We want to explore how each of these language features affects how speech2text models learn. Let’s create a custom dataset where we can implement each of these language rules in isolation. To do that, we’ll build out our own language. Sounds daunting — but there are only a key few building blocks that matter to us. Languages are made of sentences, sentences are made of words, words are made of letters, and letters are either consonants or vowels. Let’s start with that.&lt;/p&gt; &lt;p&gt;From &lt;d-cite key=&quot;prehistoric_speech&quot;&gt;&lt;/d-cite&gt;, languages have 22 consonants on average and about 9 vowels on average so that’s what we’ll have in our language too. We represent consonants as positive integers from 1 to 23 and vowels as negative integers from -9 to -1. After all, letters are just symbols!&lt;/p&gt; &lt;p&gt;A word, at it’s most crude representation, is just a string of these consonants and vowels at some random length. To make sentences, we just string these words together with spaces, represented by 0, together.&lt;/p&gt; &lt;p&gt;Here’s a sample sentence in our language:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[14 -2 -9 13 0 8 16 -8 -2 0 -3 -8 16 12 0 10 20 -3 -7 0 14 18 -9 -4 0 16 -3 -5 14 0 -3 9 -8 3 0 -9 -1 22 7 0 12 -5 6 -7 0 -7 22 12 -2 0 22 -9 2 -2 0 17 -2 -8 9 0 1 -4 18 -9 0 19 -7 20 -2 0 8 18 -4 -2 0 -9 8 -4 15 0 -9 -2 22 18] &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Ok, that seems a little meaningless. We don’t have to worry about meaning in the general semantic sense though. What we do care about, is pronouncing this language, and creating a mapping from these written sentences to an audio sample. Let’s do that next. Audio samples can be represented as spectrograms. Spectrograms give us a visual representation of audio by plotting the frequencies that make up an audio sample.&lt;/p&gt; &lt;p&gt;Here’s an example:&lt;/p&gt; &lt;p&gt;When we say &lt;strong&gt;“It’s never too early to play Christmas music”&lt;/strong&gt;, this is what it might look like visually:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/christmas_spectrogram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/christmas_spectrogram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/christmas_spectrogram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/christmas_spectrogram.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The key here is that we don’t exactly need audio samples, but rather an embedding that &lt;strong&gt;&lt;em&gt;represents&lt;/em&gt;&lt;/strong&gt; an audio sample for a written sentence. Embeddings are just low dimensional mappings that represent high dimensional data.&lt;/p&gt; &lt;p&gt;So, in our case, our spectrogram for a generated audio sample looks something like:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/gen_spectrogram-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/gen_spectrogram-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/gen_spectrogram-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/gen_spectrogram.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Even though audio samples might be complicated waveforms, the embedding for the first letter looks something like:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([[ 3.6887e-01, -9.6675e-01, 3.2892e-01, -1.2369e+00, 1.4908e+00, 8.1835e-01, -1.1171e+00, -1.9989e-01, 3.5697e-01, -1.2377e+00, 4.6225e-01, -6.7818e-01, -8.2602e-01]]) &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Again, maybe meaningless to us who haven’t really learned this new language. There are some vertical columns of the same color, and these represent the silences between each word. You might notice that these columns aren’t exactly the same color, and that’s because we’ve added a bit of Gaussian noise to the audio embedding samples to simulate noise that might occur when recording audio samples on a microphone.&lt;/p&gt; &lt;p&gt;Ok great! We’ve got this perfect language that maps the same sentence to the same audio sample. Now, let’s get to work adding some features that we talked about in the previous section to make this language a bit more complicated.&lt;/p&gt; &lt;p&gt;We narrow our feature selection to the following three:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Silent Letters:&lt;/strong&gt; letters in the written language that don’t appear in the phonetic pronunciation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Letter Combos:&lt;/strong&gt; two letters combine in the script but are still pronounced separately&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Letters out of Order:&lt;/strong&gt; phonetic pronunciation is in a different order than written language&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&quot;silent-letters&quot;&gt;Silent Letters&lt;/h3&gt; &lt;p&gt;Silent letters mean they appear in our written labels but not in our audio samples. We could just remove letters from our audio embeddings, but that’s a little funky. We don’t usually pause when we come to a silent letter — saying (pause - nite) instead of just (nite) for night. To preserve this, let’s instead add letters to our written label.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In the diagram above, we have a small written sample and some audio embeddings represented as colored blocks. We generate some rules similar to those on the left.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/silent_letters.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In this case, we add a 7 after the 3, simulating a silent letter at consonant 7. We then pad the audio sample with a silent (0) to make up for the size increase of the written label. Note that silent letters don’t add pauses during the audio.&lt;/p&gt; &lt;h3 id=&quot;combining-letters&quot;&gt;Combining Letters&lt;/h3&gt; &lt;p&gt;When combining letters, our written script changes, but our audio remains the same. We choose to combine every pair where a vowel follows a consonant. This is the most common case of letter combination in languages that have this feature.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/combo_letters.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/combo_letters.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/combo_letters.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/combo_letters.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Here we have to pad the written labels as we combine two letters into one.&lt;/p&gt; &lt;h3 id=&quot;letters-out-of-order&quot;&gt;Letters out of Order&lt;/h3&gt; &lt;p&gt;We choose some pairs of consonant and vowels. Swap the pair order for every instance of the pair in the written sample. No padding needs to be added here.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/swap.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/swap.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/swap.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/swap.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;controlled-experiments&quot;&gt;Controlled Experiments&lt;/h2&gt; &lt;p&gt;Now for the fun part! Let’s see what happens when we test our new language, which each of these rules in isolation, with some models. Regardless of the model we choose, our goal is to learn a written label for a given audio sample.&lt;/p&gt; &lt;p&gt;We’re going to test our language with the building blocks of these state of art models — transformers and RNN. The results from these experiments can inform us on the biases that these fundamental models might have in their most “vanilla” state.&lt;/p&gt; &lt;p&gt;We hypothesize that transformers will perform better because RNN’s have a limited memory size, while Transformers use attention which means they can learn orderings from anywhere in the audio sample.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/system-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/system-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/system-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/system.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/results1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/results2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/results3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/results3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;RNNs are dashed lines, Transformers are solid lines&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Hmm..so Transformers performed better, but not that much better than our RNNs. This could be because our hypothesis that attention is better for long sequences and RNNs have limited memory may not apply. When we generated our language, the consonant and vowel orderings were pretty random. Our rules have some pattern to them, but not as much as a real human language — so maybe attention can exploit these better in real human language, but doesn’t give as much of an advantage in our generated dataset.&lt;/p&gt; &lt;p&gt;As for our features, it seems that silent letters perform significantly worse than some of the other rules. This makes sense because, attention and internal memory perhaps, provides some mechanism for dealing with swapping or out of order. Transformers have the ability to “focus” on features of the sample that it is deemed important. Our rules do have some pattern, and the models just have to learn these patterns.&lt;/p&gt; &lt;p&gt;With silent letters, though there is a pattern to an audio sample not being present, the rest of the sounds succeeding the silent letters are all shifted over. This is probably why letter combos also doesn’t do too great. With letter combos and silent letters, the one-to-one mapping between a letter and it’s phonetic pronunciation (or audio embedding) is thrown off for the rest of the sequence.&lt;/p&gt; &lt;h2 id=&quot;corners-cut&quot;&gt;Corners Cut&lt;/h2&gt; &lt;p&gt;This certainly tells us a lot! But, we should take these results with a grain of salt. There are some discrepancies with human language and the way that we generated our dataset that we should consider.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Actual audio speech recognition systems mostly don’t predict letter by letter, some do subwords and others do word level recognition; but in the grand scheme of things these distinctions may be negligible — after all, they’re all units! This means our controlled experiment, for our purposes, simulates character recognition models which may misspell words (”helouw” instead of “hello”). If the model is at the subword level, misspellings may decrease, since character sequences like “ouw” would not be in the list of possible subwords, or the vocabulary. “ouw” is a very un-English sequence, see if you can find a word that contains these three letters in succession! Misspellings like “hellow” might still happen though, since it is a plausible combination of English-like sequences “hel” and “low”. If the model is at the word level, there will not be misspellings at all.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;speech2text models generally either do encoder-decoder model, or otherwise typically the input and output do not have to match in dimension. Both options mean that there is no need to pad written or audio samples to make sure they’re the same length. In our case, we have to pad our written/audio to make sure everything is the same size. Connectionist Temporal Classification &lt;d-cite key=&quot;ctc&quot;&gt;&lt;/d-cite&gt; is used to postprocess outputs and compute loss.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The way CTC works is that first it assumes that a letter may take more than one audio frame to say, which tends to be the case, especially for vowel sounds which are typically looooooooooonger than consonant sounds. There is also a special character epsilon that serves as the “character boundary” symbol, but is different from the silent symbol. The output of a CTC model is deduplicated, and epsilons are removed. Here is CTC in action from &lt;d-cite key=&quot;ctc&quot;&gt;&lt;/d-cite&gt;:&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/ctc-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/ctc-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/ctc-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/ctc.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;An effect of the letter combination script in our controlled experiment is that there will be some letter combinations that exist as a class (aka in the alphabet) but never seen in the dataset. For example (1, 12) are in the alphabet as consonants, but 112 isn’t a letter.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Actual language has tone, intonation, speed and noise that can make it harder to learn. Here is where something like Wave2Seq can help as tokens are clustered, so if someone takes a little longer to say AA, it will still register as the same pseudo token.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;real-language&quot;&gt;Real Language&lt;/h2&gt; &lt;p&gt;Alas, real-world languages are more complicated than our controlled languages. We wanted to see if the patterns we learnt in our controlled experiments would still hold true for actual datasets. For this, we needed to find a relatively phonemic language and another language that differs only by one feature. As mentioned earlier, Spanish qualifies for the former, and French qualifies for the latter. French, to the best of our knowledge, is prevalent with silent letters, but don’t really exhibit other features in our controlled experiments.&lt;/p&gt; &lt;p&gt;We’re using the CommonVoice dataset, which is a crowdsourced dataset of people reading sentences in many languages, and might be harder to train because of how unclean the dataset as a whole may be. We preprocess the audio using a standard method, which is the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First, calculate the audio spectrogram and condense the result by summing up the amplitudes of a few frequencies that belong in the same “bucket”, to yield Mel-frequency cepstral coefficients (MFCC)&lt;/li&gt; &lt;li&gt;To add some temporal context, the differential of the MFCC and its second-degree differential are calculated and concatenated to the MFCC&lt;/li&gt; &lt;li&gt;The label vocabulary is constructed, by looking at what letters exist in the dataset, and the written data is converted to numbers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Behold, an example of the preprocessed dataset for Spanish!&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/spanish-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/spanish-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/spanish-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/spanish.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;target tensor: [30, 43, 1, 41, 53, 40, 56, 39, 1, 59, 52, 39, 1, 58, 39, 56, 47, 44, 39, 1, 42, 43, 1, 43, 52, 58, 56, 39, 42, 39, 7] target sequence: Se cobra una tarifa de entrada. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;We tried training transformers and RNNs, with and without CTC, on this real-world data. Without CTC, the performances of the models are, respectfully, really bad. After a number of epochs, the only thing learnt is that the space character exists, and the 6% accuracy comes from the model predicting only spaces:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;predicted tensor: [16 39 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] predicted sequence: Ea target tensor: [71, 28, 59, 83, 1, 53, 57, 1, 54, 39, 56, 43, 41, 43, 11, 1, 36, 1, 56, 43, 57, 54, 53, 52, 42, 47, 43, 52, 42, 53, 1, 43, 50, 50, 53, 57, 5, 1, 42, 47, 48, 43, 56, 53, 52, 8, 1, 14, 59, 50, 54, 39, 42, 53, 1, 43, 57, 1, 42, 43, 1, 51, 59, 43, 56, 58, 43, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] target sequence: ¿Qué os parece? Y respondiendo ellos, dijeron: Culpado es de muerte. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Got it. Like our silent letter controlled experiment, a high mismatch between the audio frame and its written frame causes models to not be able to learn well. Let’s put in our mighty CTC Loss and see how it works! It turns out that after some 30 epochs, it still isn’t doing quite so well. Here, let’s see an example of a transformer trained on the Spanish dataset with CTC:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;predicted tensor: [ 0 39 0 57 0 54 39 0 41 0 41 0 43 0 47 0 43 0 57 0 53 0 42 0 58 0 47 0 53 0 41 0 54 0 39 0 43 0 57 0 43 0] predicted sequence: aspacceiesodtiocpaese target tensor: [71 28 59 83 1 53 57 1 54 39 56 43 41 43 11 1 36 1 56 43 57 54 53 52 42 47 43 52 42 53 1 43 50 53 57 5 1 42 47 48 43 56 53 52 8 1 14 59 50 54 39 42 53 1 43 57 1 42 43 1 51 59 43 56 58 43 7] target sequence: ¿Qué os parece? Y respondiendo elos, dijeron: Culpado es de muerte. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Perhaps the transformer is too big for this and learns pretty slowly. It is starting to pick up on some sounds, for example for “¿Qué os parece?” it seems to have picked up “as pacce” and “respondiendo” has some similarities to “esodtio,” but we really needed to squint to see that similarity. If we let it run for longer, perhaps it would get better… slowly.&lt;/p&gt; &lt;p&gt;RNNs, however, came up on top. We’re using bidirectional LSTM RNN for this, and it seems that CTC works! Here’s the RNN trained on the Spanish dataset with CTC:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;predicted tensor: [30 0 59 0 52 0 53 0 51 0 40 0 56 43 0 57 0 43 0 1 42 0 43 0 42 0 47 0 42 0 39 0 42 0 43 0 1 0 89 0 51 0 40 0 43 0 58 0 39 0 59 0 52 0 53 0 1 42 0 43 0 1 50 0 39 0 1 0 57 0 54 0 39 0 88 0 53 0 52 0 39 0 7] predicted sequence: Sunombrese dedidade ómbetauno de la spañona. target tensor: [30 59 1 52 53 51 40 56 43 1 57 43 1 42 43 56 47 60 39 1 42 43 50 1 52 53 51 40 56 43 1 58 39 86 52 53 1 42 43 1 23 39 1 16 57 54 39 88 53 50 39 7] target sequence: Su nombre se deriva del nombre taíno de La Española. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Looks great! Of course there are some word boundary mistakes, but overall it looks pretty similar. What about French? Here are transformer and RNN results for what we hypothesized is a language full of silent letter features:&lt;/p&gt; &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;predicted tensor (Transformer): [21 0] predicted sequence (Transformer): L predicted tensor (RNN): [18 0 47 0 1 0 56 0 56 40 0 54 0 44 0 1 55 0 40 0 55 1 53 0 40 0 36 0 48 40 0 49 55 0 44 0 55 0 53 40 0 36 0 49 0 1 49 50 0 53 0 1 0 1 44 0 47 0 1 40 0 1 51 0 50 0 55 0 36 0 49 0 1 54 0 40 0 47 40 0 48 40 0 49 55 0 1 71 0 1 57 0 36 0 54 0 44 0 54 6] predicted sequence (RNN): Il uuesi tet reamentitrean nor il e potan selement à vasis. target tensor: [18 47 1 36 1 36 56 54 44 1 75 55 75 1 53 75 38 40 48 40 49 55 1 44 49 55 53 50 39 56 44 55 1 40 49 1 14 56 53 50 51 40 1 50 82 1 44 47 1 40 54 55 1 51 50 55 40 49 55 44 40 47 40 48 40 49 55 1 44 49 57 36 54 44 41 6] target sequence: Il a ausi été récement introduit en Europe où il est potentielement invasif. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Wow! The transformer got stuck in the blank hole black hole, but the RNN looks not too shabby. Some word boundary issues for sure, but we can see similarities. “potan selement” and “potentielement” actually do sound similar, as do “à vasis” and “invasif.” Definitely not as good as Spanish though. Here’s a comparison of losses for the four models:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/real_results-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/real_results-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-07-Language-Bias/real_results-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-07-Language-Bias/real_results.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;One thing that’s very much worth noticing is that the validation losses plateaued or rose during training. Did we overfit our data, or are these languages too hard that they can’t be fully learnt from our data, and the high loss is due to the idiosyncrasies of language? Probably both!&lt;/p&gt; &lt;p&gt;Now did these real-world explorations match our hypotheses from controlled experiments or not? Our hypothesis from controlled experiments says that French would do worse than Spanish, which is what we’re seeing. Additionally, we see a pretty significant gap in loss between transformers and RNN models, given that CTC loss is used.&lt;/p&gt; &lt;p&gt;Here comes the confusing part. Most literature &lt;d-cite key=&quot;transf_thesis&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;rnn_study&quot;&gt;&lt;/d-cite&gt; would say that transformers should perform better than RNN, even with CTC. This matches with our controlled experiments but did not match our real-world experiments. What went wrong? For one, we think that our models might still be too small and not representative of actual real-world models. We also trained the models for quite a short amount of time with a small amount of data that might be noisy. Perhaps our recipe was just the perfect storm to cause our transformer model to be stuck in the blank hole. We found an article that documents the tendency for MLPs to get stuck in a stage of predicting blanks before moving on to predicting real characters, which sounds like what’s going on for us. &lt;d-cite key=&quot;blank_ctc&quot;&gt;&lt;/d-cite&gt; Some other sources point to the assertion that input spectrogram lengths must be longer than label lengths, and suggest refraining from padding labels with blanks. We followed their suggestions but unfortunately could not bring the transformer models out of the blank hole.&lt;/p&gt; &lt;h2 id=&quot;learnings&quot;&gt;Learnings&lt;/h2&gt; &lt;p&gt;What have we looked at?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Linguistics: we learnt how weird languages can be!&lt;/li&gt; &lt;li&gt;Models: we touched upon how speech2text models usually work&lt;/li&gt; &lt;li&gt;Hindrances: we hypothesized and tested a few features that affected model performance &lt;ul&gt; &lt;li&gt;Silent letters are our biggest enemies, followed by letter combinations and out-of-order letters&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Battle: we compared how two different foundational models for speech2text against each other &lt;ul&gt; &lt;li&gt;In our controlled experiments, it’s a pretty close call but transformer came up on top by just a slight margin&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Real: we presented what a real-world dataset looks like, the data preprocessing methods, and checked if our learnings from controlled experiments hold &lt;ul&gt; &lt;li&gt;Creating a spectrogram and a character vocabulary is the standard!&lt;/li&gt; &lt;li&gt;French (silent letter-ish) vs. Spanish (perfect-ish) matches our hypothesis!&lt;/li&gt; &lt;li&gt;CTC is the cherry on top for success but only works well with RNN, putting RNN on top by a long shot this time!&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We would like to expand our linguistics experiments further as future work, as there are many more features and combinations not explored here (for example, Arabic writing usually drops all vowels — we imagine that this feature would affect performance a lot!) Another avenue of further work is to try train on other real-world languages to see whether our hypotheses still hold true.&lt;/p&gt; </content> </entry> <entry> <title>Regularization Techniques for Attention Layers in Transformer Models</title> <link href="https://deep-learning-mit.github.io/blog/2023/attention-regularization/"/> <updated>2023-11-06T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/attention-regularization</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Transformer models are exceptionally popular and successful at completing many tasks. However, they can overfit to their training data if they are not given enough data to generalize. Frequently, part of the reason for overfitting is the overfitting of a self-attention layer, which highlights important tensors in the model. However, if there is not enough data, the attention layer can overfit to the training data and highlight some tensors too much. Therefore, researchers have proposed methods of regularizing attention layers. Adding regularization can be complex and there have been many different approaches to solving this issue, from simply smoothing attention layers to encouraging multi-headed models to approach different solutions. Therefore, there are differences in the effects of different regularization methods and some might perform better in different circumstances. There does not seem to be a standard approach to dealing with this form of regularization and while many authors have claimed their regularizations have positive effects on training, there are few comparisons of regularization methods. In this study, we will analyze previous work on regularizing self-attention layers and propose new regularization techniques to identify the advantages and disadvantages of differing models.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;There are many proposed regularization strategies for self-attention layers. We implement and utilize many of the more popular strategies in this study while also drawing inspiration from other methods in proposed methodologies. However, we could not find comparisons across regularization methods or implementations of these methods publicly available. Therefore, we implemented previously proposed strategies and proposed new regularization strategies based on methods seen in fully connected neural networks. The methods used fall into the following three categories. We will explain the exact methods and implementations used for each of these three categories in the methodology section. They represent a solid overview of the self-attention regularization space and contain the most popular methods currently in use.&lt;/p&gt; &lt;h3 id=&quot;dropout-based-methods&quot;&gt;Dropout Based Methods&lt;/h3&gt; &lt;p&gt;Dropout based methods involve randomly setting a specified fraction of the input units to zero during training time, which helps in preventing overfitting &lt;d-cite key=&quot;srivastava2014dropout&quot;&gt;&lt;/d-cite&gt;. This prevents the model from having all the information during training and therefore forces the model to generalize during training.&lt;/p&gt; &lt;h3 id=&quot;weight-smoothing-methods&quot;&gt;Weight Smoothing Methods&lt;/h3&gt; &lt;p&gt;Weight smoothing methods aim to regularize the self-attention layer by modifying the weights such that the attention weight are closer to the uniform distribution and do not overly emphasis specific inputs. This helps prevent overfitting by not allowing the model to only use a few inputs &lt;d-cite key=&quot;lohrenz2023relaxed&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;cross-head-methods&quot;&gt;Cross Head Methods&lt;/h3&gt; &lt;p&gt;Cross head methods involve techniques that operate across different attention heads, aiming to diversify the learned representations and prevent redundancy &lt;d-cite key=&quot;li2018multi&quot;&gt;&lt;/d-cite&gt;. Therefore, the goal is to prevent each head from being similar to other heads.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;h3 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h3&gt; &lt;p&gt;We begin by implementing and benchmarking a vision transformer with no regularization. We had previously implemented a transformer model as part of 6.s898 problem set 3, so we used this as basis for our models. This model follows an architecture stemming from An Image Is Worth 16X16 Words &lt;d-cite key=&quot;dosovitskiy2020image&quot;&gt;&lt;/d-cite&gt;. This transformer was easily modifiable and relatively simple and so it served as a good basis for our adjustments. The framework of the architecture goes as follows.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Take an image and split it into patches of specified size.&lt;/li&gt; &lt;li&gt;Embed these patches and add a positional encoding to their embedding.&lt;/li&gt; &lt;li&gt;Treat these embeddings as a sequence input to a transformer model.&lt;/li&gt; &lt;li&gt;Use a transformer model with multi-head self-attention to transform the input into some specified space.&lt;/li&gt; &lt;li&gt;Use this output to classify the image.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For this specific model, we use a 6 layer transformer with 5 self-attention heads and a patch size of 4. We will be focusing on the multi-head self-attention phase of the transformer model. The following is a diagram of the overall architecture of a vision transformer.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/6S898_Fall_2023_homeworks_ps3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/6S898_Fall_2023_homeworks_ps3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/6S898_Fall_2023_homeworks_ps3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/6S898_Fall_2023_homeworks_ps3.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Diagram of Vision Transformer Model &lt;d-cite key=&quot;dosovitskiy2020image&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt; &lt;p&gt;We use the CIFAR-10 and CIFAR-100 datasets for this study &lt;d-cite key=&quot;krizhevsky2009learning&quot;&gt;&lt;/d-cite&gt;. CIFAR-10 consists of 60,000 32x32 color images representing 10 different classes. These classes are airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. They are evenly distributed, such that there are 6,000 images of each class. CIFAR-100 uses the same format, but instead has 100 evenly distributed classes. We split this data into training and test sets and tested the different forms of regularization. We found that our transformer model with no regularization could easily achieve near-zero error on both sets of training data, but only achieved around 60% in test accuracy for the CIFAR-10 dataset and around 30% accuracy on the CIFAR-100 dataset. Therefore, the model is overfitting to the training data and testing regularization methods on this dataset could help the model generalize more on the test data.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/cifar_10_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/cifar_10_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/cifar_10_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/cifar_10_example.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Example of CIFAR-10 Images &lt;d-cite key=&quot;krizhevsky2009learning&quot;&gt;&lt;/d-cite&gt; &lt;/div&gt; &lt;h3 id=&quot;regularization-methods&quot;&gt;Regularization Methods&lt;/h3&gt; &lt;p&gt;We tested the following regularization methods for our model. We tested models contained within the three different categories of regularization mentioned in the background above, these being dropout based methods, weight smoothing methods, and cross-head methods.&lt;/p&gt; &lt;h4 id=&quot;dropout-based-methods-1&quot;&gt;Dropout Based Methods&lt;/h4&gt; &lt;h5 id=&quot;dropcolumn-&quot;&gt;DropColumn &lt;d-cite key=&quot;zehui2019dropattention&quot;&gt;&lt;/d-cite&gt;:&lt;/h5&gt; &lt;p&gt;This method uses dropout, a common regularization method used in fully connected neural networks, in self-attention layers. This can force the model to generalize better and not rely on specific inputs as much. The authors propose the following methodology to add dropout to the model, which is similar to standard dropout techniques in neural networks. To perform dropout, each column in the attention weight matrix is sampled from a Bernoulli distribution with some probability. We use a dropout ratio of 0.2 for these experiments. We set the sampled columns to zero weight during training. Therefore, we are able to randomly drop columns in the attention weight matrix.&lt;/p&gt; &lt;h5 id=&quot;drophead-&quot;&gt;DropHead &lt;d-cite key=&quot;zhou2020scheduled&quot;&gt;&lt;/d-cite&gt;:&lt;/h5&gt; &lt;p&gt;We can also perform dropout on the heads across the multi-head attention layer. With this method, we completely drop heads during training to reduce reliance on particular heads and increase the generalizability of the model. We use a dropout ratio of 0.2 for these experiments. This prevents the model from being dominated by a few attention heads.&lt;/p&gt; &lt;h4 id=&quot;weight-smoothing-methods-1&quot;&gt;Weight Smoothing Methods&lt;/h4&gt; &lt;h5 id=&quot;relaxed-attention-&quot;&gt;Relaxed Attention &lt;d-cite key=&quot;lohrenz2023relaxed&quot;&gt;&lt;/d-cite&gt;:&lt;/h5&gt; &lt;p&gt;This method smooths the attention weights in the self-attention layer to reduce overfitting. This helps reduce the magnitude of the highest attention scores. We do this by mixing in the uniform distribution to attention weights during training. We use some parameter $ \color{white} \gamma $ to evaluate different levels of mixing. Therefore, we apply the following function to our self-attention weights.&lt;/p&gt; &lt;p&gt;$ \color{white} A[i,j] = (1-\gamma) \times A[i,j] + \gamma \times \frac{1}{T}, \quad \forall i, j \in [0,1,…,T]$&lt;/p&gt; &lt;p&gt;We use $ \color{white} \gamma = 0.1 $ for our experiments. This adds a low level of uniformity but prevents the model from only attending upon a small number of tensors during training. Therefore, this should limit the amount of overfitting that is possible.&lt;/p&gt; &lt;h5 id=&quot;noise-injection&quot;&gt;Noise Injection&lt;/h5&gt; &lt;p&gt;Noise injection has been used to regularize fully connected neural networks, but we have not found any literature that proposes using noise injection to regularize self-attention layers. We propose two methodologies to add regularization and robustness to our model training. We inject noise into our input embeddings with the following formula.&lt;/p&gt; &lt;p&gt;$ \color{white} x_{i,j}^{noised} = x_{i,j}+ \frac{1}{100} * median(x) * N(0,1) $&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Overall Noise Injection: The first methodology involves simply adding noise to the input during training. We do this by adding Guassian random noise to the input before calculating self-attention weights in each layer of the transformer.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Individual Head Noise Injection: Our second proposed methodology takes advantage of the multi-headed transformer design. We add different Gaussian random noise to each head, such that the heads will receive different inputs. Therefore, the model must become more robust to different inputs.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h4 id=&quot;cross-head-methods-1&quot;&gt;Cross-Head Methods&lt;/h4&gt; &lt;h5 id=&quot;decorrelation&quot;&gt;Decorrelation&lt;/h5&gt; &lt;p&gt;We propose adding a decorrelation term to our loss function. The goal of this loss is the reward differences across attention heads. We begin by calculating the self-attention weights for all of the attention heads. We then compute the pairwise dot products of each head’s attention weights. This will increase the loss if there are heads that are highly correlated. This will cause the heads of the network to differ from the other heads in the network and hopefully generalize better. Therefore, we use the following loss term.&lt;/p&gt; &lt;p&gt;$ \color{white} \text{Added Loss} = \sum_{i={0,…,H},j={i+1,…,H}} \frac{\text{sum}((\Lambda_i^T \Lambda_j)^2)}{\text{Number of elements in }\Lambda_i^T \Lambda_j}$, where H is the number of heads and $ \color{white} \Lambda_i$ is the ith attention head weights.&lt;/p&gt; &lt;p&gt;This method is inspired by another method, multi-head attention with disagreement regularization &lt;d-cite key=&quot;li2018multi&quot;&gt;&lt;/d-cite&gt;. However, the disagreement regularization method relies on calculating more differences than just the attention weight matrices, which is out the of scope of these experiments.&lt;/p&gt; &lt;h5 id=&quot;normalization&quot;&gt;Normalization&lt;/h5&gt; &lt;p&gt;We propose adding the 2-norm of all elements in the attention weight matrix to the loss function to limit the emphasis of individual inputs. Therefore, this will smooth the weights and reward more uniform predictions. This should reduce overfitting and make the model more generalizable. We calculate this norm using $ \color{white} \frac{\text{torch.linalg.norm(attention weights)}}{\text{number of elements in attention weights}} $. This computes the 2-norm of all elements across attention heads and adds more loss to weights that emphasize specific inputs more than others. Therefore, this should add smoothing to the weights.&lt;/p&gt; &lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt; &lt;p&gt;We train each model for 25 epochs on the full training set with a batch size of 256. We use the AdamW optimizer, with a learning rate of 0.001. We use the following parameters for our vision transformer.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;n_channels&lt;/th&gt; &lt;th&gt;nout&lt;/th&gt; &lt;th&gt;img_size&lt;/th&gt; &lt;th&gt;patch_size&lt;/th&gt; &lt;th&gt;dim&lt;/th&gt; &lt;th&gt;attn_dim&lt;/th&gt; &lt;th&gt;mlp_dim&lt;/th&gt; &lt;th&gt;num_heads&lt;/th&gt; &lt;th&gt;num_layers&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;CIFAR-10&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;128&lt;/td&gt; &lt;td&gt;64&lt;/td&gt; &lt;td&gt;128&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CIFAR-100&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;128&lt;/td&gt; &lt;td&gt;64&lt;/td&gt; &lt;td&gt;128&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We train each model individually on both datasets.&lt;/p&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;cifar-10&quot;&gt;CIFAR-10&lt;/h3&gt; &lt;p&gt;We begin by analyzing the training results on the CIFAR-10 dataset.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Training Loss on the CIFAR-10 Dataset &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Training Accuracy on the CIFAR-10 Dataset &lt;/div&gt; &lt;p&gt;We see that most of the models, except for the dropout based models, achieve near zero error and perfect accuracy on the test set. Therefore, we see that the dropout term is stopping the model from perfectly memorizing the dataset but all other regularization techniques are not forcing the model to change the weights enough to prevent perfect accuracy.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Test Loss on the CIFAR-10 Dataset &lt;/div&gt; &lt;p&gt;Looking at the test results, the two dropout models have much lower loss achieved on the test dataset. The rest of the models have similar losses on the test dataset.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Test Accuracy on the CIFAR-10 Dataset &lt;/div&gt; &lt;p&gt;We see that the two dropout methods also have higher accuracy than the model without regularization. However, the decorrelation model has the highest test accuracy. Overall, the test dataset results are significantly lower than state of the art and a more advanced model may be needed to achieve better performance.&lt;/p&gt; &lt;h3 id=&quot;cifar-100&quot;&gt;CIFAR-100&lt;/h3&gt; &lt;p&gt;We move on to training and testing the models on the CIFAR-100 dataset. This dataset has more classes and therefore fewer examples of each class. Therefore, the model finds it more difficult to generalize on the test dataset.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss100-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss100-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss100-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_loss100.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Training Loss on the CIFAR-100 Dataset &lt;/div&gt; &lt;p&gt;Again, we see that all methods except the dropout based methods achieve near-zero error.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy100-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy100-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy100-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/training_accuracy100.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Training Accuracy on the CIFAR-100 Dataset &lt;/div&gt; &lt;p&gt;We see similar results to the CIFAR-10 dataset in training. The two dropout methods are unable to achieve perfect loss and accuracy but all other methods are able to. This includes the methods with added loss, that being the normalization method and the decorrelation method. This will depend on the parameters of the model and these models would have higher loss if we used more emphasis on the added loss.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss100-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss100-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss100-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_loss100.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Test Loss on the CIFAR-100 Dataset &lt;/div&gt; &lt;p&gt;We see that the two dropout methods have significantly lower loss on the test dataset, with all other methods performing similarly.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy100-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy100-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy100-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-06-attention-regularization/test_accuracy100.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Test Accuracy on the CIFAR-100 Dataset &lt;/div&gt; &lt;p&gt;We again see consistent results with the CIFAR-10 dataset. The two dropout methods and decorrelation improve the accuracy on the test set, while the others are about the same as without normalization. In this case, the drophead method performs the best.&lt;/p&gt; &lt;h2 id=&quot;further-research&quot;&gt;Further Research&lt;/h2&gt; &lt;p&gt;Further research is needed to further improve the generalizability of this transformer architecture for these datasets. The model still has overfitting issues, even with high regularization and so more research with different architectures or regularization methods is needed to improve the study. Further comparison of regularization methods on alternative datasets and types of data, such as text, would also be valuable to look at.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Regularization is an important tool to reduce overfitting and improve the generalizability of a model. The results show that adding various forms of regularization can improve the results of a model, but our implementations did not cause dramatic change to the ability of the model to generalize to the test set. Most of the models still had a very large gap between their training accuracy and test accuracy. However, we did see notable improvements for both the dropout models and the decorrelation model. The dropout models were the only models that added regularization such that the model could not perfectly memorize the training set. Therefore, their training accuracy was significantly lower but they also had higher test accuracy. Additionally, the decorrelation model was also successful. While the model followed a similar pattern during training to the model without regularization, the test accuracy was generally higher, suggesting the added error did force the model to learn different parameters. Therefore, based on these results, adding regularization can be helpful in improving the generalizability of transformer models, especially when they have limited data. The other methods, such as the noise based methods, normalization, and relaxation did not appear to have a significant effect on training or test outputs. It is likely that alternative parameters or architectures are needed to realize their effect. Lastly, while this analysis was only completed using vision transformers, different datasets or network architectures may have significantly different results. Therefore, these other regularization methods may be more successful in other contexts. However, these tests prove that there are circumstances in which regularization can have a beneficial effect on transformer performance and is therefore a worthwhile experiment when dealing with overfitting transformers.&lt;/p&gt; </content> </entry> <entry> <title>Neural PDEs for learning local dynamics and longer temporal rollouts</title> <link href="https://deep-learning-mit.github.io/blog/2023/neural-PDEs-long-time-dynamics/"/> <updated>2023-11-05T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/neural-PDEs-long-time-dynamics</id> <content type="html">&lt;h2 id=&quot;partial-differential-equations&quot;&gt;Partial differential equations&lt;/h2&gt; &lt;p&gt;At the continuum level, spatiotemporal physical phenomena such as reaction-diffusion processes and wave propagations can be described by partial differential equations (PDEs). By modeling PDEs, we can understand the complex dynamics of and relationships between parameters across space and time. However, PDEs usually do not have analytical solutions and are often solved numerically using methods such as the finite difference, finite volume, and finite element methods &lt;d-cite key=&quot;LoggMardalEtAl2012&quot;&gt;&lt;/d-cite&gt;. For example, the finite element method (FEM) approximates PDE solutions by first discretizing a sample domain into a mesh of interconnected elements and then solving a system of equations iteratively given a set of boundary conditions, initial conditions, and material properties.&lt;/p&gt; &lt;p&gt;In this blog, we will show two examples of PDEs, one of which is the Navier-Stokes equation which describes the dynamics of viscous fluids. The equation below shows the 2D Navier-Stokes equation for a viscous and incompressible fluid in vorticity form on a unit torus, where \(w\) is the vorticity, \(u\) the velocity field, \(\nu\) the viscosity coefficient, and \(f(x)\) is the forcing function. The solution data were from the original paper&lt;d-cite key=&quot;li2020fourier&quot;&gt;&lt;/d-cite&gt; where the problem, with a periodic boundary condition, was solved with a pseudospectral method using a 1e-4 time step with the Crank-Nicolson scheme.&lt;/p&gt; \[\begin{gather} \partial_t w(x, t) + u(x, t) \cdot \nabla w(x, t) = \nu \Delta w(x, t) + f(x), \quad x \in (0,1)^2, t \in [0,T] \\ \nabla \cdot u(x, t) = 0, \quad x \in (0,1)^2, t \in [0,T] \\ w(x, 0) = w_0(x), \quad x \in (0,1)^2 \end{gather}\] &lt;p&gt;We can visualize the 2D PDE solution over the 50 time steps:&lt;/p&gt; &lt;div class=&quot;l-body-outset&quot; style=&quot;display: flex; justify-content: center; align-items: center;&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-05-neural-PDEs-long-time-dynamics/navierstokes.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Solution of 2D Navier-Stokes PDE &lt;d-cite key=&quot;li2020fourier&quot;&gt;&lt;/d-cite&gt; - drag the slider! &lt;/div&gt; &lt;h3 id=&quot;motivations-for-neural-pdes&quot;&gt;Motivations for neural PDEs&lt;/h3&gt; &lt;p&gt;Well-established numerical methods are very successful in calculating the solutions of PDEs, however, these methods require high computational costs especially for high spatial and temporal resolutions. Furthermore, it is important to have fast and accurate surrogate models that would target problems that require uncertainty quanitifcation, inverse design, and PDE-constrained optimizations. In recent years, there have been growing interests in neural PDE models that act as a surrogate PDE solver&lt;d-cite key=&quot;PDEBench2022&quot;&gt;&lt;/d-cite&gt;, especially neural operators that aim to learn the mapping between input and output solution functions. These models are trained on numerical solutions from existing methods and inferences are orders of magnitude faster than calculating the solutions again through numerical methods.&lt;/p&gt; &lt;p&gt;In this article, I will first examine if we can apply neural networks to learn the dynamics in PDE solutions and therefore replace PDE solvers with a neural PDE as the surrogate solver. We will start with a base U-Net model with convolutional layers. Next, I will examine the neural operator methods, notably the Fourier Neural Operator (FNO). Primarily, the Fourier neural operator has proven to predict well for PDE solutions and we will use it to compare with the U-Net model on the representations learnt in the Fourier layers. Next, I will examine the FNO’s performance on another PDE with two dependent states. We will notice that the FNO is capable of learning lower frequency modes but fail to learn local dynamics and higher frequency modes. We then finally introduce some improvements to the FNO to tackle this problem involving local dynamics and long term rollout errors.&lt;/p&gt; &lt;h3 id=&quot;dataset-and-training-schemes-for-the-2d-navier-stokes-pde&quot;&gt;Dataset and training schemes for the 2D Navier-Stokes PDE&lt;/h3&gt; &lt;p&gt;For the dataset, I will start with the 2D time-dependent Navier-Stokes solution (\(\nu\) = 1e-3) that was shipped from Zongyi Li et al’s paper &lt;d-cite key=&quot;li2020fourier&quot;&gt;&lt;/d-cite&gt;. The problem for any given model would then be to learn the mapping from an input solution (vorticity) of t=[0,10] to the solution of t=(10, 40]. For all models involving Navier-Stokes, the original implementations were used, but implementations were improved or new ones were added for the second PDE problem which more details will be shared in later parts of the article. We use 1000 solutions for training and 200 for the test dataset. The models are trained with 500 epochs with an initial learning rate of 0.001, the AdamW optimizer is used with a cosine annealing scheduler. Unless otherwise specified, a relative L2 loss is used for training and prediction of each data batch. For U-Net and FNO2D, the models use 2D convolutions in the spatial domain and recurrently predict through the time domain (autoregressive training). For FNO3D, the time domain is included as the 3rd dimension in the input data for the FNO to learn both spatial and temporal dependencies within the solutions.&lt;/p&gt; &lt;h2 id=&quot;base-model-u-net&quot;&gt;Base model (U-Net)&lt;/h2&gt; &lt;p&gt;Let’s begin with examining whether a U-Net with convolutional layers can be used to learn the dynamics. U-Net&lt;d-cite key=&quot;RonnebergerFB15&quot;&gt;&lt;/d-cite&gt; is a popular model architecture for image to image predictions and image segmentation tasks. It consists of a series of downsampling and upsampling layers with skip connections, and my re-implementation is based on &lt;a href=&quot;https://github.com/khassibi/fourier-neural-operator/blob/main/UNet.py&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We can use the U-Net to learn the features from the input PDE solution frames and predict the solution in the next time step, treating the 2D solution as an image. As for the time component, the surrogate model takes the input solution from the previous k time steps to predict solution in the next k+1 time step. Then, the solution from the previous k-1 steps are concatenated with the predicted solution as the input back into the model to predict the next step, and so on. In a nutshell, the model is trained to predict autoregressively.&lt;/p&gt; &lt;div style=&quot;text-align: center; margin-right: 10px;&quot;&gt; &lt;div style=&quot;width: 70%; margin: auto;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_train_test_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_train_test_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_train_test_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_train_test_loss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;Training curve for U-Net with average relative L2 train and test loss&lt;/p&gt; &lt;/div&gt; &lt;div style=&quot;text-align: center; margin-left: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_2dt_nspred42.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_2dt_nspred42.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_2dt_nspred42.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/unet_2dt_nspred42.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;U-Net&apos;s prediction of 2D Navier-Stokes for unseen test set (id=42)&lt;/p&gt; &lt;/div&gt; &lt;p&gt;The U-Net seems to predict well for the 2D Navier-Stokes test set. However, the average final test loss of 0.0153 is still considerably high. For longer time rollout, the errors can accumulate. Let’s examine the FNO2d-t and FNO3d models next.&lt;/p&gt; &lt;h2 id=&quot;fourier-neural-operators&quot;&gt;Fourier Neural Operators&lt;/h2&gt; &lt;p&gt;Fourier neural operators (FNOs) &lt;d-cite key=&quot;li2020fourier&quot;&gt;&lt;/d-cite&gt; try to learn the mapping between input functions and solution functions &lt;d-cite key=&quot;kovachki2021neural&quot;&gt;&lt;/d-cite&gt;, for example, mapping the solutions from earlier to later time steps for time-dependent PDEs.&lt;/p&gt; &lt;p&gt;The authors introduced the Fourier layer (SpectralConv2d for FNO2d) which functions as a convolution operator in the Fourier space, and complex weights are optimized in these layers. The input functions are transformed to the frequency domain by performing fast Fourier transforms (torch.fft) and the output functions are then inverse transformed back to the physical space before they are passed through nonlinear activation functions (GeLU) to learn nonlinearity. Fourier transformations are widely used in scientific and engineering applications, such as in signal processing and filtering, where a signal / function is decomposed into its constituent frequencies. In the FNO, the number of Fourier modes is a hyperparameter of the model - the Fourier series up till the Fourier modes are kept (i.e. lower frequency modes are learnt) while higher frequency modes are truncated away. Notably, since the operator kernels are trained in the frequency domain, the model is theoretically capable of predicting solutions that are resolution-invariant.&lt;/p&gt; &lt;h3 id=&quot;applying-fno2d-and-fno3d-on-2d-navier-stokes-time-dependent-pde&quot;&gt;Applying FNO2D and FNO3D on 2D Navier-Stokes time-dependent PDE&lt;/h3&gt; &lt;p&gt;We reimplement and train the FNO2D model on the same train-test data splits for the 2D Navier-Stokes solution. Notably, the final average relative L2 loss (for test set) is 0.00602 after 500 epochs of training. Comparing this with the U-Net that is also trained and predicted with the same scheme, the FNO2D has an improved performance!&lt;/p&gt; &lt;div style=&quot;text-align: center; margin-left: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/2dt_nspred42.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/2dt_nspred42.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/2dt_nspred42.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/2dt_nspred42.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;FNO2D&apos;s prediction of 2D Navier-Stokes for unseen test set (id=42)&lt;/p&gt; &lt;/div&gt; &lt;p&gt;The predicted solutions look impressive and it seems like the dynamics of the multiscale system are learnt well, particularly the global dynamics. Likewise, the FNO3D gives similar results. Instead of just convolutions over the 2D spatial domains, the time-domain is taken in for convolutions in the Fourier space as well. According to the authors, they find that the FNO3D gives better performance than the FNO2D for time-dependent PDEs. However, it uses way more parameters (6560681) compared to FNO2D (928661 parameters) - perhaps the FNO2D with recurrent time is sufficient for most problems.&lt;/p&gt; &lt;div style=&quot;text-align: center; margin-right: 10px;&quot;&gt; &lt;div style=&quot;width: 70%; margin: auto;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/train_test_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/train_test_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/train_test_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/train_test_loss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;Training curve for FNO3D with average relative L2 train and test loss&lt;/p&gt; &lt;/div&gt; &lt;div style=&quot;text-align: center; margin-left: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/nspred42.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/nspred42.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/nspred42.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/nspred42.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;FNO3D&apos;s prediction of 2D Navier-Stokes for unseen test set (id=42)&lt;/p&gt; &lt;/div&gt; &lt;h3 id=&quot;representation-learning-in-the-fourier-layers&quot;&gt;Representation learning in the Fourier layers&lt;/h3&gt; &lt;p&gt;You might be curious how the Fourier layers learn the Navier-Stokes dynamics - let’s examine some weights in the SpectralConv3d layers (for the FNO3D). We take the magnitudes of the complex weights from a slice of each layer (4 Fourier layers were in the model).&lt;/p&gt; &lt;div style=&quot;text-align: center; margin-right: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fourierlayers-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fourierlayers-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fourierlayers-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fourierlayers.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;Visualizing weights in the Fourier layers&lt;/p&gt; &lt;/div&gt; &lt;p&gt;There seems to be some global features that are learnt in these weights. By learning in the Fourier space, the Fourier layers capture sinusoidal functions that can generalise better for dynamics according to the dynamical system’s decomposed frequency modes. For CNNs, we know that the convolutions in spatial domain would lead to the learning of more local features (such as edges of different shapes), as compared to more global features learnt in Fourier layers.&lt;/p&gt; &lt;h3 id=&quot;on-the-importance-of-positional-embeddings&quot;&gt;On the importance of positional embeddings&lt;/h3&gt; &lt;p&gt;In FNO implementations, besides the input data for the 2D + time domains, the authors also append positional encodings for both x and y dimensions so the model knows the location of each point in the 2D grid. The concatenated data (shape = (B, x, y, 12)) is then passed through the Fourier layers and so on (note: B is the batch size, x and y the spatial sizes, and 12 consists of 10 t steps and 2 channels for positional encodings along x and y). It is important to understand that the positional embedding is very important to the model performance.&lt;/p&gt; &lt;div style=&quot;display: flex; justify-content: center; align-items: center;&quot;&gt; &lt;div style=&quot;text-align: center; margin-right: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/show_dxdt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/show_dxdt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/show_dxdt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/show_dxdt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;Original with positional encoding&lt;/p&gt; &lt;/div&gt; &lt;div style=&quot;text-align: center; margin-left: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/noposencoding_dxdt-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/noposencoding_dxdt-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/noposencoding_dxdt-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/noposencoding_dxdt.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;No positional encoding&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;We train the same FNO3D on the same data but this time without the positional encodings concatenated as the input. Simply removing these positional encodings for x and y domains cause the model to underperform. Here, we are comparing between FNO3D with and without positional encoding. FNO3D has a final relative test loss of 0.0106 but the test loss is 0.0167 without positional encodings. Inspecting the change of x over t for a sample test dataset, it then becomes more visible the differences in performances. Note that we also observe the data have well-defined sinusoidal functions in the dynamics.&lt;/p&gt; &lt;h2 id=&quot;improving-accuracies-in-predicting-local-dynamics-and-long-term-rollouts-in-time-dependent-pdes&quot;&gt;Improving accuracies in predicting local dynamics and long-term rollouts in time-dependent PDEs&lt;/h2&gt; &lt;p&gt;Let’s apply the FNO to other PDEs, particularly problems where local dynamics and long-term accuracies are important. Here, I introduce another PDE as an example - a coupled reaction heat-diffusion PDE with two dependent states&lt;d-cite key=&quot;Robertson2018&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; \[\begin{gather} \kappa \frac{\partial^2 T}{\partial x^2} + \rho H_r \frac{\partial \alpha}{\partial t} = \rho C_p \frac{\partial T}{\partial t} \\ \frac{\partial \alpha}{\partial t} = A \exp \left( -\frac{E}{RT} \right) f(\alpha) \end{gather}\] &lt;p&gt;Based on the initial conditions of temperature (T) and degree of cure (alpha) and with Dirichlet boundary conditions on one end of the sample, the T and alpha propagate across the domain (here, the 1D case is examined). For certain material parameters and when initial conditions of T and alpha are varied, we can see that the dynamics can become chaotic after some time - we can visualize it below.&lt;/p&gt; &lt;div class=&quot;l-body-outset&quot;&gt; &lt;iframe src=&quot;/staging/assets/html/2023-11-05-neural-PDEs-long-time-dynamics/unstablefromp.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;750px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Solution of the above coupled PDE with 2 dependent states, solved using FEM. Drag the slider! &lt;/div&gt; &lt;p&gt;For this dataset, we aim to use the first 10 time steps of the solution (heat diffusion from x=0) as input to a neural PDE to predict the next N time steps of the solution. With 10 steps, we predict the 11th step and the prediction is concatenated with the last 9 steps to predict the next time step and so on. We first generate the training data by solving the PDE numerically using the Finite Element Method using the FEniCS package. Specifically, we use mixed finite elements with the continuous Galerkin scheme and a nonlinear solver with an algebraic multigrid preconditioner.&lt;/p&gt; &lt;p&gt;We use 1228 solutions for the training set and 308 solutions for the test set. The datasets are split into pairs of 10 trajectories, whereby the input data consists the solution of 10 time steps and the output data (to be predicted) consists the solution of the next 10 time steps. Since the neural PDE is trained to predict 10 to 1 time step, every batch is trained autoregressively and an L2 loss is taken for all 10 forward predictions before the sum is backpropagated in every batch. Likewise, the AdamW optimizer is used with an initial learning rate of 1e-4 and a cosine annealing scheduler. The models are trained for 300 epochs with a batch size of 16.&lt;/p&gt; &lt;p&gt;I initially tried the FNO1D implementation on my PDE dataset and notice that the errors accummulate with longer time rollouts using the trained model. FNO1D is used since we only have 1 spatial dimension in the 1D solution and the solutions are predicted recurrently, just like the use of FNO2D for the 2D Navier-Stokes example earlier. The FNO2D model was also used to convolve over both x and t. Both performances are not ideal within 1 cycle of forward prediction.&lt;/p&gt; &lt;h3 id=&quot;revin-and-other-training-tricks-to-improve-accuracies-in-longer-temporal-rollout&quot;&gt;RevIN and other training tricks to improve accuracies in longer temporal rollout&lt;/h3&gt; &lt;p&gt;To overcome this problem, there have been attempts to generally improve the accuracies of neural PDE models and also training tricks proposed to improve long-term accuracies in rollout. Using the FNO1D, I first tested out some training tricks, such as the pushforward and temporal bundling which are covered in the paper on message passing neural PDEs&lt;d-cite key=&quot;brandstetter2022message&quot;&gt;&lt;/d-cite&gt;. Incorporating the reversible instance normalization layer (RevIN)&lt;d-cite key=&quot;kim2022reversible&quot;&gt;&lt;/d-cite&gt; gives a more promising improvement. With ReVIN, the input solution is passed through the normalizing RevIN layer before the FNO1d and the output solution is denormalized through the RevIN layer. Some examples of these tricks for longer term robust forecasting are covered in &lt;a href=&quot;https://github.com/Rui1521/Symmetry-Tutorial/blob/main/Tutorial_Symmetry.ipynb&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Using a trained FNO1D with a RevIN layer, here is its prediction on an unseen test set starting from the first 10 time steps as the input solution. The true solution is used to predict up till 50 more time steps forward (5 full cycles forward). While the temperature is predicted with decent accuracies for first cycle (10 steps forward until t=60 shown), the errors accumulate over more steps.&lt;/p&gt; &lt;div style=&quot;text-align: center; margin-left: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt156.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt156.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt156.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt156.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;FNO1d&apos;s prediction (1)&lt;/p&gt; &lt;/div&gt; &lt;p&gt;Generally, we attribute this to the fact that the Fourier layers may not be able to learn more local changes in the dynamics since the higher frequency modes in the Fourier series are truncated away. The global dynamics of the propagating front (heat diffusion along x) are captured reasonably well (the positional encodings probably also have a large part to play). We want to build on the FNO to improve predictions for longer temporal rollout especially for multiscale dynamical systems with both global and local changes. Ideally, we want to take an input of a few time steps from a more expensive numerical solver and pass it through a trained surrogate model to predict N time steps (with N being as high as possible).&lt;/p&gt; &lt;h2 id=&quot;introducing-large-kernel-attention&quot;&gt;Introducing Large Kernel Attention&lt;/h2&gt; &lt;p&gt;To overcome the problems highlighted for this PDE, we attempt to include a large kernel attention layer (LKA) that was introduced in the Visual Attention Network paper&lt;d-cite key=&quot;guo2022visual&quot;&gt;&lt;/d-cite&gt; by Meng-Hao Guo et. al. The large kernel attention was first introduced as an alternative to the Vision Transformers (ViT) to enable higher spatial adapativity and long-range correlations. While simple to implement, the authors’ Visual Attention Network surpasses ViTs and CNNs on tasks such as object detection and pose estimation. A similar strategy was introduced last month&lt;d-cite key=&quot;zhao2023local&quot;&gt;&lt;/d-cite&gt; for 2D problems although their implementation was not shared.&lt;/p&gt; &lt;p&gt;Therefore, it may be feasible to introduce attention mechanisms to learn local dynamics in PDEs better, and this can complement the Fourier layers which capture global dynamics better. Herein, we add the LKA layers after the Fourier blocks for the FNO1D, and the new model has 5056 more parameters (583425 to 588481). The performance is found to have greatly improved, especially for local dynamics in the unstable propagations.&lt;/p&gt; &lt;div style=&quot;text-align: center; margin-left: 10px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt156.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt156.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt156.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt156.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;FNO1d + LKA&apos;s prediction (1)&lt;/p&gt; &lt;/div&gt; &lt;p&gt;For the same data, the addition of LKA gave improved accuracies over predictions in the next 50 time steps. We attribute this to the large kernel attention’s ability to focus on local dynamics at specific parts of the spatiotemporal changes. The LKA has 3 components: a spatial depth-wise convolution, a spatial depth-wise dilation long-range convolution, and a channel convolution.&lt;/p&gt; \[\begin{gather} \text{Attention} = \text{Conv}_{1 \times 1}(\text{DW-D-Conv}(\text{DW-Conv}(F))) \\ \text{Output} = \text{Attention} \otimes F \end{gather}\] &lt;p&gt;I adapted from the LKA’s &lt;a href=&quot;https://github.com/Visual-Attention-Network/VAN-Classification/blob/main/models/van.py&quot;&gt;original implementation&lt;/a&gt; to apply to our 1D PDE. Let’s examine the predictions on another test data.&lt;/p&gt; &lt;div style=&quot;display: flex; justify-content: center; align-items: center;&quot;&gt; &lt;div style=&quot;text-align: center; margin-left: 2px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt876.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt876.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt876.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/fnorevin_fromp1dt876.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;FNO1d&apos;s prediction (2)&lt;/p&gt; &lt;/div&gt; &lt;div style=&quot;text-align: center; margin-left: 2px;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt876.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt876.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt876.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-05-neural-PDEs-long-time-dynamics/LKAfno_fromp1dt876.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p style=&quot;margin-top: 0px;&quot;&gt;FNO1d + LKA&apos;s prediction (2)&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;While the predictions are significantly improved, the errors still accumulate with longer rollouts and the model fails to capture dynamics if we extend predictions till 100 steps forward. More work is needed to improve existing neural PDE methods before they can be used as foundational models for PDEs.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we have introduced the use of neural networks as potential surrogate model solvers for partial differential equations that can be expensive to solve using numerical methods. Compared to the base model U-Net, Fourier neural operators have introduced a novel and useful way of learning PDE solutions through convolutions in the frequency space. We first reimplemented the FNO2D and FNO3D on the 2D Navier-Stokes PDE solution shipped with their paper. While it achieves great performance learning global dynamics, existing models struggle to capture local dynamics (higher frequency modes are truncated away) and longer temporal rollouts. We demonstrate that despite adding a RevIN layer and several temporal training tricks, the FNO1D could not predict accurately the solutions of a coupled time-dependent PDE. With the inclusion of attention mechanism through the large kernel attention, the FNO1D’s performance significantly improved. We learn that introducing spatial attention can be useful and more work will be explored to improve predictions of multiscale spatiotemporal dynamical systems.&lt;/p&gt; </content> </entry> <entry> <title>Graph neural networks v.s. transformers for geometric graphs</title> <link href="https://deep-learning-mit.github.io/blog/2023/proposal/"/> <updated>2023-11-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2023/proposal</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Machine learning on graphs is often approached with message passing graph neural network (GNN) models, where nodes in the graph are embedded with aggregated messages passed from neighboring nodes &lt;d-cite key=&quot;zhou2020graph&quot;&gt;&lt;/d-cite&gt;. However, with the significant success of transformers in language modelling &lt;d-cite key=&quot;vaswani2017attention&quot;&gt;&lt;/d-cite&gt; and computer vision recently &lt;d-cite key=&quot;dosovitskiy2020image&quot;&gt;&lt;/d-cite&gt;, there are a growing number of transformers developed for graphs as well. In this project we investigate the application of graph neural networks compared to transformers on geometric graphs defined on point clouds. We aim to explore the performance of these two models on predicting the binding affinity for a protein-ligand interaction given the atomic coordinates of the docked protein-ligand structure, which is a highly relevant task in drug discovery. This blog post walks through an introduction into graph neural networks and transformers on molecules, our model architecture, experimental results, and a discussion comparing the two architectures.&lt;/p&gt; &lt;h2 id=&quot;background-and-relevant-work&quot;&gt;Background and relevant work&lt;/h2&gt; &lt;h3 id=&quot;graph-neural-networks-on-molecules&quot;&gt;Graph neural networks on molecules&lt;/h3&gt; &lt;p&gt;Graphs are comprised of nodes and edges, and we can model any set of objects with a defined connectivity between them as a graph. For example, social networks are a set of people and the connectivity between them is defined by on who knows who. We can also see that grid data formats, like images, are also graphs where each pixel is a node and edges are defined to the adjacent pixels. Any sequential data, such as text, can be modeled as a graph of connected words. In this section we focus on graphs of molecules where nodes are atoms and edges are defined between atoms. These edges are often defined by the molecular bonds, or for atoms with 3D coordinate information the edges can be defined by a spatial cutoff $d$ based on the Euclidean distance between nodes. Given a graph we can use a graph neural network to learn a meaningful representation of the graph and use these representations for predictive tasks such as node-level prediction, edge-level prediction, or graph-level prediction. Graph neural networks learn through successive layers of message passing between nodes and their neighboring nodes.&lt;/p&gt; &lt;p&gt;An important property of many GNNs applied on 3D molecules is SE(3)-equivariance. This means that any transformation of the input in the SE(3) symmetry group–which includes all rigid body translations and rotations in $\mathbb{R}^3$ –will result in the same transformation applied to the output. This property is important for the modelling of physical systems; for example if the prediction task is the force applied on an atom in a molecule, rotation of the molecule should result in the model predicting the same forces but rotated. In some tasks we do not need equivariance but rather SE(3)-invariance (which is a subset of SE(3)-equivariance) where any transformation of the input in the SE(3) symmetry group results in the same output. This is often the case when the task of the model is to predict a global property of the molecule which should not change if all 3D coordinates of the molecule are translated and rotated. SE(3)-invariance will be required for our model of binding affinity as global rotations and translations of the protein-ligand structure should yield the same predicted binding affinity.&lt;/p&gt; &lt;p&gt;Early SE(3)-equivariant GNNs on point clouds used directional message passing &lt;d-cite key=&quot;gasteiger2020directional&quot;&gt;&lt;/d-cite&gt; which used the pairwise distance and direction between nodes as features for the GNN, however they were soon shown to be limited in expressivity &lt;d-cite key=&quot;garg2020generalization&quot;&gt;&lt;/d-cite&gt;. Now state-of-the-art (SOTA) models in this area are based on higher order geometric properties such as dihedral angles and representations in the geometric group SO(3). Some examples include GemNet &lt;d-cite key=&quot;gasteiger2021gemnet&quot;&gt;&lt;/d-cite&gt; and e3nn &lt;d-cite key=&quot;geiger2022e3nn&quot;&gt;&lt;/d-cite&gt;. e3nn has also shown that it is much more data-efficient when learning as the model does not need to learn to be equivariant, which non-equivariant models do. For a non-equivariant model to learn to be equivariant it would have to be trained on many SE(3) transformations of the input mapping to the same output, which is very inefficient. e3nn models have led to exceptional performance for tasks related to predicting molecular forces and energies &lt;d-cite key=&quot;batzner20223&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;musaelian2023learning&quot;&gt;&lt;/d-cite&gt;. For the task of binding affinity some GNNs that achieve high performance using GNNs are ProNet &lt;d-cite key=&quot;wang2022learning&quot;&gt;&lt;/d-cite&gt; and HoloProt &lt;d-cite key=&quot;somnath2021multi&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;graph-transformers-on-molecules&quot;&gt;Graph transformers on molecules&lt;/h3&gt; &lt;p&gt;With the proliferation of transformers in the broader field of machine learning, this has also led to the development of graph transformers. In a transformer model each node attends to all other nodes in the graph via attention where the query is a projection of the feature vector of a node, and the key and value is the projection of feature vectors of all other nodes. Hence, graph transformers and transformers applied to sequences (e.g. text) are largely similar in architecture. However, differences arise in the positional encodings in a graph transformer as it is defined in relation to other nodes in the graph &lt;d-cite key=&quot;ying2021transformers&quot;&gt;&lt;/d-cite&gt;. For geometric graphs, positional encodings can be applied as a bias term on the attention value of node $u$ on $v$, where the bias is a learned value that is dependent on the distance between the nodes &lt;d-cite key=&quot;zhou2023uni&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;luo2022one&quot;&gt;&lt;/d-cite&gt;. There are also other ways of implementing positional encodings in the form of Laplacian eigenvectors, and random walk diagonals which aim to encode the centrality of each node in the graph &lt;d-cite key=&quot;rampavsek2022recipe&quot;&gt;&lt;/d-cite&gt;. Recently, in an effort to unify different methods to generate structural and positional graph encodings, Liu et al. &lt;d-cite key=&quot;liu2023graph&quot;&gt;&lt;/d-cite&gt; apply a novel pretraining approach with a multiobjective task of learning a variety of positional and structural encodings to derive more general positional and structural encodings. Graph transformers are also achieving SOTA performance for benchmarks on predicting quantum properties of molecules &lt;d-cite key=&quot;zhou2023uni&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;luo2022one&quot;&gt;&lt;/d-cite&gt; and binding affinity &lt;d-cite key=&quot;kong2023generalist&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt; &lt;p&gt;Given the growing application of both GNNs and transformers we aim to compare their performance on the same task of protein-ligand binding affinity prediction. We also aim to compare models as we can see analogies between graph transformers and GNNs, where “message passing” in the graph transformer involves messages from all nodes rather than the local neighborhood of nodes. We view protein-ligand binding affinity prediction as a suitable task to compare the two architectures as there are aspects of both the GNN and graph transformer architecture that would be advantageous for the task: binding affinity is a global prediction task for which the graph transformer may better capture global dependencies, conversely binding affinity is also driven by local structural orientations between the protein and ligand which the GNN may learn more easily.&lt;/p&gt; &lt;h2 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;The input to the model is a set of atoms for the protein pocket $X_{\mathrm{protein}}$ and ligand $X_{\mathrm{ligand}}$, for which we have the atomic identity and the 3D coordinates, and the binding affinity $y$ for the structure.&lt;/li&gt; &lt;li&gt;For the graph neural network we define a molecular graph of the protein ligand structure $G=(V,E)$ where $V$ are the $n$ nodes that represent atoms in the molecule and the edges $E$ are defined between two nodes if their 3D distance is within a radial cutoff $r$. We further define two types of edges: intramolecular edges for edges between nodes within $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$, and intermolecular edges for nodes between $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$.&lt;/li&gt; &lt;li&gt;For the graph transformer it is applied to the whole set of atoms $(X_{\mathrm{protein}}, X_{\mathrm{ligand}})$, and we can use the 3D coordinates of the atoms to derive positional encodings.&lt;/li&gt; &lt;li&gt;Performance is determined by the root mean squared error, Pearson, and Spearman correlation coefficients between true binding affinity and predicted binding affinity.&lt;/li&gt; &lt;/ul&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/problem_definition-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/problem_definition-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/problem_definition-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal/problem_definition.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1. A protein-ligand structure, Protein Data Bank (PDB) entry 1a0q. The protein backbone is shown in blue, and the ligand is shown in green. The model would be given this structure and the objective is to predict the binding affinity of the ligand to the protein. &lt;/div&gt; &lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt; &lt;p&gt;We use the PDBbind dataset for the protein-ligand structures and binding affinity. In addition, for benchmarking we use the benchmark from ATOM3D &lt;d-cite key=&quot;townshend2020atom3d&quot;&gt;&lt;/d-cite&gt; with a 30% and 60% sequence identity split on the protein to better test generalisability of the model. The sequence identity split is based on sequence similarity of proteins in the test and training datasets. The 30% sequence identity split is more challenging are there are more dissimlar proteins in the test set.&lt;/p&gt; &lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt; &lt;h3 id=&quot;graph-neural-network&quot;&gt;Graph neural network&lt;/h3&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/gnn_architecture1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/gnn_architecture1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/gnn_architecture1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal/gnn_architecture1.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2. Overview of the GNN architecture for a graph constructed from a protein-ligand structure. &lt;/div&gt; &lt;p&gt;A graph is constructed from the atomic coordinates of the atoms in the protein pocket $X_{\mathrm{protein}}$ and ligand $X_{\mathrm{ligand}}$ where the nodes are the atoms. Intramolecular edges are defined between nodes within $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$ with a distance cutoff of 3 Å, and intermolecular edges for nodes between $X_{\mathrm{protein}}$ and $X_{\mathrm{ligand}}$ with a distance cutoff of 6 Å. The model architecture is defined as follows:&lt;/p&gt; &lt;p&gt;(1) Initial feature vectors of the nodes are based on a learnable embedding of their atomic elements. The edge features are an embedding of the Euclidean distance between the atomic coordinates. The distance is embedded with a Gaussian basis embedding which is projected with a 2 layer MLP.&lt;/p&gt; &lt;p&gt;(2) We define two types of messages in the GNN, given by the two types of edges, intermolecular messages and intramolecular messages. The architecture used for the two types are messages are the same but the weights are not shared, this is to reflect that information transferred between atoms within the same molecule is chemically different to information transferred between atoms of different molecules. The message passing equation uses the tensor product network introduced by e3nn &lt;d-cite key=&quot;geiger2022e3nn&quot;&gt;&lt;/d-cite&gt;, and our implementation is based on the message passing framework used by DiffDock &lt;d-cite key=&quot;corso2022diffdock&quot;&gt;&lt;/d-cite&gt;. We omit the details of the tensor product network for simplicity but provide the overall method below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/message_passing_eqn-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/message_passing_eqn-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/message_passing_eqn-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal/message_passing_eqn.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt;where node $b$ are the neighbors of node $a$ in $G$ given by intermolecular or intramolecular edges denoted with $t$. The message is computed with tensor products between the spherical harmonic projection with rotation order $\lambda = 2$ of the unit bond direction vector, \(Y^{(\lambda)}({\hat{r}}_{a b})\), and the irreps of the feature vector of the neighbor $h_b$. This is a weighted tensor product and the weights are given by a 2-layer MLP, $\Psi^{(t)}$ , based on the scalar ($\mathrm{0e}$) features of the nodes $h_a$ and $h_b$ and the edge features $e_{ab}$. Finally, $LN$ is layer norm. Overall, the feature vectors of the nodes are updated by intermolecular and intramolecular messages given by the tensor product of feature vectors of intermolecular and intramolecular neighbors and the vector of the neighbor to the node.&lt;/p&gt; &lt;p&gt;(3) After $k$ layers of message passing we perform pooling for the nodes of $X_{\mathrm{protein}}$ and the nodes of $X_{\mathrm{ligand}}$ by message passing to the “virtual nodes” defined by the centroid of the protein and ligand, using the same message passing framework outlined above.&lt;/p&gt; &lt;p&gt;(4) Finally, we concatenate the embedding of the centroid of the protein and ligand and pass this vector to a 3 layer MLP which outputs a singular scalar, the binding affinity prediction.&lt;/p&gt; &lt;h3 id=&quot;graph-transformer&quot;&gt;Graph transformer&lt;/h3&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/graph_transformer_architecture2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/graph_transformer_architecture2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/graph_transformer_architecture2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal/graph_transformer_architecture2.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3. Overview of the graph transformer architecture for a graph constructed from a protein-ligand structure. &lt;/div&gt; &lt;p&gt;The model architecture is as follows:&lt;/p&gt; &lt;p&gt;(1) Initial feature vectors of the nodes are based on a learnable embedding of their atomic elements.&lt;/p&gt; &lt;p&gt;(2) The graph transformer architecture is based on graphormer &lt;d-cite key=&quot;ying2021transformers&quot;&gt;&lt;/d-cite&gt;. Where the input is $H \in \mathbb{R}^{n \times d}$ where $d$ is the hidden dimension and $n$ is the number of nodes. The input is projected by $W_Q \in \mathbb{R}^{d \times d_K}, W_K \in \mathbb{R}^{d \times d_K}, W_V \in \mathbb{R}^{d \times d_V}$. Since graphs have more complex positional information than sequeunces, conventional positional encoding methods used in sequence-based transformers are not applicable to graphs. Positions in a graph are defined relative to all other nodes, thus positional embeddings cannot be added at the node feature vector level but instead are added as a bias to the pairwise node attention matrix. We define $B \in \mathbb{R}^{n \times n}$, where $B_{ij}$ is given by a Gaussian basis embedding of the Euclidean distance $d_{ij}$ between node $i$ and $j$, which is passed to a 3 layer MLP that outputs a singular scalar. Then the self-attention is calculated as $Q = HW_Q, K = HW_K, V = HW_V$ and $A = \frac{QK^T + B}{\sqrt{d_k}}, Attn(H) = Softmax(A) V$. In addition to all atomic nodes, we also add a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cls&amp;gt;&lt;/code&gt; token used in the BERT model which functions as a virtual global node &lt;d-cite key=&quot;devlin2018bert&quot;&gt;&lt;/d-cite&gt;. The distance of this node to all other nodes is a learnable parameter. This process is duplicated across multiple heads and we concatenate the embeddings across all heads after $k$ layers as the updated feature vector.&lt;/p&gt; &lt;p&gt;(3) We take the final embedding of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cls&amp;gt;&lt;/code&gt; node and pass it through a 3 layer MLP which outputs a singular scalar, the binding affinity prediction.&lt;/p&gt; &lt;h3 id=&quot;loss-function&quot;&gt;Loss function&lt;/h3&gt; &lt;p&gt;Both models are trained to minimise the root mean squared error between the predicted binding affinity and true binding affinity.&lt;/p&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;In order for the results to be comparable between the two models, both models have approximately 2.8 million parameters.&lt;/p&gt; &lt;p&gt;GNN model details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2 layers of message passing, number of scalar features = 44, number of vector features = 16. Number of parameters: 2,878,011&lt;/li&gt; &lt;li&gt;4 layers of message passing, number of scalar features = 32, number of vector features = 13. Number of parameters: 2,767,269&lt;/li&gt; &lt;li&gt;6 layers of message passing, number of scalar features = 26, number of vector features = 12. Number of parameters: 2,764,431&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We compare GNNs with different numbers of layers to compare performance across models which learn embeddings from various $k$-hop neighborhoods.&lt;/p&gt; &lt;p&gt;Graph transformer model details: 8 attention heads, 8 layers, hidden dimension = 192, feed forward neural network dimension = 512. Number of parameters: 2,801,155&lt;/p&gt; &lt;p&gt;Both models were trained for 4 hours on 1 GPU with a batch size of 16, Adam optimiser, and a learning rate of $1 \times 10^{-3}$. We show the results for the 30% and 60% sequence-based splits for the protein-ligand binding affinity benchmark in Table 1 and 2 respectively.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Table 1.&lt;/strong&gt; Protein-ligand binding affinity task with 30% sequence based split. ProNet &lt;d-cite key=&quot;wang2022learning&quot;&gt;&lt;/d-cite&gt; is included as the SOTA model in this benchmark.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Root mean squared error $\downarrow$&lt;/th&gt; &lt;th&gt;Pearson correlation coefficient $\uparrow$&lt;/th&gt; &lt;th&gt;Spearman correlation coefficient $\uparrow$&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ProNet &lt;d-cite key=&quot;wang2022learning&quot;&gt;&lt;/d-cite&gt;&lt;/td&gt; &lt;td&gt;1.463&lt;/td&gt; &lt;td&gt;0.551&lt;/td&gt; &lt;td&gt;0.551&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GNN 2 layer&lt;/td&gt; &lt;td&gt;1.625&lt;/td&gt; &lt;td&gt;0.468&lt;/td&gt; &lt;td&gt;0.474&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GNN 4 layer&lt;/td&gt; &lt;td&gt;1.529&lt;/td&gt; &lt;td&gt;0.488&lt;/td&gt; &lt;td&gt;0.477&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GNN 6 layer&lt;/td&gt; &lt;td&gt;1.514&lt;/td&gt; &lt;td&gt;0.494&lt;/td&gt; &lt;td&gt;0.494&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Graph Transformer&lt;/td&gt; &lt;td&gt;1.570&lt;/td&gt; &lt;td&gt;0.476&lt;/td&gt; &lt;td&gt;0.469&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;strong&gt;Table 2.&lt;/strong&gt; Protein-ligand binding affinity task with 60% sequence based split. ProNet &lt;d-cite key=&quot;wang2022learning&quot;&gt;&lt;/d-cite&gt; is included as the SOTA model in this benchmark.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Root mean squared error $\downarrow$&lt;/th&gt; &lt;th&gt;Pearson correlation coefficient $\uparrow$&lt;/th&gt; &lt;th&gt;Spearman correlation coefficient $\uparrow$&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ProNet &lt;d-cite key=&quot;wang2022learning&quot;&gt;&lt;/d-cite&gt;&lt;/td&gt; &lt;td&gt;1.343&lt;/td&gt; &lt;td&gt;0.765&lt;/td&gt; &lt;td&gt;0.761&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GNN 2 layer&lt;/td&gt; &lt;td&gt;1.483&lt;/td&gt; &lt;td&gt;0.702&lt;/td&gt; &lt;td&gt;0.695&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GNN 4 layer&lt;/td&gt; &lt;td&gt;1.471&lt;/td&gt; &lt;td&gt;0.717&lt;/td&gt; &lt;td&gt;0.719&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GNN 6 layer&lt;/td&gt; &lt;td&gt;1.438&lt;/td&gt; &lt;td&gt;0.722&lt;/td&gt; &lt;td&gt;0.704&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Graph Transformer&lt;/td&gt; &lt;td&gt;1.737&lt;/td&gt; &lt;td&gt;0.529&lt;/td&gt; &lt;td&gt;0.534&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt; &lt;h3 id=&quot;gnns-perform-better-than-graph-transformers&quot;&gt;GNNs perform better than graph transformers&lt;/h3&gt; &lt;p&gt;From the benchmarking we can see that the graph transformer model performs worse than the GNNs for the 30% and 60% sequence split for protein-ligand binding affinity. An intuitive explanation for why graph transformers perform worse is it may be difficult for the graph transformer to learn the importance of local interactions for binding affinity prediction as it attends to all nodes in the network. Or in other words, because each update of the node involves seeing all nodes, it can be difficult to decipher which nodes are important and which nodes are not. In order to test if this is true, future experiments would involve a graph transformer with a sparse attention layer where the attention for nodes beyond a distance cutoff is 0. Converse to the lower performance of graph transformers, the results show that deeper GNNs which “see” a larger $k$-hop neighborhood perform better. However, we did not push this to the extreme of implementing a GNN with enough layers such that the $k$-hop neighborhood is the whole graph which would be most similar to a graph transformer as it attends to all nodes. This is because very deep GNNs are subject to issues like oversmoothing where all node features converge to the same value &lt;d-cite key=&quot;rusch2023survey&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;The GNN may also perform better than the graph transformer due to the higher order geometric features used by the e3nn GNN message passing framework, compared to the graph transformer which only has relative distances. To further explore this future work will involve implementing the equiformer graph transformer &lt;d-cite key=&quot;liao2022equiformer&quot;&gt;&lt;/d-cite&gt;, which is a graph transformer with higher order geometric features.&lt;/p&gt; &lt;h3 id=&quot;depth-vs-width&quot;&gt;Depth v.s. width&lt;/h3&gt; &lt;p&gt;Deeper GNNs (2 v.s. 4 v.s. 6 layers) with an approximately constant total number of parameters acheived better performance across both protein ligand binding affinity tasks. This was also observed in the image classification field with the development of AlexNet where deeper networks were shown to significantly improve performance &lt;d-cite key=&quot;krizhevsky2012imagenet&quot;&gt;&lt;/d-cite&gt;. In the context of molecular graphs, deeper GNNs allow the nodes to gain more local chemical context as their node embeddings are exposed to a larger $k$-hop neighborhoods. Thus, these node embeddings are more expressive which facilitates better task performance. There is a limit to the advantages of depth, as very deep GNNs experience oversmoothing as mentioned above &lt;d-cite key=&quot;rusch2023survey&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h3 id=&quot;model-performance-vs-graph-size&quot;&gt;Model performance v.s. graph size&lt;/h3&gt; &lt;p&gt;We compared the error of the prediction v.s. the number of atoms in the graph to test the hypothesis if larger graphs are more difficult to make predictions on. However, correlation between error and number of atoms in the graph all yielded very low pearson correlation coefficients ($&amp;lt; 0.1$) for all experiments (Figure 4). Thus, the number of atoms in the graph has minimal effect on the predictive ability of the model. This may suggest why the the graph transformer–which is able to attend to all nodes in the graph–did not perform much better as the GNN performance does not degrade significantly with larger graphs.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/error_vs_size-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/error_vs_size-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-proposal/error_vs_size-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-proposal/error_vs_size.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4. Number of nodes in graph v.s. difference between true and predicted binding affinity for graph transformers and GNNs on the 60% protein-ligand binding affinity task. There is no prominent correlation between model performance and error in prediction. &lt;/div&gt; &lt;h3 id=&quot;future-work&quot;&gt;Future work&lt;/h3&gt; &lt;p&gt;We implemented a relatively simplistic graph transformer in this project. While we concluded for this vanilla implementation of the graph transformer the GNN outperforms the graph transformer there are many more complex graph transformer architectures that we could explore to build more expressive architectures. In this section we explore some possible ideas.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Using cross-attention for better representation of protein-ligand interactions.&lt;/strong&gt; In this project, we adapted the graph transformer from graphormer &lt;d-cite key=&quot;ying2021transformers&quot;&gt;&lt;/d-cite&gt; which was developed originally for predicting the energy of one molecule. However, our task involves two interacting molecules, a protein and a ligand. Thus, graph transformer performance could be lifted if the model had a better understanding of the interactions between the protein and the ligand by using cross attention between the protein and the ligand, rather than self attention across the whole protein-ligand complex.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heirarchical pooling for better representation of amino acids.&lt;/strong&gt; Graph transformer performance could also be lifted by defining better pooling strategies than using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cls&amp;gt;&lt;/code&gt; token from a set of all atoms to predict binding affinity. In this project the graphs were defined based on the atoms in the graph. However, proteins are comprised of an alphabet of 21 amino acids. Thus, it may be easier for the model to learn more generalisable patterns to the test set if the model architecture reflected how proteins are comprised of animo acids which are comprised of atoms. This has been achieved in models using hierarchical pooling from the atom-level to the amino acid-level and finally to the graph-level &lt;d-cite key=&quot;wang2022learning&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;kong2023generalist&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A hybrid approach: GNNs with Transformers.&lt;/strong&gt; Finally, we could improve also performance further by taking a hybrid approach. That is, the GNN first learns local interactions followed by the graph transformer which learns global interactions and pools the node embeddings into a global binding affinity value. The motivation for this design is to leverage the advantages of both models. The GNN excels at learning local interactions while the graph transformer excels at learning global relationships from contextualised local interactions. This approach has been explored in other models for predicting drug-target interaction &lt;d-cite key=&quot;bai2023interpretable&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;jin2023capla&quot;&gt;&lt;/d-cite&gt;. Visualisation of the attention map of graph transformers would also be interesting to explore the importance of specific chemical motifs on protein-ligand interactions.&lt;/p&gt; &lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this project we present a direct comparison of graph transformers to GNNs for the task of predicing protein-ligand binding affinity. We show that GNNs perform better than vanilla graph transformers with the same number of model parameters across protein-ligand binding affinity benchmarks. This is likely due to the importance of capturing local interactions, which graph transformers may struggle to do. We also show that deeper GNNs perform better than wider GNNs for the same number of model parameters. Finally, future work in this area will involve a implementing more complex graph transformers, or taking a hybrid approach where we capture local interactions with a GNN and global interactions with a graph transformer.&lt;/p&gt; </content> </entry> <entry> <title>An empirical evaluation of autoencoders and diffusion models for 2D small-molecule generation</title> <link href="https://deep-learning-mit.github.io/blog/2022/molecule_generation/"/> <updated>2022-12-12T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2022/molecule_generation</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Applying deep learning techniques to 2D molecule generation is an interesting and challenging problem in the field of cheminformatics, with applications in drug discovery, materials science, and other areas of chemistry. The problem is broad in scope, since there is a variety of molecular data, representations of the generated molecules, and model frameworks or generation pipelines. Autoencoders and diffusion models are two major types of generative models. The first learns a latent distribution from actual data points and then samples from this space to produce a novel output. Diffusion models work by progressively adding noise to input data, learning the correspondence between inputs and random noise, and then working backwards from a new sample of random noise by “undoing” the noise.&lt;/p&gt; &lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt; &lt;p&gt;We use the QM9 dataset, described here. This dataset has been used extensively for cheminformatics research. The dataset contains the molecular structures and coordinates (2D and 3D) of ~134,000 organic molecules. Each molecule is represented as a set of atoms with their respective spatial (cartesian) coordinates. The dataset also contains a comprehensive set of chemical properties of each molecule.&lt;/p&gt; &lt;p&gt;We retrieved the SMILE (Simplified Molecular Input Line Entry System) notation for each molecule. The SMILE string uses ASCII characters to describe the atoms, bonds, and connectivity in a molecule, and is a standardized way to convey chemical information in textual form. The RDKit library hosts functionality for moving between SMILE strings and quantitative data (matrices, fingerprint vectors) as well as for visualizing molecules from the SMILE strings.&lt;/p&gt; &lt;p&gt;Finally, we create a secondary, restricted subset of the data that contains only simple, organic molecules by eliminating strings containing the “#” (character representing triple bonds) or elements other than C, H, O, N, P, S. For the models dealing with fingerprints, since it is challenging to go from fingerprint to an explicit representation of a model, our evaluation metric was determining whether or not the generated molecules were, in fact, similar to the chosen “simple” subset of all of the data. For models dealing with adjacency matrices, it was quite easy to determine ‘validity’ of chemical structures visually; the appearance of standard chemical structures, such as rings of 5 and 6 carbons with side-chains, was used as an indication of success.&lt;/p&gt; &lt;h2 id=&quot;autoencoder&quot;&gt;Autoencoder&lt;/h2&gt; &lt;p&gt;A very simple generative approach we can take is to use an autoencoder. Namely, we can train an autoencoder on molecules of interest — like our small-molecule-filtered dataset — and then sample from the learned latent space, decode the sample to generate a “molecule”, and evaluate the success in generation.&lt;/p&gt; &lt;p&gt;As mentioned in the introduction, it is worth considering possible data inputs and the sort of information a generative model trained on different inputs would carry. For our example, we consider the efficacy of RDKFingerprints and graph adjacency matrices as two possible input data types.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RDKFingerprints&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Molecular fingerprints are a commonly used identifier in drug discovery and virtual screening. Different types of fingerprints encode different aspects of a molecule, but they all share the characteristic of preserving features of a molecule in a spatial fashion across a bit vector. A main feature of a fingerprint scheme is that vector similarity (which can be computed in many ways) corresponds to structurally or chemically similar molecules according to the features the fingerprint intends to encode for.&lt;/p&gt; &lt;p&gt;The Python RDKit library hosts functionality for handling two such types of fingerprints — a native RDK fingerprint and a Morgan fingerprint. We use the RDK fingerprint, and our data pipeline looks something like this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;For a given molecule (via smile string) we generate a fingerprint (a 2048-long bit vector)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A set of such fingerprints is used to train an autoencoder (whose structure is a 2048 unit input layer, 2 hidden layers of 64 units activated with ReLU activations)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;We sample from the latent space and use the decoder to produce a set of generated molecules, which we associate to sets of 10 “most similar real molecules” from the original (unfiltered) dataset. Similarity is calculated using the &lt;strong&gt;Tanimoto Distance&lt;/strong&gt;, a notion of similarity between two vectors where the numerator is the number of 1s in common between the bit vectors, and the denominator is the number of 1s overall.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;We compute the percentage of these 10 similar molecules that lie in the small-molecule-filtered dataset to evaluate the success of the autoencoder in understanding the structure of small molecules at the generation step.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This approach has the benefit of using a data source explicitly designed with the goal of similarity; computing close-distance vectors to the generated RDKit fingerprint carries genuine chemical meaning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Adjacency Matrices&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Molecules lend themselves well to graph representations: atoms are like nodes, bonds are like edges. Thus, a molecule, if represented with a graph, can be associated to an adjacency matrix that carries information on interatomic and overarching molecular properties.&lt;/p&gt; &lt;p&gt;Adjacency matrices derived from the graph representation of a molecule, while not explicitly designed with the goal of molecule similarity in mind (as the fingerprint is), are historically successful in chemical deep learning, particularly as they are the workhorse of graph neural networks. The adjacency matrices available in the QM9 dataset can be decomposed into matrices at the single, double, and aromatic bond levels, so they carry a chemical information in additional to structural information. We implement a similar pipeline with adjacency matrix inputs, with a few changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The adjacency matrix for a smile string is computed&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Unliked RDK Fingerprints, which are fixed in length, the size of the adjacency matrix varies with the size of the molecule; this makes use in a fixed-input length-autoencoder difficult, so we apply a padding approach, zero-padding all matrices to the size of the largest molecule’s matrix.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The autoencoder is trained with these flattened, padded matrices.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The generated reconstructions are rearranged into a matrix shape.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The pseudo-adjacency matrix is then associated to a pseudo-molecule and corresponding pseudo-RDK fingerprint. Notably, the pseudo-molecule is created with some assumptions, such as the inclusion of only CHONPS atoms and only single bonds. Like the fingerprint framework, we find molecules in the original set with similar fingerprints to the reconstructed fingerprint, and compute the proportion of top-10 similar molecules that lie in the small-molecule set.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;autoencoder-results--rdk-fingerprints&quot;&gt;Autoencoder Results – RDK Fingerprints&lt;/h2&gt; &lt;p&gt;The first and most notable result is that over repeated trials of sampling and reconstructing from the latent space for both types of data, the proportion of top-10 similar molecules that lie in the small-molecule restricted dataset is 1.0. That is, each of the 10 most similar molecules lies in the small-molecule set in both cases, over 5 batches of 10 samples each.&lt;/p&gt; &lt;p&gt;Some detailed results follow.&lt;/p&gt; &lt;p&gt;First, here is the training curve with loss for the fingerprint autoencoder&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_fngpts-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_fngpts-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_fngpts-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_fngpts.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;An example of top-10 similarity for a sampled and reconstructed pseudo-fingerprint is shown here&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/top_10_similarity_chart-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/top_10_similarity_chart-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/top_10_similarity_chart-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/top_10_similarity_chart.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We notice that all the top-10 most similar molecules seem to be near each other, index-wise. This would make sense if the dataset is organized such that similar molecules share close indices. We can confirm this fact by inspecting a heatmap of 10 samples from a consecutive block in the dataset, like so:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/heatmatp_rdk_with_title-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/heatmatp_rdk_with_title-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/heatmatp_rdk_with_title-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/heatmatp_rdk_with_title.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We can see that indeed, closer molecules in the original dataset have higher similarity, so this result is as expected.&lt;/p&gt; &lt;h2 id=&quot;autoencoder-results---adjacency-matrix&quot;&gt;Autoencoder Results - Adjacency Matrix&lt;/h2&gt; &lt;p&gt;We then inspect the results of the adjacency matrix-based autoencoder training. First, the training curve with loss:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_adj_mat_case-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_adj_mat_case-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_adj_mat_case-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/training_loss_adj_mat_case.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Now, here is a top-10 similarity example for a pseudo-RDK fingerprint from a pseudo-adjacency matrix:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/adj_mat_top_10_similarity-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/adj_mat_top_10_similarity-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/adj_mat_top_10_similarity-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/adj_mat_top_10_similarity.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;We notice first, that the average similarity is much higher in this case, suggesting that even with the extra step of conversion and the assumptions we make about molecular form, the similarities are higher in this case. The second observation is that the top-10 similar indices are spread out farther than they were in the previous case, suggesting that the adjacency matrix to RDK fingerprint conversion moves around the similar molecules.&lt;/p&gt; &lt;p&gt;Finally, we include some photos of molecules generated in this process (we were unable to generate photos in the RDK fingerprint trained autoencoder, because we require an adjacency matrix to draw the molecules, and it is not straightforward to go from fingerprint to matrix):&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.05%20PM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.05%20PM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.05%20PM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.05%20PM.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In the photo above, we can see the lefthand side tail as a recognizable part of an organic molecule, suggesting success with some types of bonds. In the photo below, we see that the autoencoder has learnt some additional aspects beyond basic single bonds (one of the validation images we show further below includes a similar red ring).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.45%20PM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.45%20PM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.45%20PM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.45%20PM.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Finally, the photo below while the least small-molecule-like in appearance, is interesting because it appeared many times in samples of 100 images (around 20 times) despite the latent space adjacency matrices being distinct. This could perhaps have to do with the process of converting from an adjacency matrix of reals (the result of latent space sampling) to an adjacency matrix of 1/0s, which we accomplish with median thresholding.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.59%20PM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.59%20PM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.59%20PM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/Screenshot%202023-12-12%20at%207.33.59%20PM.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;For reference, a sample image from the “validation” true small-molecule dataset is shown below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/validation_molecule_1_ex-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/validation_molecule_1_ex-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/validation_molecule_1_ex-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/validation_molecule_1_ex.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Diffusion Model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;More recently, the use of diffusion models as an approach for generative modeling has become more common; as described in the introduction, denoising diffusion models operate by iteratively adding noise in a Markov manner to samples, learning the correspondence between inputs and the resultant noise, and then reverse-sampling from random noise to generate a new datapoint.&lt;/p&gt; &lt;p&gt;In the past, as seen in the E3 paper, diffusion models have been applied to 3D adjacency matrices. In this case, we adapted an image-based diffusion model to noise and then de-noise data on adjacency matrices by using 2D adjacency matrices instead.&lt;/p&gt; &lt;p&gt;The following plots provide information about the training of the diffusion model on adjacency matrices. First, is a plot of the loss over 5 training epochs at LR 0.001; this model was trained on approximately 90K training samples, so the loss was quite low even after the first epoch:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The efficacy of diffusion models as a means of generating novel adjacency matrices is evident from the following visualizations of our results. First, here are two runs of the denoising process for the diffusion model, first on an extremely limited set of approximately 1000 matrices, and then on the entire 90K dataset. As seen, even with very few inputs, it was possible to identify the emergence of a ‘bright spot’ in the top left, which represents the actual adjacency matrix (which was later encoded into actual matrices).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;In converting these adjacency matrices into actual molecule images, we aimed to visualize the backbones of these molecules (which is most informative as to the overall structure), so instead of focusing on determining atomic identity, we instead labelled all of them as carbons and proceeded.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Notably, in comparison to the molecules created by the autoencoder, these contain more of the structures which are characteristics of organic molecules, such as 5 and 6 carbon rings with molecules (potentially side chains of length &amp;gt;1) coming off. Indeed, it is possible to observe the progressively increased ordering of the adjacency matrices over times (as they become closer and closer to actual molecules), going from extremely disordered to closer and closer to something meaningful.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_4-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_4-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_4-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_4.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;The application of diffusion models to the RDKFingerprints is shown here: for two separate runs, they look like this. Notably, in order to use an image classification network for RDKFingerprints, the fingerprints were stacked into an image which looks like a series of stripes. As evident, the diffusion model was able to produce such striped images, and their simplicity is a good indication that these are indeed good learnings of information about the filtered subset.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_5-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_5-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_5-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-09-molecule_generation/shorna_5.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In this post, we used two different generative models and tested out two different encodings for information about molecular structure. In general, both models were able to learn and reproduce information about the chosen subset, but in general, the diffusion model was better at accurately reproducing molecules with ‘believable’ structures; as evident from the figures above, although the autoencoder did learn and create relatively sparse adjacency matrices, they lacked the hallmarks of small organic molecules (like rings structures). Further, although it was more difficult to discern quantitative information about the ‘accuracy’ of adjacency matrices, since they depend on larger structures than the RDKfingerprints, it was much easier to map adjacency matrices to actual (visualizable) structures. On the whole, the diffusion model was better at actually creating canonical molecular structures. Further, models trained on adjacency matrices, when converted post-generation to RDKFingerprints had higher accuracy, and adjacency matrices were generally easier to conceptualize, so we have preference for this data encoding.&lt;/p&gt; </content> </entry> <entry> <title>VIVformer</title> <link href="https://deep-learning-mit.github.io/blog/2022/VIVFormer/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2022/VIVFormer</id> <content type="html">&lt;h2 id=&quot;introduction--motivation&quot;&gt;Introduction &amp;amp; Motivation&lt;/h2&gt; &lt;p&gt;Vortex induced vibrations (VIV) are vibrations that affect bluff bodies in the presence of currents. VIV are driven by the periodic formation and shedding of vortices in the bodies’ wakes which create an alternating pressure variation causing persistent vibrations &lt;d-cite key=&quot;triantafyllou2016vortex&quot;&gt;&lt;/d-cite&gt;. The vibration amplitude in VIV is typically moderate, not exceeding about one to two body diameters &lt;d-cite key=&quot;bernitsas2019eigen&quot;&gt;&lt;/d-cite&gt;. For flexible bodies, VIV are not uniform along the body’s length (usally refered to as the span) but rather different points along the span vibrate with different amplitudes and phases.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/Intro2.jpeg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Schematic diagrams of vortex induced vibrations of flexible bodies. &lt;/div&gt; &lt;p&gt;Observations of vortex induced vibrations (VIV) date back to antiquity, when the Aeolian tones, sounds created by pressure fluctuations induced by winds passing over taut strings were recognized. The first sketches of vortices date back to Leonardo da Vinci in the early 16th century. Today, VIV have become a problem of interest to both theoreticians, due to the complex underlying mechanisms involved, and engineers, due to the practical significance of mitigating the fatigue damage VIV can cause to offshore structures and equipment such as marine risers and offshore wind turbines. In order to gain some intuition, the reader can refer to the video of a flexible body undergoing VIV in section “Data Description” (below).&lt;/p&gt; &lt;p&gt;The underlying driving mechanism of VIV is vortex formation; specifically, the periodic shedding of vortices formed in the wake behind bluff bodies placed within cross-currents &lt;d-cite key=&quot;triantafyllou2016vortex&quot;&gt;&lt;/d-cite&gt;. The vortex shedding frequency in rigid bodies is known as the Strouhal frequency. For flexibly mounted or flexible bodies, the vortex formation frequency can be entrained away from the Strouhal frequency and coincides with the frequency of vibration in a phenomenon known as lock-in &lt;d-cite key=&quot;navrose_mittal_2016&quot;&gt;&lt;/d-cite&gt;. This occurs across a wide range of oscillating frequencies resembling a nonlinear resonance &lt;d-cite key=&quot;park2016suppression&quot;&gt;&lt;/d-cite&gt;. Given that flexible body VIV are not span-wise uniform as the flexible body undergoes a spatially traveling and/or standing wave response from the forcing excerted by the fluid &lt;d-cite key=&quot;wang2021illuminating, triantafyllou2016vortex, fan2019thesis&quot;&gt;&lt;/d-cite&gt;, the observed motions are nonstationary, unsteady, and can transition to different responses even for seemingly unchanged experimental conditions.&lt;/p&gt; &lt;p&gt;VIV of flexible bodies are usually modelled by leveraging the modal decomposition technique (i.e. using a Fourier expansion of sinusoidal mode shapes with time varying coefficients), similar to the approach introduced for modelling vibrating shafts and beams &lt;d-cite key=&quot;rao1995mechanical&quot;&gt;&lt;/d-cite&gt;. Recently, Kharazmi et al. (2021) &lt;d-cite key=&quot;kharazmi2021data&quot;&gt;&lt;/d-cite&gt; attempted to learn the mode shapes and time varying coefficients using LSTM networks in modal space (LSTM-Modnet); Mentzelopoulos et al. (2023) &lt;d-cite key=&quot;mentzelopoulos2023physics&quot;&gt;&lt;/d-cite&gt; proposed learning a sparse-mode set of sinusoidal modes along with the corresponding time-varying coefficients. Both frameworks suffer from the inability to robustly forecast future motions.&lt;/p&gt; &lt;p&gt;Although leveraging transformers to expand the horizon of predictions of time series is a very active field of research &lt;d-cite key=&quot;zhou2021informer, zeng2023transformers, liu2022non, zhou2022fedformer&quot;&gt;&lt;/d-cite&gt;, transformers have not yet been used to predict VIV of flexible bodies, which are physical non-stationary time-series, to the best of the author’s knowledge. In addition, only limited work has been performed in generating physical data using generative models &lt;d-cite key=&quot;zhong2023pi, takeishi2021physics, shu2023physics&quot;&gt;&lt;/d-cite&gt; and there are no identifiable applications to VIV in the literature. Mostly appications of machine learning for VIV include solving physical equations using physics-informed neural networks (PINNs) &lt;d-cite key=&quot;bai2022machine, raissi2019deep&quot;&gt;&lt;/d-cite&gt; and learning hydrodynamic coefficients or other relevant quantities &lt;d-cite key=&quot;ma20221understanding&quot;&gt;&lt;/d-cite&gt; in order to predict the motions on average rather than instantaneously &lt;d-cite key=&quot;ma2021enhancing, rudy2021learning&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;In this work, an attempt will be made to develop a transformer network architecture to predict the VIV of a flexible body both instantaneously and on average. The transformer will be trained and tested using data collected at the MIT Towing Tank by the author. In addition, in order to make the most of the available data, a variational autoencoder (VAE) will be trained to generate more VIV samples which will then be used to train the transformer. In doing so, the capability of VAEs to create physical data which retain information of the underlying physical processes will also be examined. The rest of the blog will be organized as follows: 1. using generative-AI, specifically variational autoencoders, in order to generate physical VIV data 2. using transformers to model and forecast nonstationary flexible body VIV.&lt;/p&gt; &lt;h2 id=&quot;data-description&quot;&gt;Data Description&lt;/h2&gt; &lt;p&gt;All data used for this study were collected during experiments conducted by the author at the MIT Towing Tank, a facility consisting of a 35m x 2.5m x 1.2m water tank equipped with a towing carriage capable of reaching speeds exceeding 2 m/s as well as a flow visualization window. In this and the following sections the terms model, riser, flexible body, and flexible cylinder will be used interchangeably to refer to the flexible cylinder model used during experiments.&lt;/p&gt; &lt;p&gt;The figure below illustrates the experimental setup schematically. A solid aluminum frame was used to support the flexible cylinder; the riser model was placed vertically at the center of the structure. An ATI 6 degree of freedom force sensor was attached to the top end of the riser to measure its tension. Two GoPro Hero 11 cameras were attached to the supporting frame facing perpendicular directions to capture videos of the riser’s motion in the cross-flow and in-line directions, respectively.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/full_schema_experiment-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/full_schema_experiment-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/full_schema_experiment-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/full_schema_experiment.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Schematic of experimental setup. A riser model was placed at the center of a supporting frame which was towed resulting in a uniform incoming flow profile on the model. &lt;/div&gt; &lt;p&gt;The riser model was constructed out of urethane rubber infused with tungsten powder. Specifically, Smooth-On PMC-724 urethane rubber was mixed with powdered tungsten to increase the model’s density and achieve a mass-ratio $m^* = \frac{\rho_{model}}{\rho_{H_2O}} = 3$. The mixture was poured into a right cylindrical mold with a fishing line placed along its centerline to provide tension. The model’s length was 890 mm with a 5 mm diameter. The length-to-diameter ratio of the model riser was L/D = 178. Equidistant markers were spray-painted red on the riser model resembling a zebra-patterning to enable motion tracking using cameras. Three underwater light fixtures were used to enhance visibility underwater. The model’s ends were clamped on the supporting frame and the model was separated from the frame by a distance much greater than the body’s diameter $O( &amp;gt; 10D)$.&lt;/p&gt; &lt;p&gt;The flexible cylinder was towed at 0.7 m/s resulting in a uniform incoming flow profile along the x direction, as shown in the schematic above. Recordings of the motions were captured at a resolution of 1080p (1920x1080 pixels) and 120 fps. The Reynolds number was $ Re \approx 3,500$. A visualization of the vibration is shown below (this is a gif of the actual vibration recording downsampled in time).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/viv.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/viv.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/viv.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/viv.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Sample video recording of the cross-flow vibration of the flexible body (top) and tracking result of the motion (bottom). &lt;/div&gt; &lt;p&gt;Reconstruction of the motion was done using a machine vision framework leveraging Kalman filtering for multi-object tracking; for more information one may refer to Mentzelopoulos et al. (2024) &lt;d-cite key=&quot;mentzelopoulos2024reconstructing&quot;&gt;&lt;/d-cite&gt;. The high level process is shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Image_Processing.PNG-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Image_Processing.PNG-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Image_Processing.PNG-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/Image_Processing.PNG&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Pipeline for motion reconstruction. Red markers on the body were tracked across the video frames to obtain their motion. &lt;/div&gt; &lt;p&gt;A total of 36 locations along the span were marked red on the fexible body and their positions were tracked. The endpoints were fixed on the supporting frame and thus their displacement was zero.&lt;/p&gt; &lt;h2 id=&quot;vibration-data-as-images&quot;&gt;Vibration Data as Images&lt;/h2&gt; &lt;p&gt;The displacement of the vibrating body was recorded at 36 uniformly spaced locations along the body’s span and the video recordings were sampled at 120 fps. One may store the vibration data as 2D arrays of $N_{time}$ x $N_{sensor}$, where each row corresponds to a different time of the vibrating body’s displacement at $N_{sensor}$ locations.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/pixelated_viv-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/pixelated_viv-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/pixelated_viv-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/pixelated_viv.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/interpolated_viv-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/interpolated_viv-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/interpolated_viv-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/interpolated_viv.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Visualization of experimental data. The x-axis corresponds to sensor number (i.e. body location) and the y-axis corresponds to time step (i.e. $\Delta t = 1/fps = 1/120 \ sec$). The displacement normalized by the body&apos;s diameter is highlighted on the plane. On the left we visualize the vibration data stored in 2D arrays. On the right, we plot the interpolated values which make more intuitive sense as visualized flexible body vibrations (travelling wave response with peaks travelling to the right from location 0 to location 36). Lines parallel to the x-axis are &quot;snapshots&quot; of the flexible body vibrting in the direction perpendicular to the paper/screen (i.e. &quot;in and out of the page&quot;). &lt;/div&gt; &lt;p&gt;The stored vibration data are illustrated above and can easily be visualized and treated like single channel images! If necessary, scaling pixel values invertibly to an interval of choice, like [0,1] or [0, 255] requires just a few operations leveraging the maximum and minimum values of the data. In the images shown above, each row corresponds to a different time of the recorded vibration at all sampled locations. The time difference between consecutive time steps is $\Delta t = 1/fps = 1/120 \ sec$. The 36 “sensor locations” correspond to the uniformly spaced markers on the body (excluding the two endpoints) and thus they span approximately the full body length. Plotting the interpolated values of the array yileds a more intuitive interpretation of the vibrations. In the data shown above, a travelling wave (crests travelling) from location 0 to location 35 can be identified. For convenience, the data were stored in a single 4D array of size $N_{batch}$ x $1$ x $N_{time}$ x $N_{sensor} = N_{batch}$ x $1$ x $36$ x $36$, yielding hundreds of square arrays of size 36 x 36 which can be easily visualized and collected in batches for training models.&lt;/p&gt; &lt;h2 id=&quot;gen-ai-for-physical-vibration-data-using-variational-autoencoders&quot;&gt;Gen-AI for Physical Vibration Data using Variational Autoencoders&lt;/h2&gt; &lt;p&gt;In this section we focus on generating physical vibration data using generative-AI. We will attempt using a variational autoencoder (VAE) trained on the real experimental data described above to generate syntehtic data of the vibrations. We are interested in understanding whether the generated data preserve physicality and thus whether they can be used to train models and to understand the underlying physical generative process by studying the artificial data.&lt;/p&gt; &lt;p&gt;A VAE is a specific network architecture whose goal is to learn a probabilistic mapping from an input space to a low dimensional latent space and then back to the input space. The network architecture is comprised of an encoder network which maps data from the input space to the latent space and a decoder network which maps data from the latent space back to the input space. A schematic of the VAE used for this work is shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE.PNG-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE.PNG-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE.PNG-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE.PNG&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Schematic of the variational autoencoder architecture. &lt;/div&gt; &lt;p&gt;On a high level, the variational autoencoder acts just as a regular autoencoder, with the difference that the training ensures that the distribution of the data in the latent space is regular enough to enable a generative process when sampling from the latent space. That is, the minimized loss ensures that the distribution of the data over the latent dimensions, $q(z \mid x)$, is as close to a standard normal distribution as possible. We choose to assume a Gaussian prior on the latent space for our data since we will need to sample from it when decoding, a task which is nontrivial for arbitrary distributions. The decoder on the other hand will learn the distribution of the decoded variables, $p(x \mid z)$ given their latent representations.&lt;/p&gt; &lt;p&gt;The encoder architecture of choice was the following, accepting an input $x \in R^{36 \times 36}$:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;$x \rightarrow Linear (R^{36 \times 36}, R^{64}) \rightarrow ReLU \rightarrow Linear(R^{64}, R^{64}) \rightarrow ReLU \rightarrow x_{embedding}$&lt;/li&gt; &lt;li&gt;$x_{embedding} \rightarrow Linear(R^{64}, R^{5}) \rightarrow ReLU \rightarrow \mu \in R^5$&lt;/li&gt; &lt;li&gt;$x_{embedding} \rightarrow Linear(R^{64}, R^{5}) \rightarrow ReLU \rightarrow \sigma \in R^5$&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;where $\mu$ and $\sigma$ are the mean and variance of the posterior data distribution in the latent space. The decoder architecture was as follows accepting an input $z \in R^5$:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;$z \rightarrow Linear(R^{5}, R^{64}) \rightarrow ReLU \rightarrow Linear(R^{64}, R^{36 \times 36}) \rightarrow ReLU \rightarrow x^\prime$&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Training was done by maximizing the evidence lower bound (ELBO) on the experimental data and the outputs of the autoencoder. This is equivalent to minimizing the following loss (negative of ELBO).&lt;/p&gt; &lt;p&gt;$Loss_{ELBO} = - E_{q(z \mid x)} \bigg[ \log p(x\mid z) - D_{KL}(q(z \mid x )\mid \mid q(z)) \bigg]$&lt;/p&gt; &lt;p&gt;where $D_{KL}$ referes to the Kullback-Leibler divergence. Intuitively, maximizing the ELBO or minimizing the above $Loss_{ELBO}$, aims at maximizing the log-likelihood of the data given their representations in the latent space while minimizing the Kullback-Leibler divergence between the learned posterior of the data in the latent space and the prior assumption of a Gaussian distribution in the latent space. For the purposes of training, the data were scaled to be between [0, 1] in order to use binary cross entropy. The VAE was trained using Adam optimizer with a learning rate $lr = 0.01$. A step scheduler was set to decay the step by $\gamma = 1/2$ every 2,000 iterations. The training loss as a function of epoch is shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE_loss.PNG-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE_loss.PNG-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE_loss.PNG-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/VAE_loss.PNG&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Training loss ($Loss_{ELBO}$) for the VAE on the input real VIV data. &lt;/div&gt; &lt;p&gt;Having trained the VAE, samples from the standard normal distribution in $R^5$ were drawn, decoded, and rescaled in order to generate synthetic VIV data. Three random samples are included below (top), along with three random samples of real data observed during experiments (bottom).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/generated_VAE_3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_1-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_1-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_1-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_1.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_viv_3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Top row: Generated data using the VAE. Bottom row: Real data recorded from experiments. &lt;/div&gt; &lt;p&gt;Albeit the data generated data are certainly eye-pleasing, their promise begs the question of whether they preserve physicality. In order to address this question, we will examine whether a model trained on synthetic data can be used to predict real experimental data.&lt;/p&gt; &lt;h2 id=&quot;vivformer---a-transformer-architecture-for-viv&quot;&gt;VIVformer - A Transformer Architecture for VIV&lt;/h2&gt; &lt;p&gt;Tranformer network architectures have been widely used and are considered state of the art tools for various machine-learning tasks, particularly in natural language processing (NLP) and computer vision. The transformer architecture has become a cornerstone in deep learning and its applications span across all fields of engineering and science. In this section we will develop a transformer architecture to model and forecast the VIV of flexible bodies under the excitation of a hydrodynamic force. The transformer architecture used for this purpose is shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Transformer_architecture-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Transformer_architecture-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Transformer_architecture-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/Transformer_architecture.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; VIVformer: A transformer architecture for vortex-induced vibrations. &lt;/div&gt; &lt;p&gt;As shown schematically above, the architecture is comprised by various Residual-Attention modules followed by a final linear layer. The input to the VIVformer is a batch of vibration data as discussed in previous sections “Data Description” and “Visualizing the Data” with shape $N_{batch} \times N_{time-in} \times N_{sensor}$. The data are then passed through $N_{attn-layers}$ residual attention modules (these do not affect the shape of the input) and then scaled to the desired $N_{time-out}$ yielding an $N_{batch} \times N_{time-out} \times N_{sensor}$ output.&lt;/p&gt; &lt;p&gt;The residual-attention modules are the drivers of the data processing. These modules accept an input on which they perform two sequential tasks: 1. multi-head attention with a residual connection, and 2. pass the output of the multi-head attention module through a fully connected feedforward network (FFN) with a residual connection. The process can be visualized in the bottom left of the architecture schematic above.&lt;/p&gt; &lt;p&gt;The multi-head attention layer is comprised of $N_{heads}$ number of attention heads which calculate the self-attention of the input as proposed by Vaswani et al. (2017) &lt;d-cite key=&quot;vaswani2017attention&quot;&gt;&lt;/d-cite&gt;. The superposition of the input and output from the Multi-head attention module is then passed through the FFN. The FFN performs batch normalization, passes the output through a linear layer which sclaes the input to $mlp-dim$, then through a Gaussian Error Linear Unit (GeLU) activation and scales the output back to the original dimension by passing through a second linear layer.&lt;/p&gt; &lt;p&gt;For this work, we attempt using 20 time steps of input data in order to predict a single future time step. That is, the input to the VIVformer is 20 time steps of vibration data at 36 locations and we try to predict the next time step at the same locations. We note that the VIVformer is flexible in terms of the number of data-points in and out as well as the number of time steps in and out. Decreasing the input information (both spatial and temporal) while forecasting as much as possible in terms of spatial and temporal predictions is the recommended research direction for future work.&lt;/p&gt; &lt;p&gt;Although auto-regressive transformers are trending currently, for the purpose of forecasting vibrations this would lead to a pitfall of accumulating model errors and using them as inputs. In order to predict extended time horizons, simply adjusting the number of time-steps out would be the recommended course of action.&lt;/p&gt; &lt;p&gt;Since we are interested in making predictions of physical vibration data, a reasonable choice for our loss function is the Mean Square Error (MSE) between predicted and observed vibrations.&lt;/p&gt; &lt;h3 id=&quot;the-real-data-deal&quot;&gt;The Real (data) Deal&lt;/h3&gt; &lt;p&gt;In this section, the experimental data obtained during experiments were used to train the VIVformer. Specifically, 20 times steps at 36 locations were used as input and the next time step at the same locations was forecasted. In order to train the transformer, a dataset and dataloader was created to enable iterating over the following quantities:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Sequence_in: A 2D array of shape $N_{time-in} = 20 \times N_{sensor} = 36$.&lt;/li&gt; &lt;li&gt;Target = A 2D array of shape $N_{time-out} = 1 \times N_{sensor} = 36$.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Sequence_in refers to a single input to the VIVformer and Target is the expected output of the VIVformer. The sequences were collected in batches and then used for training. The model was trained on the MSE loss between input sequences and targets and the parameters were updated using the AdamW algorithm. The initial learning rate was set to $lr = 0.0001$ and a cosine annealing step scheduler was set to adjust the learning rate during training.&lt;/p&gt; &lt;p&gt;The training data were split into 80% for training and 20% for testing/validation. The sequences and targets of the training data were shuffled randomly and split in mini-batches while the validation data were not in order preserve the continuity of the vibrations when validating (important mainly for visualization purposes). The VIVformer was trained for a total of 50 epochs. The training results are shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_data_VIVformer_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_data_VIVformer_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_data_VIVformer_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/real_data_VIVformer_loss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; VIVformer training and validation loss trained on expreimental VIV data. &lt;/div&gt; &lt;p&gt;The training results show that the model is able to gradually decrease the MSE loss between targets and predictions. The loss on both the training set and the validation set seems to be decreasing and converging. We note that the VIVformer architecture used was heuristically optimized using a trial and error approach yielding 4 attention-residual layers, with 3 attention heads of 32 hidden units and a mlp-dim of 128 hidden units. In order to visualize the predicted vibrations, the forecasting as well as target data from a random sample of 36 continuous time steps from the validation set are shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_real-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_real-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_real-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_real.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_real-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_real-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_real-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_real.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_real-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_real-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_real-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_real.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; From left to right: Forecasting of the VIV on a ramndomly selected set of 36 continuous points from the validation set (for visualization of the predicted vibrations), target data from real experiments, absolute difference between targets and predictions. &lt;/div&gt; &lt;p&gt;As is evident from the visualized vibration predictions (above), the model can predict unseen experimental to reasonable accuracy. The expected modes are forecasted and the output is continuous. In addition, the absolute difference is almost everywhere small, although some inaccuracies do occur in the predictions. A meaningful question to ask would be how well does the model predict the root mean square (RMS) of the vibrations which gives us a sense of the prediction capabilities on average. Below we plot the RMS of the forecasted as well as the experimentally observed vibrations.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_real-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_real-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_real-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_real.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Root mean square (RMS) of the forecasted VIV as well as the experimentally observed. RMS displacement is shown on the y-axis while span (body position) is shown on the x-axis. Reasonably accurate agreement is evident between forecasting and experimental observations. &lt;/div&gt; &lt;p&gt;The RMS result shown above shows that the model can predict the vibrations reasonably accurately on average. This is a particulary important result as it allows for direct benchmarking of this method against semi-empirical models which can only predict the average vibrations.&lt;/p&gt; &lt;p&gt;Although this is not recommended practice as we described earlier, we attempt to make auto-regressive predictions using our model. That is, we start with 20 time steps of recorded vibrations as input and then use the model’s predictions gradually as more and more inputs. By 20 time steps, there would be no observed data input to the model; it would only be predicting on its outputs. The auto-regressive results are shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_forecasting_real-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_forecasting_real-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_forecasting_real-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_forecasting_real.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_targets_real-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_targets_real-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_targets_real-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_targets_real.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_difference_real-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_difference_real-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_difference_real-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/auto_regression_difference_real.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; From left to right: Auto-regressive forecasting of the VIV on a randomly selected set of 20 continuous points from the validation set, target data from real experiments, absolute difference between targets and predictions. We show 20 time steps as we assess the models capability to predict as many time steps as it uses as input. &lt;/div&gt; &lt;p&gt;Albeit the mode shapes are consistent and remain physical looking, it appears that the magnitude of the response grows with time. As expected, errors accumulate and the forecasting becomes more and more innacurate as time evolves. This can also be clearly visualized in the absolute difference plot (on the very right) where the difference increases with time.&lt;/p&gt; &lt;p&gt;In conclusion, with respect to training on real data, the transformer is reasonably accurate in terms of forecasting future motions given a sample of the experimental data. The model trains well on the MSE loss and seems to converge in about 50 epochs. The wall time of training does not exceed a few minutes on a Google-Colab T4 GPU machine.&lt;/p&gt; &lt;h3 id=&quot;the-hyper-real-gen-ai-data-deal&quot;&gt;The hyper-Real (Gen-AI data) Deal&lt;/h3&gt; &lt;p&gt;So far we have established that the VIVformer architecture can model the physical VIV of flexible bodies reasonably accurately. This section will mainly focus on addressing the question of whether synthetic VIV data generated using our VAE are physical: that is, whether the physical properties of the vibrations are preserved during the generative process. In order to address this question, we will train the VIVformer on synthetic data only and then test the trained model on the real data.&lt;/p&gt; &lt;p&gt;Sixty arrays of 36 time steps at 36 locations (this can be though of as generating 60 images similar to the ones shown in previous section “Vibration Data as Images”) were generated using the VAE trained on real experimental data. The synthetic VIV data were then organized in input and target sequences by creating a dataset and dataloader to train the VIVformer. Training was done exactly as described in section “The Real (data) Deal” with the only difference being the training data; in this case training data were only synthetic. The same split of 80% for training/validation was used on the synthetic data. The training results were as follows.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/synthetic_VIVformer_loss-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/synthetic_VIVformer_loss-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/synthetic_VIVformer_loss-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/synthetic_VIVformer_loss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; VIVformer training and validation loss trained on synthetic VIV data. &lt;/div&gt; &lt;p&gt;The VIVformer architecture seems to train on the sythetic data well. We note that both the training and validation data are from the synthetic dataset and as such we expect that they should be very similar data. We train for 50 epochs and the results seem to reach convergence. In this case we note that the error on the validation set (calculated during each epoch after optimizing on the VIVformer on the training set) seems to be be consistently smaller than the error on the training set (on average). We expect that eventually the training loss would become smaller than the validation loss although more training epochs would be required, perhaps leading to overfitting our model. Given the training results, we can be confident that the VIVformer has learned to predict the synthetic data well.&lt;/p&gt; &lt;p&gt;The more important question is however, whether the VIVformer trained on the synthetic data can accurately forecast the real experimental data. Below we show the predictions of the VIVformer on the real experimental data. We underscore that the VIVformer has NOT seen a single real datum during training: the model has trained on synthetic data only!&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_synthetic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_synthetic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_synthetic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/Forecasting_synthetic.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_synthetic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_synthetic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_synthetic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/targets_synthetic.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_synthetic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_synthetic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_synthetic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/difference_synthetic.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; From left to right: Forecasting real VIV on a ramndomly selected set of 36 continuous points from the experiments, target data from experiments, absolute difference between targets and predictions. We note that the model has trained on synthetic data only and has never seen any real data! &lt;/div&gt; &lt;p&gt;Albeit the VIVformer has not seen any real data during training, it is surprisingly reasonably accurate in predicting real data! Although certainly not perfect, the predictions are sensible. The root-mean-square of the vibrations forecasted and observed are shown below.&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_synthetic-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_synthetic-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_synthetic-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-VIVFormer/RMS_synthetic.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;caption&quot;&gt; Root mean square (RMS) of the forecasted VIV plotted on top of the RMS of the experimentally observed VIV. RMS displacement is shown on the y-axis while span (body position) is shown on the x-axis. Reasonably accurate agreement is evident between forecasting (after training on synthetic data only) and experimental observation. &lt;/div&gt; &lt;p&gt;As is evident in the above figure, the VIVformer can make reasonably accurate predictions of the RMS of the vibrations. Both the trends and amplitudes are reasonably accurately estimated.&lt;/p&gt; &lt;p&gt;Since the VIVformer has never trained on real data but can reasonably accurately predict them, we conclude that at least part of the physicality of the real data is preserved during the genrative process of the VAE. In a sense, the VAE can be though of not just as a generator which makes realistic-looking data but as a tool which learns the underlying structure and mechanisms of the physical process which generates the data; it can thus be used to better understand the data and perhaps even the physical generative process. We conclude that our VAE could certainly be used to augment scarse datasets of VIV data and in addition, that it is a powerful tool that could potentially be used to study the underlying mechanisms of the physical generative process by studying the artificially generated data!&lt;/p&gt; &lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt; &lt;p&gt;In this work, a data driven approach is employed to study physical system vibrations. Two main topics are explored: 1. Generative models for creating synthetic data similar to those obtained via physical processes and 2. employing transformers and the attention mechanism in order to model and forecast physical vibration data.&lt;/p&gt; &lt;p&gt;A variational autoencoder is trained on physical vortex-induced vibration data in order to generate sythetic data of the vibrations. The VAE is certainly able to generate data which resemble the physical data visually. Moreover, the generative process is confirmed to preserve the physicality of the data at least partially: a transformer trained on synthetic data only is capable of predicting real experimental data to reasonable accuracy. In that sense, the VAE can be viewed as a tool which learns the underlying physical traits of the data and can be used not only to augment physical datasets but also to simulate and understand the underlying physical mechanisms by examining synthetic data. With that being said, a recommended future research direction would be to examine whether the outputs of the VAE satisfy physical equations of interest and how those could perhaps be included as an additional loss term when training the VAE, i.e. having a physics-informed decoder network.&lt;/p&gt; &lt;p&gt;A transformer architecture for forecasting unsteady and nonstationary vortex-induced vibrations, the VIVformer, is developed. The VIVformer architecture combines multi-head attention modules and fully conncted network modules with residual connections in order to model and forecast the physical vibration time-series in both space and time. The optimized VIVformer architecture can forecast flexible body VIV in time-space to reasonable accuracy both instantaneously and on average. Testing the performance of the VIVformer while gradually decreasing the input information would yield a deeper understanding in the capabilities of the architecture; in addition, testing the extended time horizon predictions of the model would cretainly be a recommendation for future research.&lt;/p&gt; </content> </entry> <entry> <title>Recovering Latent Variables with VAEs despite Training Bias</title> <link href="https://deep-learning-mit.github.io/blog/2022/Recovering-Latent-Variables-with-VAEs-despite-Training-Bias/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2022/Recovering Latent Variables with VAEs despite Training Bias</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;In this age of abundant unlabeled data, unsupervised learning is capitalizing to power the generative models that are eating the world. Large Language Models and Diffusion models are focalizing most of the mainstream hype and therefore siphoning attention from other generative models. In particular, the Variational Autoencoders (VAE) is a model architecture that has been arguably overlooked in the recent onslaught for scaling transformer and diffusion-based models. VAEs are a promising latent variable model that allows for the learning of disentangled latent variables that model data.&lt;/p&gt; &lt;p&gt;As models scale in size, so is concern for the lack of interpretability associated with Neural Networks. Latent variable models offer a solution to this problem since they can learn variables that factorize the data generation process. VAEs are particularly well suited for learning latent variables in an unsupervised setting since they use an unsupervised learning objective and are regularized to learn disentangled encodings of our data. VAEs have been applied in a broad breadth of settings, such as classifying out-of-distribution data &lt;d-cite key=&quot;xiao2020likelihood&quot;&gt;&lt;/d-cite&gt;, fair decision making &lt;d-cite key=&quot;10.1145/3287560.3287564&quot;&gt;&lt;/d-cite&gt;, causal inference &lt;d-cite key=&quot;louizos2017causal&quot;&gt;&lt;/d-cite&gt;, representation learning, data augmentation, and others. Although VAEs have demonstrated the capability to recover ground truth latent variables, they often recover mock factors that can generate the training dataset but differ mechanistically from the ground truth data generation process. For instance, in lecture we demonstrated that a VAE trained on cartoon images of rivers learned to encode aggregate river curvature as a latent variable. The ground-truth data-generating random variables were an ordered set of Bernoulli random variables indicating if the river angeled its trajectory to the left or to the right at the particular timestep. The VAE’s shortcoming in recovering the real latent variables is expected from a Bayesian perspective, since we assume an isotropic Gaussian prior for continuous latent variables, and impose a bottleneck on the number of latent variables. Even though we do not recover the ground-truth data generating random variables, we learn latent variables that are qualitatively useful and capture macro latent phenomenons about the data. This segways into an interesting question—when do VAEs fail to recover useful latent variables?&lt;/p&gt; &lt;p&gt;In particular, we will choose the setting in which our training data is biased, but we still seek to learn insightful representations of the data. This is an especially well-motivated setting, since in unsupervised learning, we often do not have any guarantees about the distribution of our training data, yet we still aim to learn generalizable latent variables. It would be ideal if VAE’s ability to recover generalizable latent variables is robust to training bias. Relating to the cartoon example from lecture, if the probability parameter for the data-generating random variables was skewed so that right-curving rivers are more likely (i.e. \(P(\text{right}) = 0.9\) instead of \(P(\text{right}) = 0.5\)), would we still learn useful latent variables, or would latent variables instead model what we assume to be observational noise? If we learn the former, then we would still be able to sample in latent space to generate left-curving rivers. Intuitively, we will not be able to generate samples out of distribution with the training data (i.e. left curving rivers), however this may not be the case due to the way VAEs assume a prior. In this project, we will examine this setting to determine if higher regularization of the prior increases model robustness to training bias.&lt;/p&gt; &lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt; &lt;p&gt;VAEs are useful as encoders for downstream tasks, and as generative models. Compared to vanilla autoencoders, they offer significant advantages, since they provide some assurances regarding the distribution of its latent variables. Unlike VAEs, standard Autoencoders can have arbitrarily distributed embeddings, making them poor generative models, since there is no straightforward way to sample in latent space so that we generate samples in distribution with our training data. VAEs are similar to standard Autoencoders, however, they are trained with a modified loss function that ensures the learned embedding space is regularized towards an isotropic Gaussian (there exist alternative choices regarding which distribution we regularize towards, but Gaussian Mixture Models are the most popular as it stands due to their simple parameterization and empirical success). Additionally, instead of simply compressing the input with a neural network during the forward pass, the encoder of a VAE outputs a mean and covariance, defining a distribution from which we sample to obtain our latent variables.&lt;/p&gt; &lt;p&gt;Since the VAE loss function regularizes our latent variables towards an isotropic Gaussian, encoded data is both disentangled and interpretable. To use trained VAEs as generative models, we simply sample latent variables i.i.d. from the Gaussian distribution and pass it through the VAE decoder to generate samples in distribution with our training data. VAEs also offer significant advantages as encoders, since regularization encourages them to learn factored, disentangled representations of the data. Finally, VAEs are particularly well-suited for interpretability since regularization encourages each latent variable to capture a unique aspect of the data.&lt;/p&gt; &lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt; &lt;p&gt;There has been significant prior work studying regularization and choice of priors in VAEs. Notably, \(\beta\)-VAE &lt;d-cite key=&quot;higgins2017betavae&quot;&gt;&lt;/d-cite&gt; introduces the beta parameter to control the degree to which the VAE loss function penalizes the KL divergence of the latent variable distribution with the chosen prior (an isotropic Gaussian in their case). Higgins et al. demonstrate that introducing the beta parameter allows the VAE encoder to learn quantitatively more disentangled latent variables. They introduce a novel quantitative metric to evaluate the disentanglement of latent space and show that \(\beta\)-VAE improves on existing methods. Furthermore, they train a \(\beta\)-VAE on a dataset of faces (celebA) and qualitatively show that \(\beta\) regularization allows for the factorization of previously entangled latent variables such as azimuth and emotion.&lt;/p&gt; &lt;p&gt;There have been several iterations on \(\beta\)-VAE such as Factor-VAE &lt;d-cite key=&quot;kim2019disentangling&quot;&gt;&lt;/d-cite&gt;. Kim and Mnih point out that although \(\beta\) regularization improves disentanglement in embedding space, it does so at the cost of reconstruction quality. To reduce this trade-off and still encourage disentanglement, they introduce a term to the VAE loss function that penalizes the KL divergence between the joint distribution and the product of the marginals, instead of with an isotropic Gaussian as in \(\beta\)-VAE.&lt;/p&gt; &lt;p&gt;Selecting an appropriate data prior is fundamental when performing Bayesian inference. In vanilla VAEs, we often assume an isotropic Gaussian prior for our latent variables, however, this is not always a good assumption, making it difficult to converge &lt;d-cite key=&quot;miao2022on&quot;&gt;&lt;/d-cite&gt;. Miao et al. propose InteL-VAE, a VAE architecture capable of learning more flexible latent variables that can satisfy properties such as sparsity even when the data has significant distributional differences from a Gaussian. Their contributions allow for higher customizability of latent variables while bypassing many of the convergence issues commonplace with other methods that assume non-Gaussian priors.&lt;/p&gt; &lt;p&gt;Since that under ideal conditions, VAEs recover factorized latent variables, causal inference has become a standard setting for their application. Madras et al. propose structured causal models to recover hidden “causal effects” with the aim of improving fairness when presented with biased data &lt;d-cite key=&quot;10.1145/3287560.3287564&quot;&gt; &lt;/d-cite&gt;. They specify a framework where we want to recover the latent factors so that decision making in applications such as loan assignment and school admissions can be approached fairly. Admiddetly, Structured Causal Modeling (CSM) is arguably a better setting for futher work on our proposed research question. However, this field is largely outside of the scope of the course, so we will only observe that Madras et al. utilyze a model where causal factors, which are analaguous to our ground truth latent variables, affect a decision and an outcome, and that they utilyze a Bayesian framework to perform variational inference. Future iterations of our research should borrow methods from this field of Mathematics for maximum impact. Louizos et al. propose the Causal Effect VAE &lt;d-cite key=&quot;louizos2017causal&quot;&gt;&lt;/d-cite&gt;, marrying the adjacent fields and setting the stage for future research.&lt;/p&gt; &lt;p&gt;Although there is plenty of research adjacent to our particular question of interest, \(\beta\)-VAE investigates how \(\beta\)-regularization affects disentanglement, but not robustness to training bias. Other works that investigate the ability of latent variable models to recover the ground truth in the presence of training bias are not concerned with \(\beta\)-regularization. \(\beta\)-regularization has been shown to be effective, in addition to being extremely simple to implement, compared to other regularization techniques. Thus it is an ideal candidate for directed research on how regularization affects VAE robustness to training bias. Our question is novel, supported by adjacent research, and reasonable to implement with the resources available to an undergraduate student.&lt;/p&gt; &lt;h2 id=&quot;set-up-and-methods&quot;&gt;Set-up and Methods&lt;/h2&gt; &lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt; &lt;p&gt;More concretely, suppose that there exists a data generating function \(\mathcal{G}: Z \to X\) that generates our training dataset given random variables \(Z \sim p_{\text{data}}\). For simplicity, our data will be nxn grids of squares, where the intensity of each square is deterministically proportional to its respective random variable. To create our training dataset, we sample \(n^2\) random variables from an isotropic Gaussian distribution with mean \(\mu\) and covariance I. We then apply a sigmoid activation to the random variables so that values are in the range [0,1]. We then create a mn x mn image with mxm pixel grids for each random variable. Finally, we add Gaussian noise to the image. We choose n=3, m=7, and train a VAE for each value of \(\mu\) in the set {0, 1/2, 1, 3/2, … 5}.&lt;/p&gt; &lt;h4 id=&quot;training-data&quot;&gt;Training Data&lt;/h4&gt; &lt;p&gt;The following figure shows example training images before noising. Each row has 21 images drawn from the distribution defined by applying a sigmoid activation to a normally-distributed random variable with variance 1 and mean specified by the row index.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;And here are some images with some noise added.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training%20noised-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training%20noised-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training%20noised-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20training%20noised.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h4 id=&quot;test-dataset&quot;&gt;Test Dataset&lt;/h4&gt; &lt;p&gt;To create our test dataset, we discretize the domain of latent variables by binning. We then enumerate all possible combinaation of latent variables, and generate corresponding images without adding noise. We restict the domain generating variables to {0.1, 0,5, 0.9}, and enumerate all possible combination. This yields a test dataset of 19683 images.&lt;/p&gt; &lt;h5 id=&quot;example-test-images&quot;&gt;Example Test Images&lt;/h5&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20test%20images-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20test%20images-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20test%20images-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/example%20test%20images.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt; &lt;p&gt;With this setup, the structure of our latent space matches that of the ground-truth latent variables, creating an appropriate setting in which to test how training bias and regularization affect the quality of learned models. Our pipeline is as follows. We train a VAE on its associated training set by maximizing the ELBO. After T training steps, we then train a linear projection head from the ground-truth latent variables to our learned latent variables. Even if we fully recover the ground-truth latent variables in our model, there is no assurance that we will not learn some permutation of the ground-truth latent variables. Thus in order to test if a particular latent variable was learned in our model, we must utilize such a projection to map from ground truth to learned latent variables, then decode the sample and evaluate the generated image.&lt;/p&gt; &lt;p&gt;Although the Mutual Information between the ground truth latent variables \(z \sim p_z\) and the learned latent variables \(\hat{z} \sim p_\hat{z}\) would be a more encompassing gauge if the VAE recovered the latent variables, using a linear projection in lieu of a Mutual Information estimator such as MINE &lt;d-cite key=&quot;belghazi2021mine&quot;&gt;&lt;/d-cite&gt; is justified for the following reasons. Namely, we assume an isotropic Gaussian during training, so a good VAE will learn disentangled latent variables that will be off by at most a rotation from the ground truth latent variables. Furthermore, we control the data generation process so that data is generated by \(n^2\) normally distributed random variables. Thus we can assume that a linear projection is sufficient to recover the ground truth latent variables from our learned latent variables. Furthermore, given the time constraints and resources allocated for this project, simply training a linear projection and taking the final mean squared error as a proxy for mutual information allows for simpler implementation.&lt;/p&gt; &lt;p&gt;We train with the Adam optimizer.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Hyperparameter&lt;/th&gt; &lt;th style=&quot;text-align: right&quot;&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;VAE training steps&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;10000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Linear Projection Training Epochs&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training noise mean&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training noise variance&lt;/td&gt; &lt;td style=&quot;text-align: right&quot;&gt;0.25&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt; &lt;h3 id=&quot;training-observations&quot;&gt;Training Observations&lt;/h3&gt; &lt;p&gt;During the unsupervised training phase where we train the various VAE models on their respective training sets, we observe that dataset choice and penalization of the KL divergence (beta hyperparameter) have consistent effects on the training curves. The following charts demonstrate that increased penalization of the KL divergence results in higher training loss, as well as nosier training loss and longer convergence times. This is expected since higher regularization directly increases the loss and its associated noise. We approximate the KL divergence by drawing one sample, which is highly variable, but tends to work emperically. We also observe that higher training bias (i.e. higher pre-activation mean of the pre-activation data generating latent variables) results in higher training loss. As we increase this training bias, it becomes harder and harder to disambiguate latent features from noise. Thus models learn uninterpretable latent variables and poor decoders that learn to trivially output the dominating color (white).&lt;/p&gt; &lt;div class=&quot;row mt-3&quot;&gt; &lt;div class=&quot;col-md mt-3 mt-md-0&quot;&gt; &lt;h6&gt;Training Curves Varying Training Distribution&lt;/h6&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/beta%20=%201-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/beta%20=%201-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/beta%20=%201-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/beta%20=%201.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=&quot;col-md mt-3 mt-md-0&quot;&gt; &lt;h6&gt;Training Curves Varying $\beta$-Regularization&lt;/h6&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/mu=%5B0%5D%20training%20curves-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/mu=%5B0%5D%20training%20curves-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/mu=%5B0%5D%20training%20curves-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/mu=%5B0%5D%20training%20curves.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt; &lt;p&gt;The following figure shows a heat map of our Proxy for measuring Mutual Information (which we will refer to as PMI) between the learned latent variables \(\hat{Z}\) and the true latent variables \(Z\).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20projection%20head-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20projection%20head-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20projection%20head-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20projection%20head.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Note that when we randomly initialized a VAE and then trained linear projections from the ground truth latents to recovered latents, we achieved an PMI 0.1121 (averaged over 3 runs with identical training parameters). The heatmap shows that we almost completely recovered the ground-truth latent variables with low regularization and low training bias. As training bias increases, the model recovers less and less informative representations of the true latent variables.&lt;/p&gt; &lt;p&gt;Another heuristic that we can utilize to estimate the Mutual Information between the recovered latents and the ground truth latents is the mean squared error between \(\mathcal{G}(z)\) and \(\mathcal{D}_\text{VAE}(P(z))\) averaged over our test set, where P is the learned linear projection from \(Z \to \hat{Z}\) and \(\mathcal{D}_\text{VAE}\) is the VAE decoder. The following figure heatmap visualizes this figure.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20generating%20on%20test%20set-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20generating%20on%20test%20set-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20generating%20on%20test%20set-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-12-12-Recovering%20Latent%20Variables%20with%20VAEs%20despite%20Training%20Bias/MSE%20generating%20on%20test%20set.png&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h2&gt; &lt;p&gt;From the collected data, it is visually clear that there exists a relationship between \(\beta\)-regularization and training bias. In both heat maps, there are reasonably well-defined diagonal level surfaces, indicating that there is some relationship between regularisation towards an isotropic Gaussian prior and robustness to training bias. Validation and further experiments are required to legitimize this conclusion, however, these experiments are an indication that conscious regularization can be a useful technique to mitigate training biases of a particular form. At this point, further work is required to interpret the results, since it is not clear why we seem to observe inverse relationships between the \(\beta\)-regularization and training bias when we involve the decoder.&lt;/p&gt; &lt;p&gt;It is also worth noting that during pretraining, VAEs were trained for a fixed number of training steps, and not until convergence. Thus it is highly plausible that models with higher \(\beta\)-regularization (i.e. models with \(\beta &amp;gt; 1\)) were not trained to completion, and therefore can not be fairly evaluated with mutual information estimators without further training. Given my computational and temporal constraints, it was not reasonable to run experiments with longer training. Future work will have to validate my findings by pretraining for longer and testing a finer resolution of \(\beta\) parameters. Finally, it will be interesting to expand this work to more interesting datasets such as celebA and inject training bias by resampling the dataset according to some variables such as hair color or skin tone. Once we move beyond the assumptions assured by pet data, we can reevaluate what relationships hold true as we gradually add the complexity inherent to the real world.&lt;/p&gt; </content> </entry> <entry> <title>Recurrent Recommender System with Incentivized Search</title> <link href="https://deep-learning-mit.github.io/blog/2022/proposal_JingpengHong/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2022/proposal_JingpengHong</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;Numerous deep learning based recommender systems have been proposed recently &lt;d-cite key=&quot;10.1145/3285029&quot;&gt;&lt;/d-cite&gt;. Especially, the sequential structure of session or click-logs are highly suitable for the inductive biases provided by recurrent/convolutional neural networks &lt;d-cite key=&quot;hidasi2016sessionbased&quot;&gt;&lt;/d-cite&gt;. In such setting, the input of the network is a sequence of consumers’ search behavior, while the output is the predicted preference of the items, i.e. the likelihood of being the next in the session for each item. The ultimate goal is to pinpoint the optimal product for the consumer, thereby increasing sales. An example of where this could be applied is the “featured product” on platforms like Amazon.&lt;/p&gt; &lt;p&gt;However, a challenge with this model is the sparsity of data. It’s well-known that the products in retail has the “long-tail” feature. Only a small fraction, say 5%, of a site’s products are ever browsed or bought by customers, leaving no data on the remaining products. Additionally, customer sessions tend to be brief, limiting the amount of information we can get from any one individual. This issue is particularly acute for “data-hungry” models, which may not have sufficient training data with enough variation to accurately match products with customers.&lt;/p&gt; &lt;p&gt;My proposed solution to this issue is to recommend products that also encourage further exploration. Economic studies have shown that certain types of information structure can motivate customers to consider more options, harnessing the “wisdom of crowds” &lt;d-cite key=&quot;kremer2014implementing&quot;&gt;&lt;/d-cite&gt;&lt;d-cite key=&quot;che2018recommender&quot;&gt;&lt;/d-cite&gt;. Imagine two products: recommending the first leads to a 5% purchase likelihood, while the second has a 4% chance. But the second item prompts the customer to look at 5 additional products. This extra data allows our model to learn more, potentially enhancing recommendations for this and other customers in the future. Therefore, we might choose to recommend the second product to generate more user-driven training data.&lt;/p&gt; &lt;p&gt;In this project, we consider the multi-task learning that achieves better performance along the entire customer journey. The conventional conversion rate based model estimates&lt;/p&gt; \[P(conversion|click, impression, u_i, v_j)\] &lt;p&gt;where \(u_i\) are users’ features and \(v_j\) are items’ features.&lt;/p&gt; &lt;p&gt;We decompose the conversion rate into&lt;/p&gt; \[P(conversion, click|impression, u_i, v_j) = P(click|impression, u_i, v_j) \times P(convsersion|click, u_i, v_j)\] &lt;p&gt;Hence, we have two auxiliary tasks for predicting both the click-through rate and the conversion rate. Such approach has two advantages. First, the task for estimating the click-through rate generally has richer training data because we train on dataset with all impressions instead of the subsample with purchase. Second, we recommend products with both high probability of clicking and purchasing, leading to more training data points in future time periods. This can help us tackle the challenge of data sparsity &lt;d-cite key=&quot;ma2018entire&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;h2 id=&quot;literature&quot;&gt;Literature&lt;/h2&gt; &lt;p&gt;Recommender Systems are usually classified into three categories &lt;d-cite key=&quot;1423975&quot;&gt;&lt;/d-cite&gt;: (i) collaborative filtering (ii) content-based ,and (iii) hybrid.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Collaborative filtering. The input for the algorithm can be [User, Item, Outcome, Timestamp]. The task is to complete the matrix \(R\), where each column is an item and each row is a user, with the majority of missing elements. The memory based collaborative filtering finds pairs of user \(i\) and \(i&apos;\) using similarity metrics The model based collaborative filtering decomposes \(R^{m\times n} = U^{m\times k}I^{k\times n}\) using matrix factorization, where \(k\) is the dimension of latent factors.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Content-based. The input for the algorithm can be [User features, Item features, Outcome]. The task is to predict \(y=f(u_i, v_j)\), where \(y\) is the outcome and \(u_i\) and \(v_j\) are features of users and items respectively.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Hybrid. we consider a simple linear model &lt;d-cite key=&quot;1423975&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; \[r_{ij} = x_{ij}\mu+z_i\gamma_j+w_j\lambda_i+\epsilon_{ij}\] &lt;p&gt;where \(x_{ij}\) is the collaborative filtering component indicating the interaction, \(z_i\) are users’ features and \(w_j\) are items’ feature. \(\gamma_j\) and \(\lambda_i\) are random coefficients. We can also apply matrix factorization to reduce the dimension of interaction matrix \(x_{ij}\). A recent application in marketing can be found in &lt;d-cite key=&quot;10.1145/3523227.3547379&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;The core idea in collaborative filtering is “Similar consumers like similar products”. The similarity is defined on consumers’ revealed preference. However, the content-based approach implicitly assumes users and items should be similar if they are neighborhoods in feature space, which may or may not be true. The limitation of collaborative filtering is that we require a sufficient amount of interaction data, which is hard if we consider the sparsity and cold start problems.&lt;/p&gt; &lt;p&gt;Moreover, deep learning based recommender systems have gained significant attention by capturing the non-linear and non-trivial user-item relationships, and enable the codification of more complex abstractions as data representations in the higher layers. A nice survey for deep learning based recommender system can be found in &lt;d-cite key=&quot;10.1145/3285029&quot;&gt;&lt;/d-cite&gt;. Deep learning based recommender system can have several strength compared to conventional models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;It’s possible to capture complex non-linear user-item interactions. For example, when we model collaborative filtering by matrix factorization, we essentially use the low-dimensional linear model. The non-linear property makes it possible to deal with complex interaction patterns and precisely reflect user’s preference &lt;d-cite key=&quot;HORNIK1989359&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Architecture, such as RNN and CNN, are widely applicable and flexible in mining sequential structure in data. For example, &lt;d-cite key=&quot;10.1145/2988450.2988451&quot;&gt;&lt;/d-cite&gt; presented a co-evolutionary latent model to capture the co-evolution nature of users’ and items’ latent features. There are works dealing with the temporal dynamics of interactions and sequential patterns of user behaviours using CNN or RNN &lt;d-cite key=&quot;tang2018personalized&quot;&gt;&lt;/d-cite&gt; &lt;d-cite key=&quot;10.1145/2959100.2959167&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Representation learning can be an effective method to learn the latent factor models that are widely used in recommender systems. There are works that incorporate methods such as autoencoder in traditional recommender system frameworks we summarize above. For example, autoencoder based collaborative filtering &lt;d-cite key=&quot;10.1145/2740908.2742726&quot;&gt;&lt;/d-cite&gt;, and adversarial network (GAN) based recommendation &lt;d-cite key=&quot;10.1145/3077136.3080786&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt; &lt;p&gt;We implement the multi-task learning similar to &lt;d-cite key=&quot;ma2018entire&quot;&gt;&lt;/d-cite&gt;:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-proposal_JingpengHong/multitask-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-proposal_JingpengHong/multitask-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-10-proposal_JingpengHong/multitask-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-10-proposal_JingpengHong/multitask.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;However, we differ from the model in &lt;d-cite key=&quot;ma2018entire&quot;&gt;&lt;/d-cite&gt; in two ways:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;For user field, we implement RNN to deal with the sequential clickstream data instead of simple MLP.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;We define the loss function over the over samples of all impressions. The loss of conversion rate task and the loss of click-through rate task will not be used separately because both of them are based on subsamples (conditional on click and conditional on purchase).&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; \[L(\theta_{click}, \theta_{convsersion})=\sum_{i=1}^N l(click_i, f(u_i, v_j))+\sum_{i=1}^N l(click_i, purchase_i, f(u_i, v_j))\] &lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt; &lt;p&gt;The dataset we use is a random subsample from &lt;d-cite key=&quot;ma2018entire&quot;&gt;&lt;/d-cite&gt;, which is the traffic logs from Taobao’s recommender system. We do a 1% random sampling, though the public dataset in &lt;d-cite key=&quot;ma2018entire&quot;&gt;&lt;/d-cite&gt; has already been a 1% random sampling of the raw data. The summary statistics of the data can be found in &lt;d-cite key=&quot;ma2018entire&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;For the performance metrics, we use Area under the ROC curve (AUC).&lt;/p&gt; &lt;p&gt;Several benchmark models we use for comparsion:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;DeepFM &lt;d-cite key=&quot;10.5555/3172077.3172127&quot;&gt;&lt;/d-cite&gt;. This is a factorization-machine based neural network for click-through rate prediction. In my setting, I consider it as a single-task model with MLP structure.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;MMOE &lt;d-cite key=&quot;10.1145/3219819.3220007&quot;&gt;&lt;/d-cite&gt;. This is the multi-task setting. However, since the usecase is MovieLens, where two tasks are “finish” and “like”, it doesn’t consider the type of sequential data. In my setting, I consider it as a multi-task model with MLP structure.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;xDeepFM &lt;d-cite key=&quot;10.1145/3219819.3220023&quot;&gt;&lt;/d-cite&gt;. This model Combines both explicit and implicit feature interactions for recommender systems using a novel Compressed Interaction Network(CIN), which shares some functionalities with CNNs and RNNs. In my setting, I consider it as a single-task model with RNN/CNN structure.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Our Model, a multi-task model with RNN/CNN structure.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;test AUC&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;test click AUC&lt;/th&gt; &lt;th style=&quot;text-align: center&quot;&gt;test conversion AUC&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DeepFM&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.3233&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MMOE&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.5303&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.6053&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;xDeepFM&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.4093&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ours&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.5505&lt;/td&gt; &lt;td style=&quot;text-align: center&quot;&gt;0.6842&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </content> </entry> <entry> <title>Understanding Limitations of Vision-Language Models</title> <link href="https://deep-learning-mit.github.io/blog/2022/Vision_Language_Limitations/"/> <updated>2022-12-01T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2022/Vision_Language_Limitations</id> <content type="html">&lt;h2 id=&quot;why-are-vision-language-models-important&quot;&gt;Why are vision-language models important?&lt;/h2&gt; &lt;p&gt;The emergence of joint vision-language models such as Contrastive-Language Image Pretraining (CLIP) [1] from OpenAI, and GAIA-1 [2] from Wayve AI have had critical implications in computer vision, robotics, generative AI, self-driving, and more. The key idea of these large foundation models is that they learn meaningful data representations of labeled (text, image) pairs. Once trained, these learned representations are sufficiently versatile and can directly be deployed for a broad range of applications. Such transfer learning is referred to as zero shot learning, where the learned representations can directly be used for unseen data in a new task context without any additional training.&lt;/p&gt; &lt;h2 id=&quot;how-is-our-work-different-from-previous-related-work&quot;&gt;How is our work different from previous related work?&lt;/h2&gt; &lt;p&gt;Many follow up works have since examined how these large vision-language models perform with respect to various scenarios. Prior works study these effects in the context of transfer learning. Jain et al. looks at how performance is examined with respect to the quality of the dataset and provides examples where the performance can be improved by removing from the source dataset [5]. This can be done by utilizing linear classifiers in a scalable and automatic manner [6]. Santurkar et al. explored the impact of language supervision in vision models, and when the pre-training dataset is sufficiently large and contains relevant captions, the model will outperform other image-only models [4]. Shen et al. investigated CLIP’s advantages in outperforming widely used visual encoders through task-specific fine-tuning and combining with vision-language model pre-training [7]. While the aforementioned literature made valuable contributions in understanding the performance of vision-language models, they do not present a clear understanding of what goes on behind the “black box” of the model’s behavior and performance.&lt;/p&gt; &lt;p&gt;Our study is novel in that we provide a more in-depth, detailed analysis of both the impact of descriptive text (or the lack thereof) in vision-language models, in conjunction with the subtleties of dataset biases. We want to clearly visualize these variables’ impacts on model behavior and provide an explanation for such results. We specifically propose a (toy) expansion of prior work on understanding the role of text description [4]. Prior work claims that text descriptions with low variability will ensure that transferred features from CLIP models will outperform image only models. In our work, we will then examine how more descriptive text labels can help overcome biases in dataset and address domain shift.&lt;/p&gt; &lt;h2 id=&quot;how-are-these-models-trained&quot;&gt;How are these models trained?&lt;/h2&gt; &lt;p&gt;CLIP and GAIA are based on transformer architectures [3], which were originally developed for natural language processing and later adopted for computer vision as well. Two separate encoders, a text encoder and an image encoder, separately transform input data from their respective data modality into feature vectors. In aligning images and text in feature space, CLIP and GAIA are able to learn semantically meaningful and robust representations that are useful for several downstream applications. These models perform this embedding space alignment in different ways. CLIP performs training by predicting which image features correspond to which text embeddings in a batch of (image, text) pairs. GAIA is trained in an autoregressive manner, predicting the next token, given past image, text, and action states. GAIA is reported to have ~9 billion parameters and CLIP is reported to have ~63 million parameters. The differences between these two architectures are also related to the type of input data that is being analyzed. While CLIP operates on single images, GAIA is meant to be used for self-driving, meaning that it operates on videos rather than images. As a result, GAIA requires some notion of temporal consistency, which is why autoregression is a good architecture, and more parameters (since video data is more complex than image data). In this study, we will primarily focus on the CLIP architecture (shown below for convenience).&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/clip_model-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/clip_model-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/clip_model-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/clip_model.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; CLIP Architecture, a commonly used vision-language model [1]. (We apologize for blurring, couldn’t figure out how to get rid of it).&lt;/p&gt; &lt;h2 id=&quot;could-the-dataset-play-a-role-in-training&quot;&gt;Could the dataset play a role in training?&lt;/h2&gt; &lt;p&gt;The nature of the training process of CLIP models introduces questions about how robust the training procedure would be. The training relies on (image, text) pairs, but a single text phrase is not a unique description of an image, and a single text description can be used to describe many different scenes. This one-to-many mapping problem introduces questions about what the optimal text description of a given image should be, or if that optimal description even exists. Santurkar et al. [4] looks at how vision-language models such as CLIP and Simple framework for Contrastive Learning of visual Representations (SimCLR) exhibit different performance based on whether they are trained with or without captions and only images. We were inspired by the study’s suggestion that the descriptiveness of the dataset captions can directly influence how well the CLIP models transfer.&lt;/p&gt; &lt;p&gt;A more interesting question, that we answer in this blog post, is could having more descriptive text descriptions allow these large foundation models to mitigate or overcome dataset bias?&lt;/p&gt; &lt;p&gt;To study this question, we consider a toy example with dogs and camels in the classic domain adaptation problem. In this context, we answer the following question:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Can more descriptive text labels enable better domain adaptation in vision-language models with biased datasets?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Domain adaptation is a problem in transfer learning where we want to have a model be able to learn the model in one context, and then generalize to another context. In other words, given a source domain that the model is trained on, domain adaptation is the problem of having high model performance in the target domain. In the dog vs. camel example, the domain adaptation problem occurs when we are used to seeing dogs and camels in certain contexts. For example, we generally expect to see camels in the desert and dogs in suburban environments (e.g. on the lawn, inside the house). If a model is trained to see such examples, then is suddenly shown a camel inside a house in Cambridge, the model has a strong chance of failure. Performance failure under domain shift is indicative that the model failed to disentangle background features from the camel itself. We will study whether descriptive text labels can enhance domain adaptation ability of current transformer-based foundation models.&lt;/p&gt; &lt;h2 id=&quot;understanding-role-of-text-labels-in-clip-gaia&quot;&gt;Understanding role of text labels in CLIP, GAIA&lt;/h2&gt; &lt;p&gt;Due to the large model size, invisible datasets, and large number of GPU hours needed to train CLIP and GAIA, we perform an analysis in a toy setup using the domain adaptation problem we described above. Our goal is to align image and text features, and then visualize the embeddings corresponding to different image classes.&lt;/p&gt; &lt;p&gt;Each of the four experiments determine 1) how the models respond to dataset bias, and 2) how important the addition of descriptive text labels are in improving performance using a trade-off combination of the variables. We aim to measure and visualize the extent to which the caption aids in overcoming biases in training data.&lt;/p&gt; &lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt; &lt;p&gt;Our architecture is shown below. We have two separate transformer architectures: an image encoder and a text encoder. The output of each of these encoders is mapped to an image and text embedding, then L2-normalized. We then compute the cosine similarity of the two embeddings and use the similarity and compute a binary cross entropy loss. Note that, unlike CLIP, we do not compute similarity across all samples within a batch. We only compute cosine similarity for a sample (image, text) pair.&lt;/p&gt; &lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Image Generation.&lt;/strong&gt; We generated our own dataset using DALL-E 2. The total size of the training dataset is 196 images, with (1) 48 images of horses on grass, (2) 48 images of horses in the desert, (3) 48 images of camels in the desert, and (4) 48 images of camels on grass. Note that the DALL-E generated images are used for academic purposes, and are not intended for any commercial use, as required by DALL-E terms and conditions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Text Labels.&lt;/strong&gt; We had two cases: a descriptive label and an undescriptive label. In the descriptive label case, we used the following labels for each of the four cases above (1) “horse on the grass”, (2) “horse in the desert”, (3) “a camel in the desert”, (4) “camel on the grass”. In the undescriptive label case, we just used the labels (1) “horse”, (2) “horse”, (3) “camel”, (4) “camel”.&lt;/p&gt; &lt;h3 id=&quot;experiment-1-no-dataset-bias-undescriptive-text-labels&quot;&gt;Experiment 1: No Dataset Bias, Undescriptive Text Labels&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Description:&lt;/em&gt;&lt;/strong&gt; In the first experiment, we first baseline our text and images encoders ability to perform classification of camels and horses in the case when there is no dataset bias. We use all 196 images with undescriptive labels, so that there is an even split between all four cases (each case comprises ¼ of the dataset). The goal is to assess how well the model can learn and generalize across different classes, and provides the basis for the models’ inherent capabilities and performance without impact from external factors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Results:&lt;/em&gt;&lt;/strong&gt; We performed Principal Component Analysis (PCA) on the feature vectors of our output from the image encoder and the text encoder in order to visualize more similar labels being mapped closer to each other. We notice that camels in desert and camels in grass are closer together in the feature space, while horses in desert and horses in grass are closer together. There is some overlap between camels in grass and horses in deserts, indicating some confusion with the context of the scene. That said, there is a very clear distinction between camels in the desert and horses in the grass, implying that the model is clearly aware that they are very different classes. The overall separation is rather decent when there is no dataset bias.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_2-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_2-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_2-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_2.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; Vague separation in different environments with less descriptive labels.&lt;/p&gt; &lt;h3 id=&quot;experiment-2-no-dataset-bias-descriptive-text-labels&quot;&gt;Experiment 2: No Dataset Bias, Descriptive Text Labels&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Description:&lt;/em&gt;&lt;/strong&gt; In the second experiment, we keep the dataset unbiased, but add descriptive labels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Results:&lt;/em&gt;&lt;/strong&gt; In the plot below, we can see that using descriptive labels slightly improves the separation between classes in the unbiased dataset case. Specifically note the strong separation between red (camels in desert) and green (horses in grass). These two cases are easiest to distinguish, as is reflected in the scattered plot below. Interestingly, when we use descriptive text, the labels are getting bunched together based on context. In particular, horses and camels in the desert are being placed close together, while horses and camels in the grass are being placed close together. This is likely because the model is learning to use the context as a way to separate classes as well. There is still a general progression from red (camels in desert) → blue (horses in desert) → cyan (camels in grass) → green (horses in grass), suggesting some semantic smoothness in feature space. The transition between blue and cyan is rather abrupt though.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_3-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_3-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_3-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/fig_3.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Improvements in class separation with more descriptive labels.&lt;/p&gt; &lt;h3 id=&quot;experiment-3-dataset-bias-undescriptive-text-labels&quot;&gt;Experiment 3: Dataset Bias, Undescriptive Text Labels&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Description:&lt;/em&gt;&lt;/strong&gt; In the third experiment, we begin to investigate the role of dataset bias. The goal is to build on the results from the first experiment, reproducing a common aforementioned problem of over- or under-representation in datasets. We look at how the model responds to dataset bias and whether its performance can still stay the same, regardless of how the images are distributed in classes. Dataset bias is defined by the percentage of minority samples that we remove (minority samples are horses in desert and camels in grass). For example, we originally used 48 images of horses in the desert. 25% bias is defined as using only 12 images of horses in the desert.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Results:&lt;/em&gt;&lt;/strong&gt; These results will be jointly explained with experiment 4.&lt;/p&gt; &lt;h3 id=&quot;experiment-4-dataset-bias-descriptive-text-labels&quot;&gt;Experiment 4: Dataset Bias, Descriptive Text Labels&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Description:&lt;/em&gt;&lt;/strong&gt; In the fourth experiment, we dive deeper into the impact of dataset bias that we began exploring in the second experiment, and question whether performance will be improved when the provided text labels are more descriptive. This directly answers the question of how impactful descriptive text is in vision-language models, in addition to whether they can help overcome dataset bias.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Results:&lt;/em&gt;&lt;/strong&gt; Surprisingly, when the dataset is more biased, we find that the separation between classes is better. We believe this to be true because the model is able to identify clear separation between horses and camels based on the context alone. As a result, it is easily able to separate red and green classes as the bias increases. We notice that the minority classes (horses in desert and camels in grass) also spread out in latent space as the dataset is biased. When using descriptive labels, we notice that the blue points (horses in the desert) are able to separate themselves more from other clusters than in the undescriptive case, indicating some success with descriptive labels in the event of dataset bias. Overall, across all cases, the model generally has an easy time separating camels in the desert, which is likely due to the distinctness of the background and the object.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/dataset_bias-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/dataset_bias-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/dataset_bias-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2022-11-10-Vision_Language_Limitations/dataset_bias.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; More biased dataset can show more separation between classes.&lt;/p&gt; &lt;h2 id=&quot;limitations-and-potential-confounding-parameters&quot;&gt;Limitations and Potential Confounding Parameters&lt;/h2&gt; &lt;p&gt;There are several possible confounding parameters that may have impacted our results beyond the variables that we were looking at. They include the following:&lt;/p&gt; &lt;h3 id=&quot;dataset-1&quot;&gt;Dataset&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Generating the dataset:&lt;/em&gt; Because we used DALL-E to generate our dataset, the limitations of DALL-E itself can carry over to our performance. The inherent diversity of the data that DALL-E uses to train would directly impact our results, as well as the hyperparameters that were modified in training DALL-E. DALL-E could also have a specific image aesthetic that are different from real photography.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Size:&lt;/em&gt; Model performance can also be impacted by a limited dataset. We trained and validated our model on 196 images, which is not a large dataset. The confounding variable here would be the complexity of the images, where there may be less images with less clear distinctions of “horses in the grass” or “camels in the desert”. Furthermore, there are different breeds, sizes, colors, and shapes of horses and camels that may not have been fully explored due to less room for them.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Composition sensitivity:&lt;/em&gt; Literature review has shown that the model’s performance can be impacted by quality in addition to the quantity of the data [5]. Recent evidence has proved that removing data from a dataset can aid in transfer learning and improve downstream effectiveness. While we did not run experiments in identifying what specific composition and characteristics of the data should be removed, the analysis would have impacted our results.&lt;/p&gt; &lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Computational resources:&lt;/em&gt; Because we were restricted by GPU resources, we chose to use a smaller dataset and small self-trained Transformer architectures. We were also unable to train for more epochs, restricting having a more complex model architecture that could’ve lowered model performance. We found that increasing the batch size or increasing the number of layers lead our model to run out of computational power and continually crash.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Tuning hyperparameters:&lt;/em&gt; Batch size, learning rate, number of layers, optimization models, and other factors could also limit the exploration of optimal configurations and affect overall performance. For example, a higher learning rate in a model could converge faster and show higher performance, when in reality, it is not an accurate reflection of the model. Overfitting and different regularization parameters can also lead to over- or under-fitting.&lt;/p&gt; &lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt; &lt;p&gt;Our toy problem gives some intuition into the idea that the descriptiveness of the label can affect the clustering profile of different datasets. Note that because our experiments were done in smaller settings, we cannot make any claims with respect to scaling up to large amounts of data, compute, and model size. That said, when adding description of the context of the images (i.e. desert vs. grass), we noticed that the points in feature space began to cluster first based on context, then based on the animal type (camel vs. horse). We also noticed that under dataset bias, the majority groups (horses in grass and camels in desert) begin to have better clustering separation. However, the minority group performance decreased, which suggests the importance of accounting for dataset bias in machine learning algorithms. In our experiments, we partially found more descriptive labels to help mitigate these negative effects, but mitigating these effects more reliably is an ongoing research direction.&lt;/p&gt; &lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Radford et al., &lt;em&gt;“Learning transferable visual models from natural language supervision”&lt;/em&gt;, ICML 2021&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Hu et al., &lt;em&gt;“GAIA-1: A Generative World Model for Autonomous Driving”&lt;/em&gt;, arXiv 2023&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Vaswani et al. &lt;em&gt;“Attention Is All You Need”&lt;/em&gt;, NeurIPS 2017&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Santurkar et al., &lt;em&gt;“Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning”&lt;/em&gt;, CVPR 2022&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Jain et al., &lt;em&gt;“A Data-Based Perspective on Transfer Learning”&lt;/em&gt;, CVPR 2023&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Jain et al, &lt;em&gt;“Distilling Model Failures as Directions in Latent Space”&lt;/em&gt;, ICLR 2023&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Shen et al. &lt;em&gt;“How Much Can CLIP Benefit Vision-and-Language Tasks?”&lt;/em&gt;, arXiv 2021&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; </content> </entry> <entry> <title>Contrastive Representation Learning for Dynamical Systems</title> <link href="https://deep-learning-mit.github.io/blog/2022/contrastive-time/"/> <updated>2022-11-07T00:00:00+00:00</updated> <id>https://deep-learning-mit.github.io/staging/blog/2022/contrastive-time</id> <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;h3 id=&quot;dynamical-system&quot;&gt;Dynamical System&lt;/h3&gt; &lt;p&gt;Dynamical systems form the foundation for understanding intricate phenomena in both scientific research and engineering applications. These systems are defined by their &lt;strong&gt;state&lt;/strong&gt; (denoted as $X$) at any given time and a set of &lt;strong&gt;equations&lt;/strong&gt; (e.g., $v = \frac{dX}{dt} = f_t(X, \theta)$) that describe the evolution of these states over time ($t$), all driven by &lt;strong&gt;underlying parameters&lt;/strong&gt; $\theta$. Some real-world examples of dynamical systems include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Climate Systems&lt;/strong&gt;: Involves states like temperature, pressure, and wind velocity, with parameters such as solar radiation and greenhouse gas concentrations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Population Dynamics in Ecology&lt;/strong&gt;: Features states like population sizes, with parameters including birth and death rates, and interaction rates between species.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Economic Models&lt;/strong&gt;: Focus on states like stock prices and trading volume, influenced by parameters like interest rates and market sentiment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Control Systems in Engineering&lt;/strong&gt;: Encompasses states like the position and velocity in robotics or the aircraft’s orientation in flight dynamics, governed by parameters like physical properties and control gains.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The evolution of the system’s state over time can be observed as a time series, where system underlying parameters ($\theta$) governs the system’s behavior. In our project, we would like to determine if it would be feasible to discover the underlying system parameters given the observed trajectory. It would lay the groundwork for both robust predictive modeling and model interpretability analysis for safety-critical systems, such as clinical application and chemical engineering plants.&lt;/p&gt; &lt;h3 id=&quot;spring-mass-system&quot;&gt;Spring-Mass System&lt;/h3&gt; &lt;p&gt;Consider a spring-mass system, a fundamental model in dynamics. In a system comprising two masses, the states include positions $x$ and velocities $v = \frac{dx}{dt}$, which can be derived from the positions. Crucially, it is the underlying parameters, masses $m_1$, $m_2$ and spring constants $k_1$, $k_2$, that dictate the trajectories of $x$.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 1: A spring-mass system, classical dynamical system to illustrate project idea &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system_traj-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system_traj-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system_traj-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/spring_mass_system_traj.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 2: Spring mass system trajectory at ($k_1$, $k_2$, $m_1$, $m_2$) = (1.0, 2.0, 1.0, 2.0) &lt;/div&gt; &lt;p&gt;Different system parameters, such as mass or spring constant, result in different oscillatory and long-term behavior behaviors of the system. Below is a gif demonstrating the effect of changing parameters on the system’s trajectory; this visualization illustrates how different underlying parameter values lead to distinct dynamical behaviors.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m1.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m1.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m1.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m1.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 3: system dynamic at varying $m_1$, while keeping $k_1$, $k_2$ and $m_2$ constant. &lt;/div&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m2.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m2.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m2.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/system_dynamics_varying_m2.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 4: system dynamics at varying $m_2$, while keeping $k_1$, $k_2$, and $m_1$ constant. &lt;/div&gt; &lt;h3 id=&quot;dataset-collection--generation&quot;&gt;Dataset Collection / Generation&lt;/h3&gt; &lt;p&gt;We create a simulator for the above dynamical system to generate data based on parameters like masses $m$ and spring constants $k$. The parameters are systematically varied to generate a diverse and challenging dataset. More concretely, the dataset can be represented by a tensor of shape $(N_{param}, N_{traj}, T, d)$, where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;$N_{param}$ is the number of parameter sets. Each set of parameters would lead to different system dynamics and trajectories.&lt;/li&gt; &lt;li&gt;$N_{traj}$ is the number of trajectories generated for each parameter set. Within the same set of parameters, different initial conditions and noise level would lead to different trajectories.&lt;/li&gt; &lt;li&gt;$T$ is the number of steps in a trajectory. $T$ is dependent on 2 factors - time span in the simulation, and the time step (i.e., $dt$). Note that our system/model formulation allows $T$ to be different for different trajectories, offering more flexibility.&lt;/li&gt; &lt;li&gt;$d$ is the number of states. In the above example, $d = 4$, representing $(x_1, x_2, v_1, v_2)$.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;related-works&quot;&gt;Related Works&lt;/h3&gt; &lt;p&gt;Time-series data analysis is a crucial component in a wide array of scientific and industrial domains, ranging from dynamical systems and weather forecasting to stock market prediction. These applications often involve underlying parameters that are complex and not immediately observable from the data. Traditional time-series methodologies primarily emphasize prediction, which can result in models that operate as “black-boxes” with limited interpretability &lt;d-cite key=&quot;Lim_2021&quot;&gt;&lt;/d-cite&gt;.&lt;/p&gt; &lt;p&gt;To address this limitation, the representation learning landscape in time-series analysis has expanded recent years, with a focus on unsupervised and semi-supervised methods. Fortuin et al. &lt;d-cite key=&quot;fortuin2020gp&quot;&gt;&lt;/d-cite&gt; developed GP-VAEs, combining Gaussian processes with VAEs to handle noisy and irregularly sampled time-series data. This model addresses variabilities in time-series data, offering robust feature extraction. Franceschi et al. &lt;d-cite key=&quot;Franceschi2019UnsupervisedCL&quot;&gt;&lt;/d-cite&gt; explored self-supervised learning frameworks for time-series, demonstrating how leveraging temporal coherence can improve feature representation without reliance on labeled data.&lt;/p&gt; &lt;p&gt;Building on these advancements, recent studies like those by Eldele et al. &lt;d-cite key=&quot;eldele2021timeseries&quot;&gt;&lt;/d-cite&gt; have explored the application of contrastive learning for enhancing time-series representation. Their approach creates representations by contrasting segments of time-series, thereby learning features that distinguish different temporal patterns. This method has shown promise in enhancing classification and forecasting tasks.&lt;/p&gt; &lt;p&gt;However, there remains an unexplored potential in utilizing contrastive learning for learning the underlying parameters governing these systems. In this project, we aim to bridge this gap by applying the principles of contrastive learning to the specific challenge of identifying and understanding these hidden parameters within dynamical systems. By leveraging contrastive learning, we aim to move beyond mere prediction and delve into a deeper understanding of these parameters, thus enhancing the interpretability of time-series models, particularly applicable in safety-critical systems.&lt;/p&gt; &lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt; &lt;h3 id=&quot;contrastive-learning&quot;&gt;Contrastive Learning&lt;/h3&gt; &lt;p&gt;Contrastive learning is a self-supervised learning technique prevalent in fields such as computer vision (CV) and natural language processing (NLP). At its core, it involves minimizing the embedding similarity between similar objects (i.e., positive pairs) while distancing dissimilar ones (i.e., negative pairs).&lt;/p&gt; &lt;p&gt;In the context of dynamical systems, where the model does not have direct access to parameter values, contrastive learning is an effective method to infer underlying system parameters. In our case of spring-mass system, a positive pair consists of two trajectories generated using the same set of parameters, whereas a negative pair is two trajectories generated using different set of parameters.&lt;/p&gt; &lt;p&gt;We utilize the following InfoNCE (Information Noise-Contrastive Estimation) loss for training:&lt;/p&gt; \[L_{\text{InfoNCE}} = -\log \frac{e^{f(x)^Tf(x^+)/\tau}}{\sum_{i=0}^{N} e^{f(x)^Tf(x^-_i)/\tau}}\] &lt;ul&gt; &lt;li&gt;$f(x)$ is the generated trajectory embedding.&lt;/li&gt; &lt;li&gt;$\tau$ is a (fixed) temperature hyperparameter, which we set to default 1.&lt;/li&gt; &lt;li&gt;($x$, $x^+$) forms the positive pair (i.e., two trajectories with the same underlying parameters but different initial conditions).&lt;/li&gt; &lt;li&gt;($x$, $x_j^-$) form negative pairs (i.e. two trajectories from different underlying parameter sets).&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt; &lt;p&gt;Trajectories in dynamical systems can be represented by a time-series dataset, which is a type of sequential data. Long Short-Term Memory networks (LSTMs), a variant of Recurrent Neural Networks (RNNs), can be used process sequential data and manage long-term dependencies. A key feature of LSTMs is their use of gates, which regulate the flow of information, allowing the network to maintain pertinent information over extended periods — key characteristics for modeling dynamical systems. These gates include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Forget Gate&lt;/strong&gt;: Decides which information from the cell state should be discarded. It uses the current input and the previous hidden state to generate a value between 0 and 1 for each number in the cell state, with 1 indicating “keep this” and 0 indicating “discard this.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input Gate&lt;/strong&gt;: Determines what new information will be added to the cell state. It involves two parts: a sigmoid layer that decides which values will be updated and a tanh layer that creates a vector of new candidate values.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Gate&lt;/strong&gt;: Decides what information from the cell state will be used to generate the output. It takes the current input and the previous hidden state, passes them through a sigmoid layer, and multiplies the output by a tanh of the cell state to decide which parts of the cell state make it to the output.&lt;/li&gt; &lt;/ol&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/LSTM-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/LSTM-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/LSTM-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/LSTM.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 5: Diagram of LSTM &lt;d-cite key=&quot;towardsdatascience_lstm_gru&quot;&gt;&lt;/d-cite&gt;. &lt;/div&gt; &lt;p&gt;In the context of the contrastive learning framework, the choice of model is a design choice. Essentially, any model capable of converting a trajectory into an embedding, such as a transformer, could be utilized. While transformers have shown remarkable results in CV and NLP, their performance on smaller datasets remains an area less explored. Previous studies in dynamical systems have predominantly employed RNN-based approaches. In this project, we aim to study if LSTM is capable of capturing the dynamics of system through its hidden and cell states.&lt;/p&gt; &lt;h3 id=&quot;training-objectives&quot;&gt;Training objectives&lt;/h3&gt; &lt;p&gt;Trajectories are passed through an LSTM to generate trajectory embeddings, derived from the cell states of the LSTM’s final layer. In our training framework, there are 2 loss functions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Contrastive objective (InfoNCE loss) is applied on the trajectory embedding. This loss encourages model to create embeddings that meaningfully distinguish between different system dynamics.&lt;/li&gt; &lt;li&gt;Prediction objective (MSE) is applied between the ground truth state (i.e., $X_{t+1}$) and the prediction state (i.e., $\hat{X}_{t+1}$) at the next step. This loss encourages model to use the current state and embedding to predict next step behavior.&lt;/li&gt; &lt;/ul&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/contrastive_representation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/contrastive_representation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/contrastive_representation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/contrastive_representation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 6: Diagram of contrastive representation learning in this project. Trajectories derived from the same parameter set (e.g. top two trajectories in figure) form a positive pair; they map closely in latent space and show invariance to initial states. Trajectories from different parameter sets are distant in latent space, despite identical initial states (e.g. bottom two trajectories). &lt;/div&gt; &lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt; &lt;p&gt;The objective of the project to estimate the system parameters from observed trajectories. Therefore, the primary metric for our evaluation strategy is the &lt;strong&gt;MAE on underlying parameter estimation&lt;/strong&gt;. This involves applying linear probing to the model’s embeddings against known ground truth parameters on a subset of the training set (i.e., a linear system $X\beta = Y$ is solved, with &lt;em&gt;X&lt;/em&gt; representing the trajectory embeddings, and &lt;em&gt;y&lt;/em&gt; being the ground truth parameters). Since it is a simple linear transformation of the original features, it has limited capacity to alter feature complexity. Essentially, if a model can perform well under linear probing, it suggests that the learned embeddings themselves are robust and informative with respect to the underlying parameters.&lt;/p&gt; &lt;p&gt;The following plot shows the result of the contrastive learning framework on the validation set. Left subplot corresponds to the ground truth parameter, right subplot corresponds to the predicted parameter using the above contrastive learning framework. For a focused visualization, we only varies 2 parameter (i.e., $m_1$, $m_2$). Each point in the plot is annotated with its corresponding parameter values. For each parameter set, we evaluate on 2 trajectories with different initial conditions.&lt;/p&gt; &lt;p&gt;On the right plot, we observe similar data points are grouped together in the parameter space, indicating that the model is capable of clustering trajectories generated from the same parameter set together. Comparing the left and right plots, we observe the model is capable to predicting parameters to be close to ground truth parameters. Overall, the MAE for parameter estimation is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.043&lt;/code&gt;, underscoring the model’s precision in parameter prediction.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_estimation-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_estimation-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_estimation-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_estimation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 7: Result of parameter estimation. Left subplot corresponds to the ground truth parameter, right subplot corresponds to the predicted parameter using the above contrastive learning framework. &lt;/div&gt; &lt;p&gt;Additionally, we would also like the model to be capable of predicting the future trajectories. For this objective, the secondary metric is the &lt;strong&gt;MAE on next-step prediction&lt;/strong&gt;. High value on this metrics would indicate model’s ability to accurately forecast future states, which is a necessary but may not be sufficient step towards a more complex, weekly-supervised parameter inference tasks. The MAE on the validation set is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.00024&lt;/code&gt;, and we will discuss it more in the Experiments section.&lt;/p&gt; &lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt; &lt;p&gt;In the previous section, Figure X above shows the final result. We want to include 2 components in this section: 1) different things we attempted to reach the results in Figure X, and 2) several experiments to study how different factors affect model’s capability of discovering the underlying parameters.&lt;/p&gt; &lt;p&gt;Due to computational and time limitation, the numbers reported in this section are not from the final model, which trained for a much longer time. Instead, we ran numerous experiments and compared performance after 2000 steps, at which point the training loss has roughly plateaued.&lt;/p&gt; &lt;h3 id=&quot;effect-of-initial-conditions&quot;&gt;Effect of initial conditions&lt;/h3&gt; &lt;p&gt;The effect of different initial conditions in dynamical system is analogous to the effect of data augmentation in CV. The challenge is that different initial conditions may affect the trajectories more than the change in parameter.&lt;/p&gt; &lt;p&gt;We initially used the same initial conditions for all set of parameters and led to parameter MAE of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.01&lt;/code&gt; in the validation set. However, the model doesn’t generalize to other initial conditions; when evaluating the model on the validation set that has different initial condition, MAE increased to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.31&lt;/code&gt;, indicating overfit.&lt;/p&gt; &lt;p&gt;To ensure our model effectively discerns differences in trajectories arising from varying initial conditions, we generate 100 trajectories from each parameter set with random initial conditions, aiming to train the model to be invariant to these initial conditions and capture the essence of the system parameters. With this “data augmentation”, we bridged the gap between training and validation performance to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.061&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.065&lt;/code&gt; respectively.&lt;/p&gt; &lt;h3 id=&quot;number-of-prediction-steps&quot;&gt;Number of prediction steps&lt;/h3&gt; &lt;p&gt;We also considered the limitations of next-step prediction, particularly for high-frequency samples (i.e., small $dt$). A trivial model might simply predict state $X$ at time $t+1$ as $X_t$, and achieve a small loss since $X_{t+1} - X_t$ may be small for small $dt$. To avoid model taking shortcuts, we shift our focus from immediate next-step prediction to forecasting next-k-steps ahead. We also anticipate that accurate longer-horizon predictions would require a deeper understanding of the underlying parameters, potentially leading to improved performance in parameter estimation. This improves the parameter MAE on the validation set from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.10&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.065&lt;/code&gt;. The following figure illustrates an results of predicting 30 steps ahead.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/gt_and_pred_traj-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/gt_and_pred_traj-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/gt_and_pred_traj-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/gt_and_pred_traj.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 8: Ground truth and predicted trajectory for predicting 30 steps (i.e., 3 seconds). &lt;/div&gt; &lt;h3 id=&quot;decouple-state-and-parameter-embedding&quot;&gt;Decouple state and parameter embedding&lt;/h3&gt; &lt;p&gt;In our hypothesis, the latent space of a trajectory encodes dual forms of information: “long-term” information pertaining to system parameters, and “short-term” information reflective of the current state. Traditional approaches applying contrastive learning across the entire latent vector may not optimally capture this duality.&lt;/p&gt; &lt;p&gt;To address this, we propose to decouple the state and parameter embedding space. Concretely, for positive pairs emerging from identical parameters but divergent initial conditions, our approach focuses on computing the InfoNCE loss solely on the segment of the embedding representing the parameter. This is operationalized by limiting contrastive learning to the initial W dimensions of the latent vector, denoted as $z[:W]$. This strategy aims to specialize $z[:W]$ in encoding system parameters, while allowing the remaining part of the vector, $z[W:]$, the flexibility to encapsulate other trajectory aspects, such as initial conditions and inherent noise.&lt;/p&gt; &lt;p&gt;However, the performance didn’t increase across various values of $W$. This stagnation might stem from our use of the LSTM cell state as the latent embedding. Given that the cell state inherently integrates “long-term” information, with “short-term” data predominantly residing in the hidden states, restricting ourselves to $z[:W]$ potentially reduces the representational power of our contrastive learning framework.&lt;/p&gt; &lt;h3 id=&quot;effect-of-key-hyperparameters&quot;&gt;Effect of key hyperparameters&lt;/h3&gt; &lt;p&gt;We utilized WandB for a hyperparameter sweep to investigate their impact on the model’s performance in next-steps prediction and underlying parameter estimation. Key hyperparameters explored include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Embedding Size&lt;/strong&gt;: We observed that increasing the embedding size from 10 to 200 led to a reduction in the InfoNCE loss from 0.862 to 0.007, and the corresponding parameter estimation estimation MAE peaked when embedding size reached 100. This suggests a larger embedding size can increase the capacity to more effectively inferring underlying system parameters. However, maintaining the embedding size at a balanced level is crucial to ensure the model concentrates on the most pivotal aspects of data variation, rather than overfitting to minor system details.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Number of LSTM Layers&lt;/strong&gt;: Increasing the number of LSTM layers improved both next-step prediction and parameter estimation. Notably, with more LSTM layers, a smaller embedding size became sufficient for achieving desirable outcomes in both prediction and parameter inference. This implies a deeper LSTM architecture can capture more complex pattern in the data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prediction Horizon (Predict Ahead)&lt;/strong&gt;: We observe a modest improvement in performance on parameter estimation MAE (i.e., 0.04) as the prediction horizon increases. This improvement, while positive, was less pronounced than anticipated. In our model, contrastive learning serves as the primary mechanism for learning about system parameters, with next-k-step prediction intended to supplement this learning process. Theoretically, as the prediction horizon (k) increases, the complexity of the next-k-step prediction task escalates. This demands more focus from the model, potentially at the expense of its capacity for contrastive learning. Consequently, the variable k emerges as a hyperparameter to strike an optimal balance between two competing objectives: facilitating overall learning (where a larger k is advantageous), and maintaining a focus on contrastive learning (where a smaller k is beneficial).&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&quot;noise-level-in-data-generation&quot;&gt;Noise level in data generation&lt;/h3&gt; &lt;p&gt;In real-world applications, models often lack direct access to state values due to the inherent stochasticity of systems or observation noise. In high-precision engineering applications, this noise is typically constrained to below 1%. However, in less precise scenarios, the noise in observed data can reach levels as high as 20%. It’s important to note that these errors are not merely observational errors, which can be assumed to be independent and identically distributed (i.i.d). Rather, these errors are intertwined with the state itself and can propagate over time, affecting subsequent observations. The figure below illustrates how noise can significantly alter trajectories. For instance, at a 20% noise level, the state variable $x_1$ markedly diverges from its intended path around the 8-second mar&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/traj_with_noise-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/traj_with_noise-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/traj_with_noise-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/traj_with_noise.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 9: Comparison of state trajectories with no noise (top subplot) and 20% noise (bottom subplot). &lt;/div&gt; &lt;p&gt;The following section evaluates the model’s performance using noisy observed data. During trajectory generation, we introduce random noise according to the formula $X_{obs} = X_{true} (1 + \alpha \mathit{N}(0, 1))$ where $\alpha$ is the noise-to-signal ratio. We studied the model’s performance across various noise levels, ranging from $\alpha = 0.0$ to $\alpha = 0.2$, and the results are plotting in the following figure.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_noise-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_noise-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_noise-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_noise.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 10: Model performance across different noise levels. The blue bars represent a model trained on a clean dataset, and directly applied to a noisy validation dataset. The orange bars represent a model trained and evaluated on datasets with equivalent noise levels. &lt;/div&gt; &lt;p&gt;Directly applying a model trained with a clean dataset on a noisy dataset would lead to drastic performance drop as shown in the blue bars. During model deployment, it’s a natural choice to train on a dataset with the same noise amount. This could mitigate the drastic performance drop, especially for low to moderate amount of noise (e.g., $\alpha &amp;lt; 0.1$), as shown in the orange bars. However, when noise amount rises to 20%, training on noisy dataset doesn’t help either due to significant deviation from clean data.&lt;/p&gt; &lt;p&gt;Applying a model trained on a clean dataset to a noisy dataset leads to a significant drop in performance, as indicated by the blue bars. In practical model deployment, it’s common to train the model on a dataset with a comparable level of noise. This approach can substantially mitigate performance degradation, particularly at low to moderate noise levels (e.g., $\alpha &amp;lt; 0.1$), as demonstrated by the orange bars. However, at higher noise levels, such as 20%, training on a noisy dataset proves less effective due to the substantial deviation from the clean data.&lt;/p&gt; &lt;h3 id=&quot;generalizability-to-unseen-parameters&quot;&gt;Generalizability to unseen parameters&lt;/h3&gt; &lt;p&gt;In this section, we delve into the model’s generalizability across unseen parameters. Our investigation comprises experiments on both in-distribution and out-of-distribution system parameters. The results of these experiments are illustrated in the following figures.&lt;/p&gt; &lt;p&gt;For in-distribution analysis, our focus was to assess the model’s proficiency in adapting to system parameters that, while differing from those in the training set, still fall within the same predefined range. This aspect of the study aims to understand how well the model can interpolate within the known parameter space.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_in_distribution-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_in_distribution-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_in_distribution-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_in_distribution.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 11: Result of parameter estimation to unseen parameters in-distribution. &lt;/div&gt; &lt;p&gt;On the other hand, the out-of-distribution experiments were designed to challenge the model further by introducing system parameters that lie outside the range encountered during training. This approach tests the model’s ability to extrapolate beyond its training confines.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_out_distribution-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_out_distribution-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_out_distribution-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/param_est_unseen_out_distribution.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 12: Result of parameter estimation to unseen parameters out-of-distribution. &lt;/div&gt; &lt;p&gt;Remarkably, our model demonstrated a robust ability to generalize across both in-distribution and out-of-distribution parameters. It achieved a Mean Absolute Error (MAE) of 0.032 in the former and 0.082 in the latter scenario. These findings suggest that the model not only learns the underlying patterns within the training data but also retains a significant degree of flexibility to adapt to new, unseen parameter sets.&lt;/p&gt; &lt;h2 id=&quot;another-framework---generative-modeling&quot;&gt;Another Framework - Generative Modeling&lt;/h2&gt; &lt;p&gt;While the previously discussed contrastive learning framework shows promise in inferring underlying parameters through a weakly-supervised learning approach, it relies on prior knowledge about the relationship between trajectories and their corresponding parameter sets. Such information may not always be readily available in practical scenarios. To address this challenge, our research pivots towards employing a generative modeling framework, enabling the learning of system parameters in an unsupervised manner.&lt;/p&gt; &lt;p&gt;We transition from contrastive learning to incorporating a &lt;strong&gt;variational autoencoder (VAE) structure&lt;/strong&gt;. This setup operates without explicit knowledge of parameter sets, compelling the model to decipher the underlying patterns solely from the observed trajectories. The VAE framework consists of three primary components: 1) an encoder LSTM that transforms an observed trajectory into a latent representation, 2) a reparameterization layer that molds this latent representation into a specific distribution, and 3) a decoder LSTM that uses the latent representation and initial conditions to reconstruct the trajectory.&lt;/p&gt; &lt;p&gt;Training focuses on 1) the reconstruction loss between real and a generated trajectories, and 2) Mean Absolute Error (MAE) for next-k-step predictions made by the encoder LSTM. This method is designed to challenge the model’s capability to extract insights about the system’s dynamics independently, without relying on any prior information about the trajectories. The framework thus becomes a critical platform for testing the model’s ability to autonomously learn the system’s underlying parameters, requiring an advanced level of unsupervised learning.&lt;/p&gt; &lt;p&gt;The evaluation metrics for this second framework are aligned with the first, utilizing MAE to assess both the underlying parameter estimation and the next k-step prediction accuracy of the encoder LSTM. A key addition in this framework is the &lt;strong&gt;MAE on Reconstruction Loss&lt;/strong&gt;.This metric is used to gauge the model’s ability to accurately reconstruct input sequences, thereby reflecting its understanding of the data’s fundamental structure. A lower reconstruction loss implies that the model has effectively internalized the essential characteristics of the data distribution. Our expectation is that this deeper grasp of data structure will enable the model to infer underlying system parameters independently, without prior exposure to specific parameter set information.&lt;/p&gt; &lt;h2 id=&quot;experiments---generative-modeling&quot;&gt;Experiments - Generative Modeling&lt;/h2&gt; &lt;h3 id=&quot;autoencoder-vs-variational-autoencoder&quot;&gt;Autoencoder v.s. Variational Autoencoder&lt;/h3&gt; &lt;p&gt;In addition to exploring the Variational Autoencoder (VAE) framework, we also experimented with a traditional autoencoder setup. This variant mirrors the architecture of the VAE but excludes the computation of the mean ($\mu$) and log variance ($\log \sigma^2$), thereby omitting the variational element. This modification streamlines the model, narrowing its focus to purely reconstructing input data from its latent representations.&lt;/p&gt; &lt;p&gt;Our findings reveal that the autoencoder configuration surpassed the VAE in both parameter estimation and reconstruction. For parameter estimation MAE, autoencoder and VAE achieved 0.12 and 0.23 respectively. For reconstruction MAE, autoencoder and VAE achieved 0.02 and 0.49 respectively. This performance disparity can be attributed to the inherent constraints of each model. The autoencoder is primarily limited by the dimensionality of the embedding in its latent space. In contrast, the VAE faces an additional constraint due to its need to model the distribution within the latent space.&lt;/p&gt; &lt;p&gt;These results suggest that the variational component, a defining feature of VAEs and instrumental in modeling data distributions, might not be essential for capturing the dynamics specific to our system. By removing the variational aspect, the autoencoder model is enabled to concentrate more effectively on capturing the most salient features for reconstruction and parameter inference. This simpler approach avoids the additional complexity of encoding the data distribution in the latent space, potentially leading to more efficient and targeted learning relevant to our system’s dynamics.&lt;/p&gt; &lt;h3 id=&quot;beyond-reconstruction-evaluating-future-prediction-capabilities&quot;&gt;Beyond Reconstruction: Evaluating Future Prediction Capabilities&lt;/h3&gt; &lt;p&gt;To evaluate our AE model’s generalizability and future prediction capabilities, we expanded its function beyond reconstruction to include forecasting additional steps. The figure presented here compares the ground truth states $x_1$ and $x_2$ (displacements for $m_1$ and $m_2$) against the model’s outputs for both reconstruction and prediction. The model processes input trajectories of 100 time steps and generates outputs for 199 steps, with the initial 99 steps dedicated to reconstruction and the subsequent 100 steps for prediction (unseen by the model during training). The results illustrate effective reconstruction performance but relatively weaker predictive accuracy.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/recon_and_pred-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/recon_and_pred-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/recon_and_pred-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/recon_and_pred.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 13: Reconstruction and further prediction of unseen states. &lt;/div&gt; &lt;p&gt;Given that our autoencoder (AE) framework surpasses the Variational Autoencoder (VAE) in reconstruction and parameter estimation, we speculated whether VAE’s variational component might enhance future predictions. Therefore, we compared the reconstruction and prediction losses between the AE and VAE frameworks.&lt;/p&gt; &lt;p&gt;The corresponding figure, presenting the mean and standard deviation of these losses, reveals that in both frameworks, reconstruction losses and their variability are substantially lower than prediction losses. This trend highlights the ongoing difficulty in achieving precise future predictions within our model configurations.&lt;/p&gt; &lt;p&gt;Furthermore, the AE framework demonstrated superior performance over the VAE in both reconstruction and future step prediction. This outcome suggests that the VAE’s variational component does not necessarily contribute to improved future predictions. Echoing our earlier findings on parameter estimation and reconstruction, the variational aspect might not be pivotal for capturing the dynamics specific to our system. Instead, it could introduce additional complexity by encoding the data distribution in the latent space, which appears to be less relevant for reconstruction and future step prediction tasks.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_recon_and_pred-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_recon_and_pred-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_recon_and_pred-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/comparison_recon_and_pred.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 14: Results of reconstruction loss (first 99 steps) and future prediction loss (last 100 steps) for autoencoder (top subplot) and VAE (bottom subplot). &lt;/div&gt; &lt;h3 id=&quot;effect-of-latent-variables-on-generated-trajectories&quot;&gt;Effect of Latent Variables on Generated Trajectories&lt;/h3&gt; &lt;p&gt;In this section, our objective is to glean insights into the latent variables by manipulating them and observing the resultant changes in the generated trajectories. Given that the embedding dimension (i.e., |z|) exceeds the dimension of the parameters (i.e., |$\theta$|), we initially establish a linear mapping from from $z$ to $\theta$. The following gif demonstrates how the trajectory evolves in response to alterations in the variable $m_1$. The upper part of the gif represents the simulation, while the lower part reflects the output from the decoder of our autoencoder.&lt;/p&gt; &lt;p&gt;A notable observation is that, as m1 undergoes modifications, the predicted trajectories adeptly resemble the period of the simulation trajectories. However, a discrepancy arises in their magnitude, with the predicted trajectories exhibiting a notably smaller scale compared to the ground truth trajectories. This pattern suggests that while the embedding successfully captures certain characteristics of the trajectories, it does not fully encapsulate all their properties.&lt;/p&gt; &lt;p&gt;We hypothesize that enhancing the complexity of the encoder/decoder architecture (e.g., larger number of layers of LSTM layers) might facilitate a more comprehensive capture of trajectory attributes. However, our experimental scope is currently constrained by limitations in CUDA memory, particularly due to the decoder’s requirement to process 99 time steps. This constraint hinders our ability to experiment with architectures involving a greater number of layers, which might otherwise allow for a richer representation and understanding of the trajectory data.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/latent_to_traj.gif-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/latent_to_traj.gif-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-11-08-contrastive-time/latent_to_traj.gif-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/staging/assets/img/2023-11-08-contrastive-time/latent_to_traj.gif&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Figure 15: Effect of latent variables on generated trajectories. &lt;/div&gt; &lt;h2 id=&quot;conclusion-and-future-works&quot;&gt;Conclusion and Future Works&lt;/h2&gt; &lt;p&gt;In contrast to current machine learning literature that predominantly focuses on predicting future states of dynamical systems, our work is geared towards uncovering the underlying system parameters from observed trajectories. Our key contributions include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Implementing two frameworks: an autoregressive LSTM with contrastive learning, and a variational autoencoder architecture. While contrastive learning yields superior parameter estimation, the autoencoder enables unsupervised learning without relying on prior knowledge.&lt;/li&gt; &lt;li&gt;Demonstrating our model’s generalizability to both in-distribution and out-of-distribution unseen parameters, and its effective performance with noisy datasets, sustaining a noise-to-signal ratio of up to 10%.&lt;/li&gt; &lt;li&gt;Conducting thorough experiments to explore the impact of various factors like initial conditions, prediction horizons, and the interplay between state and parameters embeddings. We also examined the influence of latent variables on trajectory generation and the model’s predictive capabilities beyond the confines of the training set.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The ability to accurately estimate underlying system parameters significantly enhances model interpretability, which is crucial in scientific and engineering applications where decision-making stakes are high. We hope our findings will help researchers and students interested in interpretable machine learning for dynamical systems.&lt;/p&gt; &lt;p&gt;While this project did extensive analysis on a spring-mass system, future work may extend this analysis to a broader range of dynamical systems. Moreover, future work can integrate the strengths of both frameworks to incorporate contrastive learning within an unsupervised context, possibly through data augmentation strategies. Further advancements could also focus on refining the impact of latent variables on trajectory generation. Such progress is expected to bolster trust in AI solutions and facilitate their integration into essential decision-making frameworks across various domains.&lt;/p&gt; &lt;p&gt;Here’s the link to our Github Repo: &lt;a href=&quot;https://github.com/martinzwm/meta_param_est&quot;&gt;https://github.com/martinzwm/meta_param_est&lt;/a&gt;&lt;/p&gt; &lt;!-- Proposal ## Introduction Time-series data analysis is pivotal in numerous scientific and industrial applications, including dynamical system, weather forecasting, and stock market prediction. The underlying parameters governing the time-series data can often be complex and not directly observable. Unlike traditional time series approaches, which predominantly focus on prediction tasks, leading to a &quot;black-box&quot; prediction &lt;d-cite key=&quot;Lim_2021&quot;&gt;&lt;/d-cite&gt;. Recent literatures have explored using contrastive learning to learn time-series representation, but none has explored learning underlying system parameters &lt;d-cite key=&quot;eldele2021timeseries&quot;&gt;&lt;/d-cite&gt;. In this project, we want to leverage the contrastive learning approach studied in class to learn underlying system parameters parameters. A deep comprehension of these underlying parameters, if successfully achieved, can lead to 2 benefits - 1) enhanced model capability for making accurate future predictions, and 2) a better understanding of the underlying system. The latter is particularly important in scientific, where the goal is to understand the underlying system, and engineering, where safety and reliability are of paramount importance. To achieve the above goals, we proposed the following experiments and setups to study the insights of using contrastive approach to learn latent parameters for time-series representation. ## Objectives The primary objective of this research is to investigate the effectiveness of contrastive loss learning in capturing the system underlying parameters ($$\theta_i$$) of time-series data. We aim to: 1. Test the capability of contrastive learning approach to extract embeddings from time-series data that correlate strongly with system underlying parameters. 2. Study different neural network architecture for encoding time-series trajectories into informative embeddings. 3. Explore the impact of various factors such as function forms, number of parameters and distributions, trajectory length, noise levels, and loss functions on the model’s performance. 4. Evaluate the precision of the predictive models in terms of their ability to make accurate future predictions based on learned latent variables, particularly in few-shot learning scenarios. ## Hypothesis With contrastive loss learning, the embeddings of trajectories from the same parameter set will be closer together in the embedding space than to those from different sets. Therefore, our central hypothesis is that the embeddings produced by a model trained with contrastive loss learning will reflect the underlying parameters of time-series data. It is anticipated that a linear projection of these embeddings back onto the parameter space will yield predictions that are congruent with the original parameter values. Moreover, we postulate that the model will be able to make more precise future predictions by effectively capturing the essence of the latent variables governing the time-series data. ## Experimental Setup ### Trajectories Simulation We will generate synthetic time-series data based on underlying deterministic and stochastic processes (e.g., spring-mass dynamical system). - The system can be defined by a set of parameters $$\theta_i$$. We have $H$ set of parameters. - For each set of parameters, a trajectory, $$\{x_{ij}\}$$ of length $T$ can be draw with different initial conditions and noise. We will sample $K$ trajectories for each set of parameters. ### Models We will evaluate three different neural network architectures: 1. Recurrent Neural Network (RNN) 2. Long Short-Term Memory (LSTM) 3. Transformer (utilizing attention mechanisms) A model $$M$$ will output an embedding vector $$v_{ij}$$ for a given input trajectory $$\{x_{ij}\}$$. ### Experimentation We want to evaluate the contrastive approach in extracting system parameter under the following scenarios: 1. **System Functional Forms:** We will test linear, non-linear, and complex periodic functions to generate the trajectories. 2. **Number of Parameters ($$\lvert \theta \rvert$$):** We will explore varying the number of parameters to understand how it affects the model’s ability to learn. 3. **Parameter Distribution:** We will use different distributions (uniform, normal, bimodal, etc.) of parameters (i.e., $\theta_i$) to study the impact on the learning process. 4. **Trajectory Length ($$T$$):** We will vary the length to assess the effect on the model’s performance. 5. **Noise Levels:** Different amounts of Gaussian noise will be added to the trajectories to simulate real-world data imperfections. 6. **Loss Functions:** Alongside contrastive loss, does add a loss function for model prediction of next time stamp help performance? ## Conclusion This proposal presents a structured plan to investigate the potential of contrastive loss approach in learning system underlying parameters of time-series data. The insights gained from this research could pave the way for advancements in various fields where time-series analysis is crucial. We hope the insights from our project can contribute to the field of machine learning and its applications in time-series analysis. --&gt; </content> </entry> </feed>
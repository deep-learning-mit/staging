<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Multimodal Commonsense Proposal | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="6.S898 project proposal for analyzing and evaluating the commonsense reasoning performance of multimodal vs text-only models."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/dev/submit/2023-11-09-multimodal-commonsense/2023-11-09-multimodal-commonsense/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Multimodal Commonsense Proposal",
      "description": "6.S898 project proposal for analyzing and evaluating the commonsense reasoning performance of multimodal vs text-only models.",
      "published": "November 8, 2023",
      "authors": [
        {
          "author": "Vincent Lin",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Multimodal Commonsense Proposal</h1> <p>6.S898 project proposal for analyzing and evaluating the commonsense reasoning performance of multimodal vs text-only models.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#background">Background</a></div> <div><a href="#related-work">Related Work</a></div> <div><a href="#implementation-evaluation">Implementation & Evaluation</a></div> </nav> </d-contents> <h2 id="background">Background</h2> <p>In recent years, language models have been proven to be quite proficient in producing human-like text, computing somewhat semantically-meaningful and human-interpretable word and token embeddings, and generating realistic conversation. However, there is a vast distinction between mimicking human linguistics from data and forming an understanding of the world and its abstract connections from data. The latter describes the commonsense knowledge of a language model, or its ability to reason about simple relationships, interactions, and general logic of the world.</p> <p>Previous work has been completed evaluating the commonsense capabilities of langauge models, and with the vast sizes of LMs and the even vaster data availble today, language models’ commonsense performance has grown increasingly close to human performance – but not quite <d-cite key="li2021"></d-cite>. From textual data alone, models still perform worse than humans with a significant margin of error. Yet, humans don’t learn to reason about the world from text alone; many, many different modes of perception contribute to our knowledge of reality. Can we imbue deep learning models with other modes of input to similarly augment their reasoning skills?</p> <p>In this project, I propose an investigation and evaluation of multimodal deep learning models for commonsense reasoning. When compared to standard language models, multimodal models have a more diverse set of input/training data that, perhaps, grants them a richer representation of the data. For example, vision-text models can be trained on the same textual data as language models, but the association of images and visualized objects with text embeddings provides a more comprehensive “understanding” of the objects and their interactions with their environment. Do different types of auxiliary inputs types provide multimodal models with any additional commonsense information? In the context of model representations and embeddings, how do the multimodal representations differ from those of the (text-only) unimodal? How are they similar? When observing the relationships between embeddings within the multimodal model (e.g., latent-space distances), does the multimodal affect the relative similarity between words/objects? Do these augmented relationships benefit multimodal models in commonsense reasoning at all?</p> <h2 id="related-work">Related Work</h2> <p>Several works have evaluated the commonsense capabilities of unimodal language models. Li et al., 2021 <d-cite key="li2021"></d-cite> analyzes the performance of the Gopher language model in zero-shot and few-shot learning with varying model sizes. They find that their LM performed relatively well in physical commonsense (explained further below), but worse in social commonsense. Zhao et al., 2023 <d-cite key="zhao2023"></d-cite> measure large language models’ commonsense performance in the context of simple task planning, e.g., in robotics, observing that performance varies depending on the particular task and the length of the descrption for the task. Saharia et al., 2022 <d-cite key="saharia2022"></d-cite> propose a text-to-image multimodal model and evaluate the depth of its text language understanding.</p> <h2 id="implementation--evaluation">Implementation &amp; Evaluation</h2> <p>For this project, I will choose to focus on vision-text models to evaluate multimodal performance. It’s important to note that different types of commonsense exist, and vision-text models may, intuitively, perform better at physical commonsense tasks than, say, social tasks, which will be a crucial distinction in evaluation. Reliable and relatively compact language models already exist with pretrained weights and relatively solid performance in general NLP tasks (e.g., transformer models from Huggingface <d-cite key="huggingface"></d-cite>), so I will plan to use these as reference. I may choose to implement more of the vision-text model from scratch (though carefully, so as not to have lackluster text processing in the multimodal model impact any comparison with the reference LM). However, if complications do arise, preimplemented multimodal models may also be used for reference <d-cite key="saharia2022"></d-cite>.</p> <p>Many benchmarks are available for evaluating the commonsense capabilities of language models. I will focus on multiple choice evaluation, where given a short story or background prompt, a model must choose the most reasonable answer or continuation. Multiple choice benchmarks provide a more concrete and reliable metric for determining similarity to “human” judgement. A brief summary of some potential benchmarks is given below:</p> <p><strong>HellaSwag</strong><d-cite key="zellers2019"></d-cite>: Designed to evaluate physical, grounded, and temporal common sense. Given a short description/prompt, the model must choose the correct continuation from four choices. The “stories” are produced from video captions or other passages.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag-1400.webp"/> <img src="/staging/assets/img/2023-11-09-multimodal-commonsense/hellaswag.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Social IQa</strong><d-cite key="sap2019"></d-cite>: Evaluates a model’s social common sense. This dataset is comprised of social situations of interactions between people, evaluating a model’s knowledge of emotion, mental states, etc.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/socialiqa-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/socialiqa-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/socialiqa-1400.webp"/> <img src="/staging/assets/img/2023-11-09-multimodal-commonsense/socialiqa.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>PIQA</strong><d-cite key="bisk2019"></d-cite>: Another physical common sense benchmark, where given a short question or situational prompt, models must select a solution between two options. PIQA focuses on physical interaction.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/piqa-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/piqa-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-multimodal-commonsense/piqa-1400.webp"/> <img src="/staging/assets/img/2023-11-09-multimodal-commonsense/piqa.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-09-multimodal-commonsense.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>
<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Augmenting Expert Domain Image Inputs for Enhancing Visual Language Models Performance | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="This blog post explores enhancing visual language models, particularly for expert domains like scientific literature, where standard models struggle. By integrating domain-specific knowledge and advanced image embeddings, the research aims to refine the performance of visual language models such as OpenFlamingo. Leveraging graphical structured embeddings and graph neural networks, the study tests different methods of representing images to improve the models' interpretive capabilities."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/vig-algorithm-flow-project-proposal/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Augmenting Expert Domain Image Inputs for Enhancing Visual Language Models Performance",
      "description": "This blog post explores enhancing visual language models, particularly for expert domains like scientific literature, where standard models struggle. By integrating domain-specific knowledge and advanced image embeddings, the research aims to refine the performance of visual language models such as OpenFlamingo. Leveraging graphical structured embeddings and graph neural networks, the study tests different methods of representing images to improve the models' interpretive capabilities.",
      "published": "December 12, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Augmenting Expert Domain Image Inputs for Enhancing Visual Language Models Performance</h1> <p>This blog post explores enhancing visual language models, particularly for expert domains like scientific literature, where standard models struggle. By integrating domain-specific knowledge and advanced image embeddings, the research aims to refine the performance of visual language models such as OpenFlamingo. Leveraging graphical structured embeddings and graph neural networks, the study tests different methods of representing images to improve the models' interpretive capabilities.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#related-works">Related Works</a></div> <div><a href="#methodology">Methodology</a></div> <ul> <li><a href="#dataset">Dataset</a></li> <li><a href="#experimentation">Experimentation</a></li> <li><a href="#evaluation-metrics">Evaluation Metrics</a></li> </ul><div><a href="#results">Results</a></div> <ul> <li><a href="#superiority-in-extracting-nodes">Superiority in Extracting Nodes</a></li> <li><a href="#difficulties-with-edge-dectection">Difficulties with Edge Dectection</a></li> <li><a href="#outside-information">Outside information</a></li> </ul><div><a href="#conclusions-and-future-steps">Conclusions and Future Steps</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Over the past few years, we have seen a surge in creation, adoption, and excitement around visual language models, specifically around Open AI’s CLIP model. Visual language models can bridge the gap between image and text, allowing tokenized understanding of the visual world around us. For instance, Meta released Segment Anything, a model with enhanced object detection through multimodal inputs like defined bounding boxes and text.</p> <p>After the recent surge with ChatGPT, we have begun to see advancements in the visual language model space to combine the image analysis and conversational tool. While the recent developments with Bard, GPT4-v, LLava, and many others have progressed the visual language model domain, the overall capabilities of the models are limited to the type of images provided. Most of the models have been trained and finetuned on common day objects, specializing in every-day normal tasks.</p> <p>However, theses models continue to struggle with answering images derived from an expert domain, especially scientific literature. Images from these domains can be challenging for the model, as they require common background knowledge, domain knowledge, and interpretation of the diagram.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/chat_example.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Question: What is the path from 1 to 4? <br/><br/> GPT4-v Answer: The image you've uploaded appears to show a diagram with four numbered points, possibly representing steps or locations connected by a path... However, as an AI, I can't visually trace paths or analyze images in the way a human would... </div> </div> <p>How can we assist visual language models to improve performance in expert domains?</p> <h2 id="past-works">Past Works</h2> <p>Visual Language Models have become very popular in the recent years with their ability to connect image to text. Open Flamingo<d-cite key="openflamingo"></d-cite> is an open source model with a similar architecture to Flamingo<d-cite key="flamingo"></d-cite>: images are sent to (1) visual encoders, (2) perceived resampler, and (3) through a dense group of layers. Through few shot learning, text completion, and image analysis, Open Flamingo allows users to have a conversation involving images.</p> <p>Currently, popular visual language models, like Flamingo, utilize CLIP<d-cite key="clip"></d-cite> as the visual encoder to perform the image-to-embedding conversion. Behind the scenes, CLIP uses a Vision Transformer architecture as the encoder. However, when we analyze the embeddings output from CLIP, they do not necessarily have a clear representation to the human eye. This makes it really challenging to inject domain-specific knowledge through the embedding to help perform specific tasks.</p> <p>The project, FlowchartQA<d-cite key="flowchart"></d-cite>, contributed by creating a dataset of flowcharts and code, revealing insights into the relationships forming code. However, no research has tried to understand the way visual language models interpret graphs with nodes and edges, specifically for domain specific questions. Through various changes to text and image inputs, we can learn about the way a visual language model understands graphical structures to improve future performance of VLMs.</p> <h2 id="procedure">Procedure</h2> <h3 id="dataset-creation">Dataset Creation</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/data_augmentation_pipeline.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Data Augmentation Pipeline for the Inputs </div> <p>To learn more about the graphical understanding of VLMs, a dataset had to be curated to test various conditions. The original images of the flowcharts are sampled from the BizGraphQA dataset<d-cite key="bizgraphqa"></d-cite>. Due to the compute and time contraints, only a sample of ten images were utilized for the analysis. This subset of images is passed through the Segment Anything Model from Meta <d-cite key="sam"></d-cite> to extract a mask of each of the nodes. From there, OCR is performed on each node to retrieve the text and hand coloring is performed to color the image. In the end, we get dataset triples of the original image, colored image, and the list of nodes in the image.</p> <p>For example, for this image, we would have the following dataset.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Original Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Colored Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Node List </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/0_43_labeled_styled.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_43_labeled_styled.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> [Finanze Tyco Roadsafe Insurance, Greyhound Bbbytf Limited, Etablissements Lgac Lthalat Incorporated Indiana, Korte Pro-Cut Grayhawk Insurace North Carolina, Hutchins Sandvik Maryland, Nm Suc. Krsx Limited Michigan] </div> </div> <h3 id="experimentation">Experimentation</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/experimentation_pipeline.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Experimentation Pipeline for the Inputs </div> <p>Bard uses Flamingo, a visual language model, to answer queries. We will provide an input image with or without the augmentation and a question about the graph into Flamingo, as illustrated in the figure above. Each image will be paired with a question in a specific category. For this analysis, we will focus on four major types of questions to evaluate the VLM’s understanding of graph connectivity. These questions are to be asked in tandem with the original image, the colored image, and the original image paired with the list of nodes in the image. We ask the following questions:</p> <ol> <li>Based on the image, is there a cycle in the graph?</li> <li>Based on the image, what is the path from <strong>__ to __</strong>_? (The ground truth path involves nodes that only have one child node.)</li> <li>Based on the image, what is the path from <strong>__ to __</strong>_? (The ground truth path involves nodes that have multiple child nodes.)</li> <li>Based on the image, how many child/parent nodes does _____ have?</li> </ol> <p>For the same image from above, here are the questions and relevant answers:</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center">Question</th> <th style="text-align: center">Answer</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">Is there a cycle in this graph?</td> <td style="text-align: center">No</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">What is the organization hierarchy path from Etablissements Lgac Lthalat Incorporated, Indiana to Nm Suc. Krsx Limited - Michigan?</td> <td style="text-align: center">The path is Etablissements Lgac Lthalat Incorporated, Indiana to Korte Pro-Cut Grayhawk Insurance, North Carolina to Nm Suc. Krsx Limited - Michigan</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">What is the organization hierarchy path from Finanze Tyco Roadsafe Insurance to Nm Suc. Krsx Limited - Michigan?</td> <td style="text-align: center">The path is from Finanze Tyco Roadsafe Insurance to Greyhound Bbbytf Limited to Nm Suc. Krsx Limited - Michigan</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">How many child companies does Greyhound Bbbytf Limited have holdings in?</td> <td style="text-align: center">Two</td> </tr> </tbody> </table> <p>But, you must be wondering: why ask these questions specifically? Each question tests understanding of graphical elements without background understanding of the topic. This should serve as a baseline for the way that VLMs understand graphical structures and the common questions to be asked.</p> <h3 id="evaluation-metrics">Evaluation Metrics</h3> <p>To evaluate the success of our model, we will conduct both qualitative and quantitative analyses on the dataset, given that quantitative evaluation of generative models can be challenging. The control group will provide a baseline for normalizing the results.</p> <p>Qualitatively, we will perform a manual analysis of the generated outputs. By using prompts, images, and answer, we will subjectively compare the prompt, the image, and the resulting answer. Our primary goal is to assess how effectively the visual language model generates the answer based on the prompt while being constrained by the graph.</p> <p>Quantitatively, we plan to utilize an accuracy score will be employed to evaluate the percentage of questions that have been answered correctly in each metric. Incorporating these metrics will provide a more comprehensive understanding of the model’s performance and allow for better comparisons with other generative models.</p> <p>This comparative analysis will facilitate a robust evaluation of the methods employed to represent graphical information within visual language models.</p> <h2 id="results">Results</h2> <p>The following table provides the accuracy metric of the aforementioned experiment:</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center">Question 1</th> <th style="text-align: center">Question 2</th> <th style="text-align: center">Question 3</th> <th style="text-align: center">Question 4</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Original Image</td> <td style="text-align: center">.7</td> <td style="text-align: center">.4</td> <td style="text-align: center">0</td> <td style="text-align: center">0</td> </tr> <tr> <td style="text-align: center">Original Image with Node List</td> <td style="text-align: center">0</td> <td style="text-align: center">.3</td> <td style="text-align: center">0</td> <td style="text-align: center">.1</td> </tr> <tr> <td style="text-align: center">Colored Image</td> <td style="text-align: center">1</td> <td style="text-align: center">.3</td> <td style="text-align: center">0.1</td> <td style="text-align: center">0.2</td> </tr> </tbody> </table> <p>We clearly see that the Colored Image performs better than the Original Image in most of the scenarios. Also, we unfortunately see that the node list did not provide gains in performance for the VLM. Overall, the performance is quite limited when asking specific questions about path finding and children nodes. Let’s dive a bit deeper to understand why.</p> <p>Preface: In the following sections, there are tables with prompts, images, and answers from Bard for various mini explorations.</p> <h3 id="superiority-in-extracting-nodes">Superiority in Extracting Nodes</h3> <p>When first embraking on this project, I believed that the VLM would struggle to pair the text and nodes together. However, the VLM actually proved to be quite impressive with the extraction of nodes and the names associated with, making the need for a node list not quite neccesary.</p> <p>Using the input figure from above without any augmentations, I asked the VLM how many child nodes a specific node in the graph had. The VLM outputs the wrong answer, but it actually provides generated code for its reasoning. Here is a portion of the code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Define the corporate structure as a dictionary
corporate_structure = {
    "Etablissements Lgac Lthalat Incorporated, Indiana": {
        "Nm Suc. Krsx Limited - Michigan": {}
    },
    "Nm Suc. Krsx Limited - Michigan": {},
    "Krsx Incorporated, California": {
        "Nm Suc. Krsx Limited - Michigan": {}
    },
    "Krsx Incorporated, Texas": {},
    "Lthalat Holdings, Delaware": {
        "Etablissements Lgac Lthalat Incorporated, Indiana": {},
        "Krsx Incorporated, California": {},
        "Krsx Incorporated, Texas": {}
    },
    "Greyhound Bbbytf Limited": {
        "Krsx Incorporated, California": {},
        "Krsx Incorporated, Texas": {}
    }
}
</code></pre></div></div> <p>The VLM was actually quite impressive with obtaining the specific nodes in the graph, recognizing the text through OCR, and pairing it with the nodes. While all the nodes are not correct, we can see that the VLM can follow a procedure to determine this. While this is a good example of node extraction, the capability is still ambiguous.</p> <p>To poke this topic a bit more, I wanted to test out the VLM’s ability to extract the nodes if the colors are the same or different. I designed a basic figure with just nodes to test this. The same prompt was passed into Bard with the images below. The only difference between the two images is the fact that in one image, the colors of the nodes are same, and in the other image, the colors of the nodes are different. In the results below, we can clearly see that the VLM is able to perform better with the colored nodes, as the VLM is able to distinguish between different nodes.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Original Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Colored Image </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt: What are the names of the nodes in this graph? </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt: What are the names of the nodes in this graph? </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer: B1, B2, B3, B4, B5. </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer: B1, B2, B3, B4. </div> </div> <p><br/> To support this argument, we look at the attention that CLIP places on segments of the image based on a caption. We specifically use CLIP because CLIP is the visual encoder in Flamingo. While this isn’t necessarily a rigorous proof, we can see that the attention on the nodes is placed stronger in the colored graph example rather than the regular graph example.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Caption: Node B1, B2, B3, B4 </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Caption: Node B1, B2, B3, B4 </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes_explainability_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes_explainability_example_1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Through the examples and tests above, we can clearly see the VLM’s ability to extract nodes, especially with a visually distinugishing factor between the nodes like color. Since the VLM can do a pretty decent job of extracting the nodes, it makes sense that providing the VLM with the node list may not allow for great improvements in performance.</p> <p>So, if the VLM can extract the nodes relatively well, why is the performance still subpar?</p> <h3 id="difficulties-with-edge-dectection">Difficulties with Edge Dectection</h3> <p>Aside from nodes, most graphs have edges, and for the questions asked in the experiments, understanding the connectivity was crucial to providing the correct answer. We actually observed that the colored graphs had answers that were closer to 100% accuracy in comparison to the regular graphs. To explore how VLMs understand the connections between nodes, I decided to ask Bard about some simple graphs to determine how it responded.</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/nodes1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/colored_nodes1.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Can you please tell me what the edges in the graph are? </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Can you please tell me what the edges in the graph are? </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The edges in the graph are: a -&gt; b, b -&gt; c, c -&gt; a </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The edges in the graph are: B -&gt; A, C -&gt; A </div> </div> <p>Wow! It’s really surprising that the VLM is creating edges where there aren’t even edges? Also, the direction of the edges are wrong. Why would this be the case?</p> <p>Let’s explore a bit more about the individual arrows. Can a VLM accurately describe the arrow direction?</p> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Image </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/big_arrow.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow-1400.webp"/> <img src="/staging/assets/img/2023-11-08-vig-algorithm-flow-project-proposal/small_arrow.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Prompt </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Which way does the arrow point? </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Which way does the arrow point? </div> </div> <div class="row" style="border: 1px solid #000"> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> Answer </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The arrow in the image points to the right. </div> <div class="col-sm mt-3 mt-md-0" style="border: 1px solid #000"> The arrow in the image points to the left. </div> </div> <p>Well, if you take a look at the responses above, we notice that the VLM can accurately provide the arrow direction for larger arrows incomparison to smaller arrows. Since most of the graphs involve smaller arrows, we can see why the VLM could not tell the directionality of the arrow.</p> <h3 id="outside-information">Outside information</h3> <p>Beyond the graphical structure of the nodes, I found that Bard typically incorporated outside information that required background knowledge about the topic. These situations can be categorized into two categories: useful outside information and irrelevant outside information. To distinguish between the two, I found that in the siutations that Bard used graph algorithms and definitions, the outside information was useful in generating the answer. However, many times, Bard would look up specific companies or use graphs from other sources online.</p> <p>In this example, Bard was asked if there was a cycle in the graph, and Bard outputted the answer below. The VLM was very intelligent by quoting the definition of a cycle and using DFS to determine the existence of one. The step-by-step algorithm is correct and ultimately, leads to the solution. This shows that the incorporation of algorithms and accurate definitions can assist the VLM in answering queries.</p> <blockquote> <p>No, there is no cycle in the graph of the Nove 1-66Express organizational structure. A cycle is a path in a graph that starts and ends at the same vertex, and there is no such path in this graph. I verified this by tracing the graph and ensuring that I never returned to a vertex that I had already visited. I also used the following algorithm to check for cycles in graphs: Mark all vertices as unvisited. Choose a vertex and start a depth-first search (DFS). As you traverse the graph, mark each vertex as visited. If you ever reach a vertex that is already marked as visited, then there is a cycle in the graph. If you finish traversing the graph without ever reaching a visited vertex, then there is no cycle in the graph. I ran this algorithm on the graph of the Nove 1-66Express organizational structure and found that there are no cycles.</p> </blockquote> <p>However, outside information can be just as harmful as it is beneficial. Pulling from outside sources can be tricky for a VLM. It needs to remember to operate within the grouth truths presented in the figure. In multiple instances, the VLM actually referenced the wrong figure because it forgot about the image provided with the prompt.</p> <p>Therefore, the VLM needs to be fine-tuned to filter the outside information it brings in to improve the accuracy of its answers.</p> <h2 id="conclusions-and-future-steps">Conclusions and Future Steps</h2> <p>In this study, I delved into the capabilities and limitations of VLMs, like Open Flamingo, in interpreting complex graphical structures within expert domains, such as scientific literature. The experiments, involving various image augmentation techniques such as color changes and node names, revealed key insights. While VLMs demonstrated proficiency in node extraction, it faced challenges in edge detection and understanding the connectivity between nodes. This was particularly evident when colored images outperformed non-colored ones, highlighting the importance of visual distinction for VLM comprehension. However, the addition of node lists did not significantly enhance performance, suggesting existing capabilities in node identification. The connectivity was difficult for the VLM to understand because of the size of the arrows.</p> <p>The findings of this research highlight a crucial challenge for VLMs: integrating domain-specific knowledge, especially for non-standard images like scientific diagrams. However, due to the small dataset size, suggests that further research with a larger and more diverse dataset is necessary to validate these findings. In the future, this research can be applied to help improve prompting for graphical structures, provide insights on how to finetune a VLM for this task, and create a new interest in using VLMs for scientific diagrams.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-08-vig-algorithm-flow-project-proposal.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>
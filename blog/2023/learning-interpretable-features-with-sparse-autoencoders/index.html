<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Learning Interpretable Features with Sparse Auto-Encoders | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Staging website for the 2023 ICLR Blogposts track "/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/learning-interpretable-features-with-sparse-autoencoders/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Learning Interpretable Features with Sparse Auto-Encoders",
      "description": "",
      "published": "November 9, 2023",
      "authors": [
        {
          "author": "Sam Mitchell",
          "authorURL": "",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Learning Interpretable Features with Sparse Auto-Encoders</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#superposition-hypothesis">Superposition Hypothesis</a></div> <div><a href="#sparse-auto-encoders">Sparse Auto-Encoders</a></div> <div><a href="#research-question">Research Question</a></div> <div><a href="#codebase">Codebase</a></div> <div><a href="#language-models">Language Models</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>The field of Machine Learning is becoming increasingly promising as humanity endeavors to create intelligent systems, with models outperforming humans on many tasks. As models become increasingly capable, its important that humans are able to interpret a model’s internal decision making process to mitigate the risk of negative outcomes. While significant progress has been made on interpreting important parts of models like <a href="https://transformer-circuits.pub/2021/framework/index.html">attention heads</a> <d-cite key="elhage2021mathematical"></d-cite>, it’s also the case that hidden layers in deep neural networks have remained notoriously hard to interpret.</p> <h2 id="superposition-hypothesis">Superposition Hypothesis</h2> <p>One hypothesis for why it can be challenging to interpret individual neurons is because they are simultaneously representing multiple concepts. One may wonder why a network would have its neurons learn to represent multiple concepts. At a first glance, this approach to encoding information feels unintuitive and messy. The key idea comes from the Johnson–Lindenstrauss lemma: In $n$ dimensions, you can have at most $n$ pairwise orthogonal vectors, but the number of pairwise “almost orthogonal” vectors (i.e. cosine similarity at most $\epsilon$) you can have is exponential in $n$. This enables a layer to encode for many more concepts than it has neurons. So long as each neuron is only activated by a sparse combination of concepts, we can reconstruct these concepts from a given activation with minimal interference between the concepts, since they are “almost orthogonal”. This hypothesis is known as <strong><a href="https://transformer-circuits.pub/2022/toy_model/index.html">superposition</a></strong> <d-cite key="elhage2022superposition"></d-cite>, and offers an explanation for why neurons have been observed in practice to be polysemantic.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/superposition.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Diagram depicting a larger model with disentngled features and a lower dimensional projection simulating this larger network using polysemanticity. Source <d-cite key="elhage2022superposition"></d-cite> </div> <h2 id="sparse-auto-encoders">Sparse Auto-Encoders</h2> <p>Since deep neural networks are strongly biased towards making neurons polysemantic during training, humans might try to understand the model’s decision making process by “unwrapping” the network into the sparse features that the neurons in some particular layer are simulating. To do this, a concept called a Sparse Auto-Encoder (SAE) is used. An SAE is similar to a normal autoencoder, with two main differences: (1) the encoding layer is larger than the neuron layer, often by a factor of 4x. (2) the loss function penalizes not only for the MSE loss, but also for the sparsity of the encoder matrix, frequently represented as L1 loss. A sparse autoencoder lets us learn a sparse representation for a vector, but in a higher dimensional space. SAEs were first proposed in a <a href="https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition">blogpost</a> by Lee Sharkey in December 2022, and in September 2023 more research was published on SAEs, both by a group of <a href="https://arxiv.org/abs/2309.08600">independent researchers</a> <d-cite key="cunningham2023sparse"></d-cite> and by <a href="https://transformer-circuits.pub/2023/monosemantic-features/">Anthropic</a> <d-cite key="bricken2023monosemanticity"></d-cite> demonstrating that not only can SAEs be learned at a specific layer, but the features they learn are human interpretable.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/SAE.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Diagram depicting an SAE architecture for a transformer language model. Source <d-cite key="cunningham2023sparse"></d-cite> </div> <h2 id="research-question">Research Question</h2> <p>This inspired a new idea: what if we could take a neural network, unwrap each layer into a larger, sparse, interpretable set of features, and then learn a sparse weight matrix connecting all pairs of two consecutive feature layers? This would mean that we could take a neural network, and transform it into a new neural network simulating the old neural network, with the nice property that the computations are sparse and hopefully interpretable.</p> <p>The main question we wish to explore is: Can we unwrap a deep neural network into a larger sparse network and learn sparse weights between consecutive feature layers without losing performance?</p> <h2 id="initial-mathematics">Initial Mathematics</h2> <p>Let’s begin by looking at $L_1$ and $L_2$, two consecutive layers in a deep neural network with ReLU activations. Let $W$ and $b$ be the matrix and bias respectively that connects these two layers. Then we have</p> \[L_2 = \text{ReLU}(W L_1 + b)\] <p>We create autoencoders such that</p> \[L_1 = D_1 \text{ReLU}(E_1 L_1 + e_1) \equiv D_1 F_1\] \[L_2 = D_2 \text{ReLU}(E_2 L_2 + e_2) \equiv D_2 F_2\] <p>where $D_i$ is the decoder for layer $i$, $E_i$ and $e_i$ are the weights of the encoder and encoder bias, and $F_i$ is the feature vector.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_diagram.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Biases excluded from diagram for clarity. The hockey sticks on top of $F_1$, $L_2$, and $F_2$ indicate that a ReLU is applied to get the activations at that layer. If our autoencoder is good (which it should be), we have $L_1=L_1'$ and $L_2=L_2'$. </div> <p>Thus we have</p> \[\begin{align} F_2 &amp;= \text{ReLU}(E_2 L_2 + e_2) \\ &amp;= \text{ReLU}(E_2 \text{ReLU}(W L_1 + b) + e_2) \\ &amp;= \text{ReLU}(E_2 \text{ReLU}(W D_1 F_1 + b) + e_2). \end{align}\] <p>In general, an approximation of the form</p> \[F_2 = \text{ReLU}(W_2 F_1 + b_2)\] <p>would be pretty terrible since we cannot easily approximate a double ReLU function with a single ReLU function. However, because of the way $F_1$ and $F_2$ are created from $L_1$ and $L_2$, the relationships are actually very sparse in nature, so we will try to learn the approximation above. Perhaps there is a clever initialization that will allow us to learn this more easily.</p> <p>If we just ignored the inside ReLU in the definition of $F_2$, then we’d have</p> \[F_2 = \text{ReLU}(E_2 W D_1 F_1 + E_2 b + e_2)\] <p>which suggests the following could be a good initialization for our learned weight $W_2$ and bias $b_2$.</p> \[W_2 = E_2 W D_1\] \[b_2 = E_2 b + e_2\] <p>While this initialization seemed reasonable at the start of the project, it turned out that during training this results in a local minimum, and you can actually get much lower loss if you randomly initialize $W_2$ and $b_2$.</p> <h2 id="codebase">Codebase</h2> <p>To answer this main question, the first step was to build out a <a href="https://drive.google.com/file/d/1_0g_Qq76AqJByCrj_i-tYr76KPeAfIem/view?usp=sharing">codebase</a> that had all the implementations necessary to run experiements to explore this question. The codebase was developed from scratch to ensure I understood how each part of the code worked.</p> <h3 id="model">Model</h3> <p>The first part of the code trains a four layer neural network to classify MNIST images. After training we got a validation loss of 0.09 and a validation accuracy: 0.98, indicating the model does well. For clarity, all losses described in this section will refer to loss on the validation set.</p> <h3 id="saes">SAEs</h3> <p>Next, two autoencoder architectures are implemented, one that learns both an encoder and decoder, and one that learns only an encoder as its decoder is tied as the transpose of the encoder. Empirically, the tied autoencoder seemed to perform better and achieved an L1 (sparsity) loss of 0.04928, and an L2 (MSE) loss of 0.03970. Seeing these numbers close in magnitude is good, indicating that the model is neither penalizing too much nor too little for L1 sparsity loss.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/autoencoder.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> For a random input: The top diagram depicts neuron activations (blue) and reconstructed neuron activations from the SAE (orange), indicating the SAE has low L2 loss and reconstructs the input well. The bottom diagram depicts the feature activations for the same input, showing they are sparse. Notably, 38/64 of the neuron activations have magnitude above 0.3, but only 7/256 of the encoded features have magnitude above 0.3. </div> <h3 id="feature-connectors">Feature Connectors</h3> <p>Then, a feature connector was implemented, which learns the matrices $W_2$ and $b_2$ descibed above mapping one layer to another layer. The inputs are the set of all feature $i$ activations and the outputs are the set of all feature $i+1$ activations, allowing us to gradient descent over loss (which consists of L1 sparsity and L2 MSE) to optimize $W_2$ and $b_2$. The L1 (sparsity) loss was 0.02114 and the L2 (MSE) loss: 0.03209, indicating that there is a good tradeoff between L1 and L2 penalty.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/layer_weights.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Weights matrix connecting neuron layer 1 to neuron layer 2. This is a mess. 2205 weights have magnitude greater than 0.1. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_weights.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Weights matrix connecting encoded features in layer 1 to encoded features in layer 2. This is nice and sparse. 458 weights have magnitude greater than 0.1. </div> <p>Below is what the feature connector matrix looks like after each epoch of training.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/feature_connector1_2.gif" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="simulating-the-feature-network">Simulating the Feature Network</h3> <p>Finally, we replace neuron connections with feature connections. This means that when we pass an input through the network, we immediately encode it as a feature and propogate it through the feature connector weights, skipping the neuron layer weights. In this network, removing two neuron to neuron layers and substituting them with feature to feature layers results in a decrease from 97.8% accuracy to 94% accuracy, which is pretty good considering we made our network much sparser.</p> <p>Next, I tried to visualize the features using a variety of methods (both inspired by a class lecture and a <a href="https://distill.pub/2017/feature-visualization">Distill blogpost</a> <d-cite key="olah2017feature"></d-cite>). Unfortunately, I did not find the features to be much more interpretable than the neurons for the MNIST dataset. Still, our results are cool: we can take a network, and with only a fraction of the parameters maintain comparable performance.</p> <h2 id="language-models">Language Models</h2> <p>I shared these results with Logan Riggs, one of the <a href="https://arxiv.org/abs/2309.08600">independent researchers</a> <d-cite key="cunningham2023sparse"></d-cite> who published about SAEs in October 2023. Excited about the possibility, we collaborated to see if we could achieve the same results for language models, anticipating that the learned features might be more interpretable. We and a couple other collaborators published a <a href="https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms">blogpost</a> showing that the learned features in Pythia-70M are indeed interpretable, and there are cool relationships! (the remainder of this section is adapted from that blogpost)</p> <p>Below we show some examples of sparse linear feature connections. For the curious reader, additional examples can be found <a href="https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4?pvs=4">here</a>.</p> <h3 id="or-example">OR Example</h3> <p>In Layer 1, we have:</p> \[OF_{30} = 0.26IF_{2797} + 0.23IF_{259} + 0.10IF_{946}\] <p>where OF is output feature (in MLP_out), and IF is input feature (in Residual Stream before the MLP)</p> <p>Below is input feature 2797, activating strongly on the token “former”</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former.webp" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> This is 5 examples. For each example, the top row of words are feature activation e.g. token "former" activated 9.4. The bottom blank row is: if we removed this feature, how much worse does the model get at predicting these tokens? e.g. Soviet is 5.5 logits worse when the model can't use this "former" feature. </div> <p>Below is input feature 259, activating strongly on the token “old”</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/old.webp" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Below is input feature 946, activating on the token “young”</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/young.webp" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In the output feature, we see the tokens former, old, and young all activate, with young activating about half as strongly as “former” and “old” as we would expect from the weight coefficients.</p> <p>\(OF_{30} = 0.26IF_{former} + 0.23IF_{old} + 0.10IF_{young}\)</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/former_old_young.webp" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can view this computation as a weighted logical OR. Output Feature 30 activates on former OR old OR young.</p> <h3 id="negative-weight-example">Negative Weight Example</h3> <p>In Layer 1, we have:</p> \[OF_{505} = 0.68IF_{3021} -0.21IF_{729}\] <p>where OF is output feature, and IF is input feature.</p> <p>Below is input feature 3021, activating strongly on tokens like “said” which in almost all cases appear not after a quote.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/all_said.webp" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Below is input feature 729, activating strongly on tokens like “said” when they appear shortly after a quote.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_quotes.webp" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Below we see the output feature activates on tokens like “said” that have no prior quote tokens. We’ve “subtracted out” with a large negative weight, so to speak, the examples where “said” appears after a quote, and now the feature only activates when “said” appears without any prior quotes.</p> \[OF_{505} = 0.68IF_{(\text{"said" in many contexts})} -0.21IF_{(\text{"said" after quotes})}\] <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp-1400.webp"/> <img src="/staging/assets/img/2023-11-09-learning-interpretable-features-with-sparse-autoencoders/said_no_quotes.webp" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can view this computation as a weighted logical AND. Output Feature 505 activates on A AND ~B. In the case where A is a superset of B, this is the complement of B e.g. I have the set of all fruits and all yellow fruits, so now I can find all non-yellow fruits.</p> <h2 id="conclusion">Conclusion</h2> <p>Our exploration into interpreting neural networks using Sparse Auto-Encoders has shown promising results. The ability to unwrap the layers of a neural network into a more interpretable, sparse representation without a significant loss in performance supports the superposition hypothesis. Even if the features were only interpretable on some architectures/datasets, I am optimistic that Sparse Auto-Encoders will not only make deep neural networks more interpretable, but they will also allow for quicker parallelized inference since each output feature will depend on a small fraction of the total possible input features.</p> <p>I’d like to thank everyone who has contributed to my deep learning education this semester. I have learned a tremendous amount and really enjoyed working on this project.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-09-learning-interpretable-features-with-sparse-autoencoders.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>
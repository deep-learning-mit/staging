<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Prompt to Prompt | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Text-based image editing via cross-attention mechanisms - the research of hyperparameters and novel mechanisms to enhance existing frameworks"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/prompt-to-prompt/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Prompt to Prompt",
      "description": "Text-based image editing via cross-attention mechanisms - the research of hyperparameters and novel mechanisms to enhance existing frameworks",
      "published": "November 7, 2023",
      "authors": [
        {
          "author": "Carla Lorente",
          "authorURL": "https://www.linkedin.com/in/carla-lorente/",
          "affiliations": [
            {
              "name": "MIT EECS 2025",
              "url": ""
            }
          ]
        },
        {
          "author": "Linn Bieske",
          "authorURL": "https://www.linkedin.com/in/linn-bieske-189b9b138//",
          "affiliations": [
            {
              "name": "MIT EECS 2025",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Prompt to Prompt</h1> <p>Text-based image editing via cross-attention mechanisms - the research of hyperparameters and novel mechanisms to enhance existing frameworks</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#literature-review">Literature Review</a></div> <div><a href="#outline-of-our-research">Outline of our research</a></div> <div><a href="#a-hyperparameter-study-of-prompt-to-prompt-editing-method-word-swap">A. Hyperparameter Study of prompt-to-prompt editing method "word swap"</a></div> <div><a href="#a1-exploration-of-silhouette-threshold-hyperparameter-k">A1. Exploration of silhouette threshold hyperparameter ("k")</a></div> <div><a href="#a2-exploration-of-cross-attention-injection-hyperparameter-cross-replace-steps">A2. Exploration of cross-attention injection hyperparameter ("cross replace steps")</a></div> <div><a href="#a3-exploration-of-self-attention-hyperparameter-self-replace-steps">A3. Exploration of self-attention hyperparameter ("self replace steps")</a></div> <div><a href="#a4-cycle-consistency-of-method">A4. Cycle Consistency of method</a></div> <div><a href="#b-generalization-of-optimized-hyperparameters-to-attention-re-weight-method">B. Generalization of optimized hyperparameters to "attention re-weight method"</a></div> <div><a href="#our-proposed-method">Our proposed method</a></div> <div><a href="#future-work">Future work</a></div> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Recently, the techniques to edit images have advanced from methodologies that require the user to edit individual pixels to deep learning-based image editing. The latter employ for example large image generation models (e.g., stable diffusion models). While these deep learning-based image editing techniques initially required the user to mark particular areas that should be edited (Nichol et al., 2021 <d-cite key="nichol2021glide"></d-cite>; Avrahami et al., 2022a<d-cite key="avrahami2022blendeddiffusion"></d-cite>; Ramesh et al., 2022), recently the work by (Hertz et al, 2022 <d-cite key="hertz2022prompttoprompt"></d-cite>) has shown that this becomes unnecessary. Instead, image editing can be performed using a cross-attention mechanism. In particular, the proposed prompt-to-prompt editing framework enables the controlling of image edits by text only. The section below provides an overview of how this prompt-to-prompt framework works (Figure 1, by (Hertz et al, 2022<d-cite key="hertz2022prompttoprompt"></d-cite>)).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/1-cross_attention_masks.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 1: Cross-attention method overview. Top: visual and textual embedding are fused using cross-attention layers that produce attention maps for each textual token. Bottom: we control the spatial layout and geometry of the generated image using the attention maps of a source image. This enables various editing tasks through editing the textual prompt only. When swapping a word in the prompt, we inject the source image maps Mt, overriding the target maps M ∗ t . In the case of adding a refinement phrase, we inject only the maps that correspond to the unchanged part of the prompt. To amplify or attenuate the semantic effect of a word, we re-weight the corresponding attention map. (Hertz et al, 2022 <d-cite key="hertz2022prompttoprompt"></d-cite>).</em></p> <p>While this proposed framework has significantly advanced the image editing research field, its performance leaves still room for improvement such that open research questions remain. For example, when performing an image editing operation that changes the hair color of a woman, significant variability across the woman’s face can be observed (Figure 2). This is undesirable, as the user would expect to see the same female face across all four images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/2-Experimentation_proposed_prompt_to_prompt.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 2: Experimentation with the proposed prompt-to-prompt image editing framework presented by (Hertz et al, 2022<d-cite key="hertz2022prompttoprompt"></d-cite>). The faces of the women show significant variability even though they should remain invariant across all four generated/ edited images.</em></p> <p>Within our work, we will start to further benchmark the proposed framework’s performance, explore its hyperparameters’ impact on the image editing process, and research opportunities to improve the current performance.</p> <h2 id="literature-review">Literature Review</h2> <p>Before delving into the details of the prompt-to-prompt editing method, let’s briefly recap some existing techniques to edit images with diffusion models that have paved the way for this revolutionary approach:</p> <h3 id="1-adding-noise-to-an-image-and-denoising-with-a-prompt">1. Adding noise to an image and denoising with a prompt</h3> <p>In <strong>SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</strong> <d-cite key="meng2021sdedit"></d-cite> , the user takes an image, introduces noise and then denoises it according to a user-provided prompt. As an example, given an image, users can specify how they want the edited image to look using pixel patches copied from other reference images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/SDEdit.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A similar approach is used in the paper <strong>MagicMix: Semantic Mixing with Diffusion Models</strong> <d-cite key="liew2022magicmix"></d-cite> which uses a pre-trained text-to-image diffusion based generative model to extract and mix two semantics. The figure below showcases the detailed pipeline of MagicMix (image-text mixing). Given an image x<sub>0</sub> of layout semantics, they first craft its corresponding layout noises from step Kmin to K<sub>max</sub>. Starting from K<sub>max</sub>, the conditional generation process progressively mixes the two concepts by denoising given the conditioning content semantics (“coffee machine” in this example). For each step k in [K<sub>min</sub>; K<sub>max</sub>], the generated noise of mixed semantics is interpolated with the layout noise x<sub>k</sub> to preserve more layout details.</p> <div style="text-align:center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_1.png" class="img-fluid" width="100" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/corgi_coffee_machine_2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="2-take-an-image-add-noise-and-denoise-it-with-a-prompt--add-a-mask">2. Take an image, add noise and denoise it with a prompt + Add a mask</h3> <p>In the paper <strong>Blended Diffusion: Text-Driven Editing of Natural Images</strong> <d-cite key="avrahami2022blended"></d-cite> , given an input of an image and a mask, the blended diffusion modifies the masked area according to a guided text prompt, without affecting the unmasked regions. One limitation of this is that it relies on the user having to produce this mask to indicate the editing region.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Blended_Difussion.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>An advanced version of this diffusion mode is discussed in the paper <strong>Text-based inpainting with CLIPSef and Stable Diffusion</strong> <d-cite key="luddecke2022image"></d-cite>. In this paper, the novelty is that the user doesn’t have to do the mask manually. Instead, it can use an existing segmentation model (e.g. ClipSef). Another alternative is presented in the paper <strong>DiffEdit: Diffusion-based semantic image editing with mask guidance</strong> <d-cite key="couairon2022diffedit"></d-cite> where the mask is generated directly from the diffusion model.</p> <h3 id="3-fine-tune-overfit-on-a-single-image-and-then-generate-with-the-fine-tuned-model">3. Fine-tune (“overfit”) on a single image and then generate with the fine-tuned model</h3> <p>In the paper <strong>Imagic: Text-based real image editing with diffusion models</strong> <d-cite key="kawar2023imagic"></d-cite> and <strong>Unitune: Text-driven image editing by fine-tuning a diffusion model on a single image</strong> <d-cite key="valevski2023unitune"></d-cite>, the authors perform extensive fine-tuning on either the entire diffusion model or specific sections of it. This process is computationally and memory-intensive, setting it apart from alternative methods.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/literature_review/Fine_Tuning.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="prompt-to-prompt">Prompt-to-prompt</h3> <p>The prompt-to-prompt editing method is a significant advancement compared with the existing image editing techniques that rely on diffusion models. Unlike the methods explained above that involve adding noise, using masks, or fine-tuning, the prompt-to-prompt method stands out because of its simplicity, flexibility, and user-friendliness. In the former methods, users often face challenges such as manually creating masks or undergoing resource-intensive fine-tuning processes, which can be both time-consuming and technically demanding. In contrast, the prompt-to-prompt editing method streamlines the editing process by allowing users to directly specify their desired edits through language prompts. This approach eliminates the need for intricate masking or extensive model training as well as leverages the power of human language to precisely convey editing intentions.</p> <p>Throughout our research, we will adopt the prompt-to-prompt editing method as our starting point, with the aim of enhancing its performance.</p> <h2 id="outline-of-our-research">Outline of our research</h2> <p>To perform our research, we plan to build upon the code which complemented the paper published by (Hertz et al, 2022 <d-cite key="hertz2022prompttoprompt"></d-cite>, <a href="https://github.com/google/prompt-to-prompt/">Link to code</a>). Concretely, we will rely on a stable diffusion model from hugging face which we will access via Python. No model training is required as we will solely work with attention layers that capture spatial information about the images.</p> <p>Our study will be divided into 3 main subsections:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2001%20-%20outline.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="a-hyperparameter-study-of-prompt-to-prompt-editing-method-word-swap">A. Hyperparameter Study of prompt-to-prompt editing method “word swap”</h2> <p>In the forthcoming subsection, we delve into a comprehensive analysis of the hyperparameters pertaining to the “word swap” method within the prompt-to-prompt editing framework. Before delving into the specifics, it’s crucial to understand the significance of these hyperparameters and their default values, as originally outlined in the seminal work by Hertz et al<d-cite key="hertz2022prompttoprompt"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2002%20-%20outline%20section%20A.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2003%20-%20Local%20editing.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2004%20-%20Cross%20replace%20steps%20explanation.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2005%20-%20Sel-attention%20explanation.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We will systematically explore various hypotheses regarding each hyperparameter and present our empirical findings, shedding light on their individual impacts on the editing process. This examination aims to provide valuable insights into optimizing the performance of the “word swap” method and enhancing its practical utility.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2006%20-%20Hypothesis%20and%20findings.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="a1-exploration-of-silhouette-threshold-hyperparameter-k">A1. Exploration of silhouette threshold hyperparameter (“k”)</h2> <p>In this section, we embark on an exploration of the silhouette threshold hyperparameter (“k”). We aim to unravel the influence of varying this parameter while using the prompt ‘<em>“A woman’s face with blond hair”</em>’ and making alterations to different hair colors (brown, red, black). The GIF below showcases the representation of these experiments.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/change_threshold_womens_face2.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2007%20-%20Results%20silhouette%20parameter%20k%20-%20faces.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Additionally, we present a comparative analysis of the impact of this hyperparameter on editing tasks related to landscapes. For instance, we employ the prompt ‘<em>“A river between mountains”</em>’ and manipulate the landscape, including options like streets, forests, and deserts. The results of this landscape-oriented analysis can be seen in the figure below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/threshold_k/attention_replace_rivers.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2008%20-%20Results%20silhouette%20parameter%20k%20-%20landscape.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="a2-exploration-of-cross-attention-injection-hyperparameter-cross-replace-steps">A2. Exploration of cross-attention injection hyperparameter (“cross replace steps”)</h2> <p>Below we showcase the effect of the silhouette threshold hyperparameter (“k”) and the cross-attention injection hyperparameter(“cross_replace_steps”). We manipulate the “k” value, setting it to 3 different levels: 0, 0.3 (default literature value), and 0.6. The experiment was performed for both women’s faces and landscapes, providing a comprehensive understanding of how these hyperparameters affect the editing process. The following GIFs showcase the results of our exploration.</p> <h3 id="with-k--0">With k = 0:</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_women.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_zero_cross_replace_river.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="with-k--03">With k = 0.3:</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_women.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point3_cross_replace_river.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="with-k--06">With k = 0.6:</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_women.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/cross_replace_steps/k_point6_cross_replace_river.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Below, we present the key insights found for the prompt <em>“A woman’s face with blond hair”</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2009%20-%20Results%20cross%20replace%20steps%20-%20faces.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Below, we present the key insights found for the prompt <em>“A river between mountains”</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2010%20-%20Results%20cross%20replace%20steps%20-%20landscape.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="a3-exploration-of-self-attention-hyperparameter-self-replace-steps">A3. Exploration of self-attention hyperparameter (“self replace steps”)</h2> <p>In our investigation of the self-attention hyperparameter known as “self_replace_steps,” we conducted a series of experiments with careful consideration of the interplay between this parameter and two other critical factors: “k” (the silhouette threshold) and “cross_replace_steps” (the cross-attention injection parameter). To comprehensively assess the influence of “self_replace_steps,” we designed two distinct experimental scenarios.</p> <p>In the first scenario, we set “k” and “cross_replace_steps” to their default values in the literature review (0.3 and 0.8 respectively), creating an environment conducive to exploring the effects of self-attention within these threshold parameters. Concurrently, in the second scenario, we opted for more extreme settings by keeping “k” at 0 (no silhouette threshold) and “cross_replace_steps” at 0.2, thereby intensifying the impact of the self-attention hyperparameter.</p> <h3 id="with-k--03-and-cross_replace_steps--08">With k = 0.3 and cross_replace_steps = 0.8:</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_women.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_point3_self_replace_river.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="with-k--0-and-cross_replace_steps--02">With k = 0 and cross_replace_steps = 0.2:</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_women.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/self_replace_steps/k_zero_crossattention_point2_self_replace_river.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Below, we present the key insights for the hyperparameter “self_replace_steps” within the context of the prompt <em>“A woman’s face with blond hair”</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2011%20-%20Results%20self%20replace%20steps%20-%20faces.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Below, we present the key insights for the hyperparameter “self_replace_steps” found for the prompt <em>“A river between mountains”</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2012%20-%20Results%20self%20replace%20steps%20-%20landscape.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="a4-cycle-consistency-of-method">A4. Cycle Consistency of method</h2> <p>Our primary goal is to delve into the notion of “Cycle Consistency” within our methodology. This concept revolves around the seamless reversal of text prompt modifications back to their original form, ensuring that the resulting image closely mirrors the initial prompt. This bidirectional editing process serves as the central focus of our research, and in the subsequent sections, we present our findings on this crucial aspect.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2013%20-%20Cycle%20consistency.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2014%20-%20Cycle%20consistency%20-%20hyperparameter%20impact.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="b-generalization-of-optimized-hyperparameters-to-attention-re-weight-method">B. Generalization of optimized hyperparameters to “attention re-weight method”</h2> <p>After identifying the optimal parameters, we conducted a comparative analysis to assess their generalizability across other methods, including attention re-weighting. In the visual presentation, we used GIFs to showcase image generation under two different parameter configurations for the prompt <em>“A woman’s face with long wavy blond hair”</em>.</p> <p>On the left side, images were generated using default values (k=0.3; cross_replace_steps = 0.8; self_replace_steps = 0.2) while varying the assigned weights. Notably, negative weights led to instability and less desirable outcomes, as evidenced by the results on the left.</p> <p>On the right side, we employed our optimized hyperparameter values (k = 0; cross_replace_steps = 0.2; self_replace_steps = 0.8). These images demonstrated improved stability while consistently producing the desired output. This visual comparison highlights the effectiveness of our optimized parameters and their superior performance, particularly when dealing with attention re-weighting method.</p> <div style="display: flex;"> <div style="flex: 1; padding: 10px;"> Literature suggested parameters <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly.gif" class="img-fluid" width="200" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="flex: 1; padding: 10px;"> Newly optimized parameters <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/GIFs/c_value/c_value_women_curly_improved_self_replace.gif" class="img-fluid" width="50" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="our-proposed-method">Our Proposed Method</h2> <p>As our research has demonstrated, the current prompt-to-prompt method, as reported in the literature <d-cite key="hertz2022prompttoprompt"></d-cite>, exhibits significant limitations. Specifically, with the current settings for the silhouette, cross-attention injection, and self-attention injection parameters, the method fails to perform the prompted edits with precision. A comparative analysis of the generated target images against the geometry of the reference images reveals undesired deviations. The existing method over-constrains the geometry due to excessively high k values and cross-attention injection values. Additionally, it underutilizes self-attention injection. Furthermore, the current method lacks cycle consistency. To address these shortcomings, we propose a new framework: the <em>“CL P2P”</em> prompt-to-prompt image editing framework. This framework offers several key improvements over the existing method:</p> <p><strong>Optimization of Critical Hyperparameters</strong>: Our research indicates that optimizing the values of critical hyperparameters results in higher prompt-to-prompt image editing precision and a more accurate similarity between the reference and target images for desired features. We propose the following adjusted values, particularly for editing faces and hairstyles:</p> <ul> <li>Local editing (silhouette parameter k): 0.0</li> <li>Cross-attention injection (cross replace steps): 0.2</li> <li>Self-attention injections (self-replace steps): 0.8</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2016%20-%20Current%20vs%20new%20method%20comparision%20of%20output.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>By selecting these values, the following changes are introduced to the prompt-to-prompt editing method:</p> <ul> <li><span style="color:red">Remove</span>: Local editing can be removed from the method, as it did not lead to significant improvements compared to the precision achieved by the elongated injection of self-attention.</li> <li><span style="color:orange">Reduce</span>: The cross-attention (query-key-value attention) injection should be reduced to allow greater geometric adaptability and better convergence between the reference and target images.</li> <li><span style="color:green">Increase</span>: Self-attention injection should be substantially elongated from 20% to 80% of the diffusion steps. This is crucial, especially for editing hairstyles, as it allows for the greatest geometric adaptability and ensures the convergence between desired reference and target image features.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method-1400.webp"/> <img src="/staging/assets/img/2023-11-07-prompt-to-prompt/analysis/Figure%2015%20-%20Current%20vs%20new%20method.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Addressing Cycle-Inconsistency</strong>: To remedy the cycle-inconsistency, we propose balancing the asymmetry of the current method with regards to the V values of the underlying transformer model. The current method is cycle-inconsistent, even though the same embeddings are used for both the reference and target prompts. Traditionally, the method has only employed the V values of the reference prompt, neglecting those of the target prompt. This characteristic likely introduces asymmetry, breaking the cycle-consistency of the model. We propose an additional injection mechanism for the “CL P2P” framework, a V value injection method, allowing for the consideration of both the V values of the reference and target images. To control the number of injection steps, we introduce an additional hyperparameter, “V value injection steps”. The V value injection function is defined based on the logic highlighted in the footnote of the image.</p> <h2 id="future-work">Future work</h2> <p>The development of the “CL P2P” framework is a significant advancement in prompt-to-prompt image editing methods. However, there are still areas where further research will be needed. A critical area of exploration lies in the enhancement of cycle-consistency within the prompt-to-prompt editing process. Further research is required to ascertain and refine the optimal values for the V value injection steps, a key component in achieving cycle-consistency.</p> <p>Additionally, the existing frameworks predominantly focus on singular reference and target prompts. While this approach has opened new pathways in human-computer interaction, several research questions remain unexplored. A notable inquiry is the potential to integrate various prompt-to-prompt editing methods, such as “word swap”, “attention re-weighting,” and “prompt refinement.” This integration aims to facilitate a dynamic, conversational interaction between users and generated images, enabling a continuous and iterative editing process. Current state-of-the-art generative image models, such as mid-journey models, do not inherently support such iterative mechanisms. The realization of this functionality necessitates extensive research and development, offering an exciting challenge for future advancements in the field.</p> <h2 id="conclusion">Conclusion</h2> <p>Image generation models, inherently stochastic in nature, exhibit variability in outcomes even when similar prompts are applied. This stochasticity can result in significant deviations in the generated images. For instance, prompts like “A woman’s face with blond hair” and “A woman’s face with red hair” may yield images with markedly different facial features, demonstrating the algorithm’s underlying randomness.</p> <p>In response to this challenge, prompt-to-prompt image generation and editing techniques have emerged as a significant area of interest in recent years. These methods, while constituting a potent tool in the arsenal of image editing alongside fine-tuning, semantic mixing, and masking approaches, are not without limitations. Specifically, the precision of edits and the geometric alignment between reference and target images often fall short of expectations.</p> <p>Our research delves into the influence of critical hyperparameters on the outcomes of a cross-attention-based prompt-to-prompt method. We aimed to dissect the impact of each hyperparameter on image editing and geometric adaptation between the reference and target images. Our findings make substantive contributions to enhancing the precision and geometric convergence in prompt-to-prompt methods, with the following key insights:</p> <ul> <li>An extensive analysis of three critical hyperparameters (silhouette selection, cross-attention injection, and self-attention injection) was conducted, focusing on their effect on the precision of an attention-based prompt-to-prompt editing method.</li> <li>Contrary to existing literature<d-cite key="hertz2022prompttoprompt"></d-cite>, our study reveals that self-attention injection plays a more pivotal role than previously recognized. We recommend incorporating self-attention injection from the reference image for approximately 80% of the diffusion steps during the target image generation process.</li> <li>We introduce the novel <em>“CL P2P”</em> framework, designed to elevate the efficacy of prompt-to-prompt editing.</li> </ul> <p>Our research not only deepens the understanding of prompt-to-prompt editing methods but also achieves enhanced editing precision and improved similarity between reference and target images.</p> <p>Looking ahead, the <em>“CL P2P”</em> framework paves the way for further exploration, particularly in addressing the cycle consistency of prompt-to-prompt methods. Additionally, exploring strategies to seamlessly integrate different prompts into a continuous dialogue could revolutionize human-computer interaction, enabling users to edit generated images through conversational engagement.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-07-prompt-to-prompt.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>
<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Accelerating large model inference with speculative decoding - 6.s898 | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="An investigation into methods to speed up autoregressive inference through increased parallelization, specifically through speculative sampling and decoding."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/2023/speculative-decoding/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/staging/assets/js/distillpub/template.v2.js"></script> <script src="/staging/assets/js/distillpub/transforms.v2.js"></script> <script src="/staging/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Accelerating large model inference with speculative decoding - 6.s898",
      "description": "An investigation into methods to speed up autoregressive inference through increased parallelization, specifically through speculative sampling and decoding.",
      "published": "November 16, 2023",
      "authors": [
        {
          "author": "Dakota Goldberg",
          "authorURL": "/#",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/staging/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Accelerating large model inference with speculative decoding - 6.s898</h1> <p>An investigation into methods to speed up autoregressive inference through increased parallelization, specifically through speculative sampling and decoding.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <ul> <li><a href="#inference-in-autoregressive-models">Inference in autoregressive models</a></li> <li><a href="#speculative-execution-in-processors">Speculative execution in processors</a></li> <li><a href="#applying-speculative-execution-to-model-inference">Applying speculative execution to model inference</a></li> <li><a href="#hierarchical-speculative-decoding">Hierarchical speculative decoding</a></li> </ul><div><a href="#current-work">Current Work</a></div> <ul> <li><a href="#general-setup">General setup</a></li> <li><a href="#sampling-p-x">Sampling $p(x)$</a></li> <li><a href="#the-algorithm">The Algorithm</a></li> <li><a href="#evaluation">Evaluation</a></li> </ul><div><a href="#hierarchical-speculative-decoding">Hierarchical Speculative Decoding</a></div> <div><a href="#experiments">Experiments</a></div> <ul> <li><a href="#general-set-up-for-experiments">General set-up for experiments</a></li> <li><a href="#how-many-orders-of-magnitude-larger-should-m-p-be-than-m-q">How many orders of magnitude larger should $M_p$ be than $M_q$?</a></li> <li><a href="#set-up-for-hierarchical-speculative-decoding">Set-up for hierarchical speculative decoding</a></li> </ul><div><a href="#results">Results</a></div> <ul> <li><a href="#calculating-c-for-each-model-pair">Calculating $c$ for each model pair</a></li> <li><a href="#the-general-effect-of-speculative-decoding">The general effect of speculative decoding</a></li> <li><a href="#acceptance-rates-and-wall-time-given-m-p-and-m-q">Acceptance rates and wall time given $M_p$ and $M_q$</a></li> <li><a href="#results-of-hierarchical-speculative-decoding">Results of hierarchical speculative decoding</a></li> </ul><div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <h3 id="inference-in-autoregressive-models">Inference in autoregressive models</h3> <p>Autoregressive models, particularly transformers and RNNs, play a crucial role in tasks involving sequential data processing, such as natural language processing and time series analysis. However, a significant limitation of these models is their slow inference speed. The primary bottleneck in these models is associated with memory reads and writes, rather than arithmetic computations. This is especially problematic in larger models with vast parameter spaces, where efficient memory management is critical to performance. Further, these models generate outputs sequentially, one token at a time, with each new token depending on all previously generated tokens. This inherent sequential dependency limits the model’s ability to parallelize the token generation process, leading to inference latency much greater than that of models capable of processing data in parallel. The challenge is to overcome this sequential bottleneck without compromising the model’s ability to accurately capture dependencies in the data.</p> <p>The central question this project addresses is whether it’s possible to introduce parallelism into the inference process of autoregressive models. A more specific aspect of this problem is whether probabilities for multiple tokens can be computed simultaneously, rather than processing each token individually. This project aims to enhance methods that have been proposed for parallelizing the decoding process, focusing on solutions that draw inspiration from speculative execution in processors and other systems design strategies.</p> <h3 id="speculative-execution-in-processors">Speculative execution in processors</h3> <p>Speculative execution is a technique used in CPU architecture to improve processing speed. Instead of waiting for sequential execution of instructions, processors predict which instructions are likely to be executed next and start processing them in advance. If the prediction is correct, this leads to a significant reduction in latency, as the processor has preemptively executed necessary instructions. If the prediction is incorrect, the processor discards the speculative results and reverts to the correct execution path. This method effectively utilizes CPU resources that would otherwise remain idle during the waiting period, thus optimizing the overall processing speed and reducing latency.</p> <h3 id="applying-speculative-execution-to-model-inference">Applying speculative execution to model inference</h3> <p>Inspired by speculative execution in processors, this project explores how similar principles can be applied to accelerate inference in large autoregressive models. The concept involves generating multiple potential outputs in parallel, using a smaller or draft model, and then evaluating these outputs with the larger target model. This mimics the speculative execution process where multiple paths are explored simultaneously, with the most promising path being selected as the final output. This approach, referred to as “speculative sampling” or “speculative decoding,” aims to introduce a level of parallelism in the inference process, enabling faster generation of outputs without compromising the quality or accuracy of the model’s predictions.</p> <h3 id="hierarchical-speculative-decoding">Hierarchical speculative decoding</h3> <p>In addition to implementing already proposed speculative decoding techniques, this project investigates a strategy that has the potential further speed up inference: hierarchical speculative decoding. This method aims to accelerate the smaller approximation model with an even smaller, faster model. While I experiment with two-layer (traditional) and three-layer hierarchies in this project, one could theoretically extend this idea to create an <em>n</em> layer hierarchy, assuming sufficient memory. Although researchers developing speculative decoding algorithms and sampling methods have mentioned the potential viability of hierarchical speculative decoding, none have tried to implement it. Thus, this project aims to find an efficient implementation of the approach and determine if it actually further speeds up inference.</p> <h2 id="current-work">Current Work</h2> <p>Multiple papers have presented novel speculative decoding algorithms, with the nuance typically in the way that sampling is performed. The two most-referenced papers in this space are DeepMind’s Accelerating Large Language Model Decoding with Speculative Sampling (Chen et al.) <a href="https://arxiv.org/pdf/2302.01318.pdf">(paper)</a> and Google Research’s Fast Inference from Transformers via Speculative Decoding (Leviathan et al.) <a href="https://arxiv.org/pdf/2211.17192.pdf">(paper)</a>. This project draws its architecture from the latter, so we will more explore its approach in-depth and describe how its shortcomings motivated the experiments in this project.</p> <h3 id="general-setup">General setup</h3> <p>The approach presented in Fast Inference from Transformers via Speculative Decoding (Leviathan et al.) aims to accelerate inference from a target transformer-like model $M_p$. We present a distilled version of the speculative decoding set-up, algorithm, and evaluation here.</p> <p>We start with two models:</p> <ol> <li>$M_p$ (the target model)</li> <li>$M_q$ (a smaller approximation model)</li> </ol> <table> <tbody> <tr> <td>$p(x_{t}</td> <td>x_{&lt;t})$ describes the sampling of token $x_t$ given pretext $x_{&lt;t}$, and we will refer to this as just $p(x)$. The shorthand applies for $q(x)$.</td> </tr> </tbody> </table> <p>Our goal is to generate $\gamma \in \mathbb{Z}^{+}$ completions quickly with the approximation model, check that the probability of those generations are identical to the target model’s (in parallel), and then reject and resample starting from the first “wrong” generation.</p> <h3 id="sampling-px">Sampling $p(x)$</h3> <p>In order to sample $p(x)$, we will sample $x \sim q(x)$ instead.</p> <ol> <li>If $q(x)\leq p(x)$, we keep $x$</li> <li>Otherwise, we reject $x$ with a $1-\frac{p(x)}{q(x)}$ probability. <ul> <li>If we end up rejecting $x$, we resample $x\sim\text{norm}(\max(0, p(x)-q(x)))$.</li> </ul> </li> </ol> <p>Basically, we want $x\sim p(x)$ to be <em>at least</em> as likely as $x \sim q(x)$. Following the steps above is equivalent to just sampling $x \sim q(x)$, and the paper provides a comprehensive proof of this in its appendix.</p> <h3 id="the-algorithm">The Algorithm</h3> <p>We use an implementation of the following algorithm from Leviathan et al. We start with some conditioning $prefix$ (our starting tokens) and generate between $1$ and $\gamma+1$ tokens at once.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1-1400.webp"/> <img src="/staging/assets/img/2023-11-16-speculative-decoding/Algorithm1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="evaluation">Evaluation</h3> <p>To evaluate the effectiveness of this approach, we need to calculate the total wall time improvement of speculative decoding versus normal inference on the target model.</p> <p>To make this evaluation more simple, assume we can run $\gamma + 1$ concurrent evaluations of $M_p$ in parallel. Now, we just need to get the cost of running $M_q$ (the approximation model).</p> <p>Let $c$ = the cost coefficient, which is the ratio between the time for a single run of $M_q$ and a single run of $M_p$. $c$ will depend only on our hardware and software implementation details.</p> <p>Now, we need some measure of how well $M_q$ approximates $M_p$.</p> <p>Let $\beta$ be the <em>acceptance rate</em>.</p> <ul> <li> <table> <tbody> <tr> <td>$\beta_{x&lt;t}$ is the probability of accepting $x_{t}\sim q(x_{t}</td> <td>x_{&lt;t})$ by speculative sampling.</td> </tr> </tbody> </table> </li> <li>Assume that the $\beta$s are i.i.d.</li> </ul> <p>Let $\alpha=E(\beta)$. This gives us the average acceptance rate across many samples, which is a good measure of how well $M_q$ approximates $M_p$.</p> <p>The expectation of the number of generated tokens is now a bounded geometric function of $\alpha$ (bounded by $\gamma$) :\(E(\text{# of generated tokens}) = \frac{1-\alpha^{\gamma + 1}}{1-\alpha}\)Given this relationship, we can derive the expected improvement factor for the total wall time (assuming longer generations):\(\frac{1-\alpha^{\gamma+1}}{(1-\alpha)(\gamma c+1)}\) For the sake of conciseness, we leave the full proof to the paper, but the general sketch relies on the fact that each run of Algorithm 1 costs $Tc\gamma + T$ (where $T$ is the cost of running one step of $M_p$). We run $M_q$ $\gamma$ times and $M_p$ once, and each run of Algorithm 1 produces $\frac{1-\alpha^{\gamma + 1}}{1-\alpha}$ tokens. Since the cost of producing a single token with a standard algorithm is $T$, we get the above improvement.</p> <h2 id="hierarchical-speculative-decoding-1">Hierarchical Speculative Decoding</h2> <p>How much faster can we make model inference by accelerating the approximation model with an even smaller, faster model? Let’s look at the case where we have three models:</p> <ol> <li><strong>$M_p$:</strong> The target model</li> <li><strong>$M_q$:</strong> The first-level approximation model, used to approximate $M_p$.</li> <li><strong>$M_r$:</strong> The second-level, even smaller approximation model, used to approximate $M_q$.</li> </ol> <p>With the introduction of $M_r$, we now need to consider additional parameters:</p> <ul> <li><strong>$\gamma_r$:</strong> The number of concurrent evaluations that can be run using $M_r$.</li> <li><strong>$\beta_r$:</strong> The acceptance rate for $M_r$, analogous to $\beta$ for $M_q$.</li> <li><strong>$\alpha_r = E(\beta_r)$:</strong> The average acceptance rate for $M_r$, representing how well $M_r$ approximates $M_q$.</li> </ul> <p>Now, $\beta$ for $M_q$ becomes a function of $\beta_r$, reflecting the hierarchical nature of this setup. The acceptance rate $\beta$ for $M_q$ now depends on how effectively $M_r$ approximates $M_q$, which in turn approximates $M_p$.</p> <p>We can hypothesize that the effectiveness of $M_q$ in approximating $M_p$ might now be influenced by the performance of $M_r$. This could mean that $\beta$, and consequently $\alpha$, might be a function of $\alpha_r$.</p> <p>The expectation of the number of generated tokens would now need to consider the hierarchical relationship. A new formula would be required to calculate this expectation, taking into account the performances of both $M_q$ and $M_r$.</p> <p>Finally, the expected improvement factor for the total wall time would also need to be recalculated to reflect this hierarchical structure. This would involve integrating the costs and efficiencies of $M_r$ into our existing model, which so far only considered $M_q$ and $M_p$.</p> <p>Whether or not this approach will actually speed up the model in practice is left to be determined experimentally.</p> <h2 id="experiments">Experiments</h2> <p>I experimented on multiple transformer model families, most notably <code class="language-plaintext highlighter-rouge">facebook/opt-125m</code>, <code class="language-plaintext highlighter-rouge">facebook/opt-1.3b</code>, and <code class="language-plaintext highlighter-rouge">facebook/opt-13b</code>.</p> <p>The primary research questions I investigated include:</p> <ol> <li>How many orders of magnitude larger should $M_p$ be than $M_q$ to achieve the maximal improvement?</li> <li>To what extent does hierarchical speculative decoding further speed up inference?</li> </ol> <h3 id="general-set-up-for-experiments">General set-up for experiments</h3> <ul> <li>For the standard (non-hierarchical) speculative decoding, I implemented the algorithm exactly as described above. <ul> <li>I used a gamma value of 4</li> </ul> </li> <li>I used both top-k sampling and nucleus sampling, with <code class="language-plaintext highlighter-rouge">k=20</code> and <code class="language-plaintext highlighter-rouge">p=0.9</code> constant throughout all experiments.</li> <li>I typically prompted the models with <code class="language-plaintext highlighter-rouge">input_text = "Once upon a"</code> and generated 20 tokens.</li> <li>I used consistent sets of seeds (such as <code class="language-plaintext highlighter-rouge">torch.manual_seed(898)</code>) when running the same experiment across multiple model combinations for the sake of reproducibility and so that I could more easily compare results across models on shorter generation lengths.</li> </ul> <h3 id="how-many-orders-of-magnitude-larger-should-m_p-be-than-m_q">How many orders of magnitude larger should $M_p$ be than $M_q$?</h3> <ul> <li>To investigate this, I calculated inference time (tokens per second) on each of the following (approximator, target) model pairs: <ul> <li><code class="language-plaintext highlighter-rouge">facebook/opt-125m</code>, <code class="language-plaintext highlighter-rouge">facebook/opt-1.3b</code></li> <li><code class="language-plaintext highlighter-rouge">facebook/opt-125m</code>, <code class="language-plaintext highlighter-rouge">facebook/opt-13b</code></li> <li><code class="language-plaintext highlighter-rouge">facebook/opt-1.3b</code>, <code class="language-plaintext highlighter-rouge">facebook/opt-13b</code></li> </ul> </li> </ul> <h3 id="set-up-for-hierarchical-speculative-decoding">Set-up for hierarchical speculative decoding</h3> <p>I experimented with a three-level hierarchical approach using</p> <ol> <li>Small approximation model $M_r$: <code class="language-plaintext highlighter-rouge">facebook/opt-125m</code></li> <li>Approximation model $M_q$: <code class="language-plaintext highlighter-rouge">facebook/opt-1.3b</code></li> <li>Target model $M_p$: <code class="language-plaintext highlighter-rouge">facebook/opt-13b</code></li> </ol> <p>To add hierarchical decoding to the algorithm, I replaced the sampling of $M_q$, where we typically sample $x \sim q(x)$ with a sampling process that mirrors the sampling from the target model. So we sample from $x\sim r(x)$ instead, keep if it’s at least as likely in $q(x)$, and reject proportional to the likelihood of the sample under either model, adjusting the distribution as before if we need to sample again. This made the theoretical implementation rather simple, as we could re-use a lot of the code. The implementation in practice was slightly more difficult than expected, however, as my implementation of the two-layer speculative decoding didn’t permit direct functional composition, and I had to restructure the implementation a bit.</p> <h2 id="results">Results</h2> <h3 id="calculating-c-for-each-model-pair">Calculating $c$ for each model pair</h3> <p>(The larger model is used as the target model $M_p$)</p> <table> <thead> <tr> <th> </th> <th>opt-125m</th> <th>opt-1.3b</th> <th>opt-13b</th> </tr> </thead> <tbody> <tr> <td>opt-125m</td> <td>1</td> <td>N/A</td> <td>N/A</td> </tr> <tr> <td>opt-1.3b</td> <td>0.015</td> <td>1</td> <td>N/A</td> </tr> <tr> <td>opt-13b</td> <td>0.022</td> <td>0.015</td> <td>1</td> </tr> </tbody> </table> <p>This gives insight into the relative efficiencies of the models when performing assisted inference.</p> <h3 id="the-general-effect-of-speculative-decoding">The general effect of speculative decoding</h3> <p>Wall time improvements from speculative decoding have already been documented, so these results are not novel, but I include them here for further proof that the algorithm works and for comparison with other results.</p> <table> <thead> <tr> <th>Target Model</th> <th>Approximation Model</th> <th>Tokens/Second</th> </tr> </thead> <tbody> <tr> <td>opt-13b</td> <td>None</td> <td>0.047</td> </tr> <tr> <td>opt-13b</td> <td>opt-1.3b</td> <td>0.087</td> </tr> <tr> <td>opt-13b</td> <td>opt-125m</td> <td>0.057</td> </tr> <tr> <td>opt-1.3b</td> <td>None</td> <td>0.336</td> </tr> <tr> <td>opt-1.3b</td> <td>opt-125m</td> <td>1.05</td> </tr> </tbody> </table> <p>In all cases, including an approximation model increases the model’s token per second inference rate.</p> <h3 id="acceptance-rates-and-wall-time-given-m_p-and-m_q">Acceptance rates and wall time given $M_p$ and $M_q$</h3> <table> <thead> <tr> <th>Target Model</th> <th>Approximator Model</th> <th>Tokens/Second</th> <th>Acceptance Rate</th> </tr> </thead> <tbody> <tr> <td>opt-1.3b</td> <td>opt-125m</td> <td>1.05</td> <td>38%</td> </tr> <tr> <td>opt-13b</td> <td>opt-125m</td> <td>0.057</td> <td>15%</td> </tr> <tr> <td>opt-13b</td> <td>opt-1.3b</td> <td>0.087</td> <td>19%</td> </tr> </tbody> </table> <p>These results help us answer the question: <em>How many orders of magnitude larger should $M_p$ be than $M_q$?</em></p> <p>One order of magnitude seems to yield higher acceptance rates, and the smaller models were obviously faster.</p> <h3 id="results-of-hierarchical-speculative-decoding">Results of hierarchical speculative decoding</h3> <table> <thead> <tr> <th>Target Model</th> <th>Approximation Model</th> <th>Tokens/Second</th> <th>Acceptance Rate</th> </tr> </thead> <tbody> <tr> <td>opt-13b</td> <td>None</td> <td>0.047</td> <td>N/A</td> </tr> <tr> <td>opt-13b</td> <td>opt-1.3b</td> <td>0.087</td> <td>19%</td> </tr> <tr> <td>opt-13b</td> <td>opt-125m</td> <td>0.057</td> <td>15%</td> </tr> <tr> <td>opt-13b</td> <td>opt-1.3b, opt-125m</td> <td>0.030</td> <td>17%, 33%</td> </tr> </tbody> </table> <p>I found that running the three-layer hierarchical speculative decoding <em>did not</em> speed up model inference, but I hypothesize that this is because of compute limitations. Running all three models on my computer given the parallelization requirements of the algorithm forced the program to map data to devices in a less-efficient way. I wasn’t able to find smaller pre-trained models with which I could test this on my local machine, so a future experiment should either train custom smaller models for the sake of inference in this setting or use a device with greater memory capacity.</p> <h2 id="conclusion">Conclusion</h2> <p>This project explored the potential of speculative decoding, a technique inspired by speculative execution in processors, to accelerate inference in autoregressive models like transformers. Our exploration focused on implementing and extending existing methods of speculative decoding, particularly the ones proposed in the seminal works by Chen et al. and Leviathan et al., while also introducing early experiments with concept of hierarchical speculative decoding, which is to be further investigated.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/staging/assets/bibliography/2023-11-16-speculative-decoding.bib"></d-bibliography> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2023" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>
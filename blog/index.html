<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> blog | 6.S898 Deep Learning Blogs 2023</title> <meta name="author" content="abc b c"/> <meta name="description" content="Staging website for the 2023 ICLR Blogposts track "/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/staging/assets/css/main.css"> <link rel="canonical" href="https://deep-learning-mit.github.io/staging/blog/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/staging/assets/js/theme.js"></script> <script src="/staging/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/staging/">6.S898 Deep Learning Blogs 2023</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/staging/blog/index.html">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="header-background"><div class="img"></div></div> <div class="container mt-5"> <div class="post"> <div class="header-bar"> <h1>6.S898 Deep Learning Blogs</h1> <h2>Fall 2023</h2> </div> <ul class="post-list"> <li> <h3> <a class="post-title" href="/staging/blog/2023/Investigating-neural-operator-models-for-closure-modeling-of-dynamical-systems/">Investigating Vision Transformer-Based Models for Closure Modeling of Fluid Dynamical Systems</a> </h3> <p>Project Report for 6.s898 Deep Learning (Fall 2023)</p> <p class="post-meta"> 22 min read &nbsp; &middot; &nbsp; December 19, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/watermarked-llms/">Are Watermarked Large Language Models More Prone to Hallucinations?</a> </h3> <p>In this blog post, I investigate whether watermarked LLMs are more likely to “hallucinate,” or make up facts, because of limitations imposed by the watermarking scheme.</p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/time-series-lstm-transformer/">Predicting the Future: LSTM vs Transformers for Time Series Modeling</a> </h3> <p>A comparison analysis between LSTM and Transformer models in the context of time-series forecasting. While LSTMs have long been a cornerstone, the advent of Transformers has sparked significant interest due to their attention mechanisms. In this study, we pinpoint which particular features of time series datasets could lead transformer-based models to outperform LSTM models.</p> <p class="post-meta"> 23 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/sparse-autoencoders-for-othello/">Studying the benefits and limitations of sparse auto-encoders for compositional reasoning tasks</a> </h3> <p></p> <p class="post-meta"> 28 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/solvent-encoding/">Solvent Encoding for solubility prediction using GNN</a> </h3> <p>Evaluation of different solvent-encoding methods on a public available solubility dataset</p> <p class="post-meta"> 13 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/scRNA-GNNs/">6.s898 Final Project- Investigating the biological underpinnings of latent embeddings for scRNA-seq</a> </h3> <p></p> <p class="post-meta"> 9 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/forbidden-facts/">Forbidden Facts</a> </h3> <p>A Mechanistic Interpretability Investigation of Llama 2</p> <p class="post-meta"> 16 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/elephantfish-model/">Modeling Elephantfish Communication through Deep RNNs</a> </h3> <p>Elephantfish represent a fascinating subject for study within the realms of bioacoustics and animal communication due to their unique use of electric fields for sensing and interaction. This project proposes the development of a deep learning framework to model the electrical communication signals of elephantfish, akin to language models used in natural language processing (NLP).</p> <p class="post-meta"> 14 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/contrastivediffusion-image2video/">Exploring Image-Supervised Contrastive Diffusion - A Comparative Analysis with Applications in Image-to-Video Generation</a> </h3> <p>Image-to-image (I2I) and image-to-video (I2V) may be the next frontier of generative deep learning capabilities, but current models struggle with robustness, largely due to the implicit, rather than explicit, representation learning objective during traditional diffusion model training. Hence, we propose a new technique where a custom contrastive loss function is used to leverage the innate latent space of the diffusion model’s variational autoencoder. This enables us to study the creation of lightweight models that lose less contextual information between input conditioning and target output, which we elucidate in this blog.</p> <p class="post-meta"> 27 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/combining-modalities-for-better-representation-learning/">Combining Modalities for Better Molecular Representation Learning</a> </h3> <p></p> <p class="post-meta"> 12 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/WeightDecaySpecNormEffects/">Exploring Frobenius and Spectral Normalization in MLPs and Residual networks</a> </h3> <p>This blog post compares the effects of a spectral view on weight normalization to a frobenius view on weight normalization normalization using a novel algorithm developed by us. We use two network types at multiple sizes to compare the effects of these two methods on the singular values of the weight matrices, the rank of the weight matrices, and the accuracy of the models.</p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Iterated-Representation-Learning/">Iterated Representation Learning</a> </h3> <p>Representation learning is a subfield of deep learning focused on learning meaningful lower-dimensional embeddings of input data, and rapidly emerging to popularity for its efficacy with generative models. However, most representation learning techniques, such as autoencoders and variational autoencoders, learn only one embedding from the input data, which is then used to either reconstruct the original data or generate new samples. This project seeks to study the utility of a proposed iterated representation learning framework, which repeatedly trains new latent space embeddings based on the data outputted from the last round of representation. In particular, we seek to examine whether the performance of this iterated approach on a model and input dataset are indicative of any robustness qualities of the model and latent embedding space, and potentially derive a new framework for evaluating representation stability.</p> <p class="post-meta"> 18 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/alleviating-catastrophic-forgetting-in-classification-tasks-through-strategic-filter-dropout/">A Method for Alleviating Catastrophic Forgetting With Explainability</a> </h3> <p>Using various explainability metrics to target, we freeze layers in CNNs to enable continual learning.</p> <p class="post-meta"> 21 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/graph-articulated-objects/">Graph Articulated Objects</a> </h3> <p>Pre-trained large vision-language models (VLMs), such as GPT4-Vision, uniquely encode relationships and contextual information learned about the world through copious amounts of real-world text and image information. Within the context of robotics, the recent explosion of advancements in deep learning have enabled innovation on all fronts when solving the problem of generalized embodied intelligence. Teaching a robot to perform any real-world task requires it to perceive its environment accurately, plan the steps to execute the task at hand, and accurately control the robot to perform the given task. This project explores the use of vision-language models to generate domain descriptions. These can be used for task planning, closing the gap between raw images and semantic understanding of interactions possible within an environment.</p> <p class="post-meta"> 20 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/physics-loss/">Physics Loss</a> </h3> <p>Learning a deep net to optimize an LP, based on predicting the optimal basis vector. Surveys existing approaches in the literature. Demonstrates high accuracy of feasibility and optimality on small problem instances, but documents issues when scaling to larger problems. Benchmarks against a modern optimization solver, with discussions on upfront training vs. variable inference computation times.</p> <p class="post-meta"> 14 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Tracking-Multiple-Objects/">Diffusion Models on Low-Brightness Images</a> </h3> <p>Diffusion models have been used with great success for a number of use cases, but they still remain largely unused on dim images. The primary related work has been on using a diffusion model for low-light image enhancement. However, most of these works agree that attempting to generate an image from noise generated on top of an already dim image often results in rgb shift and global degradation of the image. This is because a diffusion model adds noise to the given image and then attempts to denoise the image, so given a dim and low-contrast image, the model has a difficult time denoising. This blog post focuses on methods to improve diffusion model performance in low-light images</p> <p class="post-meta"> 14 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Exploring-Task-Specific-Data-Augmentation/">Semi-Supervised Domain Adaptation using Diffusion Models</a> </h3> <p>6.S898 Project</p> <p class="post-meta"> 18 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/superposition/">The Effect of Activation Functions On Superposition in Toy Models</a> </h3> <p>An in-depth exploration of how different activation functions influence superposition in neural networks.</p> <p class="post-meta"> 31 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/stable-diffusion-for-obs/">Stable Diffusion for Oracle Bone Script</a> </h3> <p>The project aims to train a ControlNet for Stable Diffusion on the condition of rendering traditional Chinese characters from oracle bone script samples.</p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/distill-example/">Gradient-Boosted Neural Wavlet Interpolation for Time Series (G-BiTS)</a> </h3> <p>Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.</p> <p class="post-meta"> 7 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Physics-Informed-Primal-Dual-Learning/">Challenges in Deep Learning Surrogates for Constrained Linear Optimization</a> </h3> <p>Learning a deep net to optimize an LP, based on predicting the optimal basis vector. Surveys existing approaches in the literature. Demonstrates high accuracy of feasibility and optimality on small problem instances, but documents issues when scaling to larger problems. Benchmarks against a modern optimization solver, with discussions on upfront training vs. variable inference computation times.</p> <p class="post-meta"> 20 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/CNN-activation-patching/">Activation Patching in Vision Transformers</a> </h3> <p></p> <p class="post-meta"> 13 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/A-Transformer-Based-Approach-for-Simulating-Ecological-Recovery/">Transformer-Based Approaches for Hyperspectral Imagery in Remote Sensing</a> </h3> <p>The introduction of transformer-based models in remote sensing signals a transformative shift in hyperspectral image (HSI) classification, providing advanced tools to navigate the complex data landscape. This investigation gauges the potential of vision transformers to accurately discern the detailed spectral and spatial correlations within HSI, accentuating their capacity to significantly improve detection and analysis in environmental monitoring and land management.</p> <p class="post-meta"> 22 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/transformers-as-gamers/">Learning Generals.io</a> </h3> <p>We explore the application of deep learning to the online game generals.io and discuss what is necessary to achieve superhuman performance in generals.io.</p> <p class="post-meta"> 16 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/transformer-time/">A Comparative Study of transformer on long sequence time series data</a> </h3> <p>This study evaluates Transformer models in traffic flow prediction. Focusing on long sequence time-series data, it evaluates the balance between computational efficiency and accuracy, suggesting potential combinations of methods for improved forecasting.</p> <p class="post-meta"> 16 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/transfer-resistant-model-training/">Transfer Resistant Model Training</a> </h3> <p>This blog post details our work on training neural networks that are resistant to transfer learning techniques.</p> <p class="post-meta"> 19 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/sparse-autoencoders-for-interpretable-rlhf/">Sparse Autoencoders for a More Interpretable RLHF</a> </h3> <p>Extending Anthropic's recent monosemanticity results toward a new, more interpretable way to fine-tune.</p> <p class="post-meta"> 23 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/proposal-2/">Using Synthetic Data to Minimize Real Data Requirements</a> </h3> <p>Data acquisition for some tasks in synthetic biology can be cripplingly difficult to perform at a scale necessary for machine learning... so what if we just made our data up?*</p> <p class="post-meta"> 17 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/proposal-1/">Applications of Deep Learning in Timbre Transfer</a> </h3> <p>Exploring musical timbre transfer by leveraging prior art in differential digital signal processing (DDSP) and modern deep learning structures.</p> <p class="post-meta"> 12 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/interpretability-of-toy-tasks/">The Effect of Activation Functions On Superposition in Toy Models</a> </h3> <p>An in-depth exploration of how different activation functions influence superposition in neural networks.</p> <p class="post-meta"> 31 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/generating-robust-networks/">Training Robust Networks</a> </h3> <p>Exploring ResNet on TinyImageNet, unveiling brittleness and discovering simple robustment enhancement strategies via hyperparameter optimization</p> <p class="post-meta"> 21 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/enforcing-uniformity/">Imposing uniformity through Poisson flow models</a> </h3> <p>Uniformity and alignment are used to explain the success of contrastive encoders. Can we use already trained, well-aligned features and impose uniformity to increase their quality and performance on downstream classification tasks?</p> <p class="post-meta"> 13 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/dof-visual-place-recognition-satellite/">6-DOF estimation through visual place recognition</a> </h3> <p>A neural pose-estimation solution is implemented, which could help an agent with a downward-facing camera (such as a drone) to geolocate based on prior satellite imagery of terrain. The neural encoder infers extrinsic camera parameters from camera images, enabling estimation of 6 degrees of freedom (6-DOF), namely 3-space position and orientation. By encoding priors about satellite imagery in a neural network, the need for the agent to carry a satellite imagery dataset onboard is avoided.</p> <p class="post-meta"> 15 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/conflict-causality/">Tracing the Seeds of Conflict: Advanced Semantic Parsing Techniques for Causality Detection in News Texts</a> </h3> <p>This blog post outlines a research project aiming to uncover cause-effect-relationships in the sphere of (political) conflicts using a frame-semantic parser.</p> <p class="post-meta"> 25 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/autodecoders/">To Encode or Not To Encode: The Case for the Encoder-free Autodecoder Architecture</a> </h3> <p>While the traditional autoencoder architecture consists of an encoder and a decoder to compress and reconstruct information with only the most prominent features, some recent work have begun to utilize an alternate framework, the autodecoder, in specific applications in the field of representation learning. Skipping the encoder network altogether and learning latent codes directly as parameters, we aim to compare the two architectures on practical reconstruction tasks as well as dive into the theory of autodecoders and why they work, along with certain novel features that they bring.</p> <p class="post-meta"> 20 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/PersonalizedGeneration_w_LLMAgents/">New Synthesis Approach for Personalized LLMS</a> </h3> <p></p> <p class="post-meta"> 21 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/vig-algorithm-flow-project-proposal/">Augmenting Expert Domain Image Inputs for Enhancing Visual Language Models Performance</a> </h3> <p>This blog post explores enhancing visual language models, particularly for expert domains like scientific literature, where standard models struggle. By integrating domain-specific knowledge and advanced image embeddings, the research aims to refine the performance of visual language models such as OpenFlamingo. Leveraging graphical structured embeddings and graph neural networks, the study tests different methods of representing images to improve the models' interpretive capabilities.</p> <p class="post-meta"> 21 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/spatiotemporal/">Embeddings for Spatio-temporal Forecasting</a> </h3> <p>An analysis of various embeddings methods for spatio-temporal forecasting.</p> <p class="post-meta"> 17 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/sentence-embeddings/">In the pursuit of cheap and robust word embeddings</a> </h3> <p>A study of how we can train a student word embedding model to mimic the teacher OpenAI word embedding model by using as small a training set as possible. We also investigate preprocessing tricks and robustness against poisoned data.</p> <p class="post-meta"> 18 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/representationengineering-incontextlearning/">Leveraging Representation Engineering For LLM’s In-Context-Learning</a> </h3> <p>We present a method to observe model internals whether LLMs are performing in-context learning and control the model outputs based on such Context Vectors.</p> <p class="post-meta"> 27 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/mapreason/">Reasoning with Maps: Assessing Spatial Comprehension on Maps in Pre-trained Models</a> </h3> <p>Map reasoning is an intuitive skill for humans and a fundamental skill with important applications in many domains. In this project, we aim to evaluate the capabilities of contemporary state-of-the-art Large Vision-Language Models (LVLMs) for reasoning on maps and comparing their capabilities with human participants on the coregistration task. We additionally propose and release a novel dataset to serve as an initial benchmark for map reasoning capabilities. We run an extensive analysis on the performance of open-source LVLMs showing that they struggle to achieve good performance on our dataset. Additionally, we show that coregistration is intuitive to human participants that were able to achieve close to perfect accuracy in a time-constrained manner.</p> <p class="post-meta"> 37 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/foley-to-video/">Autoen-chorder: Predicting Musical Success With Neural Nets</a> </h3> <p>In this blog, we discuss deep learning methods and results of predicting song popularity from audio features.</p> <p class="post-meta"> 17 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/double_descent/">Ensemble Learning for Mitigating Double Descent</a> </h3> <p>Exploring when and why Double Descent occurs, and how to mitigate it through Ensemble Learning.</p> <p class="post-meta"> 43 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/SmartEmbeddingInitializations/">Injecting Node Information via Embedding Initializations</a> </h3> <p>Graph Neural Networks (GNNs) have revolutionized our approach to complex data structures, enabling a deeper understanding of relationships and patterns that traditional neural networks might miss. This project looks into the potential of embedding initializations in GNNs, particularly in the context of molecular function prediction and protein retrieval tasks. By investigating the effect of intentional, information-rich initializations versus random initializations, we aim to enhance the learning efficiency and accuracy of GNNs in these domains. Our study focuses on a precision medicine knowledge graph (PrimeKG) and employs TxGNN, a GNN model initially designed for disease-drug link prediction, repurposed for protein-molecular function link prediction. We explore the impact of using ESM embeddings for protein nodes, hypothesizing that these embeddings could provide structural information not explicitly present in the graph data. Through comparisons of the latent spaces and performances, we look to see the effectiveness of these embeddings in improving the model's predictive powe of protein function.</p> <p class="post-meta"> 18 min read &nbsp; &middot; &nbsp; December 12, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/overparameterization/">Overparameterization of Neural Networks through Kernel Regression and Gaussian Processes</a> </h3> <p>In this work, we will explore the successes of overparameterization of neural networks through evaluating the relationship between the Neural Tangent Kernel (NTK), MLPs, and Gaussian processes.</p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/exploring-music-generation/">Exploring Methods for Generating Music</a> </h3> <p>Explores various machine learning techniques for generating music. Compares the performance of traditional RNNs, LSTMs, and transformers on generating sample sequences of music.</p> <p class="post-meta"> 8 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/rep-learning-for-rec-systems/">Can Constrastive Learning Recommend Me a Movie?</a> </h3> <p></p> <p class="post-meta"> 14 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/spacial-CLIP/">Improving CLIP Spatial Awareness Using Hard Negative Mining</a> </h3> <p>CLIP struggles to understand and reason spatially. We attempt to solve this issue with introducing hard negative examples during training.</p> <p class="post-meta"> 12 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/multimodal-commonsense/">Multimodal Commonsense</a> </h3> <p>6.S898 project for analyzing and evaluating the commonsense reasoning performance of multimodal vs text-only models.</p> <p class="post-meta"> 21 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Exploring-Generative-Models-In-Time-Series/">Exploring Univariate Time Series Anomaly Detection using VAE's</a> </h3> <p>In this blog post, we will take a deep dive into DONUT, a method that applies variational autoencoders to the problem of time series anomaly detection. We will begin with a overview of the original authors main ideas. Next, we will replicate some results, and perform new experiments to gain further insights into the properties, successes, and limitations of this method. Finally, we will run additional experiments that test extensions on the original formulation, and motivate future areas of exploration.</p> <p class="post-meta"> 49 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/graphs-transformers/">Graph Transformers</a> </h3> <p>A study of Transformers' understanding of fundamental graph problems, where we propose a new, tailored architecture highlighting the model's potential in graph-related tasks.</p> <p class="post-meta"> 34 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/croneillproposal/">Learning a Lifted Linearization for Switched Dynamical Systems</a> </h3> <p>A final project proposal for 6.s898 in fall 2023</p> <p class="post-meta"> 22 min read &nbsp; &middot; &nbsp; December 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/universal-features/">Sparse Autoencoder Universality - Under What Conditions are Learned Features Consistent?</a> </h3> <p>This project aims to study the universality of features in LLMs by studying sparse autoencoders trained on similar layers of different models.</p> <p class="post-meta"> 16 min read &nbsp; &middot; &nbsp; December 10, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/diaz-proposal/">Optimizations of Transformers for Small-scale Performance</a> </h3> <p>CNNs generally outperform ViTs in scenarios with limited training data. However, the narrative switches when the available training data is extensive. To bridge this gap and improve upon existing ViT methods, we explore how we can leverage recent progress in the transformer block and exploit the known structure of pre-trained ViTs.</p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; December 10, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/guided-transfer-learning/">Guided Transfer Learning and Learning How to Learn: When Is It Useful?</a> </h3> <p>For downstream tasks that involve extreme few-shot learning, it's often not enough to predispose a model with only general knowledge using traditional pre-training. In this blog, we explore the nuances and potential applications of Guided Transfer Learning, a meta-learning approach that allows a model to learn inductive biases on top of general knowledge during pre-training.</p> <p class="post-meta"> 41 min read &nbsp; &middot; &nbsp; December 10, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Visualization-of-CLIP's-Learning-and-Perceiving-Dynamics/">Alive Scene</a> </h3> <p>Inspired by the captivating Enchanted Portraits of the Harry Potter universe, my project unveils an innovative AI pipeline that transcends traditional scene-capture methods. Rather than merely recording scenes as a sequence of static images, this pipeline is intricately designed to interpret and articulate the dynamic behavior of various elements within a scene by utilizing CLIP semantic embeddings. This nuanced understanding enables the scenes to evolve autonomously and organically, mirroring the fluidity and spontaneity of living entities.</p> <p class="post-meta"> 22 min read &nbsp; &middot; &nbsp; December 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/projected-fff-networks/">Projected fast feedforward networks</a> </h3> <p>Abstract</p> <p class="post-meta"> 13 min read &nbsp; &middot; &nbsp; December 5, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/LinearMode/">Understanding Linear Mode Connectivity</a> </h3> <p>We study the pruning behavior of vision transformers (ViTs), and possible relations to linear mode connectivity. Frankle et al. (2022) showed that linear mode connectivity, the tendency of a neural network to optimize to the same linearly connected minimum when trained SGD noise, is strongly tied to the existence of "lottery networks," sparse networks that can be trained to full accuracy. We found that when initialized from a pretrained network, the ViT model showed linear mode connectivity when fine tuning on CIFAR-10. Conversely, random initialization resulted in instability during training and a lack of linear mode connectivity. We also found that using the PLATON algorithm (Zhang et al.) to generate a mask was effective for pruning the network, suggesting the existence of lottery ticket networks in ViTs, but the connection between the existence of these trainable subnetworks and linear mode connectivity remains unclear.</p> <p class="post-meta"> 15 min read &nbsp; &middot; &nbsp; December 1, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/TransformersAndRNNs/">Transformers vs. RNNs: How do findings from real-world datasets relate to the theory?</a> </h3> <p>Transformers have rapidly surpassed RNNs in popularity due to their efficiency via parallel computing without sacrificing accuracy. Transformers are seemingly able to perform better than RNNs on memory based tasks without keeping track of that recurrence. This leads researchers to wonder -- why? To contriubte towards answering that question, I'll analyze the performance of transformer and RNN based models on datasets in real-world applications. Serving as a bridge between applications and theory-based work, this will hopefully enable future developers to better decide which architecture to use in practice.</p> <p class="post-meta"> 21 min read &nbsp; &middot; &nbsp; December 1, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/latent-interpolation/">Exploring the latent space of text-to-image diffusion models</a> </h3> <p>In this blog post we explore how we can navigate through the latent space of stable diffusion and using interpolation techniques.</p> <p class="post-meta"> 15 min read &nbsp; &middot; &nbsp; December 1, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/speculative-decoding/">Accelerating large model inference with speculative decoding - 6.s898</a> </h3> <p>An investigation into methods to speed up autoregressive inference through increased parallelization, specifically through speculative sampling and decoding.</p> <p class="post-meta"> 13 min read &nbsp; &middot; &nbsp; November 16, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/unraveling-social-reasoning-in-llms/">Unraveling Social Reasoning in LLMs: A Deep Dive into the Social IQA Benchmark</a> </h3> <p>In this study, we investigate the challenge of social commonsense reasoning in large language models (LLMs), aiming to understand and categorize common errors LLMs make in social commonsense reasoning tasks.</p> <p class="post-meta"> 22 min read &nbsp; &middot; &nbsp; November 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/denoisingVAE/">Comparing data augmentation using VAEs and denoising-VAEs for limited noisy datasets</a> </h3> <p></p> <p class="post-meta"> 16 min read &nbsp; &middot; &nbsp; November 11, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/transformer-elo-prediction/">Emoji3Vec</a> </h3> <p>Our project seeks to expand on the previous attempts at "emoji2vec", or generating semantically meaningful embeddings for emojis.</p> <p class="post-meta"> 21 min read &nbsp; &middot; &nbsp; November 10, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/speech-recognition-proposal/">Modeling Human Speech Recognition with Different Network Architectures</a> </h3> <p>Evaluating a neural network's ability to effectively model human speech recognition using CNNs vs. TNNs</p> <p class="post-meta"> 15 min read &nbsp; &middot; &nbsp; November 10, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/uncertainty/">Analytic, Empirical, and Monte Carlo Bayesian Methods for Uncertainty Estimation</a> </h3> <p>In the realm of machine learning, the robustness and reliability of predictive models are important, especially when confronted with Out-of-Distribution (OOD) data that deviate from the training distribution. Bayesian models stand out for their probabilistic foundations, being able to offer ways to quantify uncertainty. This project will present a survey of already-established methods of estimating uncertainty, as well as how we adapted/generalized them.</p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/structured-physics-loss-diffusion/">Understanding LLM Attention on Useless Numbers in Word Problems (and this Title has 8 Es)</a> </h3> <p>If Jack starts out with 4 llamas and Jill takes 2 of them, then Jack gets 5 chinchillas, how many llamas does he have?</p> <p class="post-meta"> 20 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/multilingual-representations-in-embeddings-models/">Cross-Lingual Fine-Tuning for Multilingual Text Embeddings</a> </h3> <p>Exploring contrastively training text embeddings, and presenting a scalable, cheap and data-efficient method to train multilingual embedding models</p> <p class="post-meta"> 32 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/learning-interpretable-features-with-sparse-autoencoders/">Learning Interpretable Features with Sparse Auto-Encoders</a> </h3> <p></p> <p class="post-meta"> 15 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/eunhae-project/">How does model size impact catastrophic forgetting in online continual learning?</a> </h3> <p>Yes, model size matters.</p> <p class="post-meta"> 23 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/deep-connectome-clustering/">VGAE Clustering of the Fruit Fly Connectome</a> </h3> <p>An exploration of how learned Variational Graph Auto-Encoder (VGAE) embeddings compare to Spectral Embeddings to determine the function of neurons in the fruit fly brain.</p> <p class="post-meta"> 26 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/contrastivediffusion-image2video/">Robust Image to Video Generation Using Contrastive Diffusion Over Latents</a> </h3> <p>Image-to-video (I2V) may be the next frontier of generative deep learning capabilities, but current models struggle with robustness, largely due to the implicit, rather than explicit, representation learning objective during traditional diffusion model training. Hence, we propose a new technique where a pre-trained contrastive model is used to train a diffusion model with a custom contrastive loss function to operate within a learned structured latent space for I2V problems, yielding, in theory, more structurally sound videos without loss of contextual information.</p> <p class="post-meta"> 10 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/adaptive-controller-graph-eom/">Adaptive Controller with Neural Net Equations of Motion for High-DOF Robots</a> </h3> <p>This project aims to develop an adaptive control mechanism using a graph neural network to approximate the equations of motion (EoM) for high-degree-of-freedom (DOF) robotic arms bypassing the need for symbolic EoM to build an adaptive controller.</p> <p class="post-meta"> 23 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Robustness-of-self-supervised-ViT-features-in-b-mode-images/">Robustness of self-supervised ViT features in b-mode images</a> </h3> <p>Vision Transformers (ViT) trained with self-distillation with no labels (DINO) have shown striking properties for several downstream tasks regarding segmentation, classification, and image correspondence. In this work, we assess DINO-vit-s/8 on a new dataset containing b-mode ultrasound images with the ultimate goal of segmenting bone.</p> <p class="post-meta"> 18 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Symmetry-Optimization/">Investigating the Impact of Symmetric Optimization Algorithms on Learnability</a> </h3> <p>Recent theoretical papers in machine learning have raised concerns about the impact of symmetric optimization algorithms on learnability, citing hardness results from theoretical computer science. This project aims to empirically investigate and validate these theoretical claims by designing and conducting experiments as understanding the role of optimization algorithms in the learning process is crucial for advancing the field of machine learning.</p> <p class="post-meta"> 14 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/how-cnns-learn-shapes/">Can CNN learn shapes?</a> </h3> <p>One widely accepted intuition is that Convolutional Neural Networks that are trained for object classification, combine low-level features (e.g. edges) to gradually learn more complex and abstracted patterns that are useful in differentiating images. Yet it remains poorly understood how CNNs actually make their decisions, and how their recognition strategies differ from humans. Specifically, there is a major debate about the question of whether CNNs primarily rely on surface regularities of objects, or whether they are capable of exploiting the spatial arrangement of features, similar to humans.</p> <p class="post-meta"> 15 min read &nbsp; &middot; &nbsp; November 9, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/quantum-gnn/">Quantum Circuit Optimization with Graph Neural Nets</a> </h3> <p>We perform a systematic study of architectural choices of graph neural net-based reinforcement learning agents for quantum circuit optimization.</p> <p class="post-meta"> 16 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Structural_vs_Data_Inductive_Bias/">Structural vs Data Inductive Bias</a> </h3> <p>Class project proposal</p> <p class="post-meta"> 23 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/suscep/">From Scroll to Misbelief - Modeling the Unobservable Susceptibility to Misinformation on Social Media</a> </h3> <p></p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/scRNAseq-assumptions/">Examining assumptions in scRNA-seq foundation model pre-training (6.S898 Final Project)</a> </h3> <p>Final project for MIT's Deep Learning (6.S898) class.</p> <p class="post-meta"> 27 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/increasing-context-length-for-transformers/">Increasing Context Length For Transformers</a> </h3> <p>How can we make attention more efficient?</p> <p class="post-meta"> 19 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/detect-image/">Zero-Shot Machine-Generated Image Detection using Sinks of Gradient Flows</a> </h3> <p>How can we detect fake images online? A novel approach of characterizing the behavior of a diffusion model's learned score vectors.</p> <p class="post-meta"> 18 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/denoising-EMG-signals/">Denoising EMG signals</a> </h3> <p>The future of brain-computer interfaces rests on our ability to decode neural signals. Here we attempt to ensemble ML techniques to extract useful information from sEMG signals to improve downstream task performance.</p> <p class="post-meta"> 18 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/A-deeper-look-into-equivariance-for-materials-data/">A Deeper Look into Equivariance for Materials Data</a> </h3> <p>A Comparative Analysis of an E(3) Equivariant GNN and a Non-Equivariant GNN in Materials Data Tasks with a Focus on Investigating the Interpretability of Latent Geometry within the Two GNNs.</p> <p class="post-meta"> 19 min read &nbsp; &middot; &nbsp; November 8, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/prompt-to-prompt/">Prompt to Prompt</a> </h3> <p>Text-based image editing via cross-attention mechanisms - the research of hyperparameters and novel mechanisms to enhance existing frameworks</p> <p class="post-meta"> 24 min read &nbsp; &middot; &nbsp; November 7, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/Language-Bias/">Understanding Bias in Speech to Text Language Models</a> </h3> <p>Do language models have biases that make them better for latin based languages like English? To find out, we generate a custom dataset to test how various language features, like silent letters, letter combinations, and letters out of order, affect how speech2text models learn and compare these results with models trained on real human language.</p> <p class="post-meta"> 36 min read &nbsp; &middot; &nbsp; November 7, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/attention-regularization/">Regularization Techniques for Attention Layers in Transformer Models</a> </h3> <p>Attention layers are an integral part of the success of transformer models, but can also lead to overfitting on parts of input data when there is limited training data. Therefore, researchers have proposed methods to regularize attention layers to reduce overfitting and increase generalizability. This blog will analyze popular methods and explore novel approaches to regularization in attention layers.</p> <p class="post-meta"> 16 min read &nbsp; &middot; &nbsp; November 6, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/neural-PDEs-long-time-dynamics/">Neural PDEs for learning local dynamics and longer temporal rollouts</a> </h3> <p>6.S898 deep learning project</p> <p class="post-meta"> 20 min read &nbsp; &middot; &nbsp; November 5, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2023/proposal/">Graph neural networks v.s. transformers for geometric graphs</a> </h3> <p>With the recent development of graph transformers, in this project we aim to compare their performance on a molecular task of protein-ligand binding affinity prediction against the performance of message passing graph neural networks.</p> <p class="post-meta"> 22 min read &nbsp; &middot; &nbsp; November 1, 2023 </p> <p class="post-tags"> <a href="/staging/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2022/molecule_generation/">An empirical evaluation of autoencoders and diffusion models for 2D small-molecule generation</a> </h3> <p>We examine the efficacy of autoencoders and diffusion models for generating 2D molecules with certain small-molecule properties. In particular, we evaluate the success of both models in creating new molecules, containing only CHONPS atoms, and only single, double, and aromatic bonds. Secondarily, a natural question that followed was investigating the efficacy of different manners of encoding molecular data for training models - specifically, we trained with both molecular fingerprints and adjacency matrices (derived from graph embeddings of molecules). We find that small autoencoder models are successful in generating both pseudo-fingerprints and pseudo-adjacency matrices that are similar to simple small molecules’ fingerprints and adjacency matrices, but they were not able to produce ‘convincing’ simple organic molecules from the fingerprint or adjacency matrices. We find that diffusion models were considerably faster and more lightweight than autoencoders, and were generated molecules that were quantitatively closer in structure to real chemical structures than the auto-encoders were able to produce.</p> <p class="post-meta"> 15 min read &nbsp; &middot; &nbsp; December 12, 2022 </p> <p class="post-tags"> <a href="/staging/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2022/VIVFormer/">VIVformer</a> </h3> <p>A deep transformer framework trained on real experimental and synthetic gen-AI data for forecasting non-stationary time-series. Applications and insights drawn from vortex induced vibrations data collected at the MIT Towing Tank.</p> <p class="post-meta"> 33 min read &nbsp; &middot; &nbsp; December 1, 2022 </p> <p class="post-tags"> <a href="/staging/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2022/Recovering-Latent-Variables-with-VAEs-despite-Training-Bias/">Recovering Latent Variables with VAEs despite Training Bias</a> </h3> <p>Final Project Blog</p> <p class="post-meta"> 17 min read &nbsp; &middot; &nbsp; December 1, 2022 </p> <p class="post-tags"> <a href="/staging/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2022/proposal_JingpengHong/">Recurrent Recommender System with Incentivized Search</a> </h3> <p>This project considers the use of Recurrent Neural Networks (RNNs) in session-based recommender systems. We input sequences of customers' behavior, such as browsing history, to predict which product they're most likely to buy next. Our model improves upon this by taking into account how previous recommendations influence subsequent search behavior, which then serves as our training data. Our approach introduces a multi-task RNN that not only aims to recommend products with the highest likelihood of purchase but also those that are likely to encourage further customer searches. This additional search activity can enrich our training data, ultimately boosting the model's long-term performance.</p> <p class="post-meta"> 8 min read &nbsp; &middot; &nbsp; December 1, 2022 </p> <p class="post-tags"> <a href="/staging/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2022/Vision_Language_Limitations/">Understanding Limitations of Vision-Language Models</a> </h3> <p></p> <p class="post-meta"> 17 min read &nbsp; &middot; &nbsp; December 1, 2022 </p> <p class="post-tags"> <a href="/staging/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> <li> <h3> <a class="post-title" href="/staging/blog/2022/contrastive-time/">Contrastive Representation Learning for Dynamical Systems</a> </h3> <p>A deep learning method of learning system underlying parameters from observed trajectories</p> <p class="post-meta"> 35 min read &nbsp; &middot; &nbsp; November 7, 2022 </p> <p class="post-tags"> <a href="/staging/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </li> </ul> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/staging/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/staging/assets/js/zoom.js"></script> <script defer src="/staging/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>